{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Field' from 'torchtext.data' (d:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\torchtext\\data\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[188], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_split\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtt\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, BucketIterator, TabularDataset\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (d:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\torchtext\\data\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchtext as tt\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting & vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for PyTorch\n",
    "class SupplierDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Extract features\n",
    "        labels = ['id', 'status']\n",
    "        features = sample[labels].values # ignore name and address for now\n",
    "\n",
    "        # Apply transformations (e.g., convert strings/categories to numerical values)\n",
    "        if self.transform:\n",
    "            features = self.transform(features, labels).astype(np.float32)\n",
    "        # Convert to PyTorch tensor\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "        return features, 0 # 0 is a dummy label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_columns_types(dataset):\n",
    "    # Separate the text columns from the other columns\n",
    "    text_columns = [\"invoice_code\",\"customer_name\",\"customer_email\",\"customer_address\",\"customer_city\",\"customer_state\",\"customer_postal_code\",\n",
    "                    \"customer_country\",\"notes\",\"created_by\",\"updated_by\",\"shipping_address\",\"shipping_city\",\"shipping_state\",\n",
    "                    \"shipping_postal_code\",\"shipping_country\"]\n",
    "    \n",
    "    date_columns = [\"invoice_date\",\"payment_due_date\",\"created_at\",\"updated_at\",\"due_date\",\"paid_date\"]\n",
    "\n",
    "    categorical_columns = [\"payment_method\",\"status\",\"currency\",\"payment_reference\"]\n",
    "\n",
    "    numerical_columns = [\"invoice_number\",\"subtotal\",\"tax_rate\",\"tax_amount\",\"discount_rate\",\"discount_amount\",\"total\",\"exchange_rate\"]  \n",
    "\n",
    "    # Check if there is a column not in one of the above lists\n",
    "    for column in dataset.columns:\n",
    "        if column not in text_columns and column not in date_columns and column not in categorical_columns and column not in numerical_columns:\n",
    "            print(\"Column not in any list: \" + column) \n",
    "            \n",
    "    return dataset[text_columns], dataset[date_columns], dataset[categorical_columns], dataset[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical(dataset):\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "    classes = {}\n",
    "    \n",
    "    # Apply a one-hot encoding to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "\n",
    "        # Convert the strings to unique numerical indices\n",
    "        unique_classes, indices = np.unique(dataset[column], return_inverse=True)\n",
    "        classes[column] = unique_classes\n",
    "        #print(\"Column: \" + column + \" - Classes: \" + str(unique_classes))\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(indices)\n",
    "\n",
    "        # Apply one-hot encoding using torch.nn.functional.one_hot\n",
    "        one_hot_encoded = F.one_hot(tensor_data)\n",
    "        \n",
    "        # Convert the one-hot encoding tensor to a NumPy array\n",
    "        one_hot_array = one_hot_encoded.numpy()\n",
    "\n",
    "        # Add the one-hot encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(one_hot_array)\n",
    "\n",
    "    return transformed_dataset, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_numerical(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # Apply a min-max normalization to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "        # Convert the column to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(dataset[column], dtype=torch.float32)\n",
    "\n",
    "        # Range of values in the column\n",
    "        min_value = torch.min(tensor_data)\n",
    "        max_value = torch.max(tensor_data)\n",
    "\n",
    "        # Normalize the values in the column between 0 and 1en\n",
    "        normalized = (tensor_data - min_value) / (max_value - min_value)\n",
    "\n",
    "        # Apply min-max normalization using torch.nn.functional.normalize\n",
    "        #normalized = F.normalize(tensor_data)\n",
    "\n",
    "        # Convert the normalized tensor to a NumPy array\n",
    "        normalized_array = normalized.numpy()\n",
    "\n",
    "        # Add the normalized array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(normalized_array)\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "    vocabs = {}\n",
    "\n",
    "    # Apply a word embedding encoding to all columns of the tensor using torchtext\n",
    "    for column in dataset.columns:\n",
    "        # Convert the tensor to a list of strings\n",
    "        list_data = dataset[column].tolist()\n",
    "\n",
    "        # Convert the list of strings to a list of lists of strings\n",
    "        list_data = [[str(item)] for item in list_data]\n",
    "\n",
    "        # Create a torchtext field\n",
    "        field = tt.data.Field(tokenize='spacy', lower=True, batch_first=True, include_lengths=True)\n",
    "\n",
    "        # Create a torchtext dataset\n",
    "        torchtext_dataset = tt.data.Dataset(list_data, [('text', field)])\n",
    "\n",
    "        # Build the vocabulary\n",
    "        field.build_vocab(torchtext_dataset)\n",
    "\n",
    "        # Convert the text to a PyTorch tensor\n",
    "        tensor_text = field.process(torchtext_dataset)\n",
    "\n",
    "        # Convert the tensor to a NumPy array\n",
    "        tensor_text = tensor_text[0].numpy()\n",
    "\n",
    "        # Add the text array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(tensor_text)\n",
    "\n",
    "        # Add the vocabulary to the vocabs dictionary\n",
    "        vocabs[column] = field.vocab\n",
    "\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # Apply a timestamp convertion to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "        # Apply timestamp convertion using pandas.to_datetime\n",
    "        timestamp = pd.to_datetime(dataset[column]).astype(np.int64) // 10**9\n",
    "\n",
    "        # Add the timestamp to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(timestamp)\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns: (1000, 16)\n",
      "\n",
      "Date columns: (1000, 6)\n",
      "\n",
      "Categorical columns: (1000, 4)\n",
      "\n",
      "Numerical columns: (1000, 8)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv dataset from the csv file\n",
    "dataset_path = 'datasets/fake_invoice_1000.csv'\n",
    "# Creating a DataFrame from the CSV data (replace this with your actual CSV file path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Process the dataset before creating the SupplierDataset\n",
    "text_columns, date_columns, categorical_columns, numerical_columns = separate_columns_types(df)\n",
    "print(f\"Text columns: {text_columns.shape}\\n\"),                 #print(text_columns.head())\n",
    "print(f\"Date columns: {date_columns.shape}\\n\"),                 #print(date_columns.head())\n",
    "print(f\"Categorical columns: {categorical_columns.shape}\\n\"),   #print(categorical_columns.head())\n",
    "print(f\"Numerical columns: {numerical_columns.shape}\\n\"),       #print(numerical_columns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns after transformation: (1000, 4)\n",
      "\n",
      "  payment_method        status  \\\n",
      "0      [0, 1, 0]  [0, 0, 0, 1]   \n",
      "1      [1, 0, 0]  [0, 0, 1, 0]   \n",
      "2      [1, 0, 0]  [0, 1, 0, 0]   \n",
      "3      [1, 0, 0]  [1, 0, 0, 0]   \n",
      "4      [0, 0, 1]  [0, 0, 1, 0]   \n",
      "\n",
      "                                            currency payment_reference  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 1, 0]  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 1, 0]  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [1, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...         [0, 1, 0]  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [1, 0, 0]   \n",
      "\n",
      "\n",
      "Numerical columns after transformation: (1000, 8)\n",
      "\n",
      "   invoice_number  subtotal  tax_rate  tax_amount  discount_rate  \\\n",
      "0        0.248694  0.250331      0.15    0.038610           0.86   \n",
      "1        0.771873  0.368514      0.05    0.018907           0.30   \n",
      "2        0.330406  0.185498      0.20    0.038234           0.38   \n",
      "3        0.507838  0.653593      0.60    0.401614           0.90   \n",
      "4        0.311284  0.588111      0.85    0.512093           0.76   \n",
      "\n",
      "   discount_amount     total  exchange_rate  \n",
      "0         0.226812  0.133221       0.977807  \n",
      "1         0.116232  0.281886       0.115933  \n",
      "2         0.074432  0.140233       0.882845  \n",
      "3         0.617249  0.389216       0.711891  \n",
      "4         0.469142  0.413138       0.172187   \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[187], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m numerical_columns_treated \u001b[38;5;241m=\u001b[39m transform_numerical(numerical_columns)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumerical columns after transformation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumerical_columns_treated\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),       \u001b[38;5;28mprint\u001b[39m(numerical_columns_treated\u001b[38;5;241m.\u001b[39mhead(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m text_columns_treated \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText columns after transformation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_columns_treated\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),                 \u001b[38;5;28mprint\u001b[39m(text_columns_treated\u001b[38;5;241m.\u001b[39mhead(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m date_columns_treated \u001b[38;5;241m=\u001b[39m transform_date(date_columns)\n",
      "Cell \u001b[1;32mIn[184], line 15\u001b[0m, in \u001b[0;36mtransform_text\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     12\u001b[0m list_data \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mstr\u001b[39m(item)] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m list_data]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create a torchtext field\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m field \u001b[38;5;241m=\u001b[39m \u001b[43mtt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mField\u001b[49m(tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m'\u001b[39m, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Create a torchtext dataset\u001b[39;00m\n\u001b[0;32m     18\u001b[0m torchtext_dataset \u001b[38;5;241m=\u001b[39m tt\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset(list_data, [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, field)])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "# Apply transformations to the categorical columns\n",
    "categorical_colums_treated, classes = transform_categorical(categorical_columns)\n",
    "print(f\"Categorical columns after transformation: {categorical_colums_treated.shape}\\n\"),   print(categorical_colums_treated.head(), \"\\n\\n\")\n",
    "numerical_columns_treated = transform_numerical(numerical_columns)\n",
    "print(f\"Numerical columns after transformation: {numerical_columns_treated.shape}\\n\"),       print(numerical_columns_treated.head(), \"\\n\\n\")\n",
    "text_columns_treated = transform_text(text_columns)\n",
    "print(f\"Text columns after transformation: {text_columns_treated.shape}\\n\"),                 print(text_columns_treated.head(), \"\\n\\n\")\n",
    "date_columns_treated = transform_date(date_columns)\n",
    "print(f\"Date columns after transformation: {date_columns_treated.shape}\\n\"),                 print(date_columns_treated.head(), \"\\n\\n\")\n",
    "\n",
    "# Concatenate the transformed columns\n",
    "df_treated = pd.concat([text_columns_treated, date_columns_treated, categorical_colums_treated, numerical_columns_treated], axis=1)\n",
    "print(f\"Final dataset: {df_treated.shape}\\n\"), print(df_treated.head(), \"\\n\\n\")\n",
    "# Create an instance of the SupplierDataset\n",
    "supplier_dataset = SupplierDataset(dataframe=df_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[178], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43msupplier_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupplier Sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sample)\n",
      "Cell \u001b[1;32mIn[170], line 15\u001b[0m, in \u001b[0;36mSupplierDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Extract features\u001b[39;00m\n\u001b[0;32m     14\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 15\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;66;03m# ignore name and address for now\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Apply transformations (e.g., convert strings/categories to numerical values)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\series.py:1072\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     key \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(key, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m   1070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_rows_with_mask(key)\n\u001b[1;32m-> 1072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\series.py:1113\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;66;03m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[1;32m-> 1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m   1321\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1325\u001b[0m )\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1517\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[0;32m   1518\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[1;32m-> 1520\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32md:\\document\\GitHub\\POC_GAN_Test_Data_Generation\\IvaEnv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['id'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(5):\n",
    "    sample = supplier_dataset[i]\n",
    "    print(f\"supplier Sample {i + 1}:\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.2\n",
    "batch_size = 2\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(supplier_dataset))\n",
    "val_size = len(supplier_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(supplier_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2_mean = nn.Linear(512, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(512, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, 512)\n",
    "        self.fc4 = nn.Linear(512, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoder forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Reparameterization trick for sampling from a normal distribution\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Decoder forward pass\n",
    "        z = F.relu(self.fc3(z))\n",
    "        recon_x = torch.sigmoid(self.fc4(z))  # Assuming input features are normalized between 0 and 1\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Full forward pass of the VAE\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_output, input_tensor, mu, log_var):\n",
    "    print(\"reconstructed_output\", reconstructed_output)\n",
    "    print(\"input_tensor\", input_tensor)\n",
    "    BCE = nn.functional.mse_loss(reconstructed_output, input_tensor, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, epochs, device, x_dim=-1):\n",
    "    model.train()\n",
    "    startTotal = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\tEpisode\", epoch + 1, \"/\", epochs)\n",
    "        overall_loss = 0\n",
    "        start = time.time()\n",
    "        for batch_idx, (input_tensor, _) in enumerate(train_dataset):\n",
    "            print(\"\\t\\tBatch\", batch_idx + 1, \"/\", len(train_dataset))\n",
    "            input_tensor = input_tensor.view(batch_size, x_dim).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            reconstructed_output, mean, log_var = model(input_tensor)\n",
    "            loss = loss_function(reconstructed_output, input_tensor, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end = time.time()\n",
    "        print(\"\\tEpisode Result\", \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size), \"\\tStep Time: \", end - start, \"s\", \"\\tTotal Time: \", end - startTotal, \"s\",\"\\n\\n\")\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and optimizer\n",
    "input_dim = 2 # corresponds to the number of features in the dataset\n",
    "latent_dim = 2 # corresponds to the number of latent variables\n",
    "model = VAE(input_dim, latent_dim).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train(model, optimizer, train_loader, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new sample\n",
    "def generate_sample(model, latent_dim):\n",
    "    sample = torch.randn(1, latent_dim)\n",
    "    return model.decode(sample).detach().numpy()\n",
    "\n",
    "# Print or use the generated supplier data as needed\n",
    "print(\"Generated Supplier Data:\")\n",
    "for i in range(5):\n",
    "    print(generate_sample(model, latent_dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
