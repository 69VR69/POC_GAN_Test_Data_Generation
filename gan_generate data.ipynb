{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchtext as tt\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "SEPARATOR = '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting & vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for PyTorch\n",
    "class SupplierDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the element at the given index\n",
    "        element = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Convert the element to a PyTorch tensor\n",
    "        tensor = torch.tensor(element, dtype=torch.float32)\n",
    "        \n",
    "        #print(f\"shape of tensor: {tensor.shape} and type: {type(tensor)}\")\n",
    "        \n",
    "        # Apply the transform if one is given\n",
    "        if self.transform:\n",
    "            return self.transform(tensor)\n",
    "        else:\n",
    "            return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_columns_types(dataset):\n",
    "    # Separate the text columns from the other columns\n",
    "    text_columns = [\"invoice_code\", \"customer_name\",\"customer_email\",\"customer_address\",\"customer_city\",\"customer_state\",\"customer_postal_code\",\n",
    "                    \"customer_country\",\"notes\",\"created_by\",\"updated_by\",\"shipping_address\",\"shipping_city\",\"shipping_state\",\n",
    "                    \"shipping_postal_code\",\"shipping_country\"]\n",
    "    \n",
    "    date_columns = [\"invoice_date\",\"payment_due_date\",\"created_at\",\"updated_at\",\"due_date\",\"paid_date\"]\n",
    "\n",
    "    categorical_columns = [\"payment_method\",\"status\",\"currency\",\"payment_reference\"]\n",
    "\n",
    "    numerical_columns = [\"invoice_number\",\"subtotal\",\"tax_rate\",\"tax_amount\",\"discount_rate\",\"discount_amount\",\"total\",\"exchange_rate\"]  \n",
    "\n",
    "    # Check if there is a column not in one of the above lists\n",
    "    for column in dataset.columns:\n",
    "        if column not in text_columns and column not in date_columns and column not in categorical_columns and column not in numerical_columns:\n",
    "            print(\"Column not in any list: \" + column) \n",
    "            \n",
    "    return dataset[text_columns], dataset[date_columns], dataset[categorical_columns], dataset[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorical_Transformer():\n",
    "    def __init__(self):\n",
    "        self.classes = []\n",
    "        self.num_flattent_columns = 0\n",
    "\n",
    "    # Transform the column from a list of strings to a one-hot encoded array\n",
    "    def transform(self, column_data, header):\n",
    "        \n",
    "        # Convert the strings to unique numerical indices\n",
    "        unique_classes, indices = np.unique(column_data, return_inverse=True)\n",
    "        self.classes = unique_classes\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(indices)\n",
    "\n",
    "        # Apply one-hot encoding using torch.nn.functional.one_hot\n",
    "        one_hot_encoded = F.one_hot(tensor_data, len(self.classes))\n",
    "        \n",
    "        # Convert the one-hot encoding tensor to a NumPy array\n",
    "        one_hot_array = one_hot_encoded.numpy()\n",
    "\n",
    "        # Create a dataframe from the one-hot encoded array with a column for each class in format header_class\n",
    "        columns = [f\"{header}{SEPARATOR}{self.classes[i]}\" for i in range(len(self.classes))]\n",
    "        one_hot_columns = pd.DataFrame(one_hot_array, columns=columns)\n",
    "\n",
    "        return one_hot_columns\n",
    "\n",
    "    # Inverse transform the column from a one-hot encoded array to a list of strings\n",
    "    def inverse_transform(self, columns):\n",
    "\n",
    "        # convert the numpy array of flattened columns to a 2D array\n",
    "        column = np.array(columns).reshape(-1, len(self.classes))\n",
    "\n",
    "        # Convert the one-hot encoded array to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(column)\n",
    "    \n",
    "        # Convert the tensor to a NumPy array\n",
    "        numpy_data = tensor_data.numpy()\n",
    "    \n",
    "        # Convert the one-hot encoded array to a list of strings\n",
    "        indices = np.argmax(numpy_data, axis=1)\n",
    "        column_data = self.classes[indices]\n",
    "    \n",
    "        return column_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical(dataset):\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "    categories = {}\n",
    "    \n",
    "    # Apply a one-hot encoding to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "\n",
    "        # Create a Categorical_Transformer object for the column\n",
    "        categories[column] = Categorical_Transformer()\n",
    "\n",
    "        # Apply the transform to the column\n",
    "        one_hot_columns = categories[column].transform(dataset[column], column)\n",
    "\n",
    "        # Add the one-hot encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset = transformed_dataset.join(pd.DataFrame(one_hot_columns))\n",
    "\n",
    "    return transformed_dataset, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_categorical_column(column):\n",
    "    # Create a test dataset\n",
    "    test_dataset = pd.DataFrame({\"column1\": column})\n",
    "    print(\"column to transform : \",column)\n",
    "    \n",
    "    # Create a Categorical_Column object\n",
    "    categorical_column = Categorical_Transformer()\n",
    "    \n",
    "    # Transform the column\n",
    "    transformed_dataset = categorical_column.transform(test_dataset[\"column1\"], \"column1\")\n",
    "    print(\"tranformed : \", transformed_dataset)\n",
    "    \n",
    "    # Inverse transform the column\n",
    "    column_data = categorical_column.inverse_transform(transformed_dataset)\n",
    "    print(\"untransformed : \", column_data)\n",
    "    print(\"is correct : \", column == column_data, \"\\n\")\n",
    "\n",
    "# Test the Categorical_Column class\n",
    "# with few catagories\n",
    "test_categorical_column([\"a\", \"b\", \"c\", \"d\", \"a\", \"b\", \"c\", \"d\", \"a\", \"b\", \"c\", \"d\"])\n",
    "# with only one value\n",
    "test_categorical_column([\"a\"] * 12)\n",
    "# with a lot of different values\n",
    "test_categorical_column([(\"a\"+str(i)) for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Numerical_Transformer:\n",
    "    def __init__(self):\n",
    "        self.min_value = 0\n",
    "        self.max_value = 0\n",
    "        self.type = np.int_\n",
    "\n",
    "    # Transform the column by normalizing the data\n",
    "    def transform(self, column_data):\n",
    "\n",
    "        # Determine the type of the column\n",
    "        self.type = column_data.dtype\n",
    "\n",
    "        # Convert the column to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(column_data, dtype=torch.float32)\n",
    "\n",
    "        # Range of values in the column\n",
    "        self.min_value = torch.min(tensor_data)\n",
    "        self.max_value = torch.max(tensor_data)\n",
    "\n",
    "        # Normalize the values in the column between 0 and 1en\n",
    "        normalized = (tensor_data - self.min_value) / (self.max_value - self.min_value)\n",
    "\n",
    "        # Convert the normalized tensor to a NumPy array\n",
    "        normalized_array = normalized.numpy()\n",
    "\n",
    "        return normalized_array\n",
    "\n",
    "    # Inverse transform the column by denormalizing the data\n",
    "    def inverse_transform(self, column):\n",
    "        # Convert the column to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(column, dtype=torch.float32)\n",
    "\n",
    "        # Denormalize the values in the column between 0 and 1\n",
    "        denormalized = tensor_data * (self.max_value - self.min_value) + self.min_value\n",
    "\n",
    "        # Convert the denormalized tensor to a NumPy array\n",
    "        denormalized_array = denormalized.numpy()\n",
    "\n",
    "        # Convert the array to the original type\n",
    "        denormalized_array_typed = denormalized_array.astype(self.type)\n",
    "\n",
    "        return denormalized_array_typed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_numerical(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "    numericals = {}\n",
    "\n",
    "    # Apply a min-max normalization to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "        \n",
    "        numerical_column = Numerical_Transformer()\n",
    "        normalized_array = numerical_column.transform(dataset[column])\n",
    "\n",
    "        # Add the numerical column to the numericals dictionary\n",
    "        numericals[column] = numerical_column\n",
    "\n",
    "        # Add the normalized array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(normalized_array)\n",
    "\n",
    "    return transformed_dataset, numericals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_numerical_column():\n",
    "    # Create a numerical column object\n",
    "    numerical_column = Numerical_Transformer()\n",
    "\n",
    "    # Create a random column of data\n",
    "    column_data = np.random.randint(0, 100, size=100)\n",
    "    print(\"original\", column_data)\n",
    "\n",
    "    # Transform the column\n",
    "    transformed_column = numerical_column.transform(column_data)\n",
    "    print(\"transformed\", transformed_column)\n",
    "\n",
    "    # Inverse transform the column\n",
    "    inverse_transformed_column = numerical_column.inverse_transform(transformed_column)\n",
    "    print(\"untransformed\", inverse_transformed_column)\n",
    "    for i in range(len(column_data)):\n",
    "        if column_data[i] != inverse_transformed_column[i]:\n",
    "            print(f\"error : {i} - {column_data[i]} != {inverse_transformed_column[i]}\")\n",
    "\n",
    "# Test the Numerical_Column class\n",
    "test_numerical_column()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Date_Transformer():\n",
    "    def __init__(self, reference_year = 3000, reference_month = 12, reference_day = 31, reference_hour = 24, reference_minute = 60, reference_second = 60):\n",
    "        self.reference_year = reference_year\n",
    "        self.reference_month = reference_month\n",
    "        self.reference_day = reference_day\n",
    "        self.reference_hour = reference_hour\n",
    "        self.reference_minute = reference_minute\n",
    "        self.reference_second = reference_second\n",
    "        self.column_names = {}\n",
    "\n",
    "    # Transform the column by separating the date into its components and normalizing the data\n",
    "    def transform(self, column_data, column_name):\n",
    "        # Decompose the date into its components\n",
    "        date = pd.to_datetime(column_data)\n",
    "\n",
    "        transformed_dataset = pd.DataFrame()\n",
    "\n",
    "        year = date.dt.year\n",
    "        # Check if the inital column contain a year\n",
    "        if not year.empty and not year.all() == 0:\n",
    "            normalized_year = year / self.reference_year\n",
    "            year = torch.tensor(normalized_year, dtype=torch.float32)\n",
    "            column_name_year = f'{column_name}{SEPARATOR}year'\n",
    "            self.column_names[\"year\"] = column_name_year\n",
    "            transformed_dataset[column_name_year] = year\n",
    "\n",
    "        month = date.dt.month\n",
    "        if not month.empty and not month.all() == 0:\n",
    "            normalized_month = month / self.reference_month\n",
    "            month = torch.tensor(normalized_month, dtype=torch.float32)\n",
    "            column_name_month = f'{column_name}{SEPARATOR}month'\n",
    "            self.column_names[\"month\"] = column_name_month\n",
    "            transformed_dataset[column_name_month] = month\n",
    "\n",
    "        day = date.dt.day\n",
    "        if not day.empty and not day.all() == 0:\n",
    "            normalized_day = day / self.reference_day\n",
    "            day = torch.tensor(normalized_day, dtype=torch.float32)\n",
    "            column_name_day = f'{column_name}{SEPARATOR}day'\n",
    "            self.column_names[\"day\"] = column_name_day\n",
    "            transformed_dataset[column_name_day] = day\n",
    "\n",
    "        hour = date.dt.hour\n",
    "        if not hour.empty and not hour.all() == 0:\n",
    "            normalized_hour = hour / self.reference_hour\n",
    "            hour = torch.tensor(normalized_hour, dtype=torch.float32)\n",
    "            column_name_hour = f'{column_name}{SEPARATOR}hour'\n",
    "            self.column_names[\"hour\"] = column_name_hour\n",
    "            transformed_dataset[column_name_hour] = hour\n",
    "\n",
    "        minute = date.dt.minute\n",
    "        if not minute.empty and not minute.all() == 0:\n",
    "            normalized_minute = minute / self.reference_minute\n",
    "            minute = torch.tensor(normalized_minute, dtype=torch.float32)\n",
    "            column_name_minute = f'{column_name}{SEPARATOR}minute'\n",
    "            self.column_names[\"minute\"] = column_name_minute\n",
    "            transformed_dataset[column_name_minute] = minute\n",
    "\n",
    "        second = date.dt.second\n",
    "        if not second.empty and not second.all() == 0:\n",
    "            normalized_second = second / self.reference_second\n",
    "            second = torch.tensor(normalized_second, dtype=torch.float32)\n",
    "            column_name_second = f'{column_name}{SEPARATOR}second'\n",
    "            self.column_names[\"second\"] = column_name_second\n",
    "            transformed_dataset[column_name_second] = second\n",
    "\n",
    "        return transformed_dataset\n",
    "    \n",
    "    # Inverse transform the column by denormalizing the data and combining the components into a date\n",
    "    def inverse_transform(self, columns):\n",
    "        date_components = {}\n",
    "\n",
    "        # if there is a year column, denormalize it\n",
    "        if \"year\" in self.column_names:\n",
    "            print(\"year column name : \", self.column_names[\"year\"])\n",
    "            print(\"columns : \", columns)\n",
    "            year = columns[self.column_names[\"year\"]]\n",
    "            unnormalized_year = year * self.reference_year\n",
    "            date_components[\"year\"] = unnormalized_year.astype(np.int_)\n",
    "        \n",
    "        # if there is a month column, denormalize it\n",
    "        if \"month\" in self.column_names:\n",
    "            month = columns[self.column_names[\"month\"]]\n",
    "            unnormalized_month = month * self.reference_month\n",
    "            date_components[\"month\"] = unnormalized_month.astype(np.int_)\n",
    "            \n",
    "        # if there is a day column, denormalize it\n",
    "        if \"day\" in self.column_names:\n",
    "            day = columns[self.column_names[\"day\"]]\n",
    "            unnormalized_day = day * self.reference_day\n",
    "            date_components[\"day\"] = unnormalized_day.astype(np.int_)\n",
    "\n",
    "        # if there is a hour column, denormalize it\n",
    "        if \"hour\" in self.column_names:\n",
    "            hour = columns[self.column_names[\"hour\"]]\n",
    "            unnormalized_hour = hour * self.reference_hour\n",
    "            date_components[\"hour\"] = unnormalized_hour.astype(np.int_)\n",
    "\n",
    "        # if there is a minute column, denormalize it\n",
    "        if \"minute\" in self.column_names:\n",
    "            minute = columns[self.column_names[\"minute\"]]\n",
    "            unnormalized_minute = minute * self.reference_minute\n",
    "            date_components[\"minute\"] = unnormalized_minute.astype(np.int_)\n",
    "\n",
    "        # if there is a second column, denormalize it\n",
    "        if \"second\" in self.column_names:\n",
    "            second = columns[self.column_names[\"second\"]]\n",
    "            unnormalized_second = second * self.reference_second\n",
    "            date_components[\"second\"] = unnormalized_second.astype(np.int_)\n",
    "\n",
    "        # Extract the column name from the first column\n",
    "        column_name = self.column_names[\"year\"].split(SEPARATOR)[0]\n",
    "\n",
    "        print(date_components)\n",
    "\n",
    "        # Combine the date components into a date with the format YYYY-MM-DD HH:MM:SS depending on which components are present\n",
    "        date = pd.to_datetime(date_components, format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df[column_name] = date\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "    dates = {}\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        \n",
    "        date_column = Date_Transformer()\n",
    "        transformed_date = date_column.transform(dataset[column], column)\n",
    "\n",
    "        # Add the date column to the dates dictionary\n",
    "        dates[column] = date_column        \n",
    "\n",
    "        # Drop the original date column\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "\n",
    "    return transformed_dataset, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_date_columns(column):\n",
    "    dataset = pd.DataFrame()\n",
    "    dataset[\"date\"] = column\n",
    "\n",
    "    print(\"Original column:\", column)\n",
    "    date_column = Date_Transformer()\n",
    "    transformed_dataset = date_column.transform(dataset[\"date\"], \"date\")\n",
    "    print(\"Transformed dataset:\\n\", transformed_dataset)\n",
    "\n",
    "    transformed_datas = {}\n",
    "    if \"date{SEPARATOR}year\" in transformed_dataset:\n",
    "        transformed_datas[f\"date{SEPARATOR}year\"] = transformed_dataset[f\"date{SEPARATOR}year\"]\n",
    "    if \"date{SEPARATOR}month\" in transformed_dataset:\n",
    "        transformed_datas[f\"date{SEPARATOR}month\"] = transformed_dataset[f\"date{SEPARATOR}month\"]\n",
    "    if \"date{SEPARATOR}day\" in transformed_dataset:\n",
    "        transformed_datas[f\"date{SEPARATOR}day\"] = transformed_dataset[f\"date{SEPARATOR}day\"]\n",
    "    if \"date{SEPARATOR}hour\" in transformed_dataset:\n",
    "        transformed_datas[f\"date{SEPARATOR}hour\"] = transformed_dataset[f\"date{SEPARATOR}hour\"]\n",
    "    if \"date{SEPARATOR}minute\" in transformed_dataset:\n",
    "        transformed_datas[f\"date{SEPARATOR}minute\"] = transformed_dataset[f\"date{SEPARATOR}minute\"]\n",
    "    if \"date{SEPARATOR}second\" in transformed_dataset:\n",
    "        transformed_datas[f\"date{SEPARATOR}second\"] = transformed_dataset[f\"date{SEPARATOR}second\"]\n",
    "\n",
    "    untransformed_dataset = date_column.inverse_transform(transformed_datas)\n",
    "    print(\"Untransformed dataset:\\n\", untransformed_dataset)\n",
    "    \n",
    "    # Check if the untransformed dataset is equal to the original dataset and print the wrong dates if it is not\n",
    "    for i in range(len(dataset)):\n",
    "        if not dataset[\"date\"][i] == untransformed_dataset[\"date\"][i]:\n",
    "            print(\"Wrong date: \" + str(dataset[\"date\"][i]) + \" != \" + str(untransformed_dataset[\"date\"][i]))\n",
    "\n",
    "# Test the Date_Column class\n",
    "# with few dates with format YYYY-MM-DD\n",
    "test_date_columns([\"2021-01-01\", \"2021-02-02\", \"2021-03-03\", \"2021-04-04\", \"2021-05-05\"])\n",
    "# with few dates with format YYYY-MM-DD HH:MM:SS\n",
    "test_date_columns([\"2021-01-01 10:48:22\", \"2021-02-02 04:21:00\", \"2021-03-03 12:00:00\", \"2021-04-04 23:59:59\", \"2021-05-05 00:00:00\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(dataset):\n",
    "    # Prepare embeddings model\n",
    "    # embedding_dim = 100\n",
    "    # glove = GloVe(name='6B', dim=embedding_dim)\n",
    "    # tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # # Apply a word embedding encoding to all columns of the tensor using torchtext\n",
    "    # for column in dataset.columns:\n",
    "    #     # Convert the column to a string list\n",
    "    #     texts = dataset[column].astype(str).tolist()\n",
    "\n",
    "    #     # Convert the text to a list of tokens\n",
    "    #     tokens = [tokenizer(t) for t in texts]\n",
    "\n",
    "    #     # Convert the tokens to a list of indices\n",
    "    #     encoded_data = []\n",
    "    #     for token in tokens:\n",
    "    #         token_encoded = []\n",
    "    #         for word in token:\n",
    "    #             if word in glove.stoi:\n",
    "    #                 token_encoded.append(glove.stoi[word])\n",
    "    #             else:\n",
    "    #                 token_encoded.append(0)\n",
    "    #         encoded_data.append(token_encoded)\n",
    "\n",
    "    #     # Convert the indices to a PyTorch tensor\n",
    "    #     if len(encoded_data) <= 0:\n",
    "    #         continue\n",
    "\n",
    "    #     non_empty_sequences = [torch.tensor(seq) for seq in encoded_data if len(seq) > 0]\n",
    "\n",
    "    #     # Pad the sequences to the same length\n",
    "    #     padded_sequences = pad_sequence(non_empty_sequences)\n",
    "\n",
    "    #     # Add the encoded array to the transformed dataset\n",
    "    #     transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "    #     transformed_dataset[column] = padded_sequences.tolist() # TODO: fix size mismatch error\n",
    "        \n",
    "    return transformed_dataset, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv dataset from the csv file\n",
    "dataset_path = 'datasets/fake_invoice_1000.csv'\n",
    "# Creating a DataFrame from the CSV data (replace this with your actual CSV file path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Process the dataset before creating the SupplierDataset\n",
    "text_columns, date_columns, categorical_columns, numerical_columns = separate_columns_types(df)\n",
    "print(f\"Text columns: {text_columns.shape}\\n\"),                 print(text_columns.head())\n",
    "print(f\"Date columns: {date_columns.shape}\\n\"),                 print(date_columns.head())\n",
    "print(f\"Categorical columns: {categorical_columns.shape}\\n\"),   print(categorical_columns.head())\n",
    "print(f\"Numerical columns: {numerical_columns.shape}\\n\"),       print(numerical_columns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = {}\n",
    "\n",
    "# Apply transformations to the categorical columns\n",
    "categorical_columns_treated, transformers[\"categorical\"] = transform_categorical(categorical_columns)\n",
    "print(f\"Categorical columns after transformation: {categorical_columns_treated.shape}\\n\"),   print(categorical_columns_treated.head(), \"\\n\\n\")\n",
    "numerical_columns_treated, transformers[\"numerical\"] = transform_numerical(numerical_columns)\n",
    "print(f\"Numerical columns after transformation: {numerical_columns_treated.shape}\\n\"),       print(numerical_columns_treated.head(), \"\\n\\n\")\n",
    "text_columns_treated, transformers[\"text\"] = transform_text(text_columns)\n",
    "#print(f\"Text columns after transformation: {text_columns_treated.shape}\\n\"),                 print(text_columns_treated.head(), \"\\n\\n\")\n",
    "date_columns_treated, transformers[\"date\"] = transform_date(date_columns)\n",
    "print(f\"Date columns after transformation: {date_columns_treated.shape}\\n\"),                 print(date_columns_treated.head(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the transformed columns\n",
    "#text_columns_treated\n",
    "df_treated = pd.concat([ numerical_columns_treated, categorical_columns_treated,date_columns_treated], axis=1)\n",
    "print(f\"Final dataset: {df_treated.shape}\\n\"), print(df_treated.head(), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is nan values or empty strings\n",
    "print(\"Is there null values => \", df_treated.isnull().values.any())\n",
    "print(\"Is there nan values => \", df_treated.isna().values.any())\n",
    "print(\"Is there empty strings => \", df_treated.isin(['']).values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the SupplierDataset\n",
    "supplier_dataset = SupplierDataset(dataframe=df_treated)\n",
    "print(f\"Supplier dataset: {len(supplier_dataset)}\\n shape: {supplier_dataset[0].shape}\\n type: {type(supplier_dataset[0])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.2\n",
    "batch_size = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(supplier_dataset))\n",
    "val_size = len(supplier_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(supplier_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print some statistics about the dataset\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Print 5 samples from the training dataset\n",
    "print(\"\\nTraining samples:\" + \"\\n\")\n",
    "e = iter(train_loader)\n",
    "element = next(e)\n",
    "print(f\"shape of element: {len(element)} and type: {type(element)}\")\n",
    "print(f\"first element :\\n\"+ str(element) + \"\\n end of first element\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Define dynamic layers for the encoder\n",
    "        # You can experiment with different architectures\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Parameters for the mean and log-variance of the latent space\n",
    "        self.fc_mean = nn.Linear(128, latent_size)\n",
    "        self.fc_logvar = nn.Linear(128, latent_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Calculate mean and log-variance for the latent space\n",
    "        mean = self.fc_mean(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        \n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Define dynamic layers for the decoder\n",
    "        self.fc1 = nn.Linear(latent_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        \n",
    "        # Output layer for reconstruction\n",
    "        self.fc_out = nn.Linear(256, output_size)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = self.fc1(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        z = F.relu(z)\n",
    "        \n",
    "        # Output layer for reconstruction\n",
    "        x_recon = torch.sigmoid(self.fc_out(z))  # Assuming the data is normalized to [0, 1]\n",
    "        \n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        # Create instances of the Encoder and Decoder\n",
    "        self.encoder = Encoder(input_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, input_size)\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Reparameterization trick for sampling from a normal distribution\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        mean, logvar = self.encoder.forward(x)\n",
    "        print(f\"forward mean : {mean} and logvar : {logvar}\")\n",
    "        \n",
    "        # Sample from the latent space using the reparameterization trick\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        \n",
    "        # Forward pass through the decoder\n",
    "        x_recon = self.decoder.forward(z)\n",
    "        \n",
    "        return x_recon, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
    "\n",
    "    # Reconstruction Loss (e.g., Mean Squared Error or Binary Cross Entropy)\n",
    "    reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')  # Replace with appropriate loss function\n",
    "\n",
    "    # KL Divergence Loss\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Combine the Reconstruction Loss and KL Divergence Loss with the weighting factor (beta)\n",
    "    total_loss = reconstruction_loss + beta * kl_divergence\n",
    "        \n",
    "    return total_loss, reconstruction_loss, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(recon_data, mu, logvar):\n",
    "    # Recon data\n",
    "    if(recon_data is None):\n",
    "        print(\"recon_data is None\")\n",
    "        return False\n",
    "    if(recon_data.shape[0] != batch_size):\n",
    "        print(\"recon_data shape is not batch_size\")\n",
    "        return False\n",
    "       # check for nan values\n",
    "    if(torch.isnan(recon_data).any()):\n",
    "        print(\"recon_data has nan values\")\n",
    "        return False\n",
    "    # check for inf values\n",
    "    if(torch.isinf(recon_data).any()):\n",
    "        print(\"recon_data has inf values\")\n",
    "        return False\n",
    "    # check for negative values\n",
    "    if((recon_data < 0).any()):\n",
    "        print(\"recon_data has negative values\")\n",
    "        return False\n",
    "    # check for values greater than 1\n",
    "    if((recon_data > 1).any()):\n",
    "        print(\"recon_data has values greater than 1\")\n",
    "        return False\n",
    "    # check for null values\n",
    "    if(recon_data.isnull().values.any()):\n",
    "        print(\"recon_data has null values\")\n",
    "        return False\n",
    "    \n",
    "    # mu\n",
    "    if(mu is None):\n",
    "        print(\"mu is None\")\n",
    "        return False\n",
    "    if(mu.shape[0] != batch_size):\n",
    "        print(\"mu shape is not batch_size\")\n",
    "        return False\n",
    "    # check for nan values\n",
    "    if(torch.isnan(mu).any()):\n",
    "        print(\"mu has nan values\")\n",
    "        return False\n",
    "    # check for inf values\n",
    "    if(torch.isinf(mu).any()):\n",
    "        print(\"mu has inf values\")\n",
    "        return False\n",
    "    # check for null values\n",
    "    if(mu.isnull().values.any()):\n",
    "        print(\"mu has null values\")\n",
    "        return False\n",
    "    \n",
    "    # logvar\n",
    "    if(logvar is None):\n",
    "        print(\"logvar is None\")\n",
    "        return False\n",
    "    if(logvar.shape[0] != batch_size):\n",
    "        print(\"logvar shape is not batch_size\")\n",
    "        return False\n",
    "    # check for nan values\n",
    "    if(torch.isnan(logvar).any()):\n",
    "        print(\"logvar has nan values\")\n",
    "        return False\n",
    "    # check for inf values\n",
    "    if(torch.isinf(logvar).any()):\n",
    "        print(\"logvar has inf values\")\n",
    "        return False\n",
    "    # check for null values\n",
    "    if(logvar.isnull().values.any()):\n",
    "        print(\"logvar has null values\")\n",
    "        return False\n",
    "    \n",
    "    return True   \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae, train_loader, num_epochs=10, learning_rate=1e-3, beta=1.0, device='cuda', with_checker=False):\n",
    "    # Move the model to the specified device (cuda or cpu)\n",
    "    vae.to(device)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        recon_loss = 0.0\n",
    "        kl_loss = 0.0\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            # Move the batch to the specified device\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through the VAE\n",
    "            recon_data, mu, logvar = vae.forward(data)\n",
    "            \n",
    "            # Check the model return values\n",
    "            if(with_checker):\n",
    "                if(not checker(recon_data, mu, logvar)):\n",
    "                    return\n",
    "            \n",
    "            # Calculate the VAE loss\n",
    "            loss, recon_loss_batch, kl_loss_batch = loss_function(recon_data, data, mu, logvar, beta=beta)\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the running total of losses\n",
    "            total_loss += loss.item()\n",
    "            recon_loss += recon_loss_batch.item()\n",
    "            kl_loss += kl_loss_batch.item()\n",
    "            \n",
    "            # Print logs every N batches (you can adjust this value)\n",
    "            log_interval = 100\n",
    "            if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "                avg_loss = total_loss / log_interval\n",
    "                avg_recon_loss = recon_loss / log_interval\n",
    "                avg_kl_loss = kl_loss / log_interval\n",
    "                \n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], '\n",
    "                      f'Avg. Loss: {avg_loss:.4f}, Avg. Recon Loss: {avg_recon_loss:.4f}, Avg. KL Loss: {avg_kl_loss:.4f}')\n",
    "                \n",
    "                total_loss = 0.0\n",
    "                recon_loss = 0.0\n",
    "                kl_loss = 0.0\n",
    "                \n",
    "        # Print epoch-level logs\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Avg. Total Loss: {total_loss:.4f}')\n",
    "        \n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and optimizer\n",
    "# Get the input dimension from the training dataset\n",
    "input_dim =  len(train_dataset[1])\n",
    "print(f\"input_dim : {input_dim}\")\n",
    "# Get the latent dimension from the input dimension\n",
    "latent_dim = input_dim // 2\n",
    "model = VAE(input_dim, latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"input_dim: {input_dim} and latent_dim: {latent_dim}\")\n",
    "print(f\"model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train(model, train_loader, num_epochs=1, device=device, with_checker=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a new data to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_headers(headers, generated_data):\n",
    "    # Add the headers to the generated data\n",
    "    return pd.DataFrame(generated_data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new sample\n",
    "def generate_sample(model, headers):\n",
    "    sample = torch.randn(1, model.latent_size)\n",
    "    generated_data = model.decoder.forward(sample).detach().numpy()\n",
    "    return add_headers(headers, generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample = generate_sample(model, df_treated.columns)\n",
    "\n",
    "print(new_sample.shape)\n",
    "print(df_treated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print or use the generated supplier data as needed\n",
    "print(\"Generated Supplier Data:\")\n",
    "for i in range(5):\n",
    "    print(generate_sample(model, df_treated.columns))\n",
    "\n",
    "# Save the model\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "torch.save(model.state_dict(), f'model_{timestamp}.pt')\n",
    "\n",
    "# Generate a new sample\n",
    "new_sample = generate_sample(model, df_treated.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_original_columns = df.shape\n",
    "num_treated_columns = df_treated.shape\n",
    "num_generated_columns = new_sample.shape\n",
    "\n",
    "print(f\"Original columns: {num_original_columns}\")\n",
    "print(f\"Treated columns: {num_treated_columns}\")\n",
    "print(f\"Generated columns: {num_generated_columns}\")\n",
    "\n",
    "print(transformers)\n",
    "print(new_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is__in_categorical_tranfomer(column_name, transformers):\n",
    "    # Parse the column name\n",
    "    parsed_column = column_name.split(\"_\")[0]\n",
    "    \n",
    "    return parsed_column in transformers[\"categorical\"].keys()\n",
    "\n",
    "def get_categorical_inverse(column, column_name, transformers):\n",
    "    # Parse the column name\n",
    "    parsed_column = column_name.split(\"_\")[0]\n",
    "    \n",
    "    # Get the transformer for the column\n",
    "    transformer = transformers[\"categorical\"][parsed_column]\n",
    "    \n",
    "    # Inverse transform the column\n",
    "    inversed_column = transformer.inverse_transform(column)\n",
    "\n",
    "    return pd.DataFrame(inversed_column, columns=[column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is__in_numerical_tranfomer(column_name, transformers):\n",
    "    return column_name in transformers[\"numerical\"].keys()\n",
    "\n",
    "def get_numerical_inverse(column, column_name, transformers):\n",
    "    # Get the transformer for the column\n",
    "    transformer = transformers[\"numerical\"][column_name]\n",
    "    \n",
    "    # Inverse transform the column\n",
    "    inversed_column = transformer.inverse_transform(column)\n",
    "\n",
    "    return pd.DataFrame(inversed_column, columns=[column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is__in_date_tranfomer(column_name, transformers):\n",
    "    # Parse the column name\n",
    "    parsed_column = column_name.split(\"_\")[0]\n",
    "    \n",
    "    return parsed_column in transformers[\"date\"].keys()\n",
    "\n",
    "def get_date_inverse(column, column_name, transformers):\n",
    "    # Parse the column name\n",
    "    parsed_column = column_name.split(\"_\")[0]\n",
    "    \n",
    "    # Get the transformer for the column\n",
    "    transformer = transformers[\"date\"][parsed_column]\n",
    "    \n",
    "    # Inverse transform the column\n",
    "    inversed_column = transformer.inverse_transform(column)\n",
    "\n",
    "    return pd.DataFrame(inversed_column, columns=[column_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the new sample columns to find the group of columns related to the same original column\n",
    "columns_groups = {}\n",
    "\n",
    "# initialise the columns_groups with the original columns\n",
    "for column in df.columns:\n",
    "    columns_groups[column] = []\n",
    "\n",
    "print(\"Before : \", columns_groups)\n",
    "\n",
    "for column in new_sample.columns:\n",
    "    # Try parsing the column name\n",
    "    parsed_column = column.split(SEPARATOR)[0]\n",
    "\n",
    "    print(f\"column : {column} -> {parsed_column}\")\n",
    "\n",
    "    # Check wich transformer to use\n",
    "    if parsed_column in transformers[\"categorical\"].keys():\n",
    "        columns_groups[parsed_column].append(column)\n",
    "    elif parsed_column in transformers[\"numerical\"].keys():\n",
    "        columns_groups[parsed_column].append(column)\n",
    "    elif parsed_column in transformers[\"date\"].keys():\n",
    "        columns_groups[parsed_column].append(column)\n",
    "\n",
    "print(\"After : \", columns_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the new sample column names to apply correct transformer\n",
    "new_sample_columns = new_sample.columns\n",
    "new_sample_transformed = pd.DataFrame()\n",
    "\n",
    "# Apply the inverse transformations to the generated sample\n",
    "for column_name in new_sample_columns:\n",
    "    print(f'Column : {column_name}')\n",
    "\n",
    "    if is__in_categorical_tranfomer(column_name, transformers):\n",
    "        new_sample_transformed.join(get_categorical_inverse(new_sample[column_name], column_name, transformers))\n",
    "    elif is__in_numerical_tranfomer(column_name, transformers):\n",
    "        new_sample_transformed.join(get_numerical_inverse(new_sample[column_name], column_name, transformers))\n",
    "    elif is__in_date_tranfomer(column_name, transformers):\n",
    "        new_sample_transformed.join(get_date_inverse(new_sample[column_name], column_name, transformers))\n",
    "    # elif column in transformers[\"text\"].keys():\n",
    "    #     inverse_transformed = transformers[\"text\"][column].inverse_transform(new_sample[column])\n",
    "    #     new_sample_transformed[column] = inverse_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the transformed generated sample in the format 'column: value'\n",
    "for column_name in new_sample_transformed.columns:\n",
    "    print(f\"{column_name}: {new_sample_transformed[column_name][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_categorical(transformer, categorical_colums):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = categorical_colums.copy()\n",
    "\n",
    "    # Apply a one-hot encoding to all columns of the tensor\n",
    "    for column in categorical_colums.columns:\n",
    "        # Convert the one-hot encoded array to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(column)\n",
    "    \n",
    "        # Convert the tensor to a NumPy array\n",
    "        numpy_data = tensor_data.numpy()\n",
    "    \n",
    "        # Convert the one-hot encoded array to a list of strings\n",
    "        indices = np.argmax(numpy_data, axis=1)\n",
    "        column_data = transformer[column].inverse_transform(indices)\n",
    "    \n",
    "        # Add the encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = column_data\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_numerical(tranformer, numerical_columns):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = numerical_columns.copy()\n",
    "\n",
    "    # Apply a min-max normalization to all columns of the tensor\n",
    "    for column in numerical_columns.columns:\n",
    "        # Convert the column to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(numerical_columns[column], dtype=torch.float32)\n",
    "\n",
    "        # Range of values in the column\n",
    "        min_value = torch.min(tensor_data)\n",
    "        max_value = torch.max(tensor_data)\n",
    "\n",
    "        # Normalize the values in the column between 0 and 1en\n",
    "        normalized = (tensor_data - min_value) / (max_value - min_value)\n",
    "\n",
    "        # Apply min-max normalization using torch.nn.functional.normalize\n",
    "        #normalized = F.normalize(tensor_data)\n",
    "\n",
    "        # Convert the normalized tensor to a NumPy array\n",
    "        normalized_array = normalized.numpy()\n",
    "\n",
    "        # Add the normalized array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(normalized_array)\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_text(text_columns):\n",
    "    # Prepare embeddings model\n",
    "    embedding_dim = 100\n",
    "    glove = GloVe(name='6B', dim=embedding_dim)\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = text_columns.copy()\n",
    "\n",
    "    # Apply a word embedding encoding to all columns of the tensor using torchtext\n",
    "    for column in text_columns.columns:\n",
    "        # Convert the column to a string list\n",
    "        texts = text_columns[column].astype(str).tolist()\n",
    "\n",
    "        # Convert the text to a list of tokens\n",
    "        tokens = [tokenizer(t) for t in texts]\n",
    "\n",
    "        # Convert the tokens to a list of indices\n",
    "        encoded_data = []\n",
    "        for token in tokens:\n",
    "            token_encoded = []\n",
    "            for word in token:\n",
    "                if word in glove.stoi:\n",
    "                    token_encoded.append(glove.stoi[word])\n",
    "                else:\n",
    "                    token_encoded.append(0)\n",
    "            encoded_data.append(token_encoded)\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        if len(encoded_data) <= 0:\n",
    "            continue\n",
    "\n",
    "        non_empty_sequences = [torch.tensor(seq) for seq in encoded_data if len(seq) > 0]\n",
    "\n",
    "        # Pad the sequences to the same length\n",
    "        padded_sequences = pad_sequence(non_empty_sequences)\n",
    "\n",
    "        # Add the encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = padded_sequences.tolist() # TODO: fix size mismatch error\n",
    "        \n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_date(date_columns):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = date_columns.copy()\n",
    "\n",
    "    for column in date_columns.columns:\n",
    "        # Decompose the date into its components\n",
    "        date = pd.to_datetime(date_columns[column])\n",
    "\n",
    "        year = date.dt.year\n",
    "        # Check if the inital column contain a year\n",
    "        if not year.empty and not year.all() == 0:\n",
    "            year = torch.tensor(year, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_year\"] = year\n",
    "\n",
    "        month = date.dt.month\n",
    "        if not month.empty and not month.all() == 0:\n",
    "            month = torch.tensor(month / 12, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_month\"] = month\n",
    "\n",
    "        day = date.dt.day\n",
    "        if not day.empty and not day.all() == 0:\n",
    "            day = torch.tensor(day / 31, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_day\"] = day\n",
    "\n",
    "        hour = date.dt.hour\n",
    "        if not hour.empty and not hour.all() == 0:\n",
    "            hour = torch.tensor(hour / 24, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_hour\"] = hour\n",
    "\n",
    "        minute = date.dt.minute\n",
    "        if not minute.empty and not minute.all() == 0:\n",
    "            minute = torch.tensor(minute / 60, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_minute\"] = minute\n",
    "\n",
    "        second = date.dt.second\n",
    "        if not second.empty and not second.all() == 0:\n",
    "            second = torch.tensor(second / 60, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_second\"] = second\n",
    "        \n",
    "\n",
    "        # Drop the original date column\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_type(sample, categorical_columns, numerical_columns, text_columns, date_columns):\n",
    "    \n",
    "    # Retrieve the columns from the sample and assign the right column name\n",
    "    sample = pd.DataFrame(sample)\n",
    "    sample.columns = categorical_columns.columns.tolist() + numerical_columns.columns.tolist() + text_columns.columns.tolist() + date_columns.columns.tolist()\n",
    "            \n",
    "    return sample[text_columns], sample[date_columns], sample[categorical_columns], sample[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_colums_result, numerical_columns_result, text_columns_result, date_columns_result = split_by_type(new_sample, categorical_columns_treated.columns, numerical_columns_treated.columns, text_columns_treated.columns, date_columns_treated.columns)\n",
    "\n",
    "\n",
    "# Reverse the transformations applied to the categorical columns\n",
    "categorical_columns_reversed = reverse_transform_categorical(transformers[\"categorical\"], categorical_colums_result)\n",
    "\n",
    "# Reverse the transformations applied to the numerical columns\n",
    "numerical_columns_reversed = reverse_transform_numerical(transformers[\"numerical\"], numerical_columns_result)\n",
    "\n",
    "# Reverse the transformations applied to the text columns\n",
    "#text_columns_reversed = reverse_transform_text(transformers[\"text\"], text_columns_result)\n",
    "\n",
    "# Reverse the transformations applied to the date columns\n",
    "#date_columns_reversed = reverse_transform_date(transformers[\"date\"], date_columns_result)\n",
    "\n",
    "# Concatenate the reversed columns back into a single dataframe\n",
    "#text_columns_reversed, date_columns_reversed\n",
    "df_reversed = pd.concat([categorical_columns_reversed, numerical_columns_reversed], axis=1)\n",
    "\n",
    "# Print the reversed dataframe\n",
    "print(df_reversed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
