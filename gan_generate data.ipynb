{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchtext as tt\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting & vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for PyTorch\n",
    "class SupplierDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the element at the given index\n",
    "        element = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Flatten the element into a 1D array by looping through the columns and adding them to the array\n",
    "        flatten_element = []\n",
    "        for column in element:\n",
    "            # list\n",
    "            if isinstance(column, list):\n",
    "                for item in column:\n",
    "                    flatten_element.append(item)\n",
    "            # numpy array\n",
    "            elif isinstance(column, np.ndarray):\n",
    "                for item in column:\n",
    "                    flatten_element.append(item)\n",
    "            # sub tensor\n",
    "            elif isinstance(column, torch.Tensor):\n",
    "                for item in column:\n",
    "                    flatten_element.append(item)\n",
    "            else:\n",
    "                flatten_element.append(column)\n",
    "        \n",
    "        # Convert the element to a PyTorch tensor\n",
    "        tensor = torch.tensor(flatten_element, dtype=torch.float32)\n",
    "        \n",
    "        #print(f\"shape of tensor: {tensor.shape} and type: {type(tensor)}\")\n",
    "        \n",
    "        # Apply the transform if one is given\n",
    "        if self.transform:\n",
    "            return self.transform(tensor)\n",
    "        else:\n",
    "            return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_columns_types(dataset):\n",
    "    # Separate the text columns from the other columns\n",
    "    text_columns = [\"invoice_code\", \"customer_name\",\"customer_email\",\"customer_address\",\"customer_city\",\"customer_state\",\"customer_postal_code\",\n",
    "                    \"customer_country\",\"notes\",\"created_by\",\"updated_by\",\"shipping_address\",\"shipping_city\",\"shipping_state\",\n",
    "                    \"shipping_postal_code\",\"shipping_country\"]\n",
    "    \n",
    "    date_columns = [\"invoice_date\",\"payment_due_date\",\"created_at\",\"updated_at\",\"due_date\",\"paid_date\"]\n",
    "\n",
    "    categorical_columns = [\"payment_method\",\"status\",\"currency\",\"payment_reference\"]\n",
    "\n",
    "    numerical_columns = [\"invoice_number\",\"subtotal\",\"tax_rate\",\"tax_amount\",\"discount_rate\",\"discount_amount\",\"total\",\"exchange_rate\"]  \n",
    "\n",
    "    # Check if there is a column not in one of the above lists\n",
    "    for column in dataset.columns:\n",
    "        if column not in text_columns and column not in date_columns and column not in categorical_columns and column not in numerical_columns:\n",
    "            print(\"Column not in any list: \" + column) \n",
    "            \n",
    "    return dataset[text_columns], dataset[date_columns], dataset[categorical_columns], dataset[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical(dataset):\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "    classes = {}\n",
    "    \n",
    "    # Apply a one-hot encoding to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "\n",
    "        # Convert the strings to unique numerical indices\n",
    "        unique_classes, indices = np.unique(dataset[column], return_inverse=True)\n",
    "        classes[column] = unique_classes\n",
    "        #print(\"Column: \" + column + \" - Classes: \" + str(unique_classes))\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(indices)\n",
    "\n",
    "        # Apply one-hot encoding using torch.nn.functional.one_hot\n",
    "        one_hot_encoded = F.one_hot(tensor_data)\n",
    "        \n",
    "        # Convert the one-hot encoding tensor to a NumPy array\n",
    "        one_hot_array = one_hot_encoded.numpy()\n",
    "\n",
    "        # Add the one-hot encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(one_hot_array)\n",
    "\n",
    "    return transformed_dataset, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_numerical(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # Apply a min-max normalization to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "        # Convert the column to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(dataset[column], dtype=torch.float32)\n",
    "\n",
    "        # Range of values in the column\n",
    "        min_value = torch.min(tensor_data)\n",
    "        max_value = torch.max(tensor_data)\n",
    "\n",
    "        # Normalize the values in the column between 0 and 1en\n",
    "        normalized = (tensor_data - min_value) / (max_value - min_value)\n",
    "\n",
    "        # Apply min-max normalization using torch.nn.functional.normalize\n",
    "        #normalized = F.normalize(tensor_data)\n",
    "\n",
    "        # Convert the normalized tensor to a NumPy array\n",
    "        normalized_array = normalized.numpy()\n",
    "\n",
    "        # Add the normalized array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(normalized_array)\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(dataset):\n",
    "    # Prepare embeddings model\n",
    "    embedding_dim = 100\n",
    "    glove = GloVe(name='6B', dim=embedding_dim)\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # Apply a word embedding encoding to all columns of the tensor using torchtext\n",
    "    for column in dataset.columns:\n",
    "        # Convert the column to a string list\n",
    "        texts = dataset[column].astype(str).tolist()\n",
    "\n",
    "        # Convert the text to a list of tokens\n",
    "        tokens = [tokenizer(t) for t in texts]\n",
    "\n",
    "        # Convert the tokens to a list of indices\n",
    "        encoded_data = []\n",
    "        for token in tokens:\n",
    "            token_encoded = []\n",
    "            for word in token:\n",
    "                if word in glove.stoi:\n",
    "                    token_encoded.append(glove.stoi[word])\n",
    "                else:\n",
    "                    token_encoded.append(0)\n",
    "            encoded_data.append(token_encoded)\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        if len(encoded_data) <= 0:\n",
    "            continue\n",
    "\n",
    "        non_empty_sequences = [torch.tensor(seq) for seq in encoded_data if len(seq) > 0]\n",
    "\n",
    "        # Pad the sequences to the same length\n",
    "        padded_sequences = pad_sequence(non_empty_sequences)\n",
    "\n",
    "        # Add the encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = padded_sequences.tolist() # TODO: fix size mismatch error\n",
    "        \n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # Apply a timestamp convertion to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "        # Apply timestamp convertion using pandas.to_datetime\n",
    "        timestamp = pd.to_datetime(dataset[column]).astype(np.int64) // 10**9\n",
    "\n",
    "        # Add the timestamp to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(timestamp)\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns: (1000, 16)\n",
      "\n",
      "Date columns: (1000, 6)\n",
      "\n",
      "Categorical columns: (1000, 4)\n",
      "\n",
      "Numerical columns: (1000, 8)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv dataset from the csv file\n",
    "dataset_path = 'datasets/fake_invoice_1000.csv'\n",
    "# Creating a DataFrame from the CSV data (replace this with your actual CSV file path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Process the dataset before creating the SupplierDataset\n",
    "text_columns, date_columns, categorical_columns, numerical_columns = separate_columns_types(df)\n",
    "print(f\"Text columns: {text_columns.shape}\\n\"),                 #print(text_columns.head())\n",
    "print(f\"Date columns: {date_columns.shape}\\n\"),                 #print(date_columns.head())\n",
    "print(f\"Categorical columns: {categorical_columns.shape}\\n\"),   #print(categorical_columns.head())\n",
    "print(f\"Numerical columns: {numerical_columns.shape}\\n\"),       #print(numerical_columns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns after transformation: (1000, 4)\n",
      "\n",
      "  payment_method        status  \\\n",
      "0      [0, 1, 0]  [0, 0, 0, 1]   \n",
      "1      [1, 0, 0]  [0, 0, 1, 0]   \n",
      "2      [1, 0, 0]  [0, 1, 0, 0]   \n",
      "3      [1, 0, 0]  [1, 0, 0, 0]   \n",
      "4      [0, 0, 1]  [0, 0, 1, 0]   \n",
      "\n",
      "                                            currency payment_reference  \n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 1, 0]  \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 1, 0]  \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [1, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...         [0, 1, 0]  \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [1, 0, 0]   \n",
      "\n",
      "\n",
      "Numerical columns after transformation: (1000, 8)\n",
      "\n",
      "   invoice_number  subtotal  tax_rate  tax_amount  discount_rate  \\\n",
      "0        0.248694  0.250331      0.15    0.038610           0.86   \n",
      "1        0.771873  0.368514      0.05    0.018907           0.30   \n",
      "2        0.330406  0.185498      0.20    0.038234           0.38   \n",
      "3        0.507838  0.653593      0.60    0.401614           0.90   \n",
      "4        0.311284  0.588111      0.85    0.512093           0.76   \n",
      "\n",
      "   discount_amount     total  exchange_rate  \n",
      "0         0.226812  0.133221       0.977807  \n",
      "1         0.116232  0.281886       0.115933  \n",
      "2         0.074432  0.140233       0.882845  \n",
      "3         0.617249  0.389216       0.711891  \n",
      "4         0.469142  0.413138       0.172187   \n",
      "\n",
      "\n",
      "Date columns after transformation: (1000, 6)\n",
      "\n",
      "   invoice_date  payment_due_date  created_at  updated_at    due_date  \\\n",
      "0    1666569600        1671235200  1666224000  1667088000  1670544000   \n",
      "1    1651276800        1649289600  1642204800  1646784000  1668038400   \n",
      "2    1658275200        1641254400  1653955200  1654041600  1647907200   \n",
      "3    1671840000        1667606400  1665619200  1648166400  1665792000   \n",
      "4    1662422400        1666396800  1659052800  1665619200  1663286400   \n",
      "\n",
      "    paid_date  \n",
      "0  1669161600  \n",
      "1  1655769600  \n",
      "2  1668729600  \n",
      "3  1662249600  \n",
      "4  1649030400   \n",
      "\n",
      "\n",
      "Final dataset: (1000, 18)\n",
      "\n",
      "   invoice_date  payment_due_date  created_at  updated_at    due_date  \\\n",
      "0    1666569600        1671235200  1666224000  1667088000  1670544000   \n",
      "1    1651276800        1649289600  1642204800  1646784000  1668038400   \n",
      "2    1658275200        1641254400  1653955200  1654041600  1647907200   \n",
      "3    1671840000        1667606400  1665619200  1648166400  1665792000   \n",
      "4    1662422400        1666396800  1659052800  1665619200  1663286400   \n",
      "\n",
      "    paid_date payment_method        status  \\\n",
      "0  1669161600      [0, 1, 0]  [0, 0, 0, 1]   \n",
      "1  1655769600      [1, 0, 0]  [0, 0, 1, 0]   \n",
      "2  1668729600      [1, 0, 0]  [0, 1, 0, 0]   \n",
      "3  1662249600      [1, 0, 0]  [1, 0, 0, 0]   \n",
      "4  1649030400      [0, 0, 1]  [0, 0, 1, 0]   \n",
      "\n",
      "                                            currency payment_reference  \\\n",
      "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 1, 0]   \n",
      "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 1, 0]   \n",
      "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [1, 0, 0]   \n",
      "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...         [0, 1, 0]   \n",
      "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         [1, 0, 0]   \n",
      "\n",
      "   invoice_number  subtotal  tax_rate  tax_amount  discount_rate  \\\n",
      "0        0.248694  0.250331      0.15    0.038610           0.86   \n",
      "1        0.771873  0.368514      0.05    0.018907           0.30   \n",
      "2        0.330406  0.185498      0.20    0.038234           0.38   \n",
      "3        0.507838  0.653593      0.60    0.401614           0.90   \n",
      "4        0.311284  0.588111      0.85    0.512093           0.76   \n",
      "\n",
      "   discount_amount     total  exchange_rate  \n",
      "0         0.226812  0.133221       0.977807  \n",
      "1         0.116232  0.281886       0.115933  \n",
      "2         0.074432  0.140233       0.882845  \n",
      "3         0.617249  0.389216       0.711891  \n",
      "4         0.469142  0.413138       0.172187   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations to the categorical columns\n",
    "categorical_colums_treated, classes = transform_categorical(categorical_columns)\n",
    "print(f\"Categorical columns after transformation: {categorical_colums_treated.shape}\\n\"),   print(categorical_colums_treated.head(), \"\\n\\n\")\n",
    "numerical_columns_treated = transform_numerical(numerical_columns)\n",
    "print(f\"Numerical columns after transformation: {numerical_columns_treated.shape}\\n\"),       print(numerical_columns_treated.head(), \"\\n\\n\")\n",
    "#text_columns_treated = transform_text(text_columns)\n",
    "#print(f\"Text columns after transformation: {text_columns_treated.shape}\\n\"),                 print(text_columns_treated.head(), \"\\n\\n\")\n",
    "date_columns_treated = transform_date(date_columns)\n",
    "print(f\"Date columns after transformation: {date_columns_treated.shape}\\n\"),                 print(date_columns_treated.head(), \"\\n\\n\")\n",
    "\n",
    "# Concatenate the transformed columns\n",
    "#text_columns_treated, \n",
    "df_treated = pd.concat([date_columns_treated, categorical_colums_treated, numerical_columns_treated], axis=1)\n",
    "print(f\"Final dataset: {df_treated.shape}\\n\"), print(df_treated.head(), \"\\n\\n\")\n",
    "\n",
    "# Create an instance of the SupplierDataset\n",
    "supplier_dataset = SupplierDataset(dataframe=df_treated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 800\n",
      "Validation samples: 200\n",
      "Batch size: 50\n",
      "\n",
      "Training samples:\n",
      "\n",
      "shape of element: 50 and type: <class 'torch.Tensor'>\n",
      "first element :\n",
      "tensor([[1.6568e+09, 1.6521e+09, 1.6478e+09,  ..., 2.2171e-01, 1.2079e-01,\n",
      "         2.6077e-01],\n",
      "        [1.6460e+09, 1.6451e+09, 1.6721e+09,  ..., 3.0793e-01, 4.0478e-01,\n",
      "         7.0249e-01],\n",
      "        [1.6558e+09, 1.6694e+09, 1.6716e+09,  ..., 1.4041e-01, 3.6947e-01,\n",
      "         2.3391e-01],\n",
      "        ...,\n",
      "        [1.6487e+09, 1.6457e+09, 1.6676e+09,  ..., 3.2005e-02, 5.1548e-01,\n",
      "         7.3651e-01],\n",
      "        [1.6670e+09, 1.6419e+09, 1.6563e+09,  ..., 5.7757e-01, 7.0221e-01,\n",
      "         9.1571e-01],\n",
      "        [1.6558e+09, 1.6529e+09, 1.6458e+09,  ..., 3.3595e-01, 3.8091e-01,\n",
      "         8.5256e-01]])\n",
      " end of first element\n"
     ]
    }
   ],
   "source": [
    "train_percentage = 0.2\n",
    "batch_size = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(supplier_dataset))\n",
    "val_size = len(supplier_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(supplier_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print some statistics about the dataset\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Print 5 samples from the training dataset\n",
    "print(\"\\nTraining samples:\" + \"\\n\")\n",
    "e = iter(train_loader)\n",
    "element = next(e)\n",
    "print(f\"shape of element: {len(element)} and type: {type(element)}\")\n",
    "print(f\"first element :\\n\"+ str(element) + \"\\n end of first element\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Define dynamic layers for the encoder\n",
    "        # You can experiment with different architectures\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Parameters for the mean and log-variance of the latent space\n",
    "        self.fc_mean = nn.Linear(128, latent_size)\n",
    "        self.fc_logvar = nn.Linear(128, latent_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Calculate mean and log-variance for the latent space\n",
    "        mean = self.fc_mean(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        \n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Define dynamic layers for the decoder\n",
    "        self.fc1 = nn.Linear(latent_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        \n",
    "        # Output layer for reconstruction\n",
    "        self.fc_out = nn.Linear(256, output_size)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = self.fc1(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        z = F.relu(z)\n",
    "        \n",
    "        # Output layer for reconstruction\n",
    "        x_recon = torch.sigmoid(self.fc_out(z))  # Assuming the data is normalized to [0, 1]\n",
    "        \n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Create instances of the Encoder and Decoder\n",
    "        self.encoder = Encoder(input_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, input_size)\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Reparameterization trick for sampling from a normal distribution\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        mean, logvar = self.encoder.forward(x)\n",
    "        \n",
    "        # Sample from the latent space using the reparameterization trick\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        \n",
    "        # Forward pass through the decoder\n",
    "        x_recon = self.decoder.forward(z)\n",
    "        \n",
    "        return x_recon, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
    "    # Reconstruction Loss (e.g., Mean Squared Error or Binary Cross Entropy)\n",
    "    reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')  # Replace with appropriate loss function\n",
    "    \n",
    "    # KL Divergence Loss\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Combine the Reconstruction Loss and KL Divergence Loss with the weighting factor (beta)\n",
    "    total_loss = reconstruction_loss + beta * kl_divergence\n",
    "    \n",
    "    return total_loss, reconstruction_loss, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae, train_loader, num_epochs=10, learning_rate=1e-3, beta=1.0, device='cuda'):\n",
    "    # Move the model to the specified device (cuda or cpu)\n",
    "    vae.to(device)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        recon_loss = 0.0\n",
    "        kl_loss = 0.0\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            # Move the batch to the specified device\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through the VAE\n",
    "            recon_data, mu, logvar = vae.forward(data)\n",
    "            \n",
    "            # Calculate the VAE loss\n",
    "            loss, recon_loss_batch, kl_loss_batch = loss_function(recon_data, data, mu, logvar, beta=beta)\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the running total of losses\n",
    "            total_loss += loss.item()\n",
    "            recon_loss += recon_loss_batch.item()\n",
    "            kl_loss += kl_loss_batch.item()\n",
    "            \n",
    "            # Print logs every N batches (you can adjust this value)\n",
    "            log_interval = 100\n",
    "            if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "                avg_loss = total_loss / log_interval\n",
    "                avg_recon_loss = recon_loss / log_interval\n",
    "                avg_kl_loss = kl_loss / log_interval\n",
    "                \n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], '\n",
    "                      f'Avg. Loss: {avg_loss:.4f}, Avg. Recon Loss: {avg_recon_loss:.4f}, Avg. KL Loss: {avg_kl_loss:.4f}')\n",
    "                \n",
    "                total_loss = 0.0\n",
    "                recon_loss = 0.0\n",
    "                kl_loss = 0.0\n",
    "                \n",
    "        # Print epoch-level logs\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Avg. Total Loss: {total_loss:.4f}')\n",
    "        \n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and optimizer\n",
    "# Get the input dimension from the training dataset\n",
    "input_dim =  len(train_dataset[0])\n",
    "# Get the latent dimension from the input dimension\n",
    "latent_dim = input_dim // 2\n",
    "model = VAE(input_dim, latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim: 127 and latent_dim: 63\n",
      "model: VAE(\n",
      "  (encoder): Encoder(\n",
      "    (fc1): Linear(in_features=127, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc_mean): Linear(in_features=128, out_features=63, bias=True)\n",
      "    (fc_logvar): Linear(in_features=128, out_features=63, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc1): Linear(in_features=63, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=127, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch [1/10], Avg. Total Loss: nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Avg. Total Loss: nan\n",
      "Epoch [3/10], Avg. Total Loss: nan\n",
      "Epoch [4/10], Avg. Total Loss: nan\n",
      "Epoch [5/10], Avg. Total Loss: nan\n",
      "Epoch [6/10], Avg. Total Loss: nan\n",
      "Epoch [7/10], Avg. Total Loss: nan\n",
      "Epoch [8/10], Avg. Total Loss: nan\n",
      "Epoch [9/10], Avg. Total Loss: nan\n",
      "Epoch [10/10], Avg. Total Loss: nan\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "print(f\"input_dim: {input_dim} and latent_dim: {latent_dim}\")\n",
    "print(f\"model: {model}\")\n",
    "\n",
    "# train model\n",
    "train(model, train_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Supplier Data:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[257], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated Supplier Data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[257], line 4\u001b[0m, in \u001b[0;36mgenerate_sample\u001b[1;34m(model, latent_dim)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_sample\u001b[39m(model, latent_dim):\n\u001b[0;32m      3\u001b[0m     sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, latent_dim)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(sample)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32md:\\afouq\\documents\\Unity\\POC_GAN_Test_Data_Generation\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "# Generate a new sample\n",
    "def generate_sample(model, latent_dim):\n",
    "    sample = torch.randn(1, latent_dim)\n",
    "    return model.decode(sample).detach().numpy()\n",
    "\n",
    "# Print or use the generated supplier data as needed\n",
    "print(\"Generated Supplier Data:\")\n",
    "for i in range(5):\n",
    "    print(generate_sample(model, latent_dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
