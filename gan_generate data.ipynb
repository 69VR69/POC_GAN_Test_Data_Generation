{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting & vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for PyTorch\n",
    "class SupplierDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Extract features\n",
    "        labels = ['id', 'status']\n",
    "        features = sample[labels].values # ignore name and address for now\n",
    "\n",
    "        # Apply transformations (e.g., convert strings/categories to numerical values)\n",
    "        if self.transform:\n",
    "            features = self.transform(features, labels).astype(np.float32)\n",
    "        # Convert to PyTorch tensor\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "        return features, 0 # 0 is a dummy label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_date_column(dataset, column):\n",
    "    # check in the column rows if there is a date\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][column] != 0:\n",
    "            return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_columns_types(dataset):\n",
    "    # Separate the text columns from the other columns\n",
    "    text_columns = []\n",
    "    date_columns = []\n",
    "    categorical_columns = []\n",
    "    numerical_columns = []\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].dtype == 'object':\n",
    "            text_columns.append(column)\n",
    "        elif dataset[column].dtype == 'datetime64[ns]':\n",
    "            date_columns.append(column)\n",
    "        elif dataset[column].dtype == 'category':\n",
    "            categorical_columns.append(column)\n",
    "        else:\n",
    "            numerical_columns.append(column)\n",
    "            \n",
    "    return dataset[text_columns], dataset[date_columns], dataset[categorical_columns], dataset[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical(data, labels):\n",
    "    # Convert categorical variables to numerical values (you can use more advanced encoding methods)\n",
    "    status_mapping = {'draft': 0, 'val': 1, 'other': 2}\n",
    "    status_index = labels.index('status')\n",
    "    data[status_index] = status_mapping[data[status_index]]\n",
    "\n",
    "    # You can implement similar transformations for other categorical variables\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv dataset from the csv file\n",
    "dataset_path = 'datasets/fake_invoice_1000.csv'\n",
    "# Creating a DataFrame from the CSV data (replace this with your actual CSV file path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Process the dataset before creating the SupplierDataset\n",
    "text_columns, date_columns, categorical_columns, numerical_columns = separate_columns_types(df)\n",
    "print(\"Text columns:\\n\", text_columns.head())\n",
    "print(\"Date columns:\\n\", date_columns.head())\n",
    "print(\"Categorical columns:\\n\", categorical_columns.head())\n",
    "print(\"Numerical columns:\\n\", numerical_columns.head())\n",
    "\n",
    "# Create an instance of the SupplierDataset with the specified transformations\n",
    "supplier_dataset = SupplierDataset(dataframe=df, transform=transform_categorical)\n",
    "\n",
    "for i in range(5):\n",
    "    sample = supplier_dataset[i]\n",
    "    print(f\"supplier Sample {i + 1}:\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.2\n",
    "batch_size = 2\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(supplier_dataset))\n",
    "val_size = len(supplier_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(supplier_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2_mean = nn.Linear(512, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(512, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, 512)\n",
    "        self.fc4 = nn.Linear(512, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoder forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Reparameterization trick for sampling from a normal distribution\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Decoder forward pass\n",
    "        z = F.relu(self.fc3(z))\n",
    "        recon_x = torch.sigmoid(self.fc4(z))  # Assuming input features are normalized between 0 and 1\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Full forward pass of the VAE\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_output, input_tensor, mu, log_var):\n",
    "    print(\"reconstructed_output\", reconstructed_output)\n",
    "    print(\"input_tensor\", input_tensor)\n",
    "    BCE = nn.functional.mse_loss(reconstructed_output, input_tensor, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, epochs, device, x_dim=-1):\n",
    "    model.train()\n",
    "    startTotal = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\tEpisode\", epoch + 1, \"/\", epochs)\n",
    "        overall_loss = 0\n",
    "        start = time.time()\n",
    "        for batch_idx, (input_tensor, _) in enumerate(train_dataset):\n",
    "            print(\"\\t\\tBatch\", batch_idx + 1, \"/\", len(train_dataset))\n",
    "            input_tensor = input_tensor.view(batch_size, x_dim).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            reconstructed_output, mean, log_var = model(input_tensor)\n",
    "            loss = loss_function(reconstructed_output, input_tensor, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end = time.time()\n",
    "        print(\"\\tEpisode Result\", \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size), \"\\tStep Time: \", end - start, \"s\", \"\\tTotal Time: \", end - startTotal, \"s\",\"\\n\\n\")\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and optimizer\n",
    "input_dim = 2 # corresponds to the number of features in the dataset\n",
    "latent_dim = 2 # corresponds to the number of latent variables\n",
    "model = VAE(input_dim, latent_dim).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train(model, optimizer, train_loader, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new sample\n",
    "def generate_sample(model, latent_dim):\n",
    "    sample = torch.randn(1, latent_dim)\n",
    "    return model.decode(sample).detach().numpy()\n",
    "\n",
    "# Print or use the generated supplier data as needed\n",
    "print(\"Generated Supplier Data:\")\n",
    "for i in range(5):\n",
    "    print(generate_sample(model, latent_dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
