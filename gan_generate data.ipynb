{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting & vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for PyTorch\n",
    "class SupplierDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Extract features\n",
    "        labels = ['id', 'status']\n",
    "        features = sample[labels].values # ignore name and address for now\n",
    "\n",
    "        # Apply transformations (e.g., convert strings/categories to numerical values)\n",
    "        if self.transform:\n",
    "            features = self.transform(features, labels).astype(np.float32)\n",
    "        # Convert to PyTorch tensor\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "        return features, 0 # 0 is a dummy label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical(data, labels):\n",
    "    # Convert categorical variables to numerical values (you can use more advanced encoding methods)\n",
    "    status_mapping = {'draft': 0, 'val': 1, 'other': 2}\n",
    "    status_index = labels.index('status')\n",
    "    data[status_index] = status_mapping[data[status_index]]\n",
    "\n",
    "    # You can implement similar transformations for other categorical variables\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supplier Sample 1: (tensor([1., 0.]), 0)\n",
      "supplier Sample 2: (tensor([2., 1.]), 0)\n",
      "supplier Sample 3: (tensor([3., 1.]), 0)\n",
      "supplier Sample 4: (tensor([4., 0.]), 0)\n",
      "supplier Sample 5: (tensor([5., 1.]), 0)\n"
     ]
    }
   ],
   "source": [
    "# Load the csv dataset from the csv file\n",
    "dataset_path = 'datasets/fake_supplier_3.csv'\n",
    "# Creating a DataFrame from the CSV data (replace this with your actual CSV file path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "# Create an instance of the SupplierDataset with the specified transformations\n",
    "supplier_dataset = SupplierDataset(dataframe=df, transform=transform_categorical)\n",
    "\n",
    "for i in range(5):\n",
    "    sample = supplier_dataset[i]\n",
    "    print(f\"supplier Sample {i + 1}:\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.2\n",
    "batch_size = 2\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(supplier_dataset))\n",
    "val_size = len(supplier_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(supplier_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2_mean = nn.Linear(512, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(512, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, 512)\n",
    "        self.fc4 = nn.Linear(512, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoder forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Reparameterization trick for sampling from a normal distribution\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Decoder forward pass\n",
    "        z = F.relu(self.fc3(z))\n",
    "        recon_x = torch.sigmoid(self.fc4(z))  # Assuming input features are normalized between 0 and 1\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Full forward pass of the VAE\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_output, input_tensor, mu, log_var):\n",
    "    print(\"reconstructed_output\", reconstructed_output)\n",
    "    print(\"input_tensor\", input_tensor)\n",
    "    BCE = nn.functional.mse_loss(reconstructed_output, input_tensor, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, epochs, device, x_dim=-1):\n",
    "    model.train()\n",
    "    startTotal = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\tEpisode\", epoch + 1, \"/\", epochs)\n",
    "        overall_loss = 0\n",
    "        start = time.time()\n",
    "        for batch_idx, (input_tensor, _) in enumerate(train_dataset):\n",
    "            print(\"\\t\\tBatch\", batch_idx + 1, \"/\", len(train_dataset))\n",
    "            input_tensor = input_tensor.view(batch_size, x_dim).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            reconstructed_output, mean, log_var = model(input_tensor)\n",
    "            loss = loss_function(reconstructed_output, input_tensor, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end = time.time()\n",
    "        print(\"\\tEpisode Result\", \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size), \"\\tStep Time: \", end - start, \"s\", \"\\tTotal Time: \", end - startTotal, \"s\",\"\\n\\n\")\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and optimizer\n",
    "input_dim = 2 # corresponds to the number of features in the dataset\n",
    "latent_dim = 2 # corresponds to the number of latent variables\n",
    "model = VAE(input_dim, latent_dim).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpisode 1 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.7389, 0.4600],\n",
      "        [0.7257, 0.4972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.5143, 0.8420],\n",
      "        [0.4013, 0.8643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  38.782981872558594 \tStep Time:  0.010970592498779297 s \tTotal Time:  0.010970592498779297 s \n",
      "\n",
      "\n",
      "\tEpisode 2 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.8194, 0.5663],\n",
      "        [0.8748, 0.5585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.9976],\n",
      "        [0.9960, 0.9833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  27.384371757507324 \tStep Time:  0.008971214294433594 s \tTotal Time:  0.02097344398498535 s \n",
      "\n",
      "\n",
      "\tEpisode 3 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9988],\n",
      "        [0.9994, 0.9968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9990],\n",
      "        [0.9963, 0.9824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  25.762462615966797 \tStep Time:  0.007947683334350586 s \tTotal Time:  0.028921127319335938 s \n",
      "\n",
      "\n",
      "\tEpisode 4 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9825, 0.9085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4981],\n",
      "        [0.9924, 0.8130]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  25.255722999572754 \tStep Time:  0.007978439331054688 s \tTotal Time:  0.036899566650390625 s \n",
      "\n",
      "\n",
      "\tEpisode 5 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.7632],\n",
      "        [1.0000, 0.5383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.8688],\n",
      "        [0.9938, 0.9035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  27.770183086395264 \tStep Time:  0.008978128433227539 s \tTotal Time:  0.045877695083618164 s \n",
      "\n",
      "\n",
      "\tEpisode 6 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9971, 0.9296],\n",
      "        [0.9996, 0.9222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.9966],\n",
      "        [0.9970, 0.9828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  24.44252920150757 \tStep Time:  0.010968208312988281 s \tTotal Time:  0.056845903396606445 s \n",
      "\n",
      "\n",
      "\tEpisode 7 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9984],\n",
      "        [0.9989, 0.9903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9999, 0.9988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  25.049026012420654 \tStep Time:  0.010970592498779297 s \tTotal Time:  0.06781649589538574 s \n",
      "\n",
      "\n",
      "\tEpisode 8 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [0.9998, 0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9989, 0.9871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  22.46608829498291 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.0747978687286377 s \n",
      "\n",
      "\n",
      "\tEpisode 9 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [0.9989, 0.9816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9978],\n",
      "        [1.0000, 0.9992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  21.381856441497803 \tStep Time:  0.006981849670410156 s \tTotal Time:  0.08177971839904785 s \n",
      "\n",
      "\n",
      "\tEpisode 10 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9970],\n",
      "        [1.0000, 0.9989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  22.84988045692444 \tStep Time:  0.007978677749633789 s \tTotal Time:  0.08975839614868164 s \n",
      "\n",
      "\n",
      "\tEpisode 11 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9989],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.816932678222656 \tStep Time:  0.008975744247436523 s \tTotal Time:  0.09873414039611816 s \n",
      "\n",
      "\n",
      "\tEpisode 12 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.873462200164795 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.10571575164794922 s \n",
      "\n",
      "\n",
      "\tEpisode 13 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.9900],\n",
      "        [0.9998, 0.9981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  21.38160800933838 \tStep Time:  0.006982326507568359 s \tTotal Time:  0.11269807815551758 s \n",
      "\n",
      "\n",
      "\tEpisode 14 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.9917],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9995, 0.9930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.86265993118286 \tStep Time:  0.00797724723815918 s \tTotal Time:  0.12067532539367676 s \n",
      "\n",
      "\n",
      "\tEpisode 15 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.9937],\n",
      "        [1.0000, 0.9982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.906643867492676 \tStep Time:  0.007981300354003906 s \tTotal Time:  0.12865662574768066 s \n",
      "\n",
      "\n",
      "\tEpisode 16 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9992],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9965],\n",
      "        [0.9998, 0.9980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.71253252029419 \tStep Time:  0.0059833526611328125 s \tTotal Time:  0.1356348991394043 s \n",
      "\n",
      "\n",
      "\tEpisode 17 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9986],\n",
      "        [0.9999, 0.9970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [0.9995, 0.9942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.273805141448975 \tStep Time:  0.0060193538665771484 s \tTotal Time:  0.14165425300598145 s \n",
      "\n",
      "\n",
      "\tEpisode 18 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9998, 0.9979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.439982891082764 \tStep Time:  0.0069468021392822266 s \tTotal Time:  0.14860105514526367 s \n",
      "\n",
      "\n",
      "\tEpisode 19 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 0.9982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9973],\n",
      "        [0.9996, 0.9932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.195645809173584 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.15558218955993652 s \n",
      "\n",
      "\n",
      "\tEpisode 20 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9975],\n",
      "        [0.9999, 0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [0.9999, 0.9969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.10716199874878 \tStep Time:  0.007978439331054688 s \tTotal Time:  0.1635606288909912 s \n",
      "\n",
      "\n",
      "\tEpisode 21 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9987],\n",
      "        [0.9999, 0.9975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.16064739227295 \tStep Time:  0.006981849670410156 s \tTotal Time:  0.17054247856140137 s \n",
      "\n",
      "\n",
      "\tEpisode 22 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 0.9982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.165956258773804 \tStep Time:  0.006982088088989258 s \tTotal Time:  0.17752456665039062 s \n",
      "\n",
      "\n",
      "\tEpisode 23 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.9957],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9976],\n",
      "        [0.9999, 0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.096345901489258 \tStep Time:  0.007977724075317383 s \tTotal Time:  0.185502290725708 s \n",
      "\n",
      "\n",
      "\tEpisode 24 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9970],\n",
      "        [0.9998, 0.9939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.082946300506592 \tStep Time:  0.007979393005371094 s \tTotal Time:  0.1934816837310791 s \n",
      "\n",
      "\n",
      "\tEpisode 25 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.9928],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.078553438186646 \tStep Time:  0.007977485656738281 s \tTotal Time:  0.20145916938781738 s \n",
      "\n",
      "\n",
      "\tEpisode 26 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.046342849731445 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.20844054222106934 s \n",
      "\n",
      "\n",
      "\tEpisode 27 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9954],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.9930],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.080190658569336 \tStep Time:  0.006982088088989258 s \tTotal Time:  0.2154226303100586 s \n",
      "\n",
      "\n",
      "\tEpisode 28 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9985],\n",
      "        [1.0000, 0.9969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9981],\n",
      "        [0.9999, 0.9988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.042644500732422 \tStep Time:  0.00698089599609375 s \tTotal Time:  0.22240352630615234 s \n",
      "\n",
      "\n",
      "\tEpisode 29 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9989],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.033721923828125 \tStep Time:  0.007977485656738281 s \tTotal Time:  0.23038101196289062 s \n",
      "\n",
      "\n",
      "\tEpisode 30 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.9937],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.043112754821777 \tStep Time:  0.006982326507568359 s \tTotal Time:  0.23736333847045898 s \n",
      "\n",
      "\n",
      "\tEpisode 31 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.9921],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9940],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.015415906906128 \tStep Time:  0.005982637405395508 s \tTotal Time:  0.24434471130371094 s \n",
      "\n",
      "\n",
      "\tEpisode 32 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9983],\n",
      "        [0.9998, 0.9920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00986385345459 \tStep Time:  0.007978439331054688 s \tTotal Time:  0.2523231506347656 s \n",
      "\n",
      "\n",
      "\tEpisode 33 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9920],\n",
      "        [0.9999, 0.9976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9991],\n",
      "        [1.0000, 0.9989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.025951385498047 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.2593045234680176 s \n",
      "\n",
      "\n",
      "\tEpisode 34 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9993],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.016689777374268 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.26628565788269043 s \n",
      "\n",
      "\n",
      "\tEpisode 35 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9933],\n",
      "        [0.9999, 0.9968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.003552436828613 \tStep Time:  0.00797891616821289 s \tTotal Time:  0.2742645740509033 s \n",
      "\n",
      "\n",
      "\tEpisode 36 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9981],\n",
      "        [0.9999, 0.9955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.01196527481079 \tStep Time:  0.007978677749633789 s \tTotal Time:  0.2822432518005371 s \n",
      "\n",
      "\n",
      "\tEpisode 37 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.9902],\n",
      "        [0.9999, 0.9922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9981],\n",
      "        [0.9998, 0.9916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.006564617156982 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.28922486305236816 s \n",
      "\n",
      "\n",
      "\tEpisode 38 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9958],\n",
      "        [1.0000, 0.9992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.006859302520752 \tStep Time:  0.008976936340332031 s \tTotal Time:  0.2982017993927002 s \n",
      "\n",
      "\n",
      "\tEpisode 39 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9966],\n",
      "        [1.0000, 0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.003026008605957 \tStep Time:  0.006980419158935547 s \tTotal Time:  0.30518221855163574 s \n",
      "\n",
      "\n",
      "\tEpisode 40 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.9887],\n",
      "        [0.9996, 0.9915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9989],\n",
      "        [0.9998, 0.9930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.997512340545654 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.31116628646850586 s \n",
      "\n",
      "\n",
      "\tEpisode 41 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9997, 0.9925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00394868850708 \tStep Time:  0.006980419158935547 s \tTotal Time:  0.3181467056274414 s \n",
      "\n",
      "\n",
      "\tEpisode 42 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9930],\n",
      "        [0.9997, 0.9913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9998, 0.9947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.994354486465454 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.3261270523071289 s \n",
      "\n",
      "\n",
      "\tEpisode 43 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9993],\n",
      "        [0.9996, 0.9858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9892],\n",
      "        [1.0000, 0.9963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.990211725234985 \tStep Time:  0.010970354080200195 s \tTotal Time:  0.3370974063873291 s \n",
      "\n",
      "\n",
      "\tEpisode 44 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9922],\n",
      "        [1.0000, 0.9980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00180435180664 \tStep Time:  0.006979703903198242 s \tTotal Time:  0.34407711029052734 s \n",
      "\n",
      "\n",
      "\tEpisode 45 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9917],\n",
      "        [1.0000, 0.9858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9921],\n",
      "        [0.9997, 0.9793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99622631072998 \tStep Time:  0.00797891616821289 s \tTotal Time:  0.35205602645874023 s \n",
      "\n",
      "\n",
      "\tEpisode 46 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9791],\n",
      "        [0.9999, 0.9948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9987],\n",
      "        [1.0000, 0.9975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999728679656982 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.3590376377105713 s \n",
      "\n",
      "\n",
      "\tEpisode 47 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9965],\n",
      "        [1.0000, 0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9916],\n",
      "        [1.0000, 0.9975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99881649017334 \tStep Time:  0.012965679168701172 s \tTotal Time:  0.37200331687927246 s \n",
      "\n",
      "\n",
      "\tEpisode 48 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9898],\n",
      "        [1.0000, 0.9841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9756],\n",
      "        [1.0000, 0.9963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.978386640548706 \tStep Time:  0.013962507247924805 s \tTotal Time:  0.38596582412719727 s \n",
      "\n",
      "\n",
      "\tEpisode 49 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9972],\n",
      "        [1.0000, 0.9728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00071144104004 \tStep Time:  0.013963937759399414 s \tTotal Time:  0.4009275436401367 s \n",
      "\n",
      "\n",
      "\tEpisode 50 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9991],\n",
      "        [1.0000, 0.9971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9839],\n",
      "        [1.0000, 0.9659]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.969037771224976 \tStep Time:  0.00897359848022461 s \tTotal Time:  0.40990114212036133 s \n",
      "\n",
      "\n",
      "\tEpisode 51 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9880],\n",
      "        [1.0000, 0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9961],\n",
      "        [0.9999, 0.9687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99010443687439 \tStep Time:  0.007981538772583008 s \tTotal Time:  0.41788268089294434 s \n",
      "\n",
      "\n",
      "\tEpisode 52 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9957],\n",
      "        [1.0000, 0.9746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9991],\n",
      "        [0.9999, 0.9752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.998855113983154 \tStep Time:  0.008974790573120117 s \tTotal Time:  0.42685747146606445 s \n",
      "\n",
      "\n",
      "\tEpisode 53 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9932],\n",
      "        [0.9998, 0.9344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9659],\n",
      "        [1.0000, 0.9903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.997349739074707 \tStep Time:  0.011967897415161133 s \tTotal Time:  0.4388253688812256 s \n",
      "\n",
      "\n",
      "\tEpisode 54 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8848],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0076265335083 \tStep Time:  0.006983280181884766 s \tTotal Time:  0.44580864906311035 s \n",
      "\n",
      "\n",
      "\tEpisode 55 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9195],\n",
      "        [0.9998, 0.9652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9934],\n",
      "        [1.0000, 0.9347]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.97309160232544 \tStep Time:  0.006979703903198242 s \tTotal Time:  0.4527883529663086 s \n",
      "\n",
      "\n",
      "\tEpisode 56 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8904],\n",
      "        [1.0000, 0.8684]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9871],\n",
      "        [1.0000, 0.9880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.91224956512451 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.45976948738098145 s \n",
      "\n",
      "\n",
      "\tEpisode 57 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9698],\n",
      "        [1.0000, 0.9823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7547],\n",
      "        [1.0000, 0.7940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.854380130767822 \tStep Time:  0.007978677749633789 s \tTotal Time:  0.46774816513061523 s \n",
      "\n",
      "\n",
      "\tEpisode 58 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8588],\n",
      "        [1.0000, 0.9661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8205],\n",
      "        [1.0000, 0.9449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.9750816822052 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.47373223304748535 s \n",
      "\n",
      "\n",
      "\tEpisode 59 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.8522],\n",
      "        [0.9999, 0.8096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4491],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.64357280731201 \tStep Time:  0.007979154586791992 s \tTotal Time:  0.48171138763427734 s \n",
      "\n",
      "\n",
      "\tEpisode 60 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6995],\n",
      "        [1.0000, 0.9691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9947],\n",
      "        [1.0000, 0.9970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.073721408843994 \tStep Time:  0.005995750427246094 s \tTotal Time:  0.48770713806152344 s \n",
      "\n",
      "\n",
      "\tEpisode 61 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5792],\n",
      "        [1.0000, 0.8812]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9992],\n",
      "        [1.0000, 0.9933]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.70586371421814 \tStep Time:  0.0069696903228759766 s \tTotal Time:  0.4946768283843994 s \n",
      "\n",
      "\n",
      "\tEpisode 62 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1423],\n",
      "        [1.0000, 0.0113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000e+00, 2.8452e-04],\n",
      "        [9.9999e-01, 2.3220e-01]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.941678285598755 \tStep Time:  0.0069806575775146484 s \tTotal Time:  0.5016574859619141 s \n",
      "\n",
      "\n",
      "\tEpisode 63 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.8302],\n",
      "        [0.9998, 0.7552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0102],\n",
      "        [1.0000, 0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.58143901824951 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.5076417922973633 s \n",
      "\n",
      "\n",
      "\tEpisode 64 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0238],\n",
      "        [0.9999, 0.7491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4173],\n",
      "        [1.0000, 0.9963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.562118768692017 \tStep Time:  0.007978439331054688 s \tTotal Time:  0.515620231628418 s \n",
      "\n",
      "\n",
      "\tEpisode 65 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9216],\n",
      "        [0.9998, 0.5599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4167],\n",
      "        [1.0000, 0.0477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.926546812057495 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.5216042995452881 s \n",
      "\n",
      "\n",
      "\tEpisode 66 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0273],\n",
      "        [1.0000, 0.9980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.790382862091064 \tStep Time:  0.006983041763305664 s \tTotal Time:  0.5285873413085938 s \n",
      "\n",
      "\n",
      "\tEpisode 67 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9280],\n",
      "        [1.0000, 0.9952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0196],\n",
      "        [1.0000, 0.9991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.69761610031128 \tStep Time:  0.007977008819580078 s \tTotal Time:  0.5365643501281738 s \n",
      "\n",
      "\n",
      "\tEpisode 68 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9991],\n",
      "        [1.0000, 0.6896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.8552],\n",
      "        [0.9998, 0.9167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.167898178100586 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.5425481796264648 s \n",
      "\n",
      "\n",
      "\tEpisode 69 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9385],\n",
      "        [1.0000, 0.0297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0785],\n",
      "        [1.0000, 0.0069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  21.149385929107666 \tStep Time:  0.009000539779663086 s \tTotal Time:  0.5515487194061279 s \n",
      "\n",
      "\n",
      "\tEpisode 70 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2905],\n",
      "        [1.0000, 0.9950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.8981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  21.587157726287842 \tStep Time:  0.005002260208129883 s \tTotal Time:  0.5565509796142578 s \n",
      "\n",
      "\n",
      "\tEpisode 71 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2871],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  21.776944398880005 \tStep Time:  0.006940126419067383 s \tTotal Time:  0.5634911060333252 s \n",
      "\n",
      "\n",
      "\tEpisode 72 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9937],\n",
      "        [0.9999, 0.7791]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0147],\n",
      "        [1.0000, 0.0057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.398160934448242 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.5704724788665771 s \n",
      "\n",
      "\n",
      "\tEpisode 73 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.0311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.114052772521973 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.5764567852020264 s \n",
      "\n",
      "\n",
      "\tEpisode 74 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.9986],\n",
      "        [1.0000, 0.0035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9829],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.827430725097656 \tStep Time:  0.007012367248535156 s \tTotal Time:  0.5834691524505615 s \n",
      "\n",
      "\n",
      "\tEpisode 75 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9929],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.4163556098938 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.5894529819488525 s \n",
      "\n",
      "\n",
      "\tEpisode 76 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9986],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9900],\n",
      "        [1.0000, 0.9671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.321095943450928 \tStep Time:  0.0059549808502197266 s \tTotal Time:  0.5954079627990723 s \n",
      "\n",
      "\n",
      "\tEpisode 77 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9837],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9768],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.240500926971436 \tStep Time:  0.006978511810302734 s \tTotal Time:  0.602386474609375 s \n",
      "\n",
      "\n",
      "\tEpisode 78 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9552],\n",
      "        [1.0000, 0.7093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.198314666748047 \tStep Time:  0.0060160160064697266 s \tTotal Time:  0.6084024906158447 s \n",
      "\n",
      "\n",
      "\tEpisode 79 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.20046591758728 \tStep Time:  0.007978200912475586 s \tTotal Time:  0.6163806915283203 s \n",
      "\n",
      "\n",
      "\tEpisode 80 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.13893747329712 \tStep Time:  0.005985260009765625 s \tTotal Time:  0.6223659515380859 s \n",
      "\n",
      "\n",
      "\tEpisode 81 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8563],\n",
      "        [1.0000, 0.9899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.113162994384766 \tStep Time:  0.006949901580810547 s \tTotal Time:  0.6293158531188965 s \n",
      "\n",
      "\n",
      "\tEpisode 82 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5270],\n",
      "        [1.0000, 0.9794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.18297553062439 \tStep Time:  0.0069806575775146484 s \tTotal Time:  0.6362965106964111 s \n",
      "\n",
      "\n",
      "\tEpisode 83 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.764352560043335 \tStep Time:  0.00600743293762207 s \tTotal Time:  0.6423039436340332 s \n",
      "\n",
      "\n",
      "\tEpisode 84 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9999],\n",
      "        [1.0000, 0.9981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.15959930419922 \tStep Time:  0.007955312728881836 s \tTotal Time:  0.650259256362915 s \n",
      "\n",
      "\n",
      "\tEpisode 85 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9419],\n",
      "        [1.0000, 0.9972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.293283939361572 \tStep Time:  0.005983591079711914 s \tTotal Time:  0.656242847442627 s \n",
      "\n",
      "\n",
      "\tEpisode 86 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [0.9999, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.30956530570984 \tStep Time:  0.0069806575775146484 s \tTotal Time:  0.6632235050201416 s \n",
      "\n",
      "\n",
      "\tEpisode 87 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.079218864440918 \tStep Time:  0.006982088088989258 s \tTotal Time:  0.6702055931091309 s \n",
      "\n",
      "\n",
      "\tEpisode 88 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.380407333374023 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.676189661026001 s \n",
      "\n",
      "\n",
      "\tEpisode 89 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.104002475738525 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.683171272277832 s \n",
      "\n",
      "\n",
      "\tEpisode 90 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.13413906097412 \tStep Time:  0.005983591079711914 s \tTotal Time:  0.689154863357544 s \n",
      "\n",
      "\n",
      "\tEpisode 91 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.168804168701172 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.6961367130279541 s \n",
      "\n",
      "\n",
      "\tEpisode 92 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.9999],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.100557804107666 \tStep Time:  0.006981849670410156 s \tTotal Time:  0.7031185626983643 s \n",
      "\n",
      "\n",
      "\tEpisode 93 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.089898109436035 \tStep Time:  0.0059833526611328125 s \tTotal Time:  0.7091019153594971 s \n",
      "\n",
      "\n",
      "\tEpisode 94 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.040973663330078 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.7170803546905518 s \n",
      "\n",
      "\n",
      "\tEpisode 95 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.094029903411865 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.7230644226074219 s \n",
      "\n",
      "\n",
      "\tEpisode 96 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.04392671585083 \tStep Time:  0.006982564926147461 s \tTotal Time:  0.7300469875335693 s \n",
      "\n",
      "\n",
      "\tEpisode 97 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.054746627807617 \tStep Time:  0.006979703903198242 s \tTotal Time:  0.7370266914367676 s \n",
      "\n",
      "\n",
      "\tEpisode 98 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.053115367889404 \tStep Time:  0.0060155391693115234 s \tTotal Time:  0.7430422306060791 s \n",
      "\n",
      "\n",
      "\tEpisode 99 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.023892879486084 \tStep Time:  0.0069501399993896484 s \tTotal Time:  0.7499923706054688 s \n",
      "\n",
      "\n",
      "\tEpisode 100 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.035110473632812 \tStep Time:  0.00767970085144043 s \tTotal Time:  0.7576720714569092 s \n",
      "\n",
      "\n",
      "\tEpisode 101 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.012224674224854 \tStep Time:  0.005950927734375 s \tTotal Time:  0.7636229991912842 s \n",
      "\n",
      "\n",
      "\tEpisode 102 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.02963948249817 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.770604133605957 s \n",
      "\n",
      "\n",
      "\tEpisode 103 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.002479553222656 \tStep Time:  0.006011962890625 s \tTotal Time:  0.776616096496582 s \n",
      "\n",
      "\n",
      "\tEpisode 104 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.010475158691406 \tStep Time:  0.0070133209228515625 s \tTotal Time:  0.784599781036377 s \n",
      "\n",
      "\n",
      "\tEpisode 105 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.010658740997314 \tStep Time:  0.005982160568237305 s \tTotal Time:  0.7905819416046143 s \n",
      "\n",
      "\n",
      "\tEpisode 106 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.001328945159912 \tStep Time:  0.006978511810302734 s \tTotal Time:  0.797560453414917 s \n",
      "\n",
      "\n",
      "\tEpisode 107 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.009458541870117 \tStep Time:  0.006952524185180664 s \tTotal Time:  0.8045129776000977 s \n",
      "\n",
      "\n",
      "\tEpisode 108 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00147771835327 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.8104970455169678 s \n",
      "\n",
      "\n",
      "\tEpisode 109 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.003231525421143 \tStep Time:  0.007979631423950195 s \tTotal Time:  0.818476676940918 s \n",
      "\n",
      "\n",
      "\tEpisode 110 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.002914428710938 \tStep Time:  0.0059833526611328125 s \tTotal Time:  0.8244600296020508 s \n",
      "\n",
      "\n",
      "\tEpisode 111 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.002118587493896 \tStep Time:  0.007979393005371094 s \tTotal Time:  0.8324394226074219 s \n",
      "\n",
      "\n",
      "\tEpisode 112 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.001633167266846 \tStep Time:  0.0069806575775146484 s \tTotal Time:  0.8394200801849365 s \n",
      "\n",
      "\n",
      "\tEpisode 113 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000723600387573 \tStep Time:  0.006983518600463867 s \tTotal Time:  0.8464035987854004 s \n",
      "\n",
      "\n",
      "\tEpisode 114 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.001667022705078 \tStep Time:  0.006979227066040039 s \tTotal Time:  0.8533828258514404 s \n",
      "\n",
      "\n",
      "\tEpisode 115 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000169277191162 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.8603639602661133 s \n",
      "\n",
      "\n",
      "\tEpisode 116 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00089454650879 \tStep Time:  0.007014036178588867 s \tTotal Time:  0.8673779964447021 s \n",
      "\n",
      "\n",
      "\tEpisode 117 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000329971313477 \tStep Time:  0.006980180740356445 s \tTotal Time:  0.8743581771850586 s \n",
      "\n",
      "\n",
      "\tEpisode 118 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000494480133057 \tStep Time:  0.007994890213012695 s \tTotal Time:  0.8823530673980713 s \n",
      "\n",
      "\n",
      "\tEpisode 119 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00079655647278 \tStep Time:  0.005969524383544922 s \tTotal Time:  0.8883225917816162 s \n",
      "\n",
      "\n",
      "\tEpisode 120 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00054931640625 \tStep Time:  0.0069844722747802734 s \tTotal Time:  0.8953070640563965 s \n",
      "\n",
      "\n",
      "\tEpisode 121 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000374794006348 \tStep Time:  0.00697779655456543 s \tTotal Time:  0.9022848606109619 s \n",
      "\n",
      "\n",
      "\tEpisode 122 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00032949447632 \tStep Time:  0.006982326507568359 s \tTotal Time:  0.9092671871185303 s \n",
      "\n",
      "\n",
      "\tEpisode 123 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000380516052246 \tStep Time:  0.007978677749633789 s \tTotal Time:  0.9172458648681641 s \n",
      "\n",
      "\n",
      "\tEpisode 124 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000186681747437 \tStep Time:  0.005982398986816406 s \tTotal Time:  0.9232282638549805 s \n",
      "\n",
      "\n",
      "\tEpisode 125 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000568628311157 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.9302098751068115 s \n",
      "\n",
      "\n",
      "\tEpisode 126 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000614643096924 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.9361939430236816 s \n",
      "\n",
      "\n",
      "\tEpisode 127 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000430583953857 \tStep Time:  0.005982875823974609 s \tTotal Time:  0.9421768188476562 s \n",
      "\n",
      "\n",
      "\tEpisode 128 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9999, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00015354156494 \tStep Time:  0.006983280181884766 s \tTotal Time:  0.949160099029541 s \n",
      "\n",
      "\n",
      "\tEpisode 129 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00019407272339 \tStep Time:  0.006948232650756836 s \tTotal Time:  0.9561083316802979 s \n",
      "\n",
      "\n",
      "\tEpisode 130 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000622749328613 \tStep Time:  0.0060160160064697266 s \tTotal Time:  0.9621243476867676 s \n",
      "\n",
      "\n",
      "\tEpisode 131 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00040626525879 \tStep Time:  0.0069484710693359375 s \tTotal Time:  0.9690728187561035 s \n",
      "\n",
      "\n",
      "\tEpisode 132 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000003814697266 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.9750566482543945 s \n",
      "\n",
      "\n",
      "\tEpisode 133 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000048637390137 \tStep Time:  0.008011817932128906 s \tTotal Time:  0.9830684661865234 s \n",
      "\n",
      "\n",
      "\tEpisode 134 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000162839889526 \tStep Time:  0.005985260009765625 s \tTotal Time:  0.9890537261962891 s \n",
      "\n",
      "\n",
      "\tEpisode 135 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999722242355347 \tStep Time:  0.005950450897216797 s \tTotal Time:  0.9950041770935059 s \n",
      "\n",
      "\n",
      "\tEpisode 136 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00028371810913 \tStep Time:  0.007012844085693359 s \tTotal Time:  1.0020170211791992 s \n",
      "\n",
      "\n",
      "\tEpisode 137 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.9997501373291 \tStep Time:  0.005985260009765625 s \tTotal Time:  1.0080022811889648 s \n",
      "\n",
      "\n",
      "\tEpisode 138 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999999046325684 \tStep Time:  0.00698089599609375 s \tTotal Time:  1.0149831771850586 s \n",
      "\n",
      "\n",
      "\tEpisode 139 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000162601470947 \tStep Time:  0.006980180740356445 s \tTotal Time:  1.021963357925415 s \n",
      "\n",
      "\n",
      "\tEpisode 140 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000046253204346 \tStep Time:  0.005954265594482422 s \tTotal Time:  1.0279176235198975 s \n",
      "\n",
      "\n",
      "\tEpisode 141 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002670288086 \tStep Time:  0.008008241653442383 s \tTotal Time:  1.0359258651733398 s \n",
      "\n",
      "\n",
      "\tEpisode 142 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000006437301636 \tStep Time:  0.007956504821777344 s \tTotal Time:  1.0438823699951172 s \n",
      "\n",
      "\n",
      "\tEpisode 143 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000174522399902 \tStep Time:  0.007969856262207031 s \tTotal Time:  1.0518522262573242 s \n",
      "\n",
      "\n",
      "\tEpisode 144 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00046682357788 \tStep Time:  0.00698089599609375 s \tTotal Time:  1.058833122253418 s \n",
      "\n",
      "\n",
      "\tEpisode 145 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00011944770813 \tStep Time:  0.00698399543762207 s \tTotal Time:  1.06581711769104 s \n",
      "\n",
      "\n",
      "\tEpisode 146 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000025272369385 \tStep Time:  0.007010459899902344 s \tTotal Time:  1.0728275775909424 s \n",
      "\n",
      "\n",
      "\tEpisode 147 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99979019165039 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.0788118839263916 s \n",
      "\n",
      "\n",
      "\tEpisode 148 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999833822250366 \tStep Time:  0.00698089599609375 s \tTotal Time:  1.0857927799224854 s \n",
      "\n",
      "\n",
      "\tEpisode 149 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000108242034912 \tStep Time:  0.006983757019042969 s \tTotal Time:  1.0927765369415283 s \n",
      "\n",
      "\n",
      "\tEpisode 150 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000397443771362 \tStep Time:  0.006947994232177734 s \tTotal Time:  1.099724531173706 s \n",
      "\n",
      "\n",
      "\tEpisode 151 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005340576172 \tStep Time:  0.006021261215209961 s \tTotal Time:  1.105745792388916 s \n",
      "\n",
      "\n",
      "\tEpisode 152 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000096321105957 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.1127216815948486 s \n",
      "\n",
      "\n",
      "\tEpisode 153 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00006628036499 \tStep Time:  0.006982564926147461 s \tTotal Time:  1.119704246520996 s \n",
      "\n",
      "\n",
      "\tEpisode 154 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000410318374634 \tStep Time:  0.005983114242553711 s \tTotal Time:  1.1256873607635498 s \n",
      "\n",
      "\n",
      "\tEpisode 155 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00014352798462 \tStep Time:  0.007978677749633789 s \tTotal Time:  1.1336660385131836 s \n",
      "\n",
      "\n",
      "\tEpisode 156 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000092029571533 \tStep Time:  0.00598454475402832 s \tTotal Time:  1.139650583267212 s \n",
      "\n",
      "\n",
      "\tEpisode 157 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00030279159546 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.1456348896026611 s \n",
      "\n",
      "\n",
      "\tEpisode 158 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000113487243652 \tStep Time:  0.0069789886474609375 s \tTotal Time:  1.152613878250122 s \n",
      "\n",
      "\n",
      "\tEpisode 159 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999908924102783 \tStep Time:  0.005984783172607422 s \tTotal Time:  1.1585986614227295 s \n",
      "\n",
      "\n",
      "\tEpisode 160 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000089168548584 \tStep Time:  0.008944034576416016 s \tTotal Time:  1.1675426959991455 s \n",
      "\n",
      "\n",
      "\tEpisode 161 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002384185791 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.1735267639160156 s \n",
      "\n",
      "\n",
      "\tEpisode 162 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000227451324463 \tStep Time:  0.006982088088989258 s \tTotal Time:  1.1805088520050049 s \n",
      "\n",
      "\n",
      "\tEpisode 163 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00010895729065 \tStep Time:  0.006981611251831055 s \tTotal Time:  1.187490463256836 s \n",
      "\n",
      "\n",
      "\tEpisode 164 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99999189376831 \tStep Time:  0.006980180740356445 s \tTotal Time:  1.1944706439971924 s \n",
      "\n",
      "\n",
      "\tEpisode 165 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99991011619568 \tStep Time:  0.006981611251831055 s \tTotal Time:  1.2014522552490234 s \n",
      "\n",
      "\n",
      "\tEpisode 166 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000190258026123 \tStep Time:  0.00598454475402832 s \tTotal Time:  1.2074368000030518 s \n",
      "\n",
      "\n",
      "\tEpisode 167 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00001096725464 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.2144179344177246 s \n",
      "\n",
      "\n",
      "\tEpisode 168 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00037384033203 \tStep Time:  0.005983114242553711 s \tTotal Time:  1.2204010486602783 s \n",
      "\n",
      "\n",
      "\tEpisode 169 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00045418739319 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.227381706237793 s \n",
      "\n",
      "\n",
      "\tEpisode 170 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000216245651245 \tStep Time:  0.006981849670410156 s \tTotal Time:  1.2343635559082031 s \n",
      "\n",
      "\n",
      "\tEpisode 171 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00001287460327 \tStep Time:  0.006982564926147461 s \tTotal Time:  1.2413461208343506 s \n",
      "\n",
      "\n",
      "\tEpisode 172 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00024700164795 \tStep Time:  0.0059871673583984375 s \tTotal Time:  1.247333288192749 s \n",
      "\n",
      "\n",
      "\tEpisode 173 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00029993057251 \tStep Time:  0.007008075714111328 s \tTotal Time:  1.2543413639068604 s \n",
      "\n",
      "\n",
      "\tEpisode 174 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000118732452393 \tStep Time:  0.005986452102661133 s \tTotal Time:  1.2603278160095215 s \n",
      "\n",
      "\n",
      "\tEpisode 175 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000076293945312 \tStep Time:  0.006979942321777344 s \tTotal Time:  1.2673077583312988 s \n",
      "\n",
      "\n",
      "\tEpisode 176 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999756813049316 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.2732911109924316 s \n",
      "\n",
      "\n",
      "\tEpisode 177 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000248908996582 \tStep Time:  0.006982326507568359 s \tTotal Time:  1.2802734375 s \n",
      "\n",
      "\n",
      "\tEpisode 178 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00000238418579 \tStep Time:  0.006981849670410156 s \tTotal Time:  1.2872552871704102 s \n",
      "\n",
      "\n",
      "\tEpisode 179 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000115156173706 \tStep Time:  0.0049896240234375 s \tTotal Time:  1.2922449111938477 s \n",
      "\n",
      "\n",
      "\tEpisode 180 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000222206115723 \tStep Time:  0.006982564926147461 s \tTotal Time:  1.2992274761199951 s \n",
      "\n",
      "\n",
      "\tEpisode 181 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000053882598877 \tStep Time:  0.00597834587097168 s \tTotal Time:  1.3052058219909668 s \n",
      "\n",
      "\n",
      "\tEpisode 182 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999988794326782 \tStep Time:  0.005952358245849609 s \tTotal Time:  1.3111581802368164 s \n",
      "\n",
      "\n",
      "\tEpisode 183 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00009536743164 \tStep Time:  0.007013082504272461 s \tTotal Time:  1.3181712627410889 s \n",
      "\n",
      "\n",
      "\tEpisode 184 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00008726119995 \tStep Time:  0.005986213684082031 s \tTotal Time:  1.324157476425171 s \n",
      "\n",
      "\n",
      "\tEpisode 185 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999998092651367 \tStep Time:  0.0059854984283447266 s \tTotal Time:  1.3301429748535156 s \n",
      "\n",
      "\n",
      "\tEpisode 186 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000108003616333 \tStep Time:  0.005980253219604492 s \tTotal Time:  1.3361232280731201 s \n",
      "\n",
      "\n",
      "\tEpisode 187 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999656915664673 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.3421070575714111 s \n",
      "\n",
      "\n",
      "\tEpisode 188 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002098083496 \tStep Time:  0.007979869842529297 s \tTotal Time:  1.3500869274139404 s \n",
      "\n",
      "\n",
      "\tEpisode 189 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00003480911255 \tStep Time:  0.005951642990112305 s \tTotal Time:  1.3560385704040527 s \n",
      "\n",
      "\n",
      "\tEpisode 190 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000011205673218 \tStep Time:  0.006981611251831055 s \tTotal Time:  1.3630201816558838 s \n",
      "\n",
      "\n",
      "\tEpisode 191 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000089168548584 \tStep Time:  0.007977724075317383 s \tTotal Time:  1.3709979057312012 s \n",
      "\n",
      "\n",
      "\tEpisode 192 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000080108642578 \tStep Time:  0.0059850215911865234 s \tTotal Time:  1.3769829273223877 s \n",
      "\n",
      "\n",
      "\tEpisode 193 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000019550323486 \tStep Time:  0.007979154586791992 s \tTotal Time:  1.3849620819091797 s \n",
      "\n",
      "\n",
      "\tEpisode 194 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00016689300537 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.3909454345703125 s \n",
      "\n",
      "\n",
      "\tEpisode 195 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999895095825195 \tStep Time:  0.008012056350708008 s \tTotal Time:  1.3989574909210205 s \n",
      "\n",
      "\n",
      "\tEpisode 196 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000089168548584 \tStep Time:  0.0059816837310791016 s \tTotal Time:  1.4049391746520996 s \n",
      "\n",
      "\n",
      "\tEpisode 197 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00033712387085 \tStep Time:  0.005953311920166016 s \tTotal Time:  1.4108924865722656 s \n",
      "\n",
      "\n",
      "\tEpisode 198 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000053882598877 \tStep Time:  0.007013082504272461 s \tTotal Time:  1.417905569076538 s \n",
      "\n",
      "\n",
      "\tEpisode 199 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00009059906006 \tStep Time:  0.005982875823974609 s \tTotal Time:  1.4238884449005127 s \n",
      "\n",
      "\n",
      "\tEpisode 200 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000100135803223 \tStep Time:  0.00794672966003418 s \tTotal Time:  1.4328334331512451 s \n",
      "\n",
      "\n",
      "\tEpisode 201 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000128746032715 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.439814567565918 s \n",
      "\n",
      "\n",
      "\tEpisode 202 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0001437664032 \tStep Time:  0.006981611251831055 s \tTotal Time:  1.446796178817749 s \n",
      "\n",
      "\n",
      "\tEpisode 203 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000011682510376 \tStep Time:  0.007978200912475586 s \tTotal Time:  1.4547743797302246 s \n",
      "\n",
      "\n",
      "\tEpisode 204 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000019073486328 \tStep Time:  0.004987001419067383 s \tTotal Time:  1.459761381149292 s \n",
      "\n",
      "\n",
      "\tEpisode 205 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000041961669922 \tStep Time:  0.006982326507568359 s \tTotal Time:  1.467740774154663 s \n",
      "\n",
      "\n",
      "\tEpisode 206 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000354290008545 \tStep Time:  0.005983114242553711 s \tTotal Time:  1.4737238883972168 s \n",
      "\n",
      "\n",
      "\tEpisode 207 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999966144561768 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.4797077178955078 s \n",
      "\n",
      "\n",
      "\tEpisode 208 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999916076660156 \tStep Time:  0.007014036178588867 s \tTotal Time:  1.4867217540740967 s \n",
      "\n",
      "\n",
      "\tEpisode 209 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0000057220459 \tStep Time:  0.005982398986816406 s \tTotal Time:  1.492704153060913 s \n",
      "\n",
      "\n",
      "\tEpisode 210 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999800205230713 \tStep Time:  0.006983757019042969 s \tTotal Time:  1.499687910079956 s \n",
      "\n",
      "\n",
      "\tEpisode 211 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000498294830322 \tStep Time:  0.005982875823974609 s \tTotal Time:  1.5056707859039307 s \n",
      "\n",
      "\n",
      "\tEpisode 212 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0000057220459 \tStep Time:  0.0059545040130615234 s \tTotal Time:  1.5116252899169922 s \n",
      "\n",
      "\n",
      "\tEpisode 213 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000033855438232 \tStep Time:  0.00701141357421875 s \tTotal Time:  1.518636703491211 s \n",
      "\n",
      "\n",
      "\tEpisode 214 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000123977661133 \tStep Time:  0.005981922149658203 s \tTotal Time:  1.5246186256408691 s \n",
      "\n",
      "\n",
      "\tEpisode 215 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999958515167236 \tStep Time:  0.006986856460571289 s \tTotal Time:  1.5316054821014404 s \n",
      "\n",
      "\n",
      "\tEpisode 216 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999933004379272 \tStep Time:  0.005980253219604492 s \tTotal Time:  1.537585735321045 s \n",
      "\n",
      "\n",
      "\tEpisode 217 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002670288086 \tStep Time:  0.005982637405395508 s \tTotal Time:  1.5435683727264404 s \n",
      "\n",
      "\n",
      "\tEpisode 218 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000164031982422 \tStep Time:  0.00797891616821289 s \tTotal Time:  1.5515472888946533 s \n",
      "\n",
      "\n",
      "\tEpisode 219 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005865097046 \tStep Time:  0.004987001419067383 s \tTotal Time:  1.5565342903137207 s \n",
      "\n",
      "\n",
      "\tEpisode 220 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99983263015747 \tStep Time:  0.00698399543762207 s \tTotal Time:  1.5635182857513428 s \n",
      "\n",
      "\n",
      "\tEpisode 221 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999868392944336 \tStep Time:  0.0069789886474609375 s \tTotal Time:  1.5704972743988037 s \n",
      "\n",
      "\n",
      "\tEpisode 222 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999867916107178 \tStep Time:  0.005988359451293945 s \tTotal Time:  1.5764856338500977 s \n",
      "\n",
      "\n",
      "\tEpisode 223 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00006914138794 \tStep Time:  0.007941722869873047 s \tTotal Time:  1.5844273567199707 s \n",
      "\n",
      "\n",
      "\tEpisode 224 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00000262260437 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.5904114246368408 s \n",
      "\n",
      "\n",
      "\tEpisode 225 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999998569488525 \tStep Time:  0.008012533187866211 s \tTotal Time:  1.598423957824707 s \n",
      "\n",
      "\n",
      "\tEpisode 226 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00000238418579 \tStep Time:  0.005982160568237305 s \tTotal Time:  1.6044061183929443 s \n",
      "\n",
      "\n",
      "\tEpisode 227 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000001430511475 \tStep Time:  0.00598454475402832 s \tTotal Time:  1.6103906631469727 s \n",
      "\n",
      "\n",
      "\tEpisode 228 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000068187713623 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.6173720359802246 s \n",
      "\n",
      "\n",
      "\tEpisode 229 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000246047973633 \tStep Time:  0.005982875823974609 s \tTotal Time:  1.6233549118041992 s \n",
      "\n",
      "\n",
      "\tEpisode 230 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99977970123291 \tStep Time:  0.006952047348022461 s \tTotal Time:  1.6303069591522217 s \n",
      "\n",
      "\n",
      "\tEpisode 231 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999687671661377 \tStep Time:  0.007010698318481445 s \tTotal Time:  1.6373176574707031 s \n",
      "\n",
      "\n",
      "\tEpisode 232 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000041007995605 \tStep Time:  0.0059549808502197266 s \tTotal Time:  1.6432726383209229 s \n",
      "\n",
      "\n",
      "\tEpisode 233 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000015258789062 \tStep Time:  0.008010625839233398 s \tTotal Time:  1.6512832641601562 s \n",
      "\n",
      "\n",
      "\tEpisode 234 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.9999258518219 \tStep Time:  0.006979227066040039 s \tTotal Time:  1.6582624912261963 s \n",
      "\n",
      "\n",
      "\tEpisode 235 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000008583068848 \tStep Time:  0.006982088088989258 s \tTotal Time:  1.6652445793151855 s \n",
      "\n",
      "\n",
      "\tEpisode 236 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00019907951355 \tStep Time:  0.005983114242553711 s \tTotal Time:  1.6712276935577393 s \n",
      "\n",
      "\n",
      "\tEpisode 237 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999863147735596 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.6772112846374512 s \n",
      "\n",
      "\n",
      "\tEpisode 238 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000035762786865 \tStep Time:  0.00698089599609375 s \tTotal Time:  1.684192180633545 s \n",
      "\n",
      "\n",
      "\tEpisode 239 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00016689300537 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.6901764869689941 s \n",
      "\n",
      "\n",
      "\tEpisode 240 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999534130096436 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.698157548904419 s \n",
      "\n",
      "\n",
      "\tEpisode 241 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999958992004395 \tStep Time:  0.006980180740356445 s \tTotal Time:  1.7051377296447754 s \n",
      "\n",
      "\n",
      "\tEpisode 242 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000009536743164 \tStep Time:  0.0059740543365478516 s \tTotal Time:  1.7111117839813232 s \n",
      "\n",
      "\n",
      "\tEpisode 243 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000125408172607 \tStep Time:  0.006958484649658203 s \tTotal Time:  1.7180702686309814 s \n",
      "\n",
      "\n",
      "\tEpisode 244 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005292892456 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.7240543365478516 s \n",
      "\n",
      "\n",
      "\tEpisode 245 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000008583068848 \tStep Time:  0.006984233856201172 s \tTotal Time:  1.7310385704040527 s \n",
      "\n",
      "\n",
      "\tEpisode 246 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00001811981201 \tStep Time:  0.006978511810302734 s \tTotal Time:  1.7380170822143555 s \n",
      "\n",
      "\n",
      "\tEpisode 247 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00021457672119 \tStep Time:  0.006015300750732422 s \tTotal Time:  1.744032382965088 s \n",
      "\n",
      "\n",
      "\tEpisode 248 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00010061264038 \tStep Time:  0.007947683334350586 s \tTotal Time:  1.7519800662994385 s \n",
      "\n",
      "\n",
      "\tEpisode 249 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00010657310486 \tStep Time:  0.006017446517944336 s \tTotal Time:  1.7579975128173828 s \n",
      "\n",
      "\n",
      "\tEpisode 250 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999781847000122 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.7639808654785156 s \n",
      "\n",
      "\n",
      "\tEpisode 251 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002884864807 \tStep Time:  0.006982326507568359 s \tTotal Time:  1.770963191986084 s \n",
      "\n",
      "\n",
      "\tEpisode 252 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000027418136597 \tStep Time:  0.006947040557861328 s \tTotal Time:  1.7779102325439453 s \n",
      "\n",
      "\n",
      "\tEpisode 253 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000020742416382 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.7848916053771973 s \n",
      "\n",
      "\n",
      "\tEpisode 254 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00017547607422 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.7918729782104492 s \n",
      "\n",
      "\n",
      "\tEpisode 255 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000045776367188 \tStep Time:  0.007980585098266602 s \tTotal Time:  1.7998535633087158 s \n",
      "\n",
      "\n",
      "\tEpisode 256 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0000057220459 \tStep Time:  0.0070116519927978516 s \tTotal Time:  1.8068652153015137 s \n",
      "\n",
      "\n",
      "\tEpisode 257 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000006198883057 \tStep Time:  0.005951404571533203 s \tTotal Time:  1.8128166198730469 s \n",
      "\n",
      "\n",
      "\tEpisode 258 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00018835067749 \tStep Time:  0.007015228271484375 s \tTotal Time:  1.8198318481445312 s \n",
      "\n",
      "\n",
      "\tEpisode 259 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00000238418579 \tStep Time:  0.0050525665283203125 s \tTotal Time:  1.8248844146728516 s \n",
      "\n",
      "\n",
      "\tEpisode 260 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005292892456 \tStep Time:  0.006981849670410156 s \tTotal Time:  1.8318662643432617 s \n",
      "\n",
      "\n",
      "\tEpisode 261 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999993324279785 \tStep Time:  0.00598454475402832 s \tTotal Time:  1.83785080909729 s \n",
      "\n",
      "\n",
      "\tEpisode 262 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005292892456 \tStep Time:  0.0049855709075927734 s \tTotal Time:  1.8428363800048828 s \n",
      "\n",
      "\n",
      "\tEpisode 263 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000006675720215 \tStep Time:  0.007947683334350586 s \tTotal Time:  1.8507840633392334 s \n",
      "\n",
      "\n",
      "\tEpisode 264 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000048637390137 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.8567681312561035 s \n",
      "\n",
      "\n",
      "\tEpisode 265 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00014638900757 \tStep Time:  0.0069844722747802734 s \tTotal Time:  1.8637526035308838 s \n",
      "\n",
      "\n",
      "\tEpisode 266 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000032424926758 \tStep Time:  0.006978034973144531 s \tTotal Time:  1.8707306385040283 s \n",
      "\n",
      "\n",
      "\tEpisode 267 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99989128112793 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.8777117729187012 s \n",
      "\n",
      "\n",
      "\tEpisode 268 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000088214874268 \tStep Time:  0.006981611251831055 s \tTotal Time:  1.8846933841705322 s \n",
      "\n",
      "\n",
      "\tEpisode 269 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999996423721313 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.8906772136688232 s \n",
      "\n",
      "\n",
      "\tEpisode 270 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005865097046 \tStep Time:  0.007016181945800781 s \tTotal Time:  1.897693395614624 s \n",
      "\n",
      "\n",
      "\tEpisode 271 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99999165534973 \tStep Time:  0.0069468021392822266 s \tTotal Time:  1.9046401977539062 s \n",
      "\n",
      "\n",
      "\tEpisode 272 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99974250793457 \tStep Time:  0.006012439727783203 s \tTotal Time:  1.9106526374816895 s \n",
      "\n",
      "\n",
      "\tEpisode 273 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000216960906982 \tStep Time:  0.007978200912475586 s \tTotal Time:  1.9195990562438965 s \n",
      "\n",
      "\n",
      "\tEpisode 274 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999995708465576 \tStep Time:  0.006059885025024414 s \tTotal Time:  1.925658941268921 s \n",
      "\n",
      "\n",
      "\tEpisode 275 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999779224395752 \tStep Time:  0.007977485656738281 s \tTotal Time:  1.9336364269256592 s \n",
      "\n",
      "\n",
      "\tEpisode 276 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000117301940918 \tStep Time:  0.005991935729980469 s \tTotal Time:  1.9396283626556396 s \n",
      "\n",
      "\n",
      "\tEpisode 277 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999865531921387 \tStep Time:  0.005953550338745117 s \tTotal Time:  1.9465718269348145 s \n",
      "\n",
      "\n",
      "\tEpisode 278 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000041484832764 \tStep Time:  0.007977008819580078 s \tTotal Time:  1.9545488357543945 s \n",
      "\n",
      "\n",
      "\tEpisode 279 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000036239624023 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.9605326652526855 s \n",
      "\n",
      "\n",
      "\tEpisode 280 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000086784362793 \tStep Time:  0.007979154586791992 s \tTotal Time:  1.9685118198394775 s \n",
      "\n",
      "\n",
      "\tEpisode 281 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000051975250244 \tStep Time:  0.007978200912475586 s \tTotal Time:  1.9764900207519531 s \n",
      "\n",
      "\n",
      "\tEpisode 282 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000120639801025 \tStep Time:  0.008976221084594727 s \tTotal Time:  1.9854662418365479 s \n",
      "\n",
      "\n",
      "\tEpisode 283 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999985218048096 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.9924476146697998 s \n",
      "\n",
      "\n",
      "\tEpisode 284 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00001335144043 \tStep Time:  0.008010387420654297 s \tTotal Time:  2.000458002090454 s \n",
      "\n",
      "\n",
      "\tEpisode 285 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00006103515625 \tStep Time:  0.006981849670410156 s \tTotal Time:  2.0074398517608643 s \n",
      "\n",
      "\n",
      "\tEpisode 286 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99995470046997 \tStep Time:  0.0069506168365478516 s \tTotal Time:  2.014390468597412 s \n",
      "\n",
      "\n",
      "\tEpisode 287 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000062942504883 \tStep Time:  0.007012605667114258 s \tTotal Time:  2.0214030742645264 s \n",
      "\n",
      "\n",
      "\tEpisode 288 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002956390381 \tStep Time:  0.006949186325073242 s \tTotal Time:  2.0283522605895996 s \n",
      "\n",
      "\n",
      "\tEpisode 289 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000006675720215 \tStep Time:  0.00698089599609375 s \tTotal Time:  2.0353331565856934 s \n",
      "\n",
      "\n",
      "\tEpisode 290 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999682426452637 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.042314291000366 s \n",
      "\n",
      "\n",
      "\tEpisode 291 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000077724456787 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.0492959022521973 s \n",
      "\n",
      "\n",
      "\tEpisode 292 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999997854232788 \tStep Time:  0.005984783172607422 s \tTotal Time:  2.0552806854248047 s \n",
      "\n",
      "\n",
      "\tEpisode 293 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [0.9999, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000132083892822 \tStep Time:  0.00797891616821289 s \tTotal Time:  2.0632596015930176 s \n",
      "\n",
      "\n",
      "\tEpisode 294 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00001049041748 \tStep Time:  0.007978200912475586 s \tTotal Time:  2.071237802505493 s \n",
      "\n",
      "\n",
      "\tEpisode 295 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000000476837158 \tStep Time:  0.0069806575775146484 s \tTotal Time:  2.078218460083008 s \n",
      "\n",
      "\n",
      "\tEpisode 296 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999855041503906 \tStep Time:  0.00801229476928711 s \tTotal Time:  2.086230754852295 s \n",
      "\n",
      "\n",
      "\tEpisode 297 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99986505508423 \tStep Time:  0.005982398986816406 s \tTotal Time:  2.0922131538391113 s \n",
      "\n",
      "\n",
      "\tEpisode 298 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.9998836517334 \tStep Time:  0.006984233856201172 s \tTotal Time:  2.0991973876953125 s \n",
      "\n",
      "\n",
      "\tEpisode 299 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000080585479736 \tStep Time:  0.005983114242553711 s \tTotal Time:  2.105180501937866 s \n",
      "\n",
      "\n",
      "\tEpisode 300 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999860763549805 \tStep Time:  0.005950212478637695 s \tTotal Time:  2.111130714416504 s \n",
      "\n",
      "\n",
      "\tEpisode 301 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999932289123535 \tStep Time:  0.00800943374633789 s \tTotal Time:  2.119140148162842 s \n",
      "\n",
      "\n",
      "\tEpisode 302 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00009298324585 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.125124454498291 s \n",
      "\n",
      "\n",
      "\tEpisode 303 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000078201293945 \tStep Time:  0.005956172943115234 s \tTotal Time:  2.1310806274414062 s \n",
      "\n",
      "\n",
      "\tEpisode 304 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000039100646973 \tStep Time:  0.006977558135986328 s \tTotal Time:  2.1380581855773926 s \n",
      "\n",
      "\n",
      "\tEpisode 305 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999871730804443 \tStep Time:  0.0060160160064697266 s \tTotal Time:  2.1440742015838623 s \n",
      "\n",
      "\n",
      "\tEpisode 306 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000099420547485 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.151055335998535 s \n",
      "\n",
      "\n",
      "\tEpisode 307 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.1570396423339844 s \n",
      "\n",
      "\n",
      "\tEpisode 308 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999486207962036 \tStep Time:  0.006949663162231445 s \tTotal Time:  2.163989305496216 s \n",
      "\n",
      "\n",
      "\tEpisode 309 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.9999361038208 \tStep Time:  0.00701451301574707 s \tTotal Time:  2.171003818511963 s \n",
      "\n",
      "\n",
      "\tEpisode 310 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99976396560669 \tStep Time:  0.00794839859008789 s \tTotal Time:  2.178952217102051 s \n",
      "\n",
      "\n",
      "\tEpisode 311 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000000476837158 \tStep Time:  0.006978034973144531 s \tTotal Time:  2.1859302520751953 s \n",
      "\n",
      "\n",
      "\tEpisode 312 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000004529953003 \tStep Time:  0.0059850215911865234 s \tTotal Time:  2.191915273666382 s \n",
      "\n",
      "\n",
      "\tEpisode 313 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99983787536621 \tStep Time:  0.011005163192749023 s \tTotal Time:  2.202920436859131 s \n",
      "\n",
      "\n",
      "\tEpisode 314 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000131130218506 \tStep Time:  0.015922069549560547 s \tTotal Time:  2.2188425064086914 s \n",
      "\n",
      "\n",
      "\tEpisode 315 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999958038330078 \tStep Time:  0.01097249984741211 s \tTotal Time:  2.2298150062561035 s \n",
      "\n",
      "\n",
      "\tEpisode 316 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000000476837158 \tStep Time:  0.009971380233764648 s \tTotal Time:  2.239786386489868 s \n",
      "\n",
      "\n",
      "\tEpisode 317 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002908706665 \tStep Time:  0.013963460922241211 s \tTotal Time:  2.2537498474121094 s \n",
      "\n",
      "\n",
      "\tEpisode 318 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000099658966064 \tStep Time:  0.01396322250366211 s \tTotal Time:  2.2677130699157715 s \n",
      "\n",
      "\n",
      "\tEpisode 319 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9992],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999991416931152 \tStep Time:  0.011967897415161133 s \tTotal Time:  2.2796809673309326 s \n",
      "\n",
      "\n",
      "\tEpisode 320 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000041961669922 \tStep Time:  0.007977724075317383 s \tTotal Time:  2.28765869140625 s \n",
      "\n",
      "\n",
      "\tEpisode 321 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000051498413086 \tStep Time:  0.007984638214111328 s \tTotal Time:  2.2956433296203613 s \n",
      "\n",
      "\n",
      "\tEpisode 322 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000091075897217 \tStep Time:  0.006974697113037109 s \tTotal Time:  2.3026180267333984 s \n",
      "\n",
      "\n",
      "\tEpisode 323 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000002145767212 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.3095996379852295 s \n",
      "\n",
      "\n",
      "\tEpisode 324 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000001430511475 \tStep Time:  0.008975982666015625 s \tTotal Time:  2.318575620651245 s \n",
      "\n",
      "\n",
      "\tEpisode 325 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99988603591919 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.325556993484497 s \n",
      "\n",
      "\n",
      "\tEpisode 326 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000009059906006 \tStep Time:  0.00698399543762207 s \tTotal Time:  2.332540988922119 s \n",
      "\n",
      "\n",
      "\tEpisode 327 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9990],\n",
      "        [1.0000, 0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999025344848633 \tStep Time:  0.006979942321777344 s \tTotal Time:  2.340517282485962 s \n",
      "\n",
      "\n",
      "\tEpisode 328 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999995708465576 \tStep Time:  0.007979154586791992 s \tTotal Time:  2.348496437072754 s \n",
      "\n",
      "\n",
      "\tEpisode 329 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000049114227295 \tStep Time:  0.007977724075317383 s \tTotal Time:  2.3564741611480713 s \n",
      "\n",
      "\n",
      "\tEpisode 330 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999833583831787 \tStep Time:  0.005985736846923828 s \tTotal Time:  2.362459897994995 s \n",
      "\n",
      "\n",
      "\tEpisode 331 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000004768371582 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.370436906814575 s \n",
      "\n",
      "\n",
      "\tEpisode 332 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999996662139893 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.377418279647827 s \n",
      "\n",
      "\n",
      "\tEpisode 333 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000000476837158 \tStep Time:  0.007978677749633789 s \tTotal Time:  2.385396957397461 s \n",
      "\n",
      "\n",
      "\tEpisode 334 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9984],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000082969665527 \tStep Time:  0.004987478256225586 s \tTotal Time:  2.3913817405700684 s \n",
      "\n",
      "\n",
      "\tEpisode 335 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99950933456421 \tStep Time:  0.00797891616821289 s \tTotal Time:  2.3993606567382812 s \n",
      "\n",
      "\n",
      "\tEpisode 336 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00001573562622 \tStep Time:  0.006980180740356445 s \tTotal Time:  2.4063408374786377 s \n",
      "\n",
      "\n",
      "\tEpisode 337 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000047206878662 \tStep Time:  0.008980035781860352 s \tTotal Time:  2.415320873260498 s \n",
      "\n",
      "\n",
      "\tEpisode 338 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999969959259033 \tStep Time:  0.0070095062255859375 s \tTotal Time:  2.422330379486084 s \n",
      "\n",
      "\n",
      "\tEpisode 339 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00007915496826 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.428314208984375 s \n",
      "\n",
      "\n",
      "\tEpisode 340 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00002145767212 \tStep Time:  0.006980419158935547 s \tTotal Time:  2.4362616539001465 s \n",
      "\n",
      "\n",
      "\tEpisode 341 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999953269958496 \tStep Time:  0.00698089599609375 s \tTotal Time:  2.4432425498962402 s \n",
      "\n",
      "\n",
      "\tEpisode 342 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999420404434204 \tStep Time:  0.00797891616821289 s \tTotal Time:  2.451221466064453 s \n",
      "\n",
      "\n",
      "\tEpisode 343 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000078201293945 \tStep Time:  0.00698089599609375 s \tTotal Time:  2.458202362060547 s \n",
      "\n",
      "\n",
      "\tEpisode 344 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000035762786865 \tStep Time:  0.006984710693359375 s \tTotal Time:  2.4651870727539062 s \n",
      "\n",
      "\n",
      "\tEpisode 345 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000009536743164 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.473162889480591 s \n",
      "\n",
      "\n",
      "\tEpisode 346 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000065803527832 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.4801442623138428 s \n",
      "\n",
      "\n",
      "\tEpisode 347 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000088691711426 \tStep Time:  0.00698089599609375 s \tTotal Time:  2.4871251583099365 s \n",
      "\n",
      "\n",
      "\tEpisode 348 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9993],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999285459518433 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.4931089878082275 s \n",
      "\n",
      "\n",
      "\tEpisode 349 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999959707260132 \tStep Time:  0.0069828033447265625 s \tTotal Time:  2.501088857650757 s \n",
      "\n",
      "\n",
      "\tEpisode 350 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9992],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000030517578125 \tStep Time:  0.006979942321777344 s \tTotal Time:  2.508068799972534 s \n",
      "\n",
      "\n",
      "\tEpisode 351 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000084400177002 \tStep Time:  0.00698399543762207 s \tTotal Time:  2.5150527954101562 s \n",
      "\n",
      "\n",
      "\tEpisode 352 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000020027160645 \tStep Time:  0.006978511810302734 s \tTotal Time:  2.522031307220459 s \n",
      "\n",
      "\n",
      "\tEpisode 353 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000039100646973 \tStep Time:  0.0069844722747802734 s \tTotal Time:  2.5290157794952393 s \n",
      "\n",
      "\n",
      "\tEpisode 354 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00007152557373 \tStep Time:  0.007977008819580078 s \tTotal Time:  2.5369927883148193 s \n",
      "\n",
      "\n",
      "\tEpisode 355 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00004816055298 \tStep Time:  0.005982398986816406 s \tTotal Time:  2.5429751873016357 s \n",
      "\n",
      "\n",
      "\tEpisode 356 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000020503997803 \tStep Time:  0.007979869842529297 s \tTotal Time:  2.550955057144165 s \n",
      "\n",
      "\n",
      "\tEpisode 357 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005006790161 \tStep Time:  0.0069806575775146484 s \tTotal Time:  2.5579357147216797 s \n",
      "\n",
      "\n",
      "\tEpisode 358 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00003433227539 \tStep Time:  0.005985736846923828 s \tTotal Time:  2.5639214515686035 s \n",
      "\n",
      "\n",
      "\tEpisode 359 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0000057220459 \tStep Time:  0.007976531982421875 s \tTotal Time:  2.5718979835510254 s \n",
      "\n",
      "\n",
      "\tEpisode 360 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99943494796753 \tStep Time:  0.00601959228515625 s \tTotal Time:  2.5779175758361816 s \n",
      "\n",
      "\n",
      "\tEpisode 361 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9989],\n",
      "        [1.0000, 0.9976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00008201599121 \tStep Time:  0.007943868637084961 s \tTotal Time:  2.5858614444732666 s \n",
      "\n",
      "\n",
      "\tEpisode 362 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999793529510498 \tStep Time:  0.0059833526611328125 s \tTotal Time:  2.5918447971343994 s \n",
      "\n",
      "\n",
      "\tEpisode 363 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00004005432129 \tStep Time:  0.009008646011352539 s \tTotal Time:  2.600853443145752 s \n",
      "\n",
      "\n",
      "\tEpisode 364 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00007390975952 \tStep Time:  0.006949186325073242 s \tTotal Time:  2.607802629470825 s \n",
      "\n",
      "\n",
      "\tEpisode 365 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9989],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00000286102295 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.614783763885498 s \n",
      "\n",
      "\n",
      "\tEpisode 366 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999999046325684 \tStep Time:  0.007980108261108398 s \tTotal Time:  2.6227638721466064 s \n",
      "\n",
      "\n",
      "\tEpisode 367 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000016689300537 \tStep Time:  0.00797891616821289 s \tTotal Time:  2.6307427883148193 s \n",
      "\n",
      "\n",
      "\tEpisode 368 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000101327896118 \tStep Time:  0.007010698318481445 s \tTotal Time:  2.637753486633301 s \n",
      "\n",
      "\n",
      "\tEpisode 369 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000144481658936 \tStep Time:  0.0059850215911865234 s \tTotal Time:  2.6437385082244873 s \n",
      "\n",
      "\n",
      "\tEpisode 370 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999979734420776 \tStep Time:  0.008957862854003906 s \tTotal Time:  2.652696371078491 s \n",
      "\n",
      "\n",
      "\tEpisode 371 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999675750732422 \tStep Time:  0.006967782974243164 s \tTotal Time:  2.6596641540527344 s \n",
      "\n",
      "\n",
      "\tEpisode 372 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999756813049316 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.6666457653045654 s \n",
      "\n",
      "\n",
      "\tEpisode 373 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9991],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999074459075928 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.6736273765563965 s \n",
      "\n",
      "\n",
      "\tEpisode 374 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000017881393433 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.6806085109710693 s \n",
      "\n",
      "\n",
      "\tEpisode 375 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9991],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00000286102295 \tStep Time:  0.007978200912475586 s \tTotal Time:  2.688586711883545 s \n",
      "\n",
      "\n",
      "\tEpisode 376 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000016689300537 \tStep Time:  0.006015300750732422 s \tTotal Time:  2.6946020126342773 s \n",
      "\n",
      "\n",
      "\tEpisode 377 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999956607818604 \tStep Time:  0.007947206497192383 s \tTotal Time:  2.7025492191314697 s \n",
      "\n",
      "\n",
      "\tEpisode 378 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9991],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000125885009766 \tStep Time:  0.006018400192260742 s \tTotal Time:  2.7085676193237305 s \n",
      "\n",
      "\n",
      "\tEpisode 379 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00004243850708 \tStep Time:  0.0069501399993896484 s \tTotal Time:  2.71551775932312 s \n",
      "\n",
      "\n",
      "\tEpisode 380 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999454975128174 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.7224955558776855 s \n",
      "\n",
      "\n",
      "\tEpisode 381 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999016761779785 \tStep Time:  0.007016181945800781 s \tTotal Time:  2.7295117378234863 s \n",
      "\n",
      "\n",
      "\tEpisode 382 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000028133392334 \tStep Time:  0.00697779655456543 s \tTotal Time:  2.7364895343780518 s \n",
      "\n",
      "\n",
      "\tEpisode 383 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999621391296387 \tStep Time:  0.005952596664428711 s \tTotal Time:  2.7424421310424805 s \n",
      "\n",
      "\n",
      "\tEpisode 384 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0000057220459 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.7494237422943115 s \n",
      "\n",
      "\n",
      "\tEpisode 385 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000113010406494 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.7554080486297607 s \n",
      "\n",
      "\n",
      "\tEpisode 386 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9976],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9982],\n",
      "        [1.0000, 0.9970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99822759628296 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.7613916397094727 s \n",
      "\n",
      "\n",
      "\tEpisode 387 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9982],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999483108520508 \tStep Time:  0.00801229476928711 s \tTotal Time:  2.7694039344787598 s \n",
      "\n",
      "\n",
      "\tEpisode 388 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000017881393433 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.775388240814209 s \n",
      "\n",
      "\n",
      "\tEpisode 389 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00003433227539 \tStep Time:  0.00694727897644043 s \tTotal Time:  2.7823355197906494 s \n",
      "\n",
      "\n",
      "\tEpisode 390 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00004816055298 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.7893168926239014 s \n",
      "\n",
      "\n",
      "\tEpisode 391 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000008583068848 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.7953004837036133 s \n",
      "\n",
      "\n",
      "\tEpisode 392 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00005269050598 \tStep Time:  0.00897669792175293 s \tTotal Time:  2.804277181625366 s \n",
      "\n",
      "\n",
      "\tEpisode 393 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999889373779297 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.811258316040039 s \n",
      "\n",
      "\n",
      "\tEpisode 394 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.998266220092773 \tStep Time:  0.007979631423950195 s \tTotal Time:  2.8192379474639893 s \n",
      "\n",
      "\n",
      "\tEpisode 395 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9987],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9975],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99954128265381 \tStep Time:  0.006980180740356445 s \tTotal Time:  2.8262181282043457 s \n",
      "\n",
      "\n",
      "\tEpisode 396 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000022888183594 \tStep Time:  0.007979631423950195 s \tTotal Time:  2.834197759628296 s \n",
      "\n",
      "\n",
      "\tEpisode 397 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0000319480896 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.840181350708008 s \n",
      "\n",
      "\n",
      "\tEpisode 398 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00013494491577 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.8471627235412598 s \n",
      "\n",
      "\n",
      "\tEpisode 399 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000182628631592 \tStep Time:  0.007978200912475586 s \tTotal Time:  2.8551409244537354 s \n",
      "\n",
      "\n",
      "\tEpisode 400 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.997994422912598 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.8611247539520264 s \n",
      "\n",
      "\n",
      "\tEpisode 401 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00010061264038 \tStep Time:  0.007979631423950195 s \tTotal Time:  2.8691043853759766 s \n",
      "\n",
      "\n",
      "\tEpisode 402 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000059604644775 \tStep Time:  0.007978439331054688 s \tTotal Time:  2.8770828247070312 s \n",
      "\n",
      "\n",
      "\tEpisode 403 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9990],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9955],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999099254608154 \tStep Time:  0.010970354080200195 s \tTotal Time:  2.8880531787872314 s \n",
      "\n",
      "\n",
      "\tEpisode 404 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9983],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.998419761657715 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.8950343132019043 s \n",
      "\n",
      "\n",
      "\tEpisode 405 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000049114227295 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.9020159244537354 s \n",
      "\n",
      "\n",
      "\tEpisode 406 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999210834503174 \tStep Time:  0.005019426345825195 s \tTotal Time:  2.9080333709716797 s \n",
      "\n",
      "\n",
      "\tEpisode 407 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00003957748413 \tStep Time:  0.0069828033447265625 s \tTotal Time:  2.9150161743164062 s \n",
      "\n",
      "\n",
      "\tEpisode 408 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000121116638184 \tStep Time:  0.006977081298828125 s \tTotal Time:  2.9219932556152344 s \n",
      "\n",
      "\n",
      "\tEpisode 409 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999770164489746 \tStep Time:  0.005984783172607422 s \tTotal Time:  2.927978038787842 s \n",
      "\n",
      "\n",
      "\tEpisode 410 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9993],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999464988708496 \tStep Time:  0.006949901580810547 s \tTotal Time:  2.9349279403686523 s \n",
      "\n",
      "\n",
      "\tEpisode 411 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000041961669922 \tStep Time:  0.006015300750732422 s \tTotal Time:  2.9409432411193848 s \n",
      "\n",
      "\n",
      "\tEpisode 412 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9986],\n",
      "        [1.0000, 0.9983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000181198120117 \tStep Time:  0.0069904327392578125 s \tTotal Time:  2.9479336738586426 s \n",
      "\n",
      "\n",
      "\tEpisode 413 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00017738342285 \tStep Time:  0.006972789764404297 s \tTotal Time:  2.954906463623047 s \n",
      "\n",
      "\n",
      "\tEpisode 414 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9987],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.998856782913208 \tStep Time:  0.00598597526550293 s \tTotal Time:  2.96089243888855 s \n",
      "\n",
      "\n",
      "\tEpisode 415 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000412940979004 \tStep Time:  0.007944583892822266 s \tTotal Time:  2.968837022781372 s \n",
      "\n",
      "\n",
      "\tEpisode 416 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000194549560547 \tStep Time:  0.00598454475402832 s \tTotal Time:  2.9748215675354004 s \n",
      "\n",
      "\n",
      "\tEpisode 417 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00048303604126 \tStep Time:  0.00797891616821289 s \tTotal Time:  2.9828004837036133 s \n",
      "\n",
      "\n",
      "\tEpisode 418 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.995259284973145 \tStep Time:  0.007977724075317383 s \tTotal Time:  2.9907782077789307 s \n",
      "\n",
      "\n",
      "\tEpisode 419 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00129723548889 \tStep Time:  0.008977890014648438 s \tTotal Time:  2.999756097793579 s \n",
      "\n",
      "\n",
      "\tEpisode 420 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9992],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9990],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999717712402344 \tStep Time:  0.007976770401000977 s \tTotal Time:  3.00773286819458 s \n",
      "\n",
      "\n",
      "\tEpisode 421 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 0.9944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000593423843384 \tStep Time:  0.008976459503173828 s \tTotal Time:  3.016709327697754 s \n",
      "\n",
      "\n",
      "\tEpisode 422 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00082015991211 \tStep Time:  0.007979154586791992 s \tTotal Time:  3.024688482284546 s \n",
      "\n",
      "\n",
      "\tEpisode 423 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000309944152832 \tStep Time:  0.008976221084594727 s \tTotal Time:  3.0336647033691406 s \n",
      "\n",
      "\n",
      "\tEpisode 424 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9993],\n",
      "        [1.0000, 0.9975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00467801094055 \tStep Time:  0.007977724075317383 s \tTotal Time:  3.041642427444458 s \n",
      "\n",
      "\n",
      "\tEpisode 425 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9942],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.008079290390015 \tStep Time:  0.008977413177490234 s \tTotal Time:  3.0506198406219482 s \n",
      "\n",
      "\n",
      "\tEpisode 426 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9938],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.002806663513184 \tStep Time:  0.006980419158935547 s \tTotal Time:  3.057600259780884 s \n",
      "\n",
      "\n",
      "\tEpisode 427 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.002290725708008 \tStep Time:  0.0079803466796875 s \tTotal Time:  3.0655806064605713 s \n",
      "\n",
      "\n",
      "\tEpisode 428 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9990],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9994],\n",
      "        [1.0000, 0.9963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.999777793884277 \tStep Time:  0.008975505828857422 s \tTotal Time:  3.0745561122894287 s \n",
      "\n",
      "\n",
      "\tEpisode 429 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9832],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00169038772583 \tStep Time:  0.007978677749633789 s \tTotal Time:  3.0825347900390625 s \n",
      "\n",
      "\n",
      "\tEpisode 430 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9925],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9996],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000720500946045 \tStep Time:  0.006982326507568359 s \tTotal Time:  3.089517116546631 s \n",
      "\n",
      "\n",
      "\tEpisode 431 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00227451324463 \tStep Time:  0.007978439331054688 s \tTotal Time:  3.0974955558776855 s \n",
      "\n",
      "\n",
      "\tEpisode 432 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 0.9988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000296592712402 \tStep Time:  0.007013082504272461 s \tTotal Time:  3.104508638381958 s \n",
      "\n",
      "\n",
      "\tEpisode 433 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.9943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.001067399978638 \tStep Time:  0.0069789886474609375 s \tTotal Time:  3.111487627029419 s \n",
      "\n",
      "\n",
      "\tEpisode 434 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9726],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.004140853881836 \tStep Time:  0.007978200912475586 s \tTotal Time:  3.1194658279418945 s \n",
      "\n",
      "\n",
      "\tEpisode 435 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.998477935791016 \tStep Time:  0.0059854984283447266 s \tTotal Time:  3.1254513263702393 s \n",
      "\n",
      "\n",
      "\tEpisode 436 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9985],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000388383865356 \tStep Time:  0.007978439331054688 s \tTotal Time:  3.133429765701294 s \n",
      "\n",
      "\n",
      "\tEpisode 437 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9964],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9966],\n",
      "        [1.0000, 0.9878]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  19.99366307258606 \tStep Time:  0.005980253219604492 s \tTotal Time:  3.1394100189208984 s \n",
      "\n",
      "\n",
      "\tEpisode 438 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9248],\n",
      "        [1.0000, 0.9965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9980],\n",
      "        [1.0000, 0.8224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  19.974106788635254 \tStep Time:  0.006985187530517578 s \tTotal Time:  3.146395206451416 s \n",
      "\n",
      "\n",
      "\tEpisode 439 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9998],\n",
      "        [1.0000, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.356139659881592 \tStep Time:  0.007945537567138672 s \tTotal Time:  3.1543407440185547 s \n",
      "\n",
      "\n",
      "\tEpisode 440 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9995],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.93919825553894 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.160325527191162 s \n",
      "\n",
      "\n",
      "\tEpisode 441 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9984],\n",
      "        [1.0000, 0.9896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  21.58841609954834 \tStep Time:  0.00797724723815918 s \tTotal Time:  3.1683027744293213 s \n",
      "\n",
      "\n",
      "\tEpisode 442 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9989],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  21.400585651397705 \tStep Time:  0.007014274597167969 s \tTotal Time:  3.1753170490264893 s \n",
      "\n",
      "\n",
      "\tEpisode 443 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.787822246551514 \tStep Time:  0.006983518600463867 s \tTotal Time:  3.182300567626953 s \n",
      "\n",
      "\n",
      "\tEpisode 444 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9997],\n",
      "        [1.0000, 0.9997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  22.63213300704956 \tStep Time:  0.005987644195556641 s \tTotal Time:  3.1882882118225098 s \n",
      "\n",
      "\n",
      "\tEpisode 445 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.518815994262695 \tStep Time:  0.006952047348022461 s \tTotal Time:  3.1962311267852783 s \n",
      "\n",
      "\n",
      "\tEpisode 446 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  23.6782488822937 \tStep Time:  0.006979465484619141 s \tTotal Time:  3.2032105922698975 s \n",
      "\n",
      "\n",
      "\tEpisode 447 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.420845985412598 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.2091941833496094 s \n",
      "\n",
      "\n",
      "\tEpisode 448 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  22.53535556793213 \tStep Time:  0.007022857666015625 s \tTotal Time:  3.216217041015625 s \n",
      "\n",
      "\n",
      "\tEpisode 449 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.406861543655396 \tStep Time:  0.005976438522338867 s \tTotal Time:  3.222193479537964 s \n",
      "\n",
      "\n",
      "\tEpisode 450 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.844509601593018 \tStep Time:  0.0069506168365478516 s \tTotal Time:  3.2301406860351562 s \n",
      "\n",
      "\n",
      "\tEpisode 451 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.659679174423218 \tStep Time:  0.006978750228881836 s \tTotal Time:  3.237119436264038 s \n",
      "\n",
      "\n",
      "\tEpisode 452 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.121530532836914 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.243103504180908 s \n",
      "\n",
      "\n",
      "\tEpisode 453 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.409413814544678 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.25008487701416 s \n",
      "\n",
      "\n",
      "\tEpisode 454 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.501726150512695 \tStep Time:  0.006981849670410156 s \tTotal Time:  3.2570667266845703 s \n",
      "\n",
      "\n",
      "\tEpisode 455 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.19632124900818 \tStep Time:  0.00698089599609375 s \tTotal Time:  3.264047622680664 s \n",
      "\n",
      "\n",
      "\tEpisode 456 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.144789695739746 \tStep Time:  0.007013559341430664 s \tTotal Time:  3.2710611820220947 s \n",
      "\n",
      "\n",
      "\tEpisode 457 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.331480979919434 \tStep Time:  0.006983041763305664 s \tTotal Time:  3.2780442237854004 s \n",
      "\n",
      "\n",
      "\tEpisode 458 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.202391624450684 \tStep Time:  0.0069789886474609375 s \tTotal Time:  3.2850232124328613 s \n",
      "\n",
      "\n",
      "\tEpisode 459 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.039902210235596 \tStep Time:  0.006949663162231445 s \tTotal Time:  3.2919728755950928 s \n",
      "\n",
      "\n",
      "\tEpisode 460 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.081220865249634 \tStep Time:  0.007016181945800781 s \tTotal Time:  3.2989890575408936 s \n",
      "\n",
      "\n",
      "\tEpisode 461 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.110846519470215 \tStep Time:  0.005980491638183594 s \tTotal Time:  3.304969549179077 s \n",
      "\n",
      "\n",
      "\tEpisode 462 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.075984477996826 \tStep Time:  0.006983757019042969 s \tTotal Time:  3.31195330619812 s \n",
      "\n",
      "\n",
      "\tEpisode 463 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.011658430099487 \tStep Time:  0.006947994232177734 s \tTotal Time:  3.318901300430298 s \n",
      "\n",
      "\n",
      "\tEpisode 464 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.01250982284546 \tStep Time:  0.00698089599609375 s \tTotal Time:  3.3258821964263916 s \n",
      "\n",
      "\n",
      "\tEpisode 465 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.031646728515625 \tStep Time:  0.006981611251831055 s \tTotal Time:  3.3328638076782227 s \n",
      "\n",
      "\n",
      "\tEpisode 466 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.035151958465576 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.3388478755950928 s \n",
      "\n",
      "\n",
      "\tEpisode 467 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.018287658691406 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.3458292484283447 s \n",
      "\n",
      "\n",
      "\tEpisode 468 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.005859375 \tStep Time:  0.0069806575775146484 s \tTotal Time:  3.3528099060058594 s \n",
      "\n",
      "\n",
      "\tEpisode 469 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.006346702575684 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.3587939739227295 s \n",
      "\n",
      "\n",
      "\tEpisode 470 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.015113353729248 \tStep Time:  0.006016731262207031 s \tTotal Time:  3.3648107051849365 s \n",
      "\n",
      "\n",
      "\tEpisode 471 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.01322650909424 \tStep Time:  0.0069806575775146484 s \tTotal Time:  3.371791362762451 s \n",
      "\n",
      "\n",
      "\tEpisode 472 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.007297039031982 \tStep Time:  0.006949663162231445 s \tTotal Time:  3.3787410259246826 s \n",
      "\n",
      "\n",
      "\tEpisode 473 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1., 1.],\n",
      "        [1., 1.]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00141429901123 \tStep Time:  0.0069811344146728516 s \tTotal Time:  3.3857221603393555 s \n",
      "\n",
      "\n",
      "\tEpisode 474 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.003031253814697 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.3917064666748047 s \n",
      "\n",
      "\n",
      "\tEpisode 475 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.007054328918457 \tStep Time:  0.007978439331054688 s \tTotal Time:  3.3996849060058594 s \n",
      "\n",
      "\n",
      "\tEpisode 476 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00595235824585 \tStep Time:  0.006017208099365234 s \tTotal Time:  3.4057021141052246 s \n",
      "\n",
      "\n",
      "\tEpisode 477 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.002670288085938 \tStep Time:  0.006948947906494141 s \tTotal Time:  3.4126510620117188 s \n",
      "\n",
      "\n",
      "\tEpisode 478 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00045394897461 \tStep Time:  0.006981849670410156 s \tTotal Time:  3.419632911682129 s \n",
      "\n",
      "\n",
      "\tEpisode 479 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.001322269439697 \tStep Time:  0.007977724075317383 s \tTotal Time:  3.4276106357574463 s \n",
      "\n",
      "\n",
      "\tEpisode 480 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.002522945404053 \tStep Time:  0.010972023010253906 s \tTotal Time:  3.4385826587677 s \n",
      "\n",
      "\n",
      "\tEpisode 481 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00226354598999 \tStep Time:  0.0069806575775146484 s \tTotal Time:  3.445563316345215 s \n",
      "\n",
      "\n",
      "\tEpisode 482 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00095796585083 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.452544689178467 s \n",
      "\n",
      "\n",
      "\tEpisode 483 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000324726104736 \tStep Time:  0.007015228271484375 s \tTotal Time:  3.459559917449951 s \n",
      "\n",
      "\n",
      "\tEpisode 484 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000845909118652 \tStep Time:  0.007944345474243164 s \tTotal Time:  3.4675042629241943 s \n",
      "\n",
      "\n",
      "\tEpisode 485 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.001065254211426 \tStep Time:  0.00701451301574707 s \tTotal Time:  3.4745187759399414 s \n",
      "\n",
      "\n",
      "\tEpisode 486 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000932216644287 \tStep Time:  0.005982637405395508 s \tTotal Time:  3.480501413345337 s \n",
      "\n",
      "\n",
      "\tEpisode 487 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000322818756104 \tStep Time:  0.006970405578613281 s \tTotal Time:  3.4884796142578125 s \n",
      "\n",
      "\n",
      "\tEpisode 488 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000121593475342 \tStep Time:  0.006983041763305664 s \tTotal Time:  3.495462656021118 s \n",
      "\n",
      "\n",
      "\tEpisode 489 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00045943260193 \tStep Time:  0.007945060729980469 s \tTotal Time:  3.5034077167510986 s \n",
      "\n",
      "\n",
      "\tEpisode 490 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00044870376587 \tStep Time:  0.0069828033447265625 s \tTotal Time:  3.510390520095825 s \n",
      "\n",
      "\n",
      "\tEpisode 491 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000258922576904 \tStep Time:  0.007977724075317383 s \tTotal Time:  3.5183682441711426 s \n",
      "\n",
      "\n",
      "\tEpisode 492 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00013303756714 \tStep Time:  0.005983829498291016 s \tTotal Time:  3.5243520736694336 s \n",
      "\n",
      "\n",
      "\tEpisode 493 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [6., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [3., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000019073486328 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.530336856842041 s \n",
      "\n",
      "\n",
      "\tEpisode 494 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 0.9999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.0000901222229 \tStep Time:  0.006979703903198242 s \tTotal Time:  3.5373165607452393 s \n",
      "\n",
      "\n",
      "\tEpisode 495 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00018048286438 \tStep Time:  0.004986763000488281 s \tTotal Time:  3.5423033237457275 s \n",
      "\n",
      "\n",
      "\tEpisode 496 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000208377838135 \tStep Time:  0.006016969680786133 s \tTotal Time:  3.5483202934265137 s \n",
      "\n",
      "\n",
      "\tEpisode 497 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00014352798462 \tStep Time:  0.004986286163330078 s \tTotal Time:  3.554304599761963 s \n",
      "\n",
      "\n",
      "\tEpisode 498 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9999],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.00014901161194 \tStep Time:  0.005981922149658203 s \tTotal Time:  3.560286521911621 s \n",
      "\n",
      "\n",
      "\tEpisode 499 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [3., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[6., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000099658966064 \tStep Time:  0.0069844722747802734 s \tTotal Time:  3.5672709941864014 s \n",
      "\n",
      "\n",
      "\tEpisode 500 / 500\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 1.0000],\n",
      "        [1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[3., 1.],\n",
      "        [6., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  20.000081539154053 \tStep Time:  0.0059833526611328125 s \tTotal Time:  3.573254346847534 s \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40.000163078308105"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "train(model, optimizer, train_loader, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Supplier Data:\n",
      "[[1.         0.99995494]]\n",
      "[[0.9999995  0.99999976]]\n",
      "[[0.99999845 0.99999344]]\n",
      "[[1. 1.]]\n",
      "[[1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Generate a new sample\n",
    "def generate_sample(model, latent_dim):\n",
    "    sample = torch.randn(1, latent_dim)\n",
    "    return model.decode(sample).detach().numpy()\n",
    "\n",
    "# Print or use the generated supplier data as needed\n",
    "print(\"Generated Supplier Data:\")\n",
    "for i in range(5):\n",
    "    print(generate_sample(model, latent_dim))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
