{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchtext as tt\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting & vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for PyTorch\n",
    "class SupplierDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the element at the given index\n",
    "        element = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Flatten the element into a 1D array by looping through the columns and adding them to the array\n",
    "        flatten_element = []\n",
    "        for column in element:\n",
    "            # list\n",
    "            if isinstance(column, list):\n",
    "                for item in column:\n",
    "                    flatten_element.append(item)\n",
    "            # numpy array\n",
    "            elif isinstance(column, np.ndarray):\n",
    "                for item in column:\n",
    "                    flatten_element.append(item)\n",
    "            # sub tensor\n",
    "            elif isinstance(column, torch.Tensor):\n",
    "                for item in column:\n",
    "                    flatten_element.append(item)\n",
    "            else:\n",
    "                flatten_element.append(column)\n",
    "        \n",
    "        # Convert the element to a PyTorch tensor\n",
    "        tensor = torch.tensor(flatten_element, dtype=torch.float32)\n",
    "        \n",
    "        #print(f\"shape of tensor: {tensor.shape} and type: {type(tensor)}\")\n",
    "        \n",
    "        # Apply the transform if one is given\n",
    "        if self.transform:\n",
    "            return self.transform(tensor)\n",
    "        else:\n",
    "            return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_columns_types(dataset):\n",
    "    # Separate the text columns from the other columns\n",
    "    text_columns = [\"invoice_code\", \"customer_name\",\"customer_email\",\"customer_address\",\"customer_city\",\"customer_state\",\"customer_postal_code\",\n",
    "                    \"customer_country\",\"notes\",\"created_by\",\"updated_by\",\"shipping_address\",\"shipping_city\",\"shipping_state\",\n",
    "                    \"shipping_postal_code\",\"shipping_country\"]\n",
    "    \n",
    "    date_columns = [\"invoice_date\",\"payment_due_date\",\"created_at\",\"updated_at\",\"due_date\",\"paid_date\"]\n",
    "\n",
    "    categorical_columns = [\"payment_method\",\"status\",\"currency\",\"payment_reference\"]\n",
    "\n",
    "    numerical_columns = [\"invoice_number\",\"subtotal\",\"tax_rate\",\"tax_amount\",\"discount_rate\",\"discount_amount\",\"total\",\"exchange_rate\"]  \n",
    "\n",
    "    # Check if there is a column not in one of the above lists\n",
    "    for column in dataset.columns:\n",
    "        if column not in text_columns and column not in date_columns and column not in categorical_columns and column not in numerical_columns:\n",
    "            print(\"Column not in any list: \" + column) \n",
    "            \n",
    "    return dataset[text_columns], dataset[date_columns], dataset[categorical_columns], dataset[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical(dataset):\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "    classes = {}\n",
    "    \n",
    "    # Apply a one-hot encoding to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "\n",
    "        # Convert the strings to unique numerical indices\n",
    "        unique_classes, indices = np.unique(dataset[column], return_inverse=True)\n",
    "        classes[column] = unique_classes\n",
    "        #print(\"Column: \" + column + \" - Classes: \" + str(unique_classes))\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(indices)\n",
    "\n",
    "        # Apply one-hot encoding using torch.nn.functional.one_hot\n",
    "        one_hot_encoded = F.one_hot(tensor_data)\n",
    "        \n",
    "        # Convert the one-hot encoding tensor to a NumPy array\n",
    "        one_hot_array = one_hot_encoded.numpy()\n",
    "\n",
    "        # Add the one-hot encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(one_hot_array)\n",
    "\n",
    "    return transformed_dataset, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_numerical(dataset):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # Apply a min-max normalization to all columns of the tensor\n",
    "    for column in dataset.columns:\n",
    "        # Convert the column to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(dataset[column], dtype=torch.float32)\n",
    "\n",
    "        # Range of values in the column\n",
    "        min_value = torch.min(tensor_data)\n",
    "        max_value = torch.max(tensor_data)\n",
    "\n",
    "        # Normalize the values in the column between 0 and 1en\n",
    "        normalized = (tensor_data - min_value) / (max_value - min_value)\n",
    "\n",
    "        # Apply min-max normalization using torch.nn.functional.normalize\n",
    "        #normalized = F.normalize(tensor_data)\n",
    "\n",
    "        # Convert the normalized tensor to a NumPy array\n",
    "        normalized_array = normalized.numpy()\n",
    "\n",
    "        # Add the normalized array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(normalized_array)\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(dataset):\n",
    "    # Prepare embeddings model\n",
    "    # embedding_dim = 100\n",
    "    # glove = GloVe(name='6B', dim=embedding_dim)\n",
    "    # tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    # # Apply a word embedding encoding to all columns of the tensor using torchtext\n",
    "    # for column in dataset.columns:\n",
    "    #     # Convert the column to a string list\n",
    "    #     texts = dataset[column].astype(str).tolist()\n",
    "\n",
    "    #     # Convert the text to a list of tokens\n",
    "    #     tokens = [tokenizer(t) for t in texts]\n",
    "\n",
    "    #     # Convert the tokens to a list of indices\n",
    "    #     encoded_data = []\n",
    "    #     for token in tokens:\n",
    "    #         token_encoded = []\n",
    "    #         for word in token:\n",
    "    #             if word in glove.stoi:\n",
    "    #                 token_encoded.append(glove.stoi[word])\n",
    "    #             else:\n",
    "    #                 token_encoded.append(0)\n",
    "    #         encoded_data.append(token_encoded)\n",
    "\n",
    "    #     # Convert the indices to a PyTorch tensor\n",
    "    #     if len(encoded_data) <= 0:\n",
    "    #         continue\n",
    "\n",
    "    #     non_empty_sequences = [torch.tensor(seq) for seq in encoded_data if len(seq) > 0]\n",
    "\n",
    "    #     # Pad the sequences to the same length\n",
    "    #     padded_sequences = pad_sequence(non_empty_sequences)\n",
    "\n",
    "    #     # Add the encoded array to the transformed dataset\n",
    "    #     transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "    #     transformed_dataset[column] = padded_sequences.tolist() # TODO: fix size mismatch error\n",
    "        \n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(dataset, reference_year = 3000):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = dataset.copy()\n",
    "\n",
    "    for column in dataset.columns:\n",
    "        # Decompose the date into its components\n",
    "        date = pd.to_datetime(dataset[column])\n",
    "\n",
    "        year = date.dt.year\n",
    "        # Check if the inital column contain a year\n",
    "        if not year.empty and not year.all() == 0:\n",
    "            year = torch.tensor(year/reference_year, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_year\"] = year\n",
    "\n",
    "        month = date.dt.month\n",
    "        if not month.empty and not month.all() == 0:\n",
    "            month = torch.tensor(month / 12, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_month\"] = month\n",
    "\n",
    "        day = date.dt.day\n",
    "        if not day.empty and not day.all() == 0:\n",
    "            day = torch.tensor(day / 31, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_day\"] = day\n",
    "\n",
    "        hour = date.dt.hour\n",
    "        if not hour.empty and not hour.all() == 0:\n",
    "            hour = torch.tensor(hour / 24, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_hour\"] = hour\n",
    "\n",
    "        minute = date.dt.minute\n",
    "        if not minute.empty and not minute.all() == 0:\n",
    "            minute = torch.tensor(minute / 60, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_minute\"] = minute\n",
    "\n",
    "        second = date.dt.second\n",
    "        if not second.empty and not second.all() == 0:\n",
    "            second = torch.tensor(second / 60, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_second\"] = second\n",
    "        \n",
    "\n",
    "        # Drop the original date column\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text columns: (1000, 16)\n",
      "\n",
      "Date columns: (1000, 6)\n",
      "\n",
      "Categorical columns: (1000, 4)\n",
      "\n",
      "Numerical columns: (1000, 8)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv dataset from the csv file\n",
    "dataset_path = 'datasets/fake_invoice_1000.csv'\n",
    "# Creating a DataFrame from the CSV data (replace this with your actual CSV file path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Process the dataset before creating the SupplierDataset\n",
    "text_columns, date_columns, categorical_columns, numerical_columns = separate_columns_types(df)\n",
    "print(f\"Text columns: {text_columns.shape}\\n\"),                 #print(text_columns.head())\n",
    "print(f\"Date columns: {date_columns.shape}\\n\"),                 #print(date_columns.head())\n",
    "print(f\"Categorical columns: {categorical_columns.shape}\\n\"),   #print(categorical_columns.head())\n",
    "print(f\"Numerical columns: {numerical_columns.shape}\\n\"),       #print(numerical_columns.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset: (1000, 30)\n",
      "\n",
      "   invoice_number  subtotal  tax_rate  tax_amount  discount_rate  \\\n",
      "0        0.248694  0.250331      0.15    0.038610           0.86   \n",
      "1        0.771873  0.368514      0.05    0.018907           0.30   \n",
      "2        0.330406  0.185498      0.20    0.038234           0.38   \n",
      "3        0.507838  0.653593      0.60    0.401614           0.90   \n",
      "4        0.311284  0.588111      0.85    0.512093           0.76   \n",
      "\n",
      "   discount_amount     total  exchange_rate payment_method        status  ...  \\\n",
      "0         0.226812  0.133221       0.977807      [0, 1, 0]  [0, 0, 0, 1]  ...   \n",
      "1         0.116232  0.281886       0.115933      [1, 0, 0]  [0, 0, 1, 0]  ...   \n",
      "2         0.074432  0.140233       0.882845      [1, 0, 0]  [0, 1, 0, 0]  ...   \n",
      "3         0.617249  0.389216       0.711891      [1, 0, 0]  [1, 0, 0, 0]  ...   \n",
      "4         0.469142  0.413138       0.172187      [0, 0, 1]  [0, 0, 1, 0]  ...   \n",
      "\n",
      "  created_at_day updated_at_year  updated_at_month  updated_at_day  \\\n",
      "0       0.645161           0.674          0.833333        0.967742   \n",
      "1       0.483871           0.674          0.250000        0.290323   \n",
      "2       1.000000           0.674          0.500000        0.032258   \n",
      "3       0.419355           0.674          0.250000        0.806452   \n",
      "4       0.935484           0.674          0.833333        0.419355   \n",
      "\n",
      "   due_date_year  due_date_month  due_date_day  paid_date_year  \\\n",
      "0          0.674        1.000000      0.290323           0.674   \n",
      "1          0.674        0.916667      0.322581           0.674   \n",
      "2          0.674        0.250000      0.709677           0.674   \n",
      "3          0.674        0.833333      0.483871           0.674   \n",
      "4          0.674        0.750000      0.516129           0.674   \n",
      "\n",
      "   paid_date_month  paid_date_day  \n",
      "0         0.916667       0.741935  \n",
      "1         0.500000       0.677419  \n",
      "2         0.916667       0.580645  \n",
      "3         0.750000       0.129032  \n",
      "4         0.333333       0.129032  \n",
      "\n",
      "[5 rows x 30 columns] \n",
      "\n",
      "\n",
      "Is there null values =>  False\n",
      "Is there nan values =>  False\n",
      "Is there empty strings =>  False\n"
     ]
    }
   ],
   "source": [
    "# Apply transformations to the categorical columns\n",
    "categorical_columns_treated, classes = transform_categorical(categorical_columns)\n",
    "#print(f\"Categorical columns after transformation: {categorical_columns_treated.shape}\\n\"),   print(categorical_columns_treated.head(), \"\\n\\n\")\n",
    "numerical_columns_treated = transform_numerical(numerical_columns)\n",
    "#print(f\"Numerical columns after transformation: {numerical_columns_treated.shape}\\n\"),       print(numerical_columns_treated.head(), \"\\n\\n\")\n",
    "text_columns_treated = transform_text(text_columns)\n",
    "#print(f\"Text columns after transformation: {text_columns_treated.shape}\\n\"),                 print(text_columns_treated.head(), \"\\n\\n\")\n",
    "date_columns_treated = transform_date(date_columns)\n",
    "#print(f\"Date columns after transformation: {date_columns_treated.shape}\\n\"),                 print(date_columns_treated.head(), \"\\n\\n\")\n",
    "\n",
    "# Concatenate the transformed columns\n",
    "#text_columns_treated\n",
    "df_treated = pd.concat([ numerical_columns_treated, categorical_columns_treated,date_columns_treated], axis=1)\n",
    "print(f\"Final dataset: {df_treated.shape}\\n\"), print(df_treated.head(), \"\\n\\n\")\n",
    "\n",
    "# Check if there is nan values or empty strings\n",
    "print(\"Is there null values => \", df_treated.isnull().values.any())\n",
    "print(\"Is there nan values => \", df_treated.isna().values.any())\n",
    "print(\"Is there empty strings => \", df_treated.isin(['']).values.any())\n",
    "\n",
    "# Create an instance of the SupplierDataset\n",
    "supplier_dataset = SupplierDataset(dataframe=df_treated)\n",
    "print(f\"Supplier dataset: {len(supplier_dataset)}\\n shape: {supplier_dataset[0].shape}\\n type: {type(supplier_dataset[0])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 800\n",
      "Validation samples: 200\n",
      "Batch size: 50\n",
      "\n",
      "Training samples:\n",
      "\n",
      "shape of element: 50 and type: <class 'torch.Tensor'>\n",
      "first element :\n",
      "tensor([[0.9350, 0.2189, 0.8500,  ..., 0.6740, 0.1667, 0.2903],\n",
      "        [0.7331, 0.8542, 0.2500,  ..., 0.6740, 0.5000, 0.4194],\n",
      "        [0.6526, 0.5327, 0.7000,  ..., 0.6740, 0.9167, 0.4516],\n",
      "        ...,\n",
      "        [0.8568, 0.1799, 0.0500,  ..., 0.6740, 0.4167, 0.7742],\n",
      "        [0.8262, 0.1671, 0.2000,  ..., 0.6740, 0.3333, 0.9677],\n",
      "        [0.5321, 0.5473, 1.0000,  ..., 0.6740, 0.9167, 0.5806]])\n",
      " end of first element\n"
     ]
    }
   ],
   "source": [
    "train_percentage = 0.2\n",
    "batch_size = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(supplier_dataset))\n",
    "val_size = len(supplier_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(supplier_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print some statistics about the dataset\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# Print 5 samples from the training dataset\n",
    "print(\"\\nTraining samples:\" + \"\\n\")\n",
    "e = iter(train_loader)\n",
    "element = next(e)\n",
    "print(f\"shape of element: {len(element)} and type: {type(element)}\")\n",
    "print(f\"first element :\\n\"+ str(element) + \"\\n end of first element\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Define dynamic layers for the encoder\n",
    "        # You can experiment with different architectures\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Parameters for the mean and log-variance of the latent space\n",
    "        self.fc_mean = nn.Linear(128, latent_size)\n",
    "        self.fc_logvar = nn.Linear(128, latent_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Calculate mean and log-variance for the latent space\n",
    "        mean = self.fc_mean(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        \n",
    "        return mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Define dynamic layers for the decoder\n",
    "        self.fc1 = nn.Linear(latent_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        \n",
    "        # Output layer for reconstruction\n",
    "        self.fc_out = nn.Linear(256, output_size)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = self.fc1(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        z = F.relu(z)\n",
    "        \n",
    "        # Output layer for reconstruction\n",
    "        x_recon = torch.sigmoid(self.fc_out(z))  # Assuming the data is normalized to [0, 1]\n",
    "        \n",
    "        return x_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_size, latent_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        # Create instances of the Encoder and Decoder\n",
    "        self.encoder = Encoder(input_size, latent_size)\n",
    "        self.decoder = Decoder(latent_size, input_size)\n",
    "        \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Reparameterization trick for sampling from a normal distribution\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through the encoder\n",
    "        mean, logvar = self.encoder.forward(x)\n",
    "        print(f\"forward mean : {mean} and logvar : {logvar}\")\n",
    "        \n",
    "        # Sample from the latent space using the reparameterization trick\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        \n",
    "        # Forward pass through the decoder\n",
    "        x_recon = self.decoder.forward(z)\n",
    "        \n",
    "        return x_recon, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
    "\n",
    "    # Reconstruction Loss (e.g., Mean Squared Error or Binary Cross Entropy)\n",
    "    reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')  # Replace with appropriate loss function\n",
    "\n",
    "    # KL Divergence Loss\n",
    "    kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    # Combine the Reconstruction Loss and KL Divergence Loss with the weighting factor (beta)\n",
    "    total_loss = reconstruction_loss + beta * kl_divergence\n",
    "        \n",
    "    return total_loss, reconstruction_loss, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checker(recon_data, mu, logvar):\n",
    "    # Recon data\n",
    "    if(recon_data is None):\n",
    "        print(\"recon_data is None\")\n",
    "        return False\n",
    "    if(recon_data.shape[0] != batch_size):\n",
    "        print(\"recon_data shape is not batch_size\")\n",
    "        return False\n",
    "       # check for nan values\n",
    "    if(torch.isnan(recon_data).any()):\n",
    "        print(\"recon_data has nan values\")\n",
    "        return False\n",
    "    # check for inf values\n",
    "    if(torch.isinf(recon_data).any()):\n",
    "        print(\"recon_data has inf values\")\n",
    "        return False\n",
    "    # check for negative values\n",
    "    if((recon_data < 0).any()):\n",
    "        print(\"recon_data has negative values\")\n",
    "        return False\n",
    "    # check for values greater than 1\n",
    "    if((recon_data > 1).any()):\n",
    "        print(\"recon_data has values greater than 1\")\n",
    "        return False\n",
    "    # check for null values\n",
    "    if(recon_data.isnull().values.any()):\n",
    "        print(\"recon_data has null values\")\n",
    "        return False\n",
    "    \n",
    "    # mu\n",
    "    if(mu is None):\n",
    "        print(\"mu is None\")\n",
    "        return False\n",
    "    if(mu.shape[0] != batch_size):\n",
    "        print(\"mu shape is not batch_size\")\n",
    "        return False\n",
    "    # check for nan values\n",
    "    if(torch.isnan(mu).any()):\n",
    "        print(\"mu has nan values\")\n",
    "        return False\n",
    "    # check for inf values\n",
    "    if(torch.isinf(mu).any()):\n",
    "        print(\"mu has inf values\")\n",
    "        return False\n",
    "    # check for null values\n",
    "    if(mu.isnull().values.any()):\n",
    "        print(\"mu has null values\")\n",
    "        return False\n",
    "    \n",
    "    # logvar\n",
    "    if(logvar is None):\n",
    "        print(\"logvar is None\")\n",
    "        return False\n",
    "    if(logvar.shape[0] != batch_size):\n",
    "        print(\"logvar shape is not batch_size\")\n",
    "        return False\n",
    "    # check for nan values\n",
    "    if(torch.isnan(logvar).any()):\n",
    "        print(\"logvar has nan values\")\n",
    "        return False\n",
    "    # check for inf values\n",
    "    if(torch.isinf(logvar).any()):\n",
    "        print(\"logvar has inf values\")\n",
    "        return False\n",
    "    # check for null values\n",
    "    if(logvar.isnull().values.any()):\n",
    "        print(\"logvar has null values\")\n",
    "        return False\n",
    "    \n",
    "    return True   \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae, train_loader, num_epochs=10, learning_rate=1e-3, beta=1.0, device='cuda', with_checker=False):\n",
    "    # Move the model to the specified device (cuda or cpu)\n",
    "    vae.to(device)\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        recon_loss = 0.0\n",
    "        kl_loss = 0.0\n",
    "        \n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            # Move the batch to the specified device\n",
    "            data = data.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through the VAE\n",
    "            recon_data, mu, logvar = vae.forward(data)\n",
    "            \n",
    "            # Check the model return values\n",
    "            if(with_checker):\n",
    "                if(not checker(recon_data, mu, logvar)):\n",
    "                    return\n",
    "            \n",
    "            # Calculate the VAE loss\n",
    "            loss, recon_loss_batch, kl_loss_batch = loss_function(recon_data, data, mu, logvar, beta=beta)\n",
    "            \n",
    "            # Backward pass and optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the running total of losses\n",
    "            total_loss += loss.item()\n",
    "            recon_loss += recon_loss_batch.item()\n",
    "            kl_loss += kl_loss_batch.item()\n",
    "            \n",
    "            # Print logs every N batches (you can adjust this value)\n",
    "            log_interval = 100\n",
    "            if batch_idx % log_interval == 0 and batch_idx > 0:\n",
    "                avg_loss = total_loss / log_interval\n",
    "                avg_recon_loss = recon_loss / log_interval\n",
    "                avg_kl_loss = kl_loss / log_interval\n",
    "                \n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], '\n",
    "                      f'Avg. Loss: {avg_loss:.4f}, Avg. Recon Loss: {avg_recon_loss:.4f}, Avg. KL Loss: {avg_kl_loss:.4f}')\n",
    "                \n",
    "                total_loss = 0.0\n",
    "                recon_loss = 0.0\n",
    "                kl_loss = 0.0\n",
    "                \n",
    "        # Print epoch-level logs\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Avg. Total Loss: {total_loss:.4f}')\n",
    "        \n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim : 139\n"
     ]
    }
   ],
   "source": [
    "# create model and optimizer\n",
    "# Get the input dimension from the training dataset\n",
    "input_dim =  len(train_dataset[1])\n",
    "print(f\"input_dim : {input_dim}\")\n",
    "# Get the latent dimension from the input dimension\n",
    "latent_dim = input_dim // 2\n",
    "model = VAE(input_dim, latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim: 139 and latent_dim: 69\n",
      "model: VAE(\n",
      "  (encoder): Encoder(\n",
      "    (fc1): Linear(in_features=139, out_features=256, bias=True)\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (fc_mean): Linear(in_features=128, out_features=69, bias=True)\n",
      "    (fc_logvar): Linear(in_features=128, out_features=69, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (fc1): Linear(in_features=69, out_features=128, bias=True)\n",
      "    (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (fc_out): Linear(in_features=256, out_features=139, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"input_dim: {input_dim} and latent_dim: {latent_dim}\")\n",
    "print(f\"model: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward mean : tensor([[ 0.0193,  0.0156, -0.0095,  ...,  0.0531, -0.0202,  0.0627],\n",
      "        [ 0.0225,  0.0098, -0.0196,  ...,  0.0667, -0.0080,  0.0576],\n",
      "        [ 0.0133,  0.0223, -0.0038,  ...,  0.0908, -0.0010,  0.0624],\n",
      "        ...,\n",
      "        [ 0.0257,  0.0089, -0.0065,  ...,  0.0471,  0.0028,  0.0639],\n",
      "        [-0.0072,  0.0446, -0.0192,  ...,  0.0764, -0.0215,  0.0494],\n",
      "        [ 0.0184,  0.0247, -0.0380,  ...,  0.0545,  0.0099,  0.0720]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0083,  0.0380,  0.0481,  ...,  0.0751, -0.0223,  0.0024],\n",
      "        [ 0.0349,  0.0403,  0.0624,  ...,  0.1108,  0.0102,  0.0086],\n",
      "        [ 0.0388,  0.0545,  0.0289,  ...,  0.0705,  0.0121, -0.0189],\n",
      "        ...,\n",
      "        [ 0.0298,  0.0333,  0.0354,  ...,  0.0969,  0.0169,  0.0165],\n",
      "        [ 0.0099,  0.0324,  0.0314,  ...,  0.0870,  0.0182,  0.0032],\n",
      "        [ 0.0402,  0.0545,  0.0195,  ...,  0.0903,  0.0179,  0.0121]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0140,  0.0168, -0.0305,  ...,  0.0429, -0.0013,  0.0421],\n",
      "        [ 0.0062,  0.0311, -0.0426,  ...,  0.0474, -0.0028,  0.0519],\n",
      "        [ 0.0235,  0.0143, -0.0147,  ...,  0.0550, -0.0173,  0.0417],\n",
      "        ...,\n",
      "        [ 0.0162,  0.0175, -0.0307,  ...,  0.0296,  0.0126,  0.0328],\n",
      "        [ 0.0061,  0.0194, -0.0216,  ...,  0.0208, -0.0172,  0.0442],\n",
      "        [-0.0077,  0.0326, -0.0313,  ...,  0.0400,  0.0036,  0.0510]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0044,  0.0465,  0.0079,  ...,  0.0653,  0.0146, -0.0045],\n",
      "        [ 0.0003,  0.0431,  0.0017,  ...,  0.0646,  0.0317, -0.0186],\n",
      "        [ 0.0094,  0.0342,  0.0069,  ...,  0.0814,  0.0056, -0.0269],\n",
      "        ...,\n",
      "        [ 0.0260,  0.0530,  0.0148,  ...,  0.0605,  0.0126, -0.0021],\n",
      "        [-0.0038,  0.0180,  0.0100,  ...,  0.0624,  0.0091, -0.0150],\n",
      "        [ 0.0114,  0.0549, -0.0126,  ...,  0.0659,  0.0299, -0.0250]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0024,  0.0309, -0.0175,  ...,  0.0017, -0.0075,  0.0538],\n",
      "        [ 0.0315, -0.0021, -0.0516,  ...,  0.0020,  0.0108,  0.0455],\n",
      "        [ 0.0038, -0.0010, -0.0454,  ...,  0.0134, -0.0061,  0.0364],\n",
      "        ...,\n",
      "        [ 0.0230,  0.0241, -0.0224,  ...,  0.0230,  0.0336,  0.0670],\n",
      "        [-0.0014, -0.0027, -0.0427,  ...,  0.0088,  0.0057,  0.0382],\n",
      "        [-0.0031,  0.0043, -0.0070,  ...,  0.0171, -0.0023,  0.0499]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0013,  0.0113, -0.0131,  ...,  0.0678,  0.0090, -0.0193],\n",
      "        [-0.0186,  0.0382, -0.0068,  ...,  0.0571,  0.0203, -0.0121],\n",
      "        [ 0.0015,  0.0535,  0.0246,  ...,  0.0588,  0.0074, -0.0157],\n",
      "        ...,\n",
      "        [ 0.0259,  0.0441,  0.0059,  ...,  0.0521,  0.0264, -0.0196],\n",
      "        [-0.0289,  0.0223, -0.0040,  ...,  0.0764,  0.0283,  0.0010],\n",
      "        [-0.0016,  0.0474, -0.0032,  ...,  0.0452, -0.0068, -0.0226]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 2.3019e-02, -3.6715e-02, -6.6207e-02,  ..., -8.6961e-03,\n",
      "          9.8038e-03,  2.1499e-02],\n",
      "        [ 5.2755e-03, -1.9208e-02, -4.9893e-02,  ...,  1.4159e-02,\n",
      "          1.5823e-03,  6.0619e-03],\n",
      "        [-3.4760e-03, -1.9772e-02, -3.7892e-02,  ..., -6.5038e-03,\n",
      "          3.9715e-03,  1.5020e-02],\n",
      "        ...,\n",
      "        [ 3.2002e-05,  2.5472e-03, -3.3872e-02,  ...,  2.8184e-02,\n",
      "         -8.6087e-03,  3.4240e-02],\n",
      "        [-1.3959e-02,  1.3703e-02, -4.2484e-02,  ...,  9.3880e-04,\n",
      "          1.9984e-02,  3.2704e-02],\n",
      "        [-1.3426e-02,  8.1824e-03, -3.7788e-02,  ..., -1.6027e-02,\n",
      "          1.4795e-02,  2.9533e-02]], grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0408,  0.0013, -0.0009,  ...,  0.0573,  0.0277, -0.0182],\n",
      "        [-0.0143,  0.0383,  0.0114,  ...,  0.0611,  0.0119, -0.0192],\n",
      "        [-0.0310,  0.0040,  0.0031,  ...,  0.0502, -0.0026, -0.0153],\n",
      "        ...,\n",
      "        [ 0.0008,  0.0303,  0.0110,  ...,  0.0497, -0.0056, -0.0201],\n",
      "        [-0.0110,  0.0178, -0.0114,  ...,  0.0456,  0.0185, -0.0251],\n",
      "        [-0.0073,  0.0247, -0.0019,  ...,  0.0582,  0.0144, -0.0062]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0080, -0.0299, -0.0435,  ..., -0.0114,  0.0004,  0.0114],\n",
      "        [-0.0259,  0.0064, -0.0415,  ..., -0.0144,  0.0025,  0.0077],\n",
      "        [-0.0062, -0.0343, -0.0468,  ..., -0.0032, -0.0004, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0031, -0.0029, -0.0410,  ...,  0.0047,  0.0036,  0.0114],\n",
      "        [ 0.0198, -0.0230, -0.0179,  ...,  0.0176,  0.0065,  0.0095],\n",
      "        [-0.0224,  0.0167, -0.0316,  ...,  0.0088,  0.0068,  0.0428]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0380,  0.0131,  0.0210,  ...,  0.0284,  0.0051, -0.0358],\n",
      "        [-0.0113,  0.0004, -0.0012,  ...,  0.0584,  0.0128, -0.0121],\n",
      "        [-0.0318, -0.0002,  0.0018,  ...,  0.0275,  0.0243, -0.0151],\n",
      "        ...,\n",
      "        [-0.0095,  0.0051,  0.0016,  ...,  0.0446,  0.0312, -0.0302],\n",
      "        [-0.0203,  0.0043,  0.0449,  ...,  0.0392, -0.0161, -0.0459],\n",
      "        [-0.0146,  0.0265, -0.0367,  ...,  0.0335,  0.0116, -0.0294]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[-0.0023, -0.0154, -0.0347,  ..., -0.0627,  0.0402,  0.0123],\n",
      "        [-0.0182,  0.0041, -0.0491,  ..., -0.0060,  0.0324,  0.0039],\n",
      "        [-0.0115, -0.0287, -0.0603,  ..., -0.0165,  0.0349, -0.0071],\n",
      "        ...,\n",
      "        [ 0.0073, -0.0043, -0.0285,  ..., -0.0344,  0.0214,  0.0023],\n",
      "        [ 0.0029,  0.0120, -0.0525,  ..., -0.0047,  0.0291,  0.0173],\n",
      "        [-0.0358, -0.0053, -0.0678,  ..., -0.0173,  0.0266,  0.0161]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0416, -0.0061,  0.0103,  ...,  0.0293,  0.0185, -0.0319],\n",
      "        [ 0.0025,  0.0151,  0.0086,  ...,  0.0336,  0.0275, -0.0150],\n",
      "        [-0.0347,  0.0134, -0.0088,  ...,  0.0030,  0.0396,  0.0004],\n",
      "        ...,\n",
      "        [-0.0185,  0.0106, -0.0087,  ...,  0.0430,  0.0296, -0.0178],\n",
      "        [-0.0091,  0.0042,  0.0300,  ...,  0.0335,  0.0087, -0.0448],\n",
      "        [-0.0081,  0.0179, -0.0232,  ...,  0.0190,  0.0209, -0.0195]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0041, -0.0011, -0.0538,  ..., -0.0014,  0.0201, -0.0071],\n",
      "        [-0.0256, -0.0099, -0.0384,  ..., -0.0257,  0.0395, -0.0201],\n",
      "        [-0.0007,  0.0069, -0.0483,  ..., -0.0042,  0.0605, -0.0204],\n",
      "        ...,\n",
      "        [-0.0210,  0.0047, -0.0361,  ..., -0.0050,  0.0500,  0.0185],\n",
      "        [-0.0285, -0.0118, -0.0384,  ..., -0.0133,  0.0462, -0.0157],\n",
      "        [-0.0018,  0.0053, -0.0216,  ..., -0.0212,  0.0276, -0.0067]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0187, -0.0170,  0.0033,  ...,  0.0226,  0.0223, -0.0272],\n",
      "        [-0.0162, -0.0114,  0.0300,  ...,  0.0224,  0.0348, -0.0033],\n",
      "        [-0.0113, -0.0102,  0.0103,  ...,  0.0377,  0.0457, -0.0101],\n",
      "        ...,\n",
      "        [-0.0170,  0.0227, -0.0095,  ...,  0.0104,  0.0213, -0.0428],\n",
      "        [-0.0076,  0.0016,  0.0094,  ...,  0.0342,  0.0291,  0.0065],\n",
      "        [-0.0156, -0.0050,  0.0052,  ...,  0.0426,  0.0324, -0.0178]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 9.9585e-03, -1.2161e-02, -1.6489e-02,  ..., -2.4796e-02,\n",
      "          2.6371e-02,  3.6008e-03],\n",
      "        [-1.1853e-02, -3.8418e-03, -3.7111e-02,  ..., -1.1015e-02,\n",
      "          3.2492e-02, -1.4983e-02],\n",
      "        [-1.4566e-02,  1.5337e-02, -3.2620e-02,  ..., -5.3287e-05,\n",
      "          5.1613e-02, -2.8336e-03],\n",
      "        ...,\n",
      "        [-1.1539e-02, -1.6667e-02, -1.7213e-02,  ..., -3.3190e-02,\n",
      "          2.8244e-02,  3.2400e-03],\n",
      "        [-3.1842e-02, -4.7104e-03, -4.8550e-02,  ..., -3.0595e-02,\n",
      "          3.5896e-02, -5.4810e-03],\n",
      "        [-1.0684e-02,  7.9742e-03, -1.3585e-02,  ..., -2.6972e-02,\n",
      "          4.5687e-02,  5.4947e-03]], grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0231,  0.0014,  0.0294,  ...,  0.0304,  0.0349, -0.0466],\n",
      "        [-0.0113,  0.0001,  0.0122,  ...,  0.0039,  0.0442, -0.0212],\n",
      "        [-0.0166, -0.0110,  0.0046,  ...,  0.0336,  0.0143, -0.0268],\n",
      "        ...,\n",
      "        [-0.0366, -0.0277,  0.0082,  ..., -0.0008,  0.0386, -0.0283],\n",
      "        [-0.0301, -0.0270, -0.0133,  ...,  0.0160,  0.0313, -0.0162],\n",
      "        [-0.0121, -0.0046,  0.0240,  ...,  0.0017,  0.0292, -0.0387]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[-0.0195,  0.0120, -0.0575,  ..., -0.0050,  0.0545, -0.0357],\n",
      "        [-0.0051, -0.0161, -0.0219,  ..., -0.0225,  0.0559, -0.0509],\n",
      "        [-0.0245,  0.0255, -0.0375,  ...,  0.0001,  0.0240, -0.0269],\n",
      "        ...,\n",
      "        [-0.0112, -0.0244, -0.0402,  ..., -0.0205,  0.0399, -0.0166],\n",
      "        [-0.0117, -0.0101, -0.0396,  ..., -0.0227,  0.0491, -0.0231],\n",
      "        [-0.0151,  0.0440, -0.0434,  ...,  0.0065,  0.0396, -0.0098]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[ 0.0072, -0.0079,  0.0368,  ...,  0.0132,  0.0396, -0.0078],\n",
      "        [-0.0133, -0.0362,  0.0390,  ...,  0.0183,  0.0473, -0.0209],\n",
      "        [-0.0020, -0.0314,  0.0265,  ...,  0.0256,  0.0251, -0.0267],\n",
      "        ...,\n",
      "        [-0.0213, -0.0143,  0.0301,  ..., -0.0004,  0.0274, -0.0170],\n",
      "        [-0.0313, -0.0219,  0.0164,  ...,  0.0095,  0.0450, -0.0224],\n",
      "        [ 0.0075, -0.0258,  0.0031,  ...,  0.0173,  0.0321, -0.0287]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0035, -0.0162, -0.0444,  ...,  0.0192,  0.0586, -0.0347],\n",
      "        [-0.0214,  0.0164, -0.0170,  ..., -0.0509,  0.0601, -0.0408],\n",
      "        [-0.0184,  0.0187, -0.0169,  ..., -0.0102,  0.0636, -0.0326],\n",
      "        ...,\n",
      "        [-0.0013, -0.0073, -0.0162,  ..., -0.0145,  0.0673, -0.0522],\n",
      "        [-0.0142, -0.0021, -0.0256,  ..., -0.0135,  0.0406, -0.0460],\n",
      "        [-0.0066,  0.0152, -0.0191,  ..., -0.0028,  0.0473, -0.0122]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[-0.0108, -0.0046,  0.0486,  ...,  0.0080,  0.0653, -0.0226],\n",
      "        [-0.0068, -0.0464,  0.0372,  ...,  0.0018,  0.0624, -0.0140],\n",
      "        [-0.0075, -0.0053,  0.0372,  ..., -0.0085,  0.0482, -0.0236],\n",
      "        ...,\n",
      "        [-0.0095, -0.0354,  0.0497,  ...,  0.0022,  0.0591, -0.0262],\n",
      "        [-0.0078, -0.0505,  0.0392,  ...,  0.0142,  0.0420, -0.0110],\n",
      "        [-0.0074, -0.0036,  0.0526,  ...,  0.0420,  0.0407, -0.0290]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[-0.0160,  0.0109, -0.0202,  ..., -0.0255,  0.0521, -0.0523],\n",
      "        [ 0.0033,  0.0069, -0.0229,  ..., -0.0007,  0.0579, -0.0395],\n",
      "        [-0.0149,  0.0081, -0.0370,  ..., -0.0110,  0.0648, -0.0505],\n",
      "        ...,\n",
      "        [ 0.0118,  0.0245, -0.0343,  ..., -0.0025,  0.0638, -0.0370],\n",
      "        [ 0.0091,  0.0139, -0.0160,  ..., -0.0051,  0.0715, -0.0363],\n",
      "        [-0.0027,  0.0057, -0.0150,  ..., -0.0324,  0.0784, -0.0499]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[ 1.7749e-02, -3.9999e-02,  7.1794e-02,  ..., -2.8012e-03,\n",
      "          6.7061e-02, -6.7937e-03],\n",
      "        [ 1.3029e-02, -1.8124e-02,  4.5539e-02,  ...,  2.0598e-02,\n",
      "          4.1825e-02, -1.7261e-02],\n",
      "        [ 1.1416e-02, -2.2159e-02,  7.7352e-02,  ...,  3.7979e-05,\n",
      "          6.0075e-02, -9.9521e-03],\n",
      "        ...,\n",
      "        [ 2.2042e-02, -3.4453e-02,  7.0756e-02,  ...,  2.6163e-02,\n",
      "          7.7266e-02, -2.3295e-02],\n",
      "        [-4.8528e-03, -1.9150e-02,  5.4997e-02,  ...,  1.1046e-02,\n",
      "          4.8606e-02, -3.6792e-02],\n",
      "        [ 1.1397e-02, -3.4634e-02,  8.1718e-02,  ...,  1.0530e-02,\n",
      "          7.4326e-02,  1.8313e-04]], grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[-0.0013,  0.0408, -0.0484,  ...,  0.0150,  0.0835, -0.0354],\n",
      "        [-0.0074,  0.0351, -0.0454,  ...,  0.0098,  0.0733, -0.0497],\n",
      "        [ 0.0164,  0.0008, -0.0017,  ..., -0.0361,  0.0842, -0.0345],\n",
      "        ...,\n",
      "        [ 0.0180,  0.0357, -0.0288,  ...,  0.0067,  0.0903, -0.0248],\n",
      "        [-0.0071,  0.0302, -0.0459,  ...,  0.0024,  0.0621, -0.0563],\n",
      "        [-0.0042,  0.0114, -0.0387,  ...,  0.0030,  0.0780, -0.0369]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[ 0.0374,  0.0134,  0.0696,  ...,  0.0044,  0.0704, -0.0059],\n",
      "        [ 0.0408, -0.0345,  0.0775,  ...,  0.0117,  0.0653,  0.0186],\n",
      "        [-0.0093, -0.0280,  0.0609,  ...,  0.0247,  0.0687,  0.0095],\n",
      "        ...,\n",
      "        [ 0.0492, -0.0296,  0.0845,  ...,  0.0112,  0.0761,  0.0096],\n",
      "        [ 0.0391, -0.0338,  0.0889,  ...,  0.0201,  0.0659,  0.0123],\n",
      "        [ 0.0212, -0.0339,  0.0628,  ...,  0.0152,  0.0599,  0.0203]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0162,  0.0390, -0.0373,  ...,  0.0336,  0.0932, -0.0527],\n",
      "        [-0.0036,  0.0334, -0.0232,  ...,  0.0146,  0.0692, -0.0243],\n",
      "        [ 0.0197,  0.0027, -0.0084,  ...,  0.0173,  0.0513, -0.0377],\n",
      "        ...,\n",
      "        [ 0.0232,  0.0413, -0.0300,  ...,  0.0125,  0.0894, -0.0348],\n",
      "        [ 0.0131,  0.0063, -0.0273,  ...,  0.0234,  0.0675, -0.0426],\n",
      "        [-0.0019,  0.0211, -0.0199,  ...,  0.0092,  0.0605, -0.0202]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[ 0.0500, -0.0090,  0.0821,  ...,  0.0073,  0.1000,  0.0134],\n",
      "        [ 0.0421,  0.0014,  0.1027,  ..., -0.0034,  0.0948,  0.0054],\n",
      "        [ 0.0389, -0.0328,  0.0979,  ...,  0.0106,  0.0810,  0.0102],\n",
      "        ...,\n",
      "        [ 0.0437,  0.0037,  0.1035,  ...,  0.0185,  0.0872,  0.0184],\n",
      "        [ 0.0440, -0.0272,  0.0988,  ...,  0.0248,  0.0760,  0.0266],\n",
      "        [ 0.0466, -0.0147,  0.1020,  ...,  0.0124,  0.0794, -0.0011]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 2.8698e-02,  4.5195e-02, -3.0366e-02,  ...,  4.4735e-02,\n",
      "          6.4590e-02, -1.1417e-02],\n",
      "        [-6.7554e-05,  3.9937e-02, -3.7170e-02,  ...,  2.3363e-02,\n",
      "          7.5256e-02, -1.8228e-02],\n",
      "        [ 4.5955e-02,  2.1848e-02,  1.5216e-02,  ...,  1.6783e-02,\n",
      "          6.8335e-02, -2.7010e-02],\n",
      "        ...,\n",
      "        [ 2.4690e-02,  2.7727e-02, -3.9188e-02,  ...,  2.4982e-02,\n",
      "          6.5274e-02, -3.2542e-02],\n",
      "        [ 1.7595e-02,  3.3756e-02, -1.6685e-02,  ...,  2.8479e-02,\n",
      "          7.4904e-02, -1.3622e-02],\n",
      "        [ 3.0334e-02,  5.1598e-03, -1.2645e-02,  ...,  1.9993e-02,\n",
      "          6.9545e-02, -4.1911e-02]], grad_fn=<AddmmBackward0>) and logvar : tensor([[ 0.0377, -0.0128,  0.0890,  ...,  0.0245,  0.0874,  0.0241],\n",
      "        [ 0.0623,  0.0069,  0.0998,  ...,  0.0200,  0.1023,  0.0204],\n",
      "        [ 0.0465, -0.0352,  0.1148,  ...,  0.0271,  0.0924,  0.0116],\n",
      "        ...,\n",
      "        [ 0.0596, -0.0129,  0.0933,  ...,  0.0224,  0.0967,  0.0259],\n",
      "        [ 0.0621,  0.0174,  0.0995,  ...,  0.0266,  0.1100,  0.0214],\n",
      "        [ 0.0397, -0.0243,  0.1121,  ...,  0.0325,  0.0856,  0.0274]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0352,  0.0237, -0.0129,  ...,  0.0386,  0.0667, -0.0191],\n",
      "        [ 0.0424,  0.0552, -0.0248,  ...,  0.0593,  0.0666, -0.0247],\n",
      "        [ 0.0445,  0.0544, -0.0149,  ...,  0.0589,  0.0695, -0.0272],\n",
      "        ...,\n",
      "        [ 0.0389,  0.0461,  0.0097,  ...,  0.0451,  0.0646, -0.0178],\n",
      "        [ 0.0547,  0.0139, -0.0260,  ...,  0.0649,  0.0812, -0.0298],\n",
      "        [ 0.0368,  0.0550, -0.0023,  ...,  0.0440,  0.0726, -0.0254]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[ 0.0514,  0.0008,  0.1194,  ...,  0.0409,  0.1041,  0.0367],\n",
      "        [ 0.0876, -0.0259,  0.1528,  ...,  0.0384,  0.1077,  0.0448],\n",
      "        [ 0.0814,  0.0018,  0.1287,  ...,  0.0189,  0.0969,  0.0155],\n",
      "        ...,\n",
      "        [ 0.0794, -0.0109,  0.1293,  ...,  0.0408,  0.1144,  0.0184],\n",
      "        [ 0.0740, -0.0091,  0.1515,  ...,  0.0201,  0.1096,  0.0396],\n",
      "        [ 0.0862, -0.0203,  0.1453,  ...,  0.0284,  0.1133,  0.0373]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "forward mean : tensor([[ 0.0542,  0.0239, -0.0333,  ...,  0.0650,  0.0787, -0.0035],\n",
      "        [ 0.0704,  0.0569,  0.0031,  ...,  0.0591,  0.0855, -0.0056],\n",
      "        [ 0.0759,  0.0501,  0.0060,  ...,  0.0624,  0.0733, -0.0098],\n",
      "        ...,\n",
      "        [ 0.0324,  0.0545, -0.0159,  ...,  0.0625,  0.0557,  0.0023],\n",
      "        [ 0.0465,  0.0492,  0.0003,  ...,  0.0536,  0.0509,  0.0149],\n",
      "        [ 0.0355,  0.0537, -0.0146,  ...,  0.0696,  0.0489, -0.0050]],\n",
      "       grad_fn=<AddmmBackward0>) and logvar : tensor([[0.0804, 0.0192, 0.1359,  ..., 0.0452, 0.1032, 0.0603],\n",
      "        [0.0794, 0.0301, 0.1422,  ..., 0.0576, 0.0900, 0.0248],\n",
      "        [0.0951, 0.0115, 0.1383,  ..., 0.0702, 0.0939, 0.0329],\n",
      "        ...,\n",
      "        [0.0816, 0.0421, 0.1271,  ..., 0.0657, 0.1005, 0.0582],\n",
      "        [0.0951, 0.0286, 0.1424,  ..., 0.0767, 0.1128, 0.0482],\n",
      "        [0.0877, 0.0141, 0.1353,  ..., 0.0444, 0.1044, 0.0373]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Epoch [1/1], Avg. Total Loss: 15953.1074\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "train(model, train_loader, num_epochs=1, device=device, with_checker=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a new data to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_headers(headers, generated_data):\n",
    "    # Add the headers to the generated data\n",
    "    return pd.DataFrame(generated_data, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new sample\n",
    "def generate_sample(model, headers):\n",
    "    sample = torch.randn(1, model.latent_size)\n",
    "    generated_data = model.decoder.forward(sample).detach().numpy()\n",
    "    return generated_data #add_headers(headers, generated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 139)\n",
      "(1000, 30)\n"
     ]
    }
   ],
   "source": [
    "new_sample = generate_sample(model, df_treated.columns)\n",
    "\n",
    "print(new_sample.shape)\n",
    "print(df_treated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Supplier Data:\n",
      "[[0.5342029  0.5519393  0.5497189  0.20515414 0.48245504 0.22607756\n",
      "  0.30071917 0.543737   0.3065052  0.26441836 0.29737964 0.28223765\n",
      "  0.22280303 0.20591065 0.23592944 0.16978751 0.17597856 0.14744556\n",
      "  0.16255693 0.15990284 0.15269719 0.20595558 0.16248208 0.22454111\n",
      "  0.16787572 0.15482032 0.19754596 0.17155553 0.1989674  0.1466004\n",
      "  0.18269566 0.14064354 0.1833571  0.20478648 0.14259851 0.22759137\n",
      "  0.25455472 0.18321115 0.207167   0.20940945 0.18008578 0.19872123\n",
      "  0.1546141  0.11690183 0.14975587 0.16391425 0.11213035 0.21916316\n",
      "  0.25276402 0.17762621 0.15544    0.17156601 0.17446774 0.21797039\n",
      "  0.17553568 0.18889941 0.1929721  0.16285451 0.1537066  0.1361907\n",
      "  0.13263907 0.1694097  0.19437777 0.18175863 0.15365958 0.1731075\n",
      "  0.14597806 0.18833655 0.12817836 0.1696679  0.13386744 0.17309639\n",
      "  0.1268606  0.15723407 0.21049902 0.14503185 0.1425296  0.25317353\n",
      "  0.17632751 0.12248074 0.17057529 0.19498128 0.14029211 0.22199364\n",
      "  0.1538278  0.1781798  0.17723197 0.15691304 0.1727551  0.1731361\n",
      "  0.14990754 0.22466181 0.15681943 0.1495048  0.22594666 0.16488403\n",
      "  0.18217207 0.15646504 0.1712014  0.1919989  0.19808497 0.20106082\n",
      "  0.19087376 0.19165194 0.19875243 0.29432034 0.21930882 0.22754097\n",
      "  0.16078919 0.18195327 0.24308093 0.18435346 0.14051735 0.212967\n",
      "  0.21004455 0.2141668  0.16967238 0.17362007 0.3120678  0.30891353\n",
      "  0.31645465 0.75935155 0.5459515  0.52404034 0.73950684 0.61319053\n",
      "  0.5207646  0.69977593 0.51484084 0.5241192  0.7252669  0.57900804\n",
      "  0.5128204  0.7680345  0.50261354 0.46968502 0.7923303  0.59055203\n",
      "  0.5393356 ]]\n",
      "[[0.5180449  0.54078186 0.5352772  0.27433294 0.47775117 0.2567676\n",
      "  0.34230706 0.5293065  0.27540955 0.30128112 0.32983127 0.30996546\n",
      "  0.3151274  0.26099455 0.27104288 0.22355647 0.22618534 0.2120549\n",
      "  0.19543742 0.20778358 0.22365041 0.24185576 0.22433656 0.2718719\n",
      "  0.2259457  0.20235865 0.24191055 0.23297055 0.23375246 0.19915445\n",
      "  0.20821148 0.20922546 0.21758343 0.26486355 0.21795769 0.25779533\n",
      "  0.27500764 0.2422482  0.2699236  0.26029044 0.22386628 0.19479857\n",
      "  0.20223029 0.15801828 0.20260409 0.23702434 0.16390385 0.30657282\n",
      "  0.29702252 0.2236652  0.19885892 0.21441676 0.23301409 0.24588385\n",
      "  0.25634632 0.25566146 0.22068429 0.2226783  0.21873847 0.19802071\n",
      "  0.20298266 0.22603488 0.2640823  0.24685849 0.21191968 0.21881022\n",
      "  0.20276251 0.2179752  0.16566418 0.20355426 0.20339604 0.22271872\n",
      "  0.16945139 0.19793494 0.24015634 0.21269701 0.1844015  0.26987293\n",
      "  0.20876577 0.20129874 0.21843135 0.28578034 0.15569273 0.23460892\n",
      "  0.21827371 0.25396588 0.26069275 0.19871153 0.2303114  0.23848528\n",
      "  0.19651388 0.25008547 0.18884656 0.20068912 0.3016902  0.21440385\n",
      "  0.24884616 0.22884814 0.24994178 0.22164348 0.23818102 0.26579615\n",
      "  0.23691812 0.2071022  0.2780393  0.29567322 0.2663204  0.2983969\n",
      "  0.21753913 0.21370381 0.27877727 0.27794454 0.22496822 0.24899122\n",
      "  0.2280158  0.29167527 0.22000644 0.23508823 0.31847554 0.3273086\n",
      "  0.34202722 0.7351327  0.5434158  0.5190858  0.6878034  0.5839682\n",
      "  0.49543193 0.6752548  0.54466414 0.519221   0.6968747  0.5947063\n",
      "  0.51562524 0.7047619  0.5289991  0.49525222 0.73178786 0.571138\n",
      "  0.467062  ]]\n",
      "[[0.5312519  0.60352284 0.48944175 0.16682987 0.52182025 0.17255461\n",
      "  0.25459847 0.5328389  0.22774413 0.29996073 0.22766232 0.29225376\n",
      "  0.18766275 0.16449307 0.17356332 0.13772655 0.11705185 0.12227456\n",
      "  0.12122812 0.12801334 0.12457393 0.16175729 0.13923243 0.19329648\n",
      "  0.11820807 0.12007804 0.14303952 0.14606103 0.13054591 0.12262774\n",
      "  0.15787303 0.10605974 0.11841835 0.18049301 0.12287383 0.1608457\n",
      "  0.17231983 0.14585178 0.15230638 0.16999234 0.1407079  0.12780729\n",
      "  0.16713884 0.08373766 0.0839717  0.14647855 0.08966214 0.19967474\n",
      "  0.18883422 0.11037464 0.12441645 0.11914108 0.16073957 0.19822668\n",
      "  0.17667536 0.20681885 0.15115403 0.11107973 0.13135204 0.12447146\n",
      "  0.1196329  0.13342442 0.18852153 0.11768337 0.10694014 0.1399014\n",
      "  0.10909901 0.14565527 0.10023306 0.10742413 0.12553726 0.13574706\n",
      "  0.09319952 0.10737418 0.15659113 0.13282175 0.10251725 0.19346647\n",
      "  0.15889572 0.09953978 0.113458   0.13349687 0.08207285 0.16501516\n",
      "  0.13301873 0.14676069 0.14057434 0.10711893 0.11374975 0.13706832\n",
      "  0.10475034 0.14917952 0.1080987  0.11658137 0.23922238 0.15931521\n",
      "  0.134237   0.16455084 0.12137795 0.1368287  0.1609954  0.14788182\n",
      "  0.16186015 0.10351018 0.18530153 0.19711165 0.19254893 0.2051128\n",
      "  0.10914463 0.10835465 0.22819135 0.16105017 0.12805861 0.16452898\n",
      "  0.15902255 0.19169942 0.12600061 0.11519904 0.27444002 0.24709584\n",
      "  0.2620963  0.82461613 0.5785541  0.5186297  0.7349123  0.5820095\n",
      "  0.5417782  0.7811384  0.60109025 0.5694857  0.75502473 0.6519127\n",
      "  0.5298034  0.8044354  0.47000828 0.5155755  0.7604981  0.66028935\n",
      "  0.4846281 ]]\n",
      "[[0.47902733 0.563146   0.505844   0.21995844 0.52132195 0.18155147\n",
      "  0.25652155 0.56301254 0.21086851 0.26111066 0.28517142 0.25504485\n",
      "  0.25000864 0.18595497 0.18162811 0.16552742 0.12168518 0.1657151\n",
      "  0.1435117  0.15076886 0.14712454 0.17100437 0.14879894 0.22363377\n",
      "  0.13160707 0.15875    0.1866252  0.16397546 0.16062935 0.12529224\n",
      "  0.18036838 0.13556589 0.12091446 0.21529876 0.15897726 0.16580063\n",
      "  0.20798077 0.17980309 0.17379472 0.1537046  0.141233   0.1497338\n",
      "  0.17563325 0.12214424 0.09759644 0.14768326 0.1385808  0.2055144\n",
      "  0.19587524 0.15807743 0.14387792 0.13841017 0.17073603 0.18542176\n",
      "  0.18207724 0.19991915 0.18048373 0.15749007 0.12611519 0.11848199\n",
      "  0.1306852  0.12767154 0.19732866 0.17691354 0.13158397 0.16491513\n",
      "  0.14389187 0.20663488 0.10062524 0.13247997 0.13870238 0.13857421\n",
      "  0.13935488 0.12166762 0.1902334  0.16232441 0.12904008 0.19198498\n",
      "  0.1636079  0.14872114 0.1339192  0.19303952 0.10153325 0.16931467\n",
      "  0.15404569 0.16330998 0.15806518 0.13167877 0.14931013 0.16439651\n",
      "  0.13263343 0.1810156  0.1137071  0.14511119 0.21260126 0.16112027\n",
      "  0.17751837 0.17145376 0.15379843 0.13595828 0.17240824 0.18889682\n",
      "  0.2008171  0.1248809  0.2063181  0.20249951 0.25008762 0.22471105\n",
      "  0.11215364 0.15222256 0.21759117 0.16617778 0.14728205 0.21093425\n",
      "  0.1601736  0.20504509 0.13853237 0.15763202 0.3098157  0.22974578\n",
      "  0.25915292 0.7779612  0.53851724 0.4728067  0.76375425 0.5467723\n",
      "  0.56241596 0.753856   0.6101533  0.49630204 0.77423614 0.6470596\n",
      "  0.55290717 0.7976028  0.5073634  0.48344857 0.71099293 0.62284374\n",
      "  0.47516885]]\n",
      "[[0.5008872  0.52563894 0.5788882  0.20753005 0.49390295 0.2251746\n",
      "  0.3054096  0.49236202 0.25787765 0.30780935 0.26040858 0.2997799\n",
      "  0.21199954 0.17633536 0.20188446 0.20093536 0.15071702 0.13506322\n",
      "  0.16538003 0.17913435 0.14529255 0.20481588 0.18018314 0.20793313\n",
      "  0.15407632 0.14868362 0.1890399  0.14430389 0.19673418 0.13508841\n",
      "  0.17742866 0.14236815 0.16394262 0.20592102 0.13603131 0.1961763\n",
      "  0.19218285 0.16819942 0.19167024 0.1981099  0.20426367 0.16642247\n",
      "  0.15055409 0.1191828  0.14545693 0.18991292 0.0890362  0.23733012\n",
      "  0.24181096 0.13997518 0.13061771 0.16593868 0.17799723 0.22678131\n",
      "  0.19235581 0.20883009 0.17720084 0.1693681  0.17334694 0.15569091\n",
      "  0.17143157 0.1456966  0.19520244 0.17749055 0.161074   0.16285539\n",
      "  0.14838615 0.17364013 0.11590975 0.15524468 0.15487163 0.16788109\n",
      "  0.12696503 0.135109   0.2024167  0.1950861  0.13487417 0.21676989\n",
      "  0.20736653 0.13755599 0.12824386 0.17778729 0.11873569 0.19861369\n",
      "  0.1434149  0.16560923 0.19070616 0.14609553 0.1496606  0.17054918\n",
      "  0.14381364 0.18239534 0.12838046 0.13535531 0.24860756 0.20022093\n",
      "  0.16589054 0.18818837 0.1443464  0.1886788  0.19213428 0.14889145\n",
      "  0.16950275 0.13636865 0.2435991  0.2397956  0.17998917 0.24501953\n",
      "  0.1469917  0.15440871 0.24062651 0.20158868 0.16956957 0.21603967\n",
      "  0.19011067 0.22675993 0.18052399 0.15984401 0.28843516 0.28327876\n",
      "  0.30193213 0.7787063  0.541902   0.52918726 0.7257561  0.6082098\n",
      "  0.47265962 0.72402835 0.52731305 0.58190393 0.7360316  0.6369038\n",
      "  0.5174735  0.786694   0.55290294 0.57026285 0.78836906 0.629019\n",
      "  0.4901381 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print or use the generated supplier data as needed\n",
    "print(\"Generated Supplier Data:\")\n",
    "for i in range(5):\n",
    "    print(generate_sample(model, df_treated.columns))\n",
    "\n",
    "# Save the model\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "torch.save(model.state_dict(), f'model_{timestamp}.pt')\n",
    "\n",
    "# Generate a new sample\n",
    "new_sample = generate_sample(model, df_treated.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PostProcess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_categorical(categorical_colums, classes):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = categorical_colums.copy()\n",
    "\n",
    "    # Apply a one-hot encoding to all columns of the tensor\n",
    "    for column in categorical_colums.columns:\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(categorical_colums[column])\n",
    "\n",
    "        # Apply one-hot encoding using torch.nn.functional.one_hot\n",
    "        one_hot_encoded = F.one_hot(tensor_data)\n",
    "\n",
    "        # Convert the one-hot encoding tensor to a NumPy array\n",
    "        one_hot_array = one_hot_encoded.numpy()\n",
    "\n",
    "        # Convert the one-hot encoding array to a list of indices\n",
    "        indices = []\n",
    "        for row in one_hot_array:\n",
    "            indices.append(np.argmax(row))\n",
    "\n",
    "        # Convert the indices to a list of strings\n",
    "        strings = []\n",
    "        for index in indices:\n",
    "            strings.append(classes[column][index])\n",
    "\n",
    "        # Add the one-hot encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = strings\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_numerical(numerical_columns):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = numerical_columns.copy()\n",
    "\n",
    "    # Apply a min-max normalization to all columns of the tensor\n",
    "    for column in numerical_columns.columns:\n",
    "        # Convert the column to a PyTorch tensor\n",
    "        tensor_data = torch.tensor(numerical_columns[column], dtype=torch.float32)\n",
    "\n",
    "        # Range of values in the column\n",
    "        min_value = torch.min(tensor_data)\n",
    "        max_value = torch.max(tensor_data)\n",
    "\n",
    "        # Normalize the values in the column between 0 and 1en\n",
    "        normalized = (tensor_data - min_value) / (max_value - min_value)\n",
    "\n",
    "        # Apply min-max normalization using torch.nn.functional.normalize\n",
    "        #normalized = F.normalize(tensor_data)\n",
    "\n",
    "        # Convert the normalized tensor to a NumPy array\n",
    "        normalized_array = normalized.numpy()\n",
    "\n",
    "        # Add the normalized array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = list(normalized_array)\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_text(text_columns):\n",
    "    # Prepare embeddings model\n",
    "    embedding_dim = 100\n",
    "    glove = GloVe(name='6B', dim=embedding_dim)\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = text_columns.copy()\n",
    "\n",
    "    # Apply a word embedding encoding to all columns of the tensor using torchtext\n",
    "    for column in text_columns.columns:\n",
    "        # Convert the column to a string list\n",
    "        texts = text_columns[column].astype(str).tolist()\n",
    "\n",
    "        # Convert the text to a list of tokens\n",
    "        tokens = [tokenizer(t) for t in texts]\n",
    "\n",
    "        # Convert the tokens to a list of indices\n",
    "        encoded_data = []\n",
    "        for token in tokens:\n",
    "            token_encoded = []\n",
    "            for word in token:\n",
    "                if word in glove.stoi:\n",
    "                    token_encoded.append(glove.stoi[word])\n",
    "                else:\n",
    "                    token_encoded.append(0)\n",
    "            encoded_data.append(token_encoded)\n",
    "\n",
    "        # Convert the indices to a PyTorch tensor\n",
    "        if len(encoded_data) <= 0:\n",
    "            continue\n",
    "\n",
    "        non_empty_sequences = [torch.tensor(seq) for seq in encoded_data if len(seq) > 0]\n",
    "\n",
    "        # Pad the sequences to the same length\n",
    "        padded_sequences = pad_sequence(non_empty_sequences)\n",
    "\n",
    "        # Add the encoded array to the transformed dataset\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "        transformed_dataset[column] = padded_sequences.tolist() # TODO: fix size mismatch error\n",
    "        \n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform_date(date_columns):\n",
    "    # Create a copy of the dataset to avoid modifying the original\n",
    "    transformed_dataset = date_columns.copy()\n",
    "\n",
    "    for column in date_columns.columns:\n",
    "        # Decompose the date into its components\n",
    "        date = pd.to_datetime(date_columns[column])\n",
    "\n",
    "        year = date.dt.year\n",
    "        # Check if the inital column contain a year\n",
    "        if not year.empty and not year.all() == 0:\n",
    "            year = torch.tensor(year, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_year\"] = year\n",
    "\n",
    "        month = date.dt.month\n",
    "        if not month.empty and not month.all() == 0:\n",
    "            month = torch.tensor(month / 12, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_month\"] = month\n",
    "\n",
    "        day = date.dt.day\n",
    "        if not day.empty and not day.all() == 0:\n",
    "            day = torch.tensor(day / 31, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_day\"] = day\n",
    "\n",
    "        hour = date.dt.hour\n",
    "        if not hour.empty and not hour.all() == 0:\n",
    "            hour = torch.tensor(hour / 24, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_hour\"] = hour\n",
    "\n",
    "        minute = date.dt.minute\n",
    "        if not minute.empty and not minute.all() == 0:\n",
    "            minute = torch.tensor(minute / 60, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_minute\"] = minute\n",
    "\n",
    "        second = date.dt.second\n",
    "        if not second.empty and not second.all() == 0:\n",
    "            second = torch.tensor(second / 60, dtype=torch.float32)\n",
    "            transformed_dataset[column + \"_second\"] = second\n",
    "        \n",
    "\n",
    "        # Drop the original date column\n",
    "        transformed_dataset = transformed_dataset.drop(columns=[column])\n",
    "\n",
    "    return transformed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_type(sample):\n",
    "    # Separate the text columns from the other columns\n",
    "    text_columns = [\"invoice_code\", \"customer_name\",\"customer_email\",\"customer_address\",\"customer_city\",\"customer_state\",\"customer_postal_code\",\n",
    "                    \"customer_country\",\"notes\",\"created_by\",\"updated_by\",\"shipping_address\",\"shipping_city\",\"shipping_state\",\n",
    "                    \"shipping_postal_code\",\"shipping_country\"]\n",
    "    \n",
    "    date_columns = {}\n",
    "    # # Create sublist for all columns with the same prefix\n",
    "    # for column in sample.columns:\n",
    "    #     if not date_columns.__contains__(column.split(\"_\")[0]):\n",
    "    #         date_columns[column.split(\"_\")[0]] = []\n",
    "    #     date_columns[column.split(\"_\")[0]].append(column)\n",
    "\n",
    "    categorical_columns = [\"payment_method\",\"status\",\"currency\",\"payment_reference\"]\n",
    "\n",
    "    numerical_columns = [\"invoice_number\",\"subtotal\",\"tax_rate\",\"tax_amount\",\"discount_rate\",\"discount_amount\",\"total\",\"exchange_rate\"]  \n",
    "\n",
    "    # Check if there is a column not in one of the above lists\n",
    "    # for column in sample.columns:\n",
    "    #     if column not in text_columns and column not in date_columns and column not in categorical_columns and column not in numerical_columns:\n",
    "    #         print(\"Column not in any list: \" + column) \n",
    "            \n",
    "    return sample[text_columns], sample[date_columns], sample[categorical_columns], sample[numerical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "split_by_type() takes 1 positional argument but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[182], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m categorical_colums_result, numerical_columns_result, text_columns_result, date_columns_result \u001b[38;5;241m=\u001b[39m \u001b[43msplit_by_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_columns_treated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_columns_treated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_columns_treated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_columns_treated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Reverse the transformations applied to the categorical columns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m categorical_columns_reversed \u001b[38;5;241m=\u001b[39m reverse_transform_categorical(categorical_colums_result, classes)\n",
      "\u001b[1;31mTypeError\u001b[0m: split_by_type() takes 1 positional argument but 5 were given"
     ]
    }
   ],
   "source": [
    "categorical_colums_result, numerical_columns_result, text_columns_result, date_columns_result = split_by_type(new_sample, categorical_columns_treated.columns, numerical_columns_treated.columns, text_columns_treated.columns, date_columns_treated.columns)\n",
    "\n",
    "\n",
    "# Reverse the transformations applied to the categorical columns\n",
    "categorical_columns_reversed = reverse_transform_categorical(categorical_colums_result, classes)\n",
    "\n",
    "# Reverse the transformations applied to the numerical columns\n",
    "numerical_columns_reversed = reverse_transform_numerical(numerical_columns_result)\n",
    "\n",
    "# Reverse the transformations applied to the text columns\n",
    "#text_columns_reversed = reverse_transform_text(text_columns_result)\n",
    "\n",
    "# Reverse the transformations applied to the date columns\n",
    "date_columns_reversed = reverse_transform_date(date_columns_result)\n",
    "\n",
    "# Concatenate the reversed columns back into a single dataframe\n",
    "#text_columns_reversed\n",
    "df_reversed = pd.concat([categorical_columns_reversed, numerical_columns_reversed, date_columns_reversed], axis=1)\n",
    "\n",
    "# Print the reversed dataframe\n",
    "print(df_reversed)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
