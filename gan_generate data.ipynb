{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collecting & vizualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for PyTorch\n",
    "class SupplierDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Extract features\n",
    "        labels = ['id', 'status']\n",
    "        features = sample[labels].values # ignore name and address for now\n",
    "\n",
    "        # Apply transformations (e.g., convert strings/categories to numerical values)\n",
    "        if self.transform:\n",
    "            features = self.transform(features, labels).astype(np.float32)\n",
    "        # Convert to PyTorch tensor\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "        return features, 0 # 0 is a dummy label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_categorical(data, labels):\n",
    "    # Convert categorical variables to numerical values (you can use more advanced encoding methods)\n",
    "    status_mapping = {'draft': 0, 'val': 1, 'other': 2}\n",
    "    status_index = labels.index('status')\n",
    "    data[status_index] = status_mapping[data[status_index]]\n",
    "\n",
    "    # You can implement similar transformations for other categorical variables\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "supplier Sample 1: (tensor([1., 0.]), 0)\n",
      "supplier Sample 2: (tensor([2., 1.]), 0)\n",
      "supplier Sample 3: (tensor([3., 1.]), 0)\n",
      "supplier Sample 4: (tensor([4., 0.]), 0)\n",
      "supplier Sample 5: (tensor([5., 1.]), 0)\n",
      "supplier Sample 6: (tensor([6., 1.]), 0)\n"
     ]
    }
   ],
   "source": [
    "# Load the csv dataset from the csv file\n",
    "dataset_path = 'datasets/fake_supplier_3.csv'\n",
    "# Creating a DataFrame from the CSV data (replace this with your actual CSV file path)\n",
    "df = pd.read_csv(dataset_path)\n",
    "# Create an instance of the SupplierDataset with the specified transformations\n",
    "supplier_dataset = SupplierDataset(dataframe=df, transform=transform_categorical)\n",
    "\n",
    "for i in range(len(supplier_dataset)):\n",
    "    sample = supplier_dataset[i]\n",
    "    print(f\"supplier Sample {i + 1}:\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.2\n",
    "batch_size = 2\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(supplier_dataset))\n",
    "val_size = len(supplier_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(supplier_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dim=512, latent_dim=256):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.var = nn.Linear (hidden_dim, latent_dim)\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.LeakyReLU(self.linear1(x))\n",
    "        x = self.LeakyReLU(self.linear2(x))\n",
    "\n",
    "        mean = self.mean(x)\n",
    "        log_var = self.var(x)                     \n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim=784, hidden_dim=512, latent_dim=256):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.linear2 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.LeakyReLU(self.linear2(x))\n",
    "        x = self.LeakyReLU(self.linear1(x))\n",
    "        \n",
    "        x_hat = torch.sigmoid(self.output(x))\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder layers\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2_mean = nn.Linear(512, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(512, latent_dim)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.fc3 = nn.Linear(latent_dim, 512)\n",
    "        self.fc4 = nn.Linear(512, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Encoder forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mean = self.fc2_mean(x)\n",
    "        logvar = self.fc2_logvar(x)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        # Reparameterization trick for sampling from a normal distribution\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mean + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        # Decoder forward pass\n",
    "        z = F.relu(self.fc3(z))\n",
    "        recon_x = torch.sigmoid(self.fc4(z))  # Assuming input features are normalized between 0 and 1\n",
    "        return recon_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Full forward pass of the VAE\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mean, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the VAE architecture\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim = 64, latent_dim = 16):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         # self.latent_dim = latent_dim\n",
    "\n",
    "#         # Encoder\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Linear(input_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, latent_dim * 2)  # The last layer outputs mean and log-variance\n",
    "#         )\n",
    "\n",
    "#         # Decoder\n",
    "#         self.decoder = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, hidden_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(hidden_dim, input_dim),\n",
    "#             nn.Sigmoid()  # Sigmoid activation for reconstruction\n",
    "#         )\n",
    "\n",
    "#     def reparameterize(self, mu, log_var):\n",
    "#         std = torch.exp(0.5 * log_var)\n",
    "#         eps = torch.randn_like(std)\n",
    "#         return mu + eps * std\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encode\n",
    "#         encoded = self.encoder(x)\n",
    "#         mu, log_var = torch.chunk(encoded, 2, dim=-1)\n",
    "#         z = self.reparameterize(mu, log_var)\n",
    "\n",
    "#         # Decode\n",
    "#         reconstructed = self.decoder(z)\n",
    "\n",
    "#         return reconstructed, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(reconstructed_output, input_tensor, mu, log_var):\n",
    "    print(\"reconstructed_output\", reconstructed_output)\n",
    "    print(\"input_tensor\", input_tensor)\n",
    "    BCE = nn.functional.mse_loss(reconstructed_output, input_tensor, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset, epochs, device, x_dim=-1):\n",
    "    model.train()\n",
    "    startTotal = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\tEpisode\", epoch + 1, \"/\", epochs)\n",
    "        overall_loss = 0\n",
    "        start = time.time()\n",
    "        for batch_idx, (input_tensor, _) in enumerate(train_dataset):\n",
    "            print(\"\\t\\tBatch\", batch_idx + 1, \"/\", len(train_dataset))\n",
    "            input_tensor = input_tensor.view(batch_size, x_dim).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            reconstructed_output, mean, log_var = model(input_tensor)\n",
    "            loss = loss_function(reconstructed_output, input_tensor, mean, log_var)\n",
    "            \n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        end = time.time()\n",
    "        print(\"\\tEpisode Result\", \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size), \"\\tStep Time: \", end - start, \"s\", \"\\tTotal Time: \", end - startTotal, \"s\",\"\\n\\n\")\n",
    "    return overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_vae(model, data_loader, num_epochs=100, learning_rate=1e-3):\n",
    "#     criterion = nn.MSELoss()\n",
    "#     optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for data in data_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             recon_batch, mu, log_var = model(data)\n",
    "#             loss = loss_function(recon_batch, data, mu, log_var)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and optimizer\n",
    "input_dim = 2 # corresponds to the number of features in the dataset\n",
    "latent_dim = 1 # corresponds to the number of latent variables\n",
    "model = VAE(input_dim, latent_dim).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpisode 1 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9982, 0.3984],\n",
      "        [0.9994, 0.3160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.7020],\n",
      "        [0.9998, 0.7734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.22406530380249 \tStep Time:  0.009952068328857422 s \tTotal Time:  0.009952068328857422 s \n",
      "\n",
      "\n",
      "\tEpisode 2 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9978, 0.4104],\n",
      "        [0.9999, 0.8064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.2773],\n",
      "        [0.9979, 0.4013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471388816833496 \tStep Time:  0.008976221084594727 s \tTotal Time:  0.01892828941345215 s \n",
      "\n",
      "\n",
      "\tEpisode 3 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9978, 0.5255],\n",
      "        [0.9979, 0.3891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9977, 0.4198],\n",
      "        [0.9986, 0.6275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.37733268737793 \tStep Time:  0.006980419158935547 s \tTotal Time:  0.025908708572387695 s \n",
      "\n",
      "\n",
      "\tEpisode 4 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.2920],\n",
      "        [0.9995, 0.7588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9977, 0.4064],\n",
      "        [0.9980, 0.5463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539603233337402 \tStep Time:  0.008975982666015625 s \tTotal Time:  0.03488469123840332 s \n",
      "\n",
      "\n",
      "\tEpisode 5 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9977, 0.4648],\n",
      "        [0.9977, 0.4394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8865],\n",
      "        [0.9977, 0.4341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.408995151519775 \tStep Time:  0.008975744247436523 s \tTotal Time:  0.043860435485839844 s \n",
      "\n",
      "\n",
      "\tEpisode 6 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9977, 0.4635],\n",
      "        [0.9983, 0.5930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.6500],\n",
      "        [0.9990, 0.6902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61786961555481 \tStep Time:  0.00498652458190918 s \tTotal Time:  0.04884696006774902 s \n",
      "\n",
      "\n",
      "\tEpisode 7 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.2464],\n",
      "        [0.9977, 0.4563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9981, 0.5409],\n",
      "        [0.9996, 0.2111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560442924499512 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.05582857131958008 s \n",
      "\n",
      "\n",
      "\tEpisode 8 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9984, 0.5851],\n",
      "        [0.9982, 0.5592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9982, 0.3272],\n",
      "        [0.9997, 0.7955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.828814148902893 \tStep Time:  0.006983280181884766 s \tTotal Time:  0.06281185150146484 s \n",
      "\n",
      "\n",
      "\tEpisode 9 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9984, 0.5805],\n",
      "        [0.9978, 0.4003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.1763],\n",
      "        [0.9989, 0.6494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.77760934829712 \tStep Time:  0.006980180740356445 s \tTotal Time:  0.06979203224182129 s \n",
      "\n",
      "\n",
      "\tEpisode 10 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9979, 0.4933],\n",
      "        [0.9983, 0.3321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9982, 0.5288],\n",
      "        [0.9988, 0.6293]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.430071830749512 \tStep Time:  0.006979942321777344 s \tTotal Time:  0.07677197456359863 s \n",
      "\n",
      "\n",
      "\tEpisode 11 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9985, 0.5846],\n",
      "        [0.9978, 0.3881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9978, 0.3962],\n",
      "        [0.9980, 0.3611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.681885838508606 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.08275604248046875 s \n",
      "\n",
      "\n",
      "\tEpisode 12 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9979, 0.4727],\n",
      "        [0.9991, 0.6623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.2324],\n",
      "        [1.0000, 0.1922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.302210330963135 \tStep Time:  0.006982564926147461 s \tTotal Time:  0.08973860740661621 s \n",
      "\n",
      "\n",
      "\tEpisode 13 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9232],\n",
      "        [0.9983, 0.3530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.3218],\n",
      "        [0.9979, 0.4064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.368572235107422 \tStep Time:  0.007978439331054688 s \tTotal Time:  0.0977170467376709 s \n",
      "\n",
      "\n",
      "\tEpisode 14 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.3278],\n",
      "        [1.0000, 0.9468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.7775],\n",
      "        [0.9988, 0.6196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.691271781921387 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.10370087623596191 s \n",
      "\n",
      "\n",
      "\tEpisode 15 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.3211],\n",
      "        [0.9998, 0.2360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.6873],\n",
      "        [0.9979, 0.4532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.940814971923828 \tStep Time:  0.0069806575775146484 s \tTotal Time:  0.11068153381347656 s \n",
      "\n",
      "\n",
      "\tEpisode 16 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.7497],\n",
      "        [0.9987, 0.3462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9979, 0.4599],\n",
      "        [0.9990, 0.6534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34586238861084 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.11666584014892578 s \n",
      "\n",
      "\n",
      "\tEpisode 17 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.3835],\n",
      "        [0.9996, 0.7592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.2868],\n",
      "        [0.9994, 0.7180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.268447399139404 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.12364697456359863 s \n",
      "\n",
      "\n",
      "\tEpisode 18 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9980, 0.4954],\n",
      "        [0.9990, 0.6640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.6352],\n",
      "        [0.9998, 0.7971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.605853080749512 \tStep Time:  0.006979465484619141 s \tTotal Time:  0.13062644004821777 s \n",
      "\n",
      "\n",
      "\tEpisode 19 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.3039],\n",
      "        [1.0000, 0.9008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9985, 0.5678],\n",
      "        [0.9980, 0.4111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.89633560180664 \tStep Time:  0.008979558944702148 s \tTotal Time:  0.13960599899291992 s \n",
      "\n",
      "\n",
      "\tEpisode 20 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.3355],\n",
      "        [0.9984, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.2716],\n",
      "        [1.0000, 0.8877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.247693121433258 \tStep Time:  0.012965917587280273 s \tTotal Time:  0.1525719165802002 s \n",
      "\n",
      "\n",
      "\tEpisode 21 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9980, 0.4197],\n",
      "        [0.9995, 0.7169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.3457],\n",
      "        [0.9992, 0.6797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576257705688477 \tStep Time:  0.00797581672668457 s \tTotal Time:  0.16054773330688477 s \n",
      "\n",
      "\n",
      "\tEpisode 22 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9980, 0.4784],\n",
      "        [0.9980, 0.4783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.7132],\n",
      "        [0.9997, 0.7620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.87682580947876 \tStep Time:  0.011967897415161133 s \tTotal Time:  0.17351293563842773 s \n",
      "\n",
      "\n",
      "\tEpisode 23 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.3188],\n",
      "        [0.9988, 0.3412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.2930],\n",
      "        [0.9993, 0.3025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.67299747467041 \tStep Time:  0.00997304916381836 s \tTotal Time:  0.1834859848022461 s \n",
      "\n",
      "\n",
      "\tEpisode 24 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9980, 0.4495],\n",
      "        [0.9992, 0.3119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.6295],\n",
      "        [0.9983, 0.3949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.719557762145996 \tStep Time:  0.011967658996582031 s \tTotal Time:  0.19545364379882812 s \n",
      "\n",
      "\n",
      "\tEpisode 25 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9981, 0.4273],\n",
      "        [0.9983, 0.3993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.3257],\n",
      "        [0.9995, 0.6836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.409591674804688 \tStep Time:  0.013962984085083008 s \tTotal Time:  0.20941662788391113 s \n",
      "\n",
      "\n",
      "\tEpisode 26 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.3444],\n",
      "        [0.9980, 0.4660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9981, 0.4264],\n",
      "        [0.9993, 0.6503]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449230194091797 \tStep Time:  0.009974241256713867 s \tTotal Time:  0.219390869140625 s \n",
      "\n",
      "\n",
      "\tEpisode 27 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.5232],\n",
      "        [0.9997, 0.3387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9981, 0.5005],\n",
      "        [0.9982, 0.4304]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445310115814209 \tStep Time:  0.007977724075317383 s \tTotal Time:  0.22736859321594238 s \n",
      "\n",
      "\n",
      "\tEpisode 28 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.7854],\n",
      "        [0.9985, 0.5610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9981, 0.4443],\n",
      "        [0.9982, 0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.657829880714417 \tStep Time:  0.014960527420043945 s \tTotal Time:  0.24232912063598633 s \n",
      "\n",
      "\n",
      "\tEpisode 29 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.4013],\n",
      "        [0.9983, 0.4302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9981, 0.4787],\n",
      "        [0.9981, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.600460529327393 \tStep Time:  0.01196742057800293 s \tTotal Time:  0.25429654121398926 s \n",
      "\n",
      "\n",
      "\tEpisode 30 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3363],\n",
      "        [0.9993, 0.6557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.7488],\n",
      "        [0.9981, 0.4977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562295913696289 \tStep Time:  0.009975910186767578 s \tTotal Time:  0.26427245140075684 s \n",
      "\n",
      "\n",
      "\tEpisode 31 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.6201],\n",
      "        [0.9990, 0.6133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.6385],\n",
      "        [0.9983, 0.4407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.64066219329834 \tStep Time:  0.009971380233764648 s \tTotal Time:  0.2742438316345215 s \n",
      "\n",
      "\n",
      "\tEpisode 32 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.5204],\n",
      "        [0.9998, 0.3452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3609],\n",
      "        [0.9984, 0.4483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526166915893555 \tStep Time:  0.00698089599609375 s \tTotal Time:  0.28122472763061523 s \n",
      "\n",
      "\n",
      "\tEpisode 33 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.4141],\n",
      "        [0.9991, 0.6144]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.6397],\n",
      "        [0.9990, 0.5960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44989013671875 \tStep Time:  0.007978200912475586 s \tTotal Time:  0.2892029285430908 s \n",
      "\n",
      "\n",
      "\tEpisode 34 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.3825],\n",
      "        [0.9996, 0.6524]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9985, 0.5457],\n",
      "        [0.9994, 0.3865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50710904598236 \tStep Time:  0.005984783172607422 s \tTotal Time:  0.29518771171569824 s \n",
      "\n",
      "\n",
      "\tEpisode 35 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2823],\n",
      "        [0.9982, 0.4850]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9982, 0.4950],\n",
      "        [0.9985, 0.4357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474451065063477 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.30117177963256836 s \n",
      "\n",
      "\n",
      "\tEpisode 36 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5872],\n",
      "        [0.9987, 0.5587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.4538],\n",
      "        [0.9984, 0.4464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518537521362305 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.3071556091308594 s \n",
      "\n",
      "\n",
      "\tEpisode 37 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.4567],\n",
      "        [0.9982, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.5037],\n",
      "        [0.9987, 0.5592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483823299407959 \tStep Time:  0.006982088088989258 s \tTotal Time:  0.31413769721984863 s \n",
      "\n",
      "\n",
      "\tEpisode 38 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.4327],\n",
      "        [0.9983, 0.4631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6859],\n",
      "        [0.9991, 0.6010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.749073505401611 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.3211190700531006 s \n",
      "\n",
      "\n",
      "\tEpisode 39 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3340],\n",
      "        [1.0000, 0.7814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.4790],\n",
      "        [0.9987, 0.4329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.678993701934814 \tStep Time:  0.00498652458190918 s \tTotal Time:  0.32610559463500977 s \n",
      "\n",
      "\n",
      "\tEpisode 40 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6469],\n",
      "        [0.9983, 0.4953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5534],\n",
      "        [0.9996, 0.3880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.379015803337097 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.3330869674682617 s \n",
      "\n",
      "\n",
      "\tEpisode 41 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9983, 0.5004],\n",
      "        [0.9993, 0.5855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5711],\n",
      "        [1.0000, 0.6729]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53470754623413 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.34006810188293457 s \n",
      "\n",
      "\n",
      "\tEpisode 42 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9985, 0.4567],\n",
      "        [0.9997, 0.3821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9985, 0.4524],\n",
      "        [0.9991, 0.4213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511107921600342 \tStep Time:  0.0049877166748046875 s \tTotal Time:  0.34505581855773926 s \n",
      "\n",
      "\n",
      "\tEpisode 43 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9984, 0.5088],\n",
      "        [0.9984, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5434],\n",
      "        [0.9988, 0.4441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455362677574158 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.3520374298095703 s \n",
      "\n",
      "\n",
      "\tEpisode 44 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.4548],\n",
      "        [0.9990, 0.5464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3632],\n",
      "        [0.9998, 0.3671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489975929260254 \tStep Time:  0.005982875823974609 s \tTotal Time:  0.3580203056335449 s \n",
      "\n",
      "\n",
      "\tEpisode 45 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5580],\n",
      "        [0.9984, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6030],\n",
      "        [0.9984, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599534511566162 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.365001916885376 s \n",
      "\n",
      "\n",
      "\tEpisode 46 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5365],\n",
      "        [0.9988, 0.5410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5615],\n",
      "        [0.9984, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482582569122314 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.370985746383667 s \n",
      "\n",
      "\n",
      "\tEpisode 47 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.5198],\n",
      "        [0.9999, 0.3523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5388],\n",
      "        [0.9996, 0.5640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.620769023895264 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.3769698143005371 s \n",
      "\n",
      "\n",
      "\tEpisode 48 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3285],\n",
      "        [0.9997, 0.3946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5550],\n",
      "        [0.9993, 0.5457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570477485656738 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.3829538822174072 s \n",
      "\n",
      "\n",
      "\tEpisode 49 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5514],\n",
      "        [0.9986, 0.5171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9984, 0.5005],\n",
      "        [0.9995, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485178470611572 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.3899350166320801 s \n",
      "\n",
      "\n",
      "\tEpisode 50 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4523],\n",
      "        [0.9984, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9984, 0.5016],\n",
      "        [0.9990, 0.5390]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573560237884521 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.3959190845489502 s \n",
      "\n",
      "\n",
      "\tEpisode 51 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5504],\n",
      "        [0.9989, 0.5349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5391],\n",
      "        [0.9984, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53597366809845 \tStep Time:  0.00598454475402832 s \tTotal Time:  0.4019036293029785 s \n",
      "\n",
      "\n",
      "\tEpisode 52 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.5052],\n",
      "        [0.9993, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5218],\n",
      "        [0.9997, 0.5191]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497344493865967 \tStep Time:  0.0069806575775146484 s \tTotal Time:  0.40888428688049316 s \n",
      "\n",
      "\n",
      "\tEpisode 53 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5217],\n",
      "        [0.9985, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5151],\n",
      "        [0.9985, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498338222503662 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.4148681163787842 s \n",
      "\n",
      "\n",
      "\tEpisode 54 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9985, 0.5061],\n",
      "        [0.9987, 0.4972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5355],\n",
      "        [0.9994, 0.5392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549973487854004 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.4208524227142334 s \n",
      "\n",
      "\n",
      "\tEpisode 55 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4893],\n",
      "        [0.9994, 0.5381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5379],\n",
      "        [1.0000, 0.5607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497820377349854 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.4268364906311035 s \n",
      "\n",
      "\n",
      "\tEpisode 56 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4582],\n",
      "        [0.9987, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.5159],\n",
      "        [0.9986, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471394181251526 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.43282032012939453 s \n",
      "\n",
      "\n",
      "\tEpisode 57 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4701],\n",
      "        [0.9999, 0.5487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5686],\n",
      "        [0.9985, 0.4828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426313877105713 \tStep Time:  0.006983041763305664 s \tTotal Time:  0.4398033618927002 s \n",
      "\n",
      "\n",
      "\tEpisode 58 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5847],\n",
      "        [1.0000, 0.3238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.4212],\n",
      "        [0.9988, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514745712280273 \tStep Time:  0.005982160568237305 s \tTotal Time:  0.4457855224609375 s \n",
      "\n",
      "\n",
      "\tEpisode 59 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4179],\n",
      "        [0.9986, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.3957],\n",
      "        [0.9997, 0.5716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.648444175720215 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.4517695903778076 s \n",
      "\n",
      "\n",
      "\tEpisode 60 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9985, 0.4683],\n",
      "        [0.9988, 0.4429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5717],\n",
      "        [0.9994, 0.4042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448072910308838 \tStep Time:  0.007980108261108398 s \tTotal Time:  0.459749698638916 s \n",
      "\n",
      "\n",
      "\tEpisode 61 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5903],\n",
      "        [0.9988, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.3756],\n",
      "        [1.0000, 0.3018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468022584915161 \tStep Time:  0.006979703903198242 s \tTotal Time:  0.46672940254211426 s \n",
      "\n",
      "\n",
      "\tEpisode 62 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.4450],\n",
      "        [0.9994, 0.4025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5536],\n",
      "        [0.9996, 0.5769]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488572359085083 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.4737110137939453 s \n",
      "\n",
      "\n",
      "\tEpisode 63 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.5221],\n",
      "        [0.9994, 0.3999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5911],\n",
      "        [0.9992, 0.5601]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563820838928223 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.47969484329223633 s \n",
      "\n",
      "\n",
      "\tEpisode 64 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4065],\n",
      "        [0.9990, 0.5493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6266],\n",
      "        [0.9989, 0.4469]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543512642383575 \tStep Time:  0.004986763000488281 s \tTotal Time:  0.4846816062927246 s \n",
      "\n",
      "\n",
      "\tEpisode 65 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6170],\n",
      "        [0.9997, 0.5872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3511],\n",
      "        [0.9986, 0.4750]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34372615814209 \tStep Time:  0.007978677749633789 s \tTotal Time:  0.4926602840423584 s \n",
      "\n",
      "\n",
      "\tEpisode 66 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5501],\n",
      "        [0.9993, 0.4245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5694],\n",
      "        [0.9986, 0.4712]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506531298160553 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.4986441135406494 s \n",
      "\n",
      "\n",
      "\tEpisode 67 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5872],\n",
      "        [0.9988, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5851],\n",
      "        [0.9994, 0.5867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54765796661377 \tStep Time:  0.004986763000488281 s \tTotal Time:  0.5036308765411377 s \n",
      "\n",
      "\n",
      "\tEpisode 68 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6030],\n",
      "        [0.9994, 0.3952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4216],\n",
      "        [0.9995, 0.3950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436049938201904 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.5096149444580078 s \n",
      "\n",
      "\n",
      "\tEpisode 69 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.4618],\n",
      "        [0.9986, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.4993],\n",
      "        [1.0000, 0.7748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.707940101623535 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.5155990123748779 s \n",
      "\n",
      "\n",
      "\tEpisode 70 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.3647],\n",
      "        [0.9986, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5637],\n",
      "        [0.9997, 0.5951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452152132987976 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.521582841873169 s \n",
      "\n",
      "\n",
      "\tEpisode 71 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6352],\n",
      "        [0.9995, 0.3885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5863],\n",
      "        [0.9988, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44268274307251 \tStep Time:  0.004986763000488281 s \tTotal Time:  0.5265696048736572 s \n",
      "\n",
      "\n",
      "\tEpisode 72 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.4252],\n",
      "        [0.9993, 0.5557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.2957],\n",
      "        [0.9997, 0.5763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.3362717628479 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.5325539112091064 s \n",
      "\n",
      "\n",
      "\tEpisode 73 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.4524],\n",
      "        [1.0000, 0.6011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.3416],\n",
      "        [0.9986, 0.4847]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419024467468262 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.5385379791259766 s \n",
      "\n",
      "\n",
      "\tEpisode 74 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.2924],\n",
      "        [0.9986, 0.4614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6186],\n",
      "        [0.9987, 0.4461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.697206974029541 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.5455193519592285 s \n",
      "\n",
      "\n",
      "\tEpisode 75 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5300],\n",
      "        [0.9990, 0.4027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9986, 0.4712],\n",
      "        [0.9987, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555140018463135 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.5515031814575195 s \n",
      "\n",
      "\n",
      "\tEpisode 76 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.5181],\n",
      "        [0.9993, 0.5794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5579],\n",
      "        [0.9986, 0.4715]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.620336055755615 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.5574870109558105 s \n",
      "\n",
      "\n",
      "\tEpisode 77 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5234],\n",
      "        [0.9999, 0.6104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.4590],\n",
      "        [0.9987, 0.4582]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582103371620178 \tStep Time:  0.004986763000488281 s \tTotal Time:  0.5624737739562988 s \n",
      "\n",
      "\n",
      "\tEpisode 78 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5581],\n",
      "        [0.9996, 0.5739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.2756],\n",
      "        [0.9988, 0.5261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.385485649108887 \tStep Time:  0.005984783172607422 s \tTotal Time:  0.5684585571289062 s \n",
      "\n",
      "\n",
      "\tEpisode 79 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5617],\n",
      "        [0.9997, 0.5789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3075],\n",
      "        [0.9999, 0.5801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.699475765228271 \tStep Time:  0.0059833526611328125 s \tTotal Time:  0.5744419097900391 s \n",
      "\n",
      "\n",
      "\tEpisode 80 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5608],\n",
      "        [0.9987, 0.4529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.4757],\n",
      "        [0.9987, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588570296764374 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.5814230442047119 s \n",
      "\n",
      "\n",
      "\tEpisode 81 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5305],\n",
      "        [0.9987, 0.4596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.4959],\n",
      "        [0.9987, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534955024719238 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.5874073505401611 s \n",
      "\n",
      "\n",
      "\tEpisode 82 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5118],\n",
      "        [0.9997, 0.5145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4478],\n",
      "        [0.9999, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552043557167053 \tStep Time:  0.005983591079711914 s \tTotal Time:  0.593390941619873 s \n",
      "\n",
      "\n",
      "\tEpisode 83 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4340],\n",
      "        [0.9995, 0.4130]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.4841],\n",
      "        [0.9995, 0.4337]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51087236404419 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.5993750095367432 s \n",
      "\n",
      "\n",
      "\tEpisode 84 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4851],\n",
      "        [0.9993, 0.4897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4427],\n",
      "        [0.9990, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503459453582764 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.6053590774536133 s \n",
      "\n",
      "\n",
      "\tEpisode 85 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4780],\n",
      "        [0.9988, 0.4972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4688],\n",
      "        [0.9987, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520838499069214 \tStep Time:  0.005982398986816406 s \tTotal Time:  0.6113414764404297 s \n",
      "\n",
      "\n",
      "\tEpisode 86 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3173],\n",
      "        [0.9994, 0.4668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5075],\n",
      "        [0.9992, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.615789353847504 \tStep Time:  0.006983280181884766 s \tTotal Time:  0.6183247566223145 s \n",
      "\n",
      "\n",
      "\tEpisode 87 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.4908],\n",
      "        [0.9992, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4041],\n",
      "        [0.9987, 0.5045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565168857574463 \tStep Time:  0.006981611251831055 s \tTotal Time:  0.6253063678741455 s \n",
      "\n",
      "\n",
      "\tEpisode 88 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.4799],\n",
      "        [0.9994, 0.4678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5122],\n",
      "        [0.9998, 0.4122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593128204345703 \tStep Time:  0.005983591079711914 s \tTotal Time:  0.6312899589538574 s \n",
      "\n",
      "\n",
      "\tEpisode 89 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9987, 0.5054],\n",
      "        [0.9988, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4238],\n",
      "        [1.0000, 0.3831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556406021118164 \tStep Time:  0.007979631423950195 s \tTotal Time:  0.6392695903778076 s \n",
      "\n",
      "\n",
      "\tEpisode 90 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5129],\n",
      "        [0.9987, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5251],\n",
      "        [0.9990, 0.5259]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537917613983154 \tStep Time:  0.005982398986816406 s \tTotal Time:  0.645251989364624 s \n",
      "\n",
      "\n",
      "\tEpisode 91 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4508],\n",
      "        [0.9998, 0.4895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4649],\n",
      "        [0.9999, 0.4357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500576496124268 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.6512360572814941 s \n",
      "\n",
      "\n",
      "\tEpisode 92 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5415],\n",
      "        [0.9988, 0.5360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3447],\n",
      "        [0.9988, 0.5339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.627084493637085 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.6572203636169434 s \n",
      "\n",
      "\n",
      "\tEpisode 93 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5089],\n",
      "        [0.9988, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3100],\n",
      "        [0.9988, 0.5441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442401885986328 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.6632041931152344 s \n",
      "\n",
      "\n",
      "\tEpisode 94 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5639],\n",
      "        [0.9989, 0.5598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5662],\n",
      "        [0.9993, 0.5677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536199510097504 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.6691884994506836 s \n",
      "\n",
      "\n",
      "\tEpisode 95 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5453],\n",
      "        [0.9992, 0.5369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5438],\n",
      "        [0.9996, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507597208023071 \tStep Time:  0.005983591079711914 s \tTotal Time:  0.6751720905303955 s \n",
      "\n",
      "\n",
      "\tEpisode 96 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5507],\n",
      "        [0.9989, 0.5446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5633],\n",
      "        [0.9988, 0.5457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52307254076004 \tStep Time:  0.004987478256225586 s \tTotal Time:  0.6801595687866211 s \n",
      "\n",
      "\n",
      "\tEpisode 97 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5353],\n",
      "        [0.9990, 0.5706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4448],\n",
      "        [0.9988, 0.5587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488960266113281 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.687140941619873 s \n",
      "\n",
      "\n",
      "\tEpisode 98 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5754],\n",
      "        [0.9991, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5396],\n",
      "        [0.9998, 0.4511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502946853637695 \tStep Time:  0.005983114242553711 s \tTotal Time:  0.6931240558624268 s \n",
      "\n",
      "\n",
      "\tEpisode 99 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5130],\n",
      "        [0.9988, 0.5451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5616],\n",
      "        [0.9995, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522932529449463 \tStep Time:  0.004987001419067383 s \tTotal Time:  0.6981110572814941 s \n",
      "\n",
      "\n",
      "\tEpisode 100 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5741],\n",
      "        [0.9988, 0.5397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5362],\n",
      "        [0.9997, 0.5755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517392456531525 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.7040948867797852 s \n",
      "\n",
      "\n",
      "\tEpisode 101 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5734],\n",
      "        [0.9988, 0.5381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5240],\n",
      "        [0.9992, 0.5030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563279628753662 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.7100789546966553 s \n",
      "\n",
      "\n",
      "\tEpisode 102 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9988, 0.5209],\n",
      "        [1.0000, 0.3643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.4909],\n",
      "        [0.9988, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46037769317627 \tStep Time:  0.00498652458190918 s \tTotal Time:  0.7150654792785645 s \n",
      "\n",
      "\n",
      "\tEpisode 103 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.4946],\n",
      "        [0.9999, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4375],\n",
      "        [0.9993, 0.5382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439830303192139 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.7210495471954346 s \n",
      "\n",
      "\n",
      "\tEpisode 104 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5179],\n",
      "        [0.9993, 0.5409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.4966],\n",
      "        [0.9996, 0.4440]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572330474853516 \tStep Time:  0.004986763000488281 s \tTotal Time:  0.7260363101959229 s \n",
      "\n",
      "\n",
      "\tEpisode 105 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5333],\n",
      "        [1.0000, 0.2675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5005],\n",
      "        [0.9996, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.683189988136292 \tStep Time:  0.004986286163330078 s \tTotal Time:  0.7320201396942139 s \n",
      "\n",
      "\n",
      "\tEpisode 106 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5037],\n",
      "        [0.9996, 0.4454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5108],\n",
      "        [0.9997, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506650447845459 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.7380044460296631 s \n",
      "\n",
      "\n",
      "\tEpisode 107 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4091],\n",
      "        [0.9994, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5011],\n",
      "        [0.9994, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490240931510925 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.7439887523651123 s \n",
      "\n",
      "\n",
      "\tEpisode 108 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5038],\n",
      "        [0.9998, 0.4271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4760],\n",
      "        [0.9991, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550668716430664 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.750969648361206 s \n",
      "\n",
      "\n",
      "\tEpisode 109 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4797],\n",
      "        [0.9991, 0.5206]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5152],\n",
      "        [0.9989, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545926809310913 \tStep Time:  0.00498652458190918 s \tTotal Time:  0.7559561729431152 s \n",
      "\n",
      "\n",
      "\tEpisode 110 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5163],\n",
      "        [0.9990, 0.5132]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5117],\n",
      "        [0.9990, 0.5135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519544124603271 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.7619402408599854 s \n",
      "\n",
      "\n",
      "\tEpisode 111 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5095],\n",
      "        [0.9997, 0.4720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5124],\n",
      "        [0.9994, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503291308879852 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.7679243087768555 s \n",
      "\n",
      "\n",
      "\tEpisode 112 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5130],\n",
      "        [0.9989, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3693],\n",
      "        [0.9998, 0.4549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42777681350708 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.7739081382751465 s \n",
      "\n",
      "\n",
      "\tEpisode 113 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4887],\n",
      "        [0.9989, 0.5187]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5127],\n",
      "        [0.9991, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514949560165405 \tStep Time:  0.005984783172607422 s \tTotal Time:  0.7798929214477539 s \n",
      "\n",
      "\n",
      "\tEpisode 114 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5362],\n",
      "        [0.9989, 0.5177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4098],\n",
      "        [0.9999, 0.3817]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547922134399414 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.7858767509460449 s \n",
      "\n",
      "\n",
      "\tEpisode 115 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5116],\n",
      "        [0.9989, 0.5209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9989, 0.5210],\n",
      "        [0.9993, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507003605365753 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.7918605804443359 s \n",
      "\n",
      "\n",
      "\tEpisode 116 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5138],\n",
      "        [0.9990, 0.5247]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5470],\n",
      "        [0.9996, 0.4562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558631420135498 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.797844409942627 s \n",
      "\n",
      "\n",
      "\tEpisode 117 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5198],\n",
      "        [0.9992, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4649],\n",
      "        [0.9996, 0.4551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522560238838196 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.803828239440918 s \n",
      "\n",
      "\n",
      "\tEpisode 118 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5544],\n",
      "        [0.9999, 0.3687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5451],\n",
      "        [0.9994, 0.5410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439336776733398 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.8098123073577881 s \n",
      "\n",
      "\n",
      "\tEpisode 119 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5266],\n",
      "        [0.9991, 0.5012]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3480],\n",
      "        [0.9994, 0.4786]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459920883178711 \tStep Time:  0.006981372833251953 s \tTotal Time:  0.81679368019104 s \n",
      "\n",
      "\n",
      "\tEpisode 120 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5091],\n",
      "        [0.9992, 0.5497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5069],\n",
      "        [0.9990, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535975456237793 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.822777509689331 s \n",
      "\n",
      "\n",
      "\tEpisode 121 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2632],\n",
      "        [0.9990, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4237],\n",
      "        [0.9995, 0.4595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.406736016273499 \tStep Time:  0.004987001419067383 s \tTotal Time:  0.8277645111083984 s \n",
      "\n",
      "\n",
      "\tEpisode 122 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5090],\n",
      "        [0.9991, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3582],\n",
      "        [0.9994, 0.5758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.417785286903381 \tStep Time:  0.006990671157836914 s \tTotal Time:  0.8357527256011963 s \n",
      "\n",
      "\n",
      "\tEpisode 123 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5435],\n",
      "        [0.9992, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6212],\n",
      "        [0.9996, 0.5925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432971954345703 \tStep Time:  0.0069713592529296875 s \tTotal Time:  0.842724084854126 s \n",
      "\n",
      "\n",
      "\tEpisode 124 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5910],\n",
      "        [0.9993, 0.5863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5506],\n",
      "        [0.9999, 0.2880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.690601766109467 \tStep Time:  0.004987001419067383 s \tTotal Time:  0.8477110862731934 s \n",
      "\n",
      "\n",
      "\tEpisode 125 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6780],\n",
      "        [0.9997, 0.3989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5287],\n",
      "        [0.9996, 0.6251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.354525685310364 \tStep Time:  0.00698089599609375 s \tTotal Time:  0.8546919822692871 s \n",
      "\n",
      "\n",
      "\tEpisode 126 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.6148],\n",
      "        [1.0000, 0.1362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4119],\n",
      "        [0.9995, 0.6333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476236462593079 \tStep Time:  0.005984306335449219 s \tTotal Time:  0.8606762886047363 s \n",
      "\n",
      "\n",
      "\tEpisode 127 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.6296],\n",
      "        [1.0000, 0.2263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2405],\n",
      "        [0.9990, 0.5331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564000129699707 \tStep Time:  0.004987239837646484 s \tTotal Time:  0.8656635284423828 s \n",
      "\n",
      "\n",
      "\tEpisode 128 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2049],\n",
      "        [0.9992, 0.6182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5455],\n",
      "        [0.9992, 0.4996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469797134399414 \tStep Time:  0.0069811344146728516 s \tTotal Time:  0.8726446628570557 s \n",
      "\n",
      "\n",
      "\tEpisode 129 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5624],\n",
      "        [1.0000, 0.1536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.3931],\n",
      "        [0.9991, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.732121467590332 \tStep Time:  0.007978439331054688 s \tTotal Time:  0.8806231021881104 s \n",
      "\n",
      "\n",
      "\tEpisode 130 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.6356],\n",
      "        [0.9990, 0.5645]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6943],\n",
      "        [0.9998, 0.3638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.693110466003418 \tStep Time:  0.00798177719116211 s \tTotal Time:  0.8886048793792725 s \n",
      "\n",
      "\n",
      "\tEpisode 131 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6906],\n",
      "        [0.9990, 0.5550]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4385],\n",
      "        [1.0000, 0.2174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.401693224906921 \tStep Time:  0.006978034973144531 s \tTotal Time:  0.895582914352417 s \n",
      "\n",
      "\n",
      "\tEpisode 132 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5903],\n",
      "        [0.9991, 0.6018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5196],\n",
      "        [0.9993, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548723697662354 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.9015669822692871 s \n",
      "\n",
      "\n",
      "\tEpisode 133 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6788],\n",
      "        [0.9996, 0.4689]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5971],\n",
      "        [0.9992, 0.5996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532995700836182 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.9075510501861572 s \n",
      "\n",
      "\n",
      "\tEpisode 134 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.6091],\n",
      "        [0.9998, 0.3924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7590],\n",
      "        [0.9991, 0.5686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.777210712432861 \tStep Time:  0.00498652458190918 s \tTotal Time:  0.9125375747680664 s \n",
      "\n",
      "\n",
      "\tEpisode 135 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5721],\n",
      "        [0.9995, 0.6136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9990, 0.5424],\n",
      "        [0.9993, 0.5904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503805041313171 \tStep Time:  0.005982398986816406 s \tTotal Time:  0.9185199737548828 s \n",
      "\n",
      "\n",
      "\tEpisode 136 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6291],\n",
      "        [0.9998, 0.6154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4752],\n",
      "        [0.9998, 0.6087]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469900131225586 \tStep Time:  0.005985736846923828 s \tTotal Time:  0.9245057106018066 s \n",
      "\n",
      "\n",
      "\tEpisode 137 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5341],\n",
      "        [0.9992, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5384],\n",
      "        [0.9993, 0.5637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556142807006836 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.9304895401000977 s \n",
      "\n",
      "\n",
      "\tEpisode 138 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5218],\n",
      "        [0.9996, 0.4733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5836],\n",
      "        [0.9997, 0.4582]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.615167260169983 \tStep Time:  0.0059850215911865234 s \tTotal Time:  0.9364745616912842 s \n",
      "\n",
      "\n",
      "\tEpisode 139 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5321],\n",
      "        [0.9992, 0.5239]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5003],\n",
      "        [0.9992, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558664798736572 \tStep Time:  0.006980419158935547 s \tTotal Time:  0.9434549808502197 s \n",
      "\n",
      "\n",
      "\tEpisode 140 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4932],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4819],\n",
      "        [0.9999, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520347237586975 \tStep Time:  0.004987239837646484 s \tTotal Time:  0.9484422206878662 s \n",
      "\n",
      "\n",
      "\tEpisode 141 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4793],\n",
      "        [1.0000, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4852],\n",
      "        [1.0000, 0.3195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441212177276611 \tStep Time:  0.00698089599609375 s \tTotal Time:  0.95542311668396 s \n",
      "\n",
      "\n",
      "\tEpisode 142 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4682],\n",
      "        [0.9999, 0.4380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4789],\n",
      "        [0.9995, 0.4365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481272220611572 \tStep Time:  0.004987001419067383 s \tTotal Time:  0.9604101181030273 s \n",
      "\n",
      "\n",
      "\tEpisode 143 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4930],\n",
      "        [0.9991, 0.4804]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4824],\n",
      "        [0.9999, 0.4512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503902912139893 \tStep Time:  0.005983591079711914 s \tTotal Time:  0.9663937091827393 s \n",
      "\n",
      "\n",
      "\tEpisode 144 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4946],\n",
      "        [0.9999, 0.4641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4633],\n",
      "        [0.9996, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554709434509277 \tStep Time:  0.005983829498291016 s \tTotal Time:  0.9723775386810303 s \n",
      "\n",
      "\n",
      "\tEpisode 145 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4624],\n",
      "        [1.0000, 0.3467]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4054],\n",
      "        [0.9997, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418999016284943 \tStep Time:  0.005984067916870117 s \tTotal Time:  0.9783616065979004 s \n",
      "\n",
      "\n",
      "\tEpisode 146 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4693],\n",
      "        [0.9998, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4888],\n",
      "        [0.9992, 0.4332]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530502796173096 \tStep Time:  0.00598454475402832 s \tTotal Time:  0.9843461513519287 s \n",
      "\n",
      "\n",
      "\tEpisode 147 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5232],\n",
      "        [0.9991, 0.4454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4918],\n",
      "        [0.9998, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501015663146973 \tStep Time:  0.0059833526611328125 s \tTotal Time:  0.9903295040130615 s \n",
      "\n",
      "\n",
      "\tEpisode 148 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5047],\n",
      "        [0.9998, 0.3392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5261],\n",
      "        [1.0000, 0.4490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.648828983306885 \tStep Time:  0.00598454475402832 s \tTotal Time:  0.9963140487670898 s \n",
      "\n",
      "\n",
      "\tEpisode 149 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4625],\n",
      "        [0.9999, 0.3198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3427],\n",
      "        [0.9997, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507343769073486 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.002298355102539 s \n",
      "\n",
      "\n",
      "\tEpisode 150 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [0.9991, 0.4513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5108],\n",
      "        [0.9998, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456450939178467 \tStep Time:  0.004985809326171875 s \tTotal Time:  1.007284164428711 s \n",
      "\n",
      "\n",
      "\tEpisode 151 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5332],\n",
      "        [0.9995, 0.3964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5362],\n",
      "        [1.0000, 0.4859]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47572135925293 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.013268232345581 s \n",
      "\n",
      "\n",
      "\tEpisode 152 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5117],\n",
      "        [0.9996, 0.5542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5327],\n",
      "        [0.9998, 0.3468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.622172355651855 \tStep Time:  0.006980419158935547 s \tTotal Time:  1.0202486515045166 s \n",
      "\n",
      "\n",
      "\tEpisode 153 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5510],\n",
      "        [1.0000, 0.4817]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.4760],\n",
      "        [1.0000, 0.4100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595377445220947 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.0272300243377686 s \n",
      "\n",
      "\n",
      "\tEpisode 154 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5163],\n",
      "        [0.9994, 0.4211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.3829],\n",
      "        [0.9998, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535035133361816 \tStep Time:  0.00698089599609375 s \tTotal Time:  1.0342109203338623 s \n",
      "\n",
      "\n",
      "\tEpisode 155 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5578],\n",
      "        [1.0000, 0.5141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5606],\n",
      "        [0.9998, 0.3921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468996524810791 \tStep Time:  0.006982326507568359 s \tTotal Time:  1.0411932468414307 s \n",
      "\n",
      "\n",
      "\tEpisode 156 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9991, 0.5090],\n",
      "        [0.9992, 0.4890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5311],\n",
      "        [0.9993, 0.4677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512832641601562 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.0471765995025635 s \n",
      "\n",
      "\n",
      "\tEpisode 157 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4501],\n",
      "        [1.0000, 0.5531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5500],\n",
      "        [0.9999, 0.3679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66439175605774 \tStep Time:  0.0069806575775146484 s \tTotal Time:  1.0551555156707764 s \n",
      "\n",
      "\n",
      "\tEpisode 158 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5842],\n",
      "        [0.9993, 0.4782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4424],\n",
      "        [0.9993, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467065334320068 \tStep Time:  0.0049860477447509766 s \tTotal Time:  1.0601415634155273 s \n",
      "\n",
      "\n",
      "\tEpisode 159 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5848],\n",
      "        [1.0000, 0.4054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5853],\n",
      "        [0.9997, 0.4719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584454536437988 \tStep Time:  0.00598454475402832 s \tTotal Time:  1.0661261081695557 s \n",
      "\n",
      "\n",
      "\tEpisode 160 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4190],\n",
      "        [0.9992, 0.5683]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5227],\n",
      "        [1.0000, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562252521514893 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.0721101760864258 s \n",
      "\n",
      "\n",
      "\tEpisode 161 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4923],\n",
      "        [0.9997, 0.5726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5013],\n",
      "        [0.9992, 0.5611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528666496276855 \tStep Time:  0.0049860477447509766 s \tTotal Time:  1.0770962238311768 s \n",
      "\n",
      "\n",
      "\tEpisode 162 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5497],\n",
      "        [0.9993, 0.5652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5524],\n",
      "        [0.9996, 0.5631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515910625457764 \tStep Time:  0.006021738052368164 s \tTotal Time:  1.083117961883545 s \n",
      "\n",
      "\n",
      "\tEpisode 163 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5400],\n",
      "        [0.9992, 0.5467]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5513],\n",
      "        [0.9994, 0.5612]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515768826007843 \tStep Time:  0.006975412368774414 s \tTotal Time:  1.0900933742523193 s \n",
      "\n",
      "\n",
      "\tEpisode 164 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5559],\n",
      "        [0.9997, 0.5295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5519],\n",
      "        [0.9999, 0.5378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529068946838379 \tStep Time:  0.0049860477447509766 s \tTotal Time:  1.0950794219970703 s \n",
      "\n",
      "\n",
      "\tEpisode 165 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2716],\n",
      "        [0.9992, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5458],\n",
      "        [0.9992, 0.5503]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6840238571167 \tStep Time:  0.0059854984283447266 s \tTotal Time:  1.101064920425415 s \n",
      "\n",
      "\n",
      "\tEpisode 166 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4369],\n",
      "        [0.9996, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5379],\n",
      "        [1.0000, 0.3330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597407698631287 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.1070489883422852 s \n",
      "\n",
      "\n",
      "\tEpisode 167 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5489],\n",
      "        [0.9995, 0.5535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5500],\n",
      "        [1.0000, 0.4665]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561191082000732 \tStep Time:  0.0049877166748046875 s \tTotal Time:  1.1120367050170898 s \n",
      "\n",
      "\n",
      "\tEpisode 168 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5403],\n",
      "        [0.9992, 0.5422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4811],\n",
      "        [0.9994, 0.5625]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47440242767334 \tStep Time:  0.004953145980834961 s \tTotal Time:  1.1169898509979248 s \n",
      "\n",
      "\n",
      "\tEpisode 169 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5249],\n",
      "        [0.9992, 0.5333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5294],\n",
      "        [0.9992, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515976905822754 \tStep Time:  0.007012367248535156 s \tTotal Time:  1.12400221824646 s \n",
      "\n",
      "\n",
      "\tEpisode 170 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5190],\n",
      "        [0.9992, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5169],\n",
      "        [1.0000, 0.4403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4945969581604 \tStep Time:  0.004986763000488281 s \tTotal Time:  1.1289889812469482 s \n",
      "\n",
      "\n",
      "\tEpisode 171 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5154],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5491],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503273487091064 \tStep Time:  0.004987239837646484 s \tTotal Time:  1.1339762210845947 s \n",
      "\n",
      "\n",
      "\tEpisode 172 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4956],\n",
      "        [1.0000, 0.3920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4963],\n",
      "        [1.0000, 0.4188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531759262084961 \tStep Time:  0.006949186325073242 s \tTotal Time:  1.140925407409668 s \n",
      "\n",
      "\n",
      "\tEpisode 173 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4764],\n",
      "        [0.9994, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5229],\n",
      "        [0.9993, 0.5391]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531751155853271 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.1469097137451172 s \n",
      "\n",
      "\n",
      "\tEpisode 174 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4587],\n",
      "        [0.9992, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5548],\n",
      "        [1.0000, 0.5224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50600552558899 \tStep Time:  0.006017446517944336 s \tTotal Time:  1.1529271602630615 s \n",
      "\n",
      "\n",
      "\tEpisode 175 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5570],\n",
      "        [0.9993, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5551],\n",
      "        [0.9994, 0.4695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509356498718262 \tStep Time:  0.005982160568237305 s \tTotal Time:  1.1589093208312988 s \n",
      "\n",
      "\n",
      "\tEpisode 176 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5581],\n",
      "        [0.9993, 0.5411]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.4754],\n",
      "        [0.9992, 0.5227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49898087978363 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.1648929119110107 s \n",
      "\n",
      "\n",
      "\tEpisode 177 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.4864],\n",
      "        [0.9993, 0.5462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4271],\n",
      "        [0.9999, 0.5623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553662776947021 \tStep Time:  0.0059545040130615234 s \tTotal Time:  1.1708474159240723 s \n",
      "\n",
      "\n",
      "\tEpisode 178 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4017],\n",
      "        [0.9992, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5651],\n",
      "        [0.9998, 0.5650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572705268859863 \tStep Time:  0.0050182342529296875 s \tTotal Time:  1.175865650177002 s \n",
      "\n",
      "\n",
      "\tEpisode 179 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5614],\n",
      "        [0.9994, 0.4594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5607],\n",
      "        [0.9998, 0.4089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.389803409576416 \tStep Time:  0.005982875823974609 s \tTotal Time:  1.1818485260009766 s \n",
      "\n",
      "\n",
      "\tEpisode 180 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4453],\n",
      "        [0.9997, 0.4219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.4772],\n",
      "        [0.9999, 0.3766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55921459197998 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.1878321170806885 s \n",
      "\n",
      "\n",
      "\tEpisode 181 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5426],\n",
      "        [0.9995, 0.5536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4395],\n",
      "        [0.9995, 0.5550]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451665580272675 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.1938154697418213 s \n",
      "\n",
      "\n",
      "\tEpisode 182 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4459],\n",
      "        [0.9993, 0.5363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3687],\n",
      "        [0.9999, 0.4068]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554005146026611 \tStep Time:  0.005986690521240234 s \tTotal Time:  1.1998021602630615 s \n",
      "\n",
      "\n",
      "\tEpisode 183 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4257],\n",
      "        [0.9992, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4634],\n",
      "        [0.9992, 0.4866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512384414672852 \tStep Time:  0.005986690521240234 s \tTotal Time:  1.2057888507843018 s \n",
      "\n",
      "\n",
      "\tEpisode 184 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4385],\n",
      "        [0.9997, 0.5499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5531],\n",
      "        [0.9993, 0.4750]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498594760894775 \tStep Time:  0.005947113037109375 s \tTotal Time:  1.2117359638214111 s \n",
      "\n",
      "\n",
      "\tEpisode 185 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4973],\n",
      "        [0.9993, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5465],\n",
      "        [1.0000, 0.3946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537870407104492 \tStep Time:  0.0060155391693115234 s \tTotal Time:  1.2177515029907227 s \n",
      "\n",
      "\n",
      "\tEpisode 186 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4298],\n",
      "        [0.9998, 0.5345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9992, 0.5007],\n",
      "        [0.9994, 0.5302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484222412109375 \tStep Time:  0.006966114044189453 s \tTotal Time:  1.224717617034912 s \n",
      "\n",
      "\n",
      "\tEpisode 187 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4757],\n",
      "        [0.9993, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4731],\n",
      "        [0.9993, 0.5206]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466808080673218 \tStep Time:  0.004985809326171875 s \tTotal Time:  1.2306866645812988 s \n",
      "\n",
      "\n",
      "\tEpisode 188 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5362],\n",
      "        [0.9995, 0.5364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4541],\n",
      "        [0.9999, 0.4660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44756269454956 \tStep Time:  0.007979154586791992 s \tTotal Time:  1.2386658191680908 s \n",
      "\n",
      "\n",
      "\tEpisode 189 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5170],\n",
      "        [0.9995, 0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5109],\n",
      "        [0.9993, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527488946914673 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.2456471920013428 s \n",
      "\n",
      "\n",
      "\tEpisode 190 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5073],\n",
      "        [0.9993, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4816],\n",
      "        [0.9993, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53199577331543 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.2516307830810547 s \n",
      "\n",
      "\n",
      "\tEpisode 191 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5172],\n",
      "        [0.9999, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4267],\n",
      "        [0.9993, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567870140075684 \tStep Time:  0.007977485656738281 s \tTotal Time:  1.259608268737793 s \n",
      "\n",
      "\n",
      "\tEpisode 192 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5206],\n",
      "        [0.9999, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5231],\n",
      "        [0.9993, 0.5232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507347583770752 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.2665894031524658 s \n",
      "\n",
      "\n",
      "\tEpisode 193 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5213],\n",
      "        [1.0000, 0.3467]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5220],\n",
      "        [0.9995, 0.5252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.431162357330322 \tStep Time:  0.006981611251831055 s \tTotal Time:  1.2735710144042969 s \n",
      "\n",
      "\n",
      "\tEpisode 194 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5272],\n",
      "        [0.9995, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4123],\n",
      "        [1.0000, 0.2853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.369020462036133 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.2805521488189697 s \n",
      "\n",
      "\n",
      "\tEpisode 195 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5633],\n",
      "        [0.9996, 0.5535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5965],\n",
      "        [0.9995, 0.5670]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54270601272583 \tStep Time:  0.006983041763305664 s \tTotal Time:  1.2875351905822754 s \n",
      "\n",
      "\n",
      "\tEpisode 196 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3729],\n",
      "        [0.9993, 0.5562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5285],\n",
      "        [0.9993, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455145835876465 \tStep Time:  0.005982160568237305 s \tTotal Time:  1.2935173511505127 s \n",
      "\n",
      "\n",
      "\tEpisode 197 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3399],\n",
      "        [0.9994, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5916],\n",
      "        [0.9993, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571115970611572 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.3004987239837646 s \n",
      "\n",
      "\n",
      "\tEpisode 198 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5156],\n",
      "        [1.0000, 0.5900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4809],\n",
      "        [0.9994, 0.5632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441396713256836 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.3064827919006348 s \n",
      "\n",
      "\n",
      "\tEpisode 199 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4670],\n",
      "        [0.9993, 0.5314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5543],\n",
      "        [0.9995, 0.5973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467081546783447 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.3124668598175049 s \n",
      "\n",
      "\n",
      "\tEpisode 200 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.6052],\n",
      "        [0.9998, 0.3799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1197],\n",
      "        [0.9993, 0.5523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.753979206085205 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.318450689315796 s \n",
      "\n",
      "\n",
      "\tEpisode 201 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4237],\n",
      "        [0.9994, 0.5770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6192],\n",
      "        [0.9993, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552854537963867 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.324434757232666 s \n",
      "\n",
      "\n",
      "\tEpisode 202 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5434],\n",
      "        [0.9993, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.2793],\n",
      "        [0.9997, 0.6321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.361920833587646 \tStep Time:  0.004986763000488281 s \tTotal Time:  1.3294215202331543 s \n",
      "\n",
      "\n",
      "\tEpisode 203 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.6080],\n",
      "        [1.0000, 0.1654]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5761],\n",
      "        [0.9993, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.333076000213623 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.3354058265686035 s \n",
      "\n",
      "\n",
      "\tEpisode 204 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5947],\n",
      "        [1.0000, 0.7233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.3836],\n",
      "        [0.9999, 0.2720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.256110191345215 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.3423871994018555 s \n",
      "\n",
      "\n",
      "\tEpisode 205 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3369],\n",
      "        [1.0000, 0.2074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.6454],\n",
      "        [0.9996, 0.6720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528719007968903 \tStep Time:  0.004986286163330078 s \tTotal Time:  1.3473734855651855 s \n",
      "\n",
      "\n",
      "\tEpisode 206 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5809],\n",
      "        [1.0000, 0.8162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.6319],\n",
      "        [0.9999, 0.2809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555234432220459 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.3543546199798584 s \n",
      "\n",
      "\n",
      "\tEpisode 207 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8125],\n",
      "        [0.9993, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.3671],\n",
      "        [0.9994, 0.6331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.88490915298462 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.3603389263153076 s \n",
      "\n",
      "\n",
      "\tEpisode 208 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4353],\n",
      "        [0.9993, 0.5296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1891],\n",
      "        [0.9996, 0.6522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.356977939605713 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.3663227558135986 s \n",
      "\n",
      "\n",
      "\tEpisode 209 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.6275],\n",
      "        [0.9993, 0.5330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.6087],\n",
      "        [0.9995, 0.6260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509304642677307 \tStep Time:  0.0060160160064697266 s \tTotal Time:  1.3723387718200684 s \n",
      "\n",
      "\n",
      "\tEpisode 210 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5507],\n",
      "        [0.9995, 0.4557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5794],\n",
      "        [0.9999, 0.7094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.700256824493408 \tStep Time:  0.0059850215911865234 s \tTotal Time:  1.3783237934112549 s \n",
      "\n",
      "\n",
      "\tEpisode 211 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5215],\n",
      "        [0.9998, 0.6697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4851],\n",
      "        [0.9998, 0.2909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.398017883300781 \tStep Time:  0.004990577697753906 s \tTotal Time:  1.3833143711090088 s \n",
      "\n",
      "\n",
      "\tEpisode 212 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5976],\n",
      "        [0.9997, 0.3814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4400],\n",
      "        [0.9995, 0.4537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.664571046829224 \tStep Time:  0.006975889205932617 s \tTotal Time:  1.3902902603149414 s \n",
      "\n",
      "\n",
      "\tEpisode 213 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1312],\n",
      "        [0.9996, 0.6220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.2655],\n",
      "        [0.9994, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53673791885376 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.3962745666503906 s \n",
      "\n",
      "\n",
      "\tEpisode 214 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9993, 0.5267],\n",
      "        [0.9996, 0.4222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6243],\n",
      "        [0.9995, 0.4669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458566665649414 \tStep Time:  0.004986286163330078 s \tTotal Time:  1.4012608528137207 s \n",
      "\n",
      "\n",
      "\tEpisode 215 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1139],\n",
      "        [0.9998, 0.6350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.2528],\n",
      "        [0.9996, 0.6202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.589894652366638 \tStep Time:  0.005986213684082031 s \tTotal Time:  1.4072470664978027 s \n",
      "\n",
      "\n",
      "\tEpisode 216 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5566],\n",
      "        [1.0000, 0.6588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0675],\n",
      "        [0.9999, 0.2566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525728225708008 \tStep Time:  0.005983114242553711 s \tTotal Time:  1.4132301807403564 s \n",
      "\n",
      "\n",
      "\tEpisode 217 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.6077],\n",
      "        [0.9998, 0.6419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5865],\n",
      "        [0.9993, 0.5485]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565248966217041 \tStep Time:  0.004987001419067383 s \tTotal Time:  1.4182171821594238 s \n",
      "\n",
      "\n",
      "\tEpisode 218 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3519],\n",
      "        [1.0000, 0.6559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2450],\n",
      "        [0.9995, 0.5871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60604178905487 \tStep Time:  0.006949901580810547 s \tTotal Time:  1.4251670837402344 s \n",
      "\n",
      "\n",
      "\tEpisode 219 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5516],\n",
      "        [0.9999, 0.6231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5866],\n",
      "        [0.9996, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4466872215271 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.4311506748199463 s \n",
      "\n",
      "\n",
      "\tEpisode 220 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6013],\n",
      "        [1.0000, 0.1166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4715],\n",
      "        [0.9994, 0.5727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.431960105895996 \tStep Time:  0.006982564926147461 s \tTotal Time:  1.4381332397460938 s \n",
      "\n",
      "\n",
      "\tEpisode 221 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3751],\n",
      "        [0.9994, 0.5314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2791],\n",
      "        [0.9994, 0.5692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501646041870117 \tStep Time:  0.007978200912475586 s \tTotal Time:  1.4461114406585693 s \n",
      "\n",
      "\n",
      "\tEpisode 222 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5445],\n",
      "        [0.9994, 0.5588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4233],\n",
      "        [1.0000, 0.2860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.350183963775635 \tStep Time:  0.00698089599609375 s \tTotal Time:  1.453092336654663 s \n",
      "\n",
      "\n",
      "\tEpisode 223 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5449],\n",
      "        [1.0000, 0.6157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6195],\n",
      "        [1.0000, 0.2493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.364677786827087 \tStep Time:  0.0059850215911865234 s \tTotal Time:  1.4590773582458496 s \n",
      "\n",
      "\n",
      "\tEpisode 224 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.6080],\n",
      "        [0.9998, 0.6297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3059],\n",
      "        [0.9995, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.643879413604736 \tStep Time:  0.005983114242553711 s \tTotal Time:  1.466057538986206 s \n",
      "\n",
      "\n",
      "\tEpisode 225 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3419],\n",
      "        [1.0000, 0.2796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4706],\n",
      "        [0.9999, 0.6535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444905936717987 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.473038911819458 s \n",
      "\n",
      "\n",
      "\tEpisode 226 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6543],\n",
      "        [0.9994, 0.5902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4894],\n",
      "        [0.9999, 0.3706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541170597076416 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.4790229797363281 s \n",
      "\n",
      "\n",
      "\tEpisode 227 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4919],\n",
      "        [0.9996, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2713],\n",
      "        [0.9997, 0.6546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.392574191093445 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.4850068092346191 s \n",
      "\n",
      "\n",
      "\tEpisode 228 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2510],\n",
      "        [0.9998, 0.6690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.6330],\n",
      "        [0.9995, 0.5407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.704140186309814 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.491988182067871 s \n",
      "\n",
      "\n",
      "\tEpisode 229 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1323],\n",
      "        [0.9999, 0.3718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5826],\n",
      "        [0.9994, 0.5661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489192962646484 \tStep Time:  0.00598454475402832 s \tTotal Time:  1.4979727268218994 s \n",
      "\n",
      "\n",
      "\tEpisode 230 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2195],\n",
      "        [0.9995, 0.5348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5235],\n",
      "        [0.9995, 0.5640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.720017433166504 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.5039565563201904 s \n",
      "\n",
      "\n",
      "\tEpisode 231 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.6660],\n",
      "        [1.0000, 0.7118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6993],\n",
      "        [1.0000, 0.6974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.643884181976318 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.5099401473999023 s \n",
      "\n",
      "\n",
      "\tEpisode 232 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6687],\n",
      "        [0.9994, 0.6100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.6418],\n",
      "        [0.9995, 0.5557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487833499908447 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.5159242153167725 s \n",
      "\n",
      "\n",
      "\tEpisode 233 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5741],\n",
      "        [0.9994, 0.5648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6392],\n",
      "        [0.9994, 0.5639]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580945014953613 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.5229053497314453 s \n",
      "\n",
      "\n",
      "\tEpisode 234 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4054],\n",
      "        [0.9995, 0.6007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5748],\n",
      "        [0.9998, 0.6080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46558427810669 \tStep Time:  0.004986763000488281 s \tTotal Time:  1.5278921127319336 s \n",
      "\n",
      "\n",
      "\tEpisode 235 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5430],\n",
      "        [0.9996, 0.5055]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5389],\n",
      "        [0.9998, 0.5914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537771701812744 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.5338761806488037 s \n",
      "\n",
      "\n",
      "\tEpisode 236 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3618],\n",
      "        [0.9995, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5622],\n",
      "        [0.9994, 0.5274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46582806110382 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.5408575534820557 s \n",
      "\n",
      "\n",
      "\tEpisode 237 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4759],\n",
      "        [0.9997, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5460],\n",
      "        [0.9996, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556358814239502 \tStep Time:  0.004986763000488281 s \tTotal Time:  1.545844316482544 s \n",
      "\n",
      "\n",
      "\tEpisode 238 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5372],\n",
      "        [1.0000, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5199],\n",
      "        [0.9996, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504267692565918 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.551828145980835 s \n",
      "\n",
      "\n",
      "\tEpisode 239 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5117],\n",
      "        [0.9994, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4462],\n",
      "        [0.9997, 0.4804]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496452808380127 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.557812213897705 s \n",
      "\n",
      "\n",
      "\tEpisode 240 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4965],\n",
      "        [0.9998, 0.4414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4623],\n",
      "        [0.9994, 0.4968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500973582267761 \tStep Time:  0.004986763000488281 s \tTotal Time:  1.5637962818145752 s \n",
      "\n",
      "\n",
      "\tEpisode 241 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4673],\n",
      "        [0.9999, 0.4141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4816],\n",
      "        [1.0000, 0.3423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438366532325745 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.5697801113128662 s \n",
      "\n",
      "\n",
      "\tEpisode 242 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2683],\n",
      "        [1.0000, 0.3786]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4191],\n",
      "        [0.9996, 0.4951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450857162475586 \tStep Time:  0.004987478256225586 s \tTotal Time:  1.5747675895690918 s \n",
      "\n",
      "\n",
      "\tEpisode 243 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4865],\n",
      "        [0.9997, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4179],\n",
      "        [0.9998, 0.4746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559512078762054 \tStep Time:  0.004986763000488281 s \tTotal Time:  1.5807509422302246 s \n",
      "\n",
      "\n",
      "\tEpisode 244 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4831],\n",
      "        [0.9996, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3619],\n",
      "        [0.9994, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579326629638672 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.5867350101470947 s \n",
      "\n",
      "\n",
      "\tEpisode 245 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4269],\n",
      "        [0.9995, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4896],\n",
      "        [0.9999, 0.3934]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.613930225372314 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.5927188396453857 s \n",
      "\n",
      "\n",
      "\tEpisode 246 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4936],\n",
      "        [0.9995, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4859],\n",
      "        [0.9998, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534873485565186 \tStep Time:  0.00498652458190918 s \tTotal Time:  1.597705364227295 s \n",
      "\n",
      "\n",
      "\tEpisode 247 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4703],\n",
      "        [0.9995, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4797],\n",
      "        [0.9999, 0.3623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599704265594482 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.6036896705627441 s \n",
      "\n",
      "\n",
      "\tEpisode 248 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3664],\n",
      "        [0.9994, 0.5145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.4977],\n",
      "        [0.9995, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.454739570617676 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.6096735000610352 s \n",
      "\n",
      "\n",
      "\tEpisode 249 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2114],\n",
      "        [0.9994, 0.5238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4936],\n",
      "        [0.9995, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.706348896026611 \tStep Time:  0.00498652458190918 s \tTotal Time:  1.6146600246429443 s \n",
      "\n",
      "\n",
      "\tEpisode 250 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4339],\n",
      "        [0.9997, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5453],\n",
      "        [0.9994, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.602907180786133 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.6206440925598145 s \n",
      "\n",
      "\n",
      "\tEpisode 251 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4893],\n",
      "        [0.9997, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5492],\n",
      "        [1.0000, 0.4421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61393529176712 \tStep Time:  0.006981372833251953 s \tTotal Time:  1.6276254653930664 s \n",
      "\n",
      "\n",
      "\tEpisode 252 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4943],\n",
      "        [0.9995, 0.5514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5681],\n",
      "        [0.9996, 0.5675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506640672683716 \tStep Time:  0.00498652458190918 s \tTotal Time:  1.6326119899749756 s \n",
      "\n",
      "\n",
      "\tEpisode 253 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5539],\n",
      "        [0.9994, 0.5564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5870],\n",
      "        [1.0000, 0.3081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451752185821533 \tStep Time:  0.0069811344146728516 s \tTotal Time:  1.6395931243896484 s \n",
      "\n",
      "\n",
      "\tEpisode 254 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5635],\n",
      "        [0.9997, 0.5912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4374],\n",
      "        [0.9996, 0.5982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487226963043213 \tStep Time:  0.006980180740356445 s \tTotal Time:  1.6465733051300049 s \n",
      "\n",
      "\n",
      "\tEpisode 255 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4947],\n",
      "        [0.9994, 0.5654]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9994, 0.5680],\n",
      "        [0.9996, 0.5358]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526148319244385 \tStep Time:  0.0059854984283447266 s \tTotal Time:  1.6525588035583496 s \n",
      "\n",
      "\n",
      "\tEpisode 256 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5998],\n",
      "        [0.9999, 0.6033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5474],\n",
      "        [0.9998, 0.5993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578904628753662 \tStep Time:  0.006014347076416016 s \tTotal Time:  1.6585731506347656 s \n",
      "\n",
      "\n",
      "\tEpisode 257 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4295],\n",
      "        [0.9995, 0.5613]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5014],\n",
      "        [0.9997, 0.5888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473595142364502 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.6645567417144775 s \n",
      "\n",
      "\n",
      "\tEpisode 258 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4827],\n",
      "        [0.9997, 0.4733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5729],\n",
      "        [0.9999, 0.5938]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.640312671661377 \tStep Time:  0.004956483840942383 s \tTotal Time:  1.66951322555542 s \n",
      "\n",
      "\n",
      "\tEpisode 259 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5321],\n",
      "        [0.9996, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4720],\n",
      "        [0.9996, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54516887664795 \tStep Time:  0.007012605667114258 s \tTotal Time:  1.6765258312225342 s \n",
      "\n",
      "\n",
      "\tEpisode 260 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3390],\n",
      "        [0.9995, 0.5265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5389],\n",
      "        [0.9997, 0.5651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445913434028625 \tStep Time:  0.004987239837646484 s \tTotal Time:  1.6815130710601807 s \n",
      "\n",
      "\n",
      "\tEpisode 261 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5452],\n",
      "        [0.9996, 0.5556]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3339],\n",
      "        [0.9995, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418965339660645 \tStep Time:  0.00598597526550293 s \tTotal Time:  1.6874990463256836 s \n",
      "\n",
      "\n",
      "\tEpisode 262 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5362],\n",
      "        [0.9999, 0.3763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3455],\n",
      "        [0.9995, 0.5454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531967878341675 \tStep Time:  0.005980014801025391 s \tTotal Time:  1.693479061126709 s \n",
      "\n",
      "\n",
      "\tEpisode 263 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5376],\n",
      "        [0.9995, 0.5265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4463],\n",
      "        [0.9998, 0.4665]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520410537719727 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.699463129043579 s \n",
      "\n",
      "\n",
      "\tEpisode 264 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5305],\n",
      "        [1.0000, 0.2874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5433],\n",
      "        [0.9999, 0.5656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.688758373260498 \tStep Time:  0.004987955093383789 s \tTotal Time:  1.704451084136963 s \n",
      "\n",
      "\n",
      "\tEpisode 265 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5676],\n",
      "        [0.9998, 0.4497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5220],\n",
      "        [0.9999, 0.4059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557117462158203 \tStep Time:  0.005985260009765625 s \tTotal Time:  1.7104363441467285 s \n",
      "\n",
      "\n",
      "\tEpisode 266 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3855],\n",
      "        [0.9999, 0.4287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5589],\n",
      "        [0.9997, 0.5606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562220573425293 \tStep Time:  0.00498509407043457 s \tTotal Time:  1.715421438217163 s \n",
      "\n",
      "\n",
      "\tEpisode 267 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5534],\n",
      "        [0.9999, 0.4131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5349],\n",
      "        [0.9995, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57975149154663 \tStep Time:  0.00499272346496582 s \tTotal Time:  1.720414161682129 s \n",
      "\n",
      "\n",
      "\tEpisode 268 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5523],\n",
      "        [0.9995, 0.5419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5163],\n",
      "        [1.0000, 0.3968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46036672592163 \tStep Time:  0.004980564117431641 s \tTotal Time:  1.7253947257995605 s \n",
      "\n",
      "\n",
      "\tEpisode 269 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5564],\n",
      "        [0.9999, 0.4577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5367],\n",
      "        [0.9999, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514484286308289 \tStep Time:  0.0059909820556640625 s \tTotal Time:  1.7313857078552246 s \n",
      "\n",
      "\n",
      "\tEpisode 270 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5495],\n",
      "        [0.9995, 0.5480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4944],\n",
      "        [0.9998, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456790924072266 \tStep Time:  0.005958080291748047 s \tTotal Time:  1.7373437881469727 s \n",
      "\n",
      "\n",
      "\tEpisode 271 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5492],\n",
      "        [1.0000, 0.3885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5557],\n",
      "        [0.9995, 0.5663]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.620839595794678 \tStep Time:  0.005998849868774414 s \tTotal Time:  1.743342638015747 s \n",
      "\n",
      "\n",
      "\tEpisode 272 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5516],\n",
      "        [0.9997, 0.5814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5673],\n",
      "        [0.9999, 0.4284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461337089538574 \tStep Time:  0.005993843078613281 s \tTotal Time:  1.7493364810943604 s \n",
      "\n",
      "\n",
      "\tEpisode 273 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5729],\n",
      "        [1.0000, 0.3203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5206],\n",
      "        [1.0000, 0.3191]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534255981445312 \tStep Time:  0.0059735774993896484 s \tTotal Time:  1.75531005859375 s \n",
      "\n",
      "\n",
      "\tEpisode 274 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4506],\n",
      "        [1.0000, 0.3059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6171],\n",
      "        [0.9995, 0.5724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446134269237518 \tStep Time:  0.004991292953491211 s \tTotal Time:  1.7603013515472412 s \n",
      "\n",
      "\n",
      "\tEpisode 275 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5227],\n",
      "        [0.9996, 0.5820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6066],\n",
      "        [0.9999, 0.4318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502243995666504 \tStep Time:  0.005985736846923828 s \tTotal Time:  1.766287088394165 s \n",
      "\n",
      "\n",
      "\tEpisode 276 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6080],\n",
      "        [0.9998, 0.6196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6295],\n",
      "        [0.9996, 0.6010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52413272857666 \tStep Time:  0.00498509407043457 s \tTotal Time:  1.7712721824645996 s \n",
      "\n",
      "\n",
      "\tEpisode 277 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5197],\n",
      "        [1.0000, 0.3432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6094],\n",
      "        [0.9999, 0.4290]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.355499744415283 \tStep Time:  0.004986286163330078 s \tTotal Time:  1.7762584686279297 s \n",
      "\n",
      "\n",
      "\tEpisode 278 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5445],\n",
      "        [1.0000, 0.6497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4426],\n",
      "        [0.9995, 0.5372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.631125271320343 \tStep Time:  0.005986690521240234 s \tTotal Time:  1.78224515914917 s \n",
      "\n",
      "\n",
      "\tEpisode 279 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5369],\n",
      "        [0.9995, 0.5552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.6042],\n",
      "        [0.9996, 0.5775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570195198059082 \tStep Time:  0.005979299545288086 s \tTotal Time:  1.788224458694458 s \n",
      "\n",
      "\n",
      "\tEpisode 280 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5816],\n",
      "        [1.0000, 0.6101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5919],\n",
      "        [0.9995, 0.5311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56459379196167 \tStep Time:  0.0059871673583984375 s \tTotal Time:  1.7942116260528564 s \n",
      "\n",
      "\n",
      "\tEpisode 281 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5139],\n",
      "        [0.9995, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5631],\n",
      "        [0.9996, 0.5368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516307353973389 \tStep Time:  0.0049555301666259766 s \tTotal Time:  1.7991671562194824 s \n",
      "\n",
      "\n",
      "\tEpisode 282 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5435],\n",
      "        [0.9998, 0.5481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4961],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56130838394165 \tStep Time:  0.0060138702392578125 s \tTotal Time:  1.8051810264587402 s \n",
      "\n",
      "\n",
      "\tEpisode 283 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4858],\n",
      "        [1.0000, 0.2041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5032],\n",
      "        [0.9996, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.40230417251587 \tStep Time:  0.0049555301666259766 s \tTotal Time:  1.8101365566253662 s \n",
      "\n",
      "\n",
      "\tEpisode 284 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4999],\n",
      "        [0.9995, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.4788],\n",
      "        [1.0000, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504021644592285 \tStep Time:  0.006024599075317383 s \tTotal Time:  1.8161611557006836 s \n",
      "\n",
      "\n",
      "\tEpisode 285 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5008],\n",
      "        [0.9999, 0.3949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4958],\n",
      "        [0.9996, 0.4766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555324614048004 \tStep Time:  0.005942821502685547 s \tTotal Time:  1.8221039772033691 s \n",
      "\n",
      "\n",
      "\tEpisode 286 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4982],\n",
      "        [0.9996, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2265],\n",
      "        [1.0000, 0.3471]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.355211734771729 \tStep Time:  0.0069828033447265625 s \tTotal Time:  1.8290867805480957 s \n",
      "\n",
      "\n",
      "\tEpisode 287 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3034],\n",
      "        [0.9996, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5214],\n",
      "        [0.9996, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.621541142463684 \tStep Time:  0.00498652458190918 s \tTotal Time:  1.8340733051300049 s \n",
      "\n",
      "\n",
      "\tEpisode 288 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5021],\n",
      "        [0.9995, 0.5012]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5013],\n",
      "        [1.0000, 0.3353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436503171920776 \tStep Time:  0.006982088088989258 s \tTotal Time:  1.842052698135376 s \n",
      "\n",
      "\n",
      "\tEpisode 289 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5391],\n",
      "        [0.9999, 0.4013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4278],\n",
      "        [0.9996, 0.4981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41019582748413 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.8480360507965088 s \n",
      "\n",
      "\n",
      "\tEpisode 290 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5149],\n",
      "        [0.9999, 0.4123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4904],\n",
      "        [0.9999, 0.5684]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.602068901062012 \tStep Time:  0.006990909576416016 s \tTotal Time:  1.8550269603729248 s \n",
      "\n",
      "\n",
      "\tEpisode 291 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5318],\n",
      "        [0.9996, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5286],\n",
      "        [0.9995, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509040415287018 \tStep Time:  0.006972312927246094 s \tTotal Time:  1.861999273300171 s \n",
      "\n",
      "\n",
      "\tEpisode 292 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5157],\n",
      "        [0.9996, 0.5416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3964],\n",
      "        [0.9995, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.609135329723358 \tStep Time:  0.004986286163330078 s \tTotal Time:  1.866985559463501 s \n",
      "\n",
      "\n",
      "\tEpisode 293 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5122],\n",
      "        [0.9995, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5172],\n",
      "        [0.9995, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512571811676025 \tStep Time:  0.007981300354003906 s \tTotal Time:  1.8749668598175049 s \n",
      "\n",
      "\n",
      "\tEpisode 294 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5091],\n",
      "        [0.9998, 0.4837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5124],\n",
      "        [0.9999, 0.4468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552355289459229 \tStep Time:  0.0069789886474609375 s \tTotal Time:  1.8819458484649658 s \n",
      "\n",
      "\n",
      "\tEpisode 295 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4983],\n",
      "        [1.0000, 0.3964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4767],\n",
      "        [1.0000, 0.4135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515811443328857 \tStep Time:  0.005988597869873047 s \tTotal Time:  1.8879344463348389 s \n",
      "\n",
      "\n",
      "\tEpisode 296 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4004],\n",
      "        [0.9995, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4769],\n",
      "        [0.9999, 0.4405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555131912231445 \tStep Time:  0.00697636604309082 s \tTotal Time:  1.8949108123779297 s \n",
      "\n",
      "\n",
      "\tEpisode 297 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5013],\n",
      "        [0.9995, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4881],\n",
      "        [0.9999, 0.4745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514676094055176 \tStep Time:  0.00498652458190918 s \tTotal Time:  1.9008946418762207 s \n",
      "\n",
      "\n",
      "\tEpisode 298 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5009],\n",
      "        [0.9998, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4967],\n",
      "        [0.9997, 0.4833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534608840942383 \tStep Time:  0.005984783172607422 s \tTotal Time:  1.9068794250488281 s \n",
      "\n",
      "\n",
      "\tEpisode 299 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4859],\n",
      "        [0.9995, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5326],\n",
      "        [0.9995, 0.5264]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528748035430908 \tStep Time:  0.0059833526611328125 s \tTotal Time:  1.912862777709961 s \n",
      "\n",
      "\n",
      "\tEpisode 300 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5064],\n",
      "        [0.9998, 0.5341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5418],\n",
      "        [0.9995, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522018730640411 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.918846607208252 s \n",
      "\n",
      "\n",
      "\tEpisode 301 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5058],\n",
      "        [0.9996, 0.5434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4864],\n",
      "        [0.9997, 0.4993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525795221328735 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.9248309135437012 s \n",
      "\n",
      "\n",
      "\tEpisode 302 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5068],\n",
      "        [0.9996, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5142],\n",
      "        [1.0000, 0.3258]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62267541885376 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.9308149814605713 s \n",
      "\n",
      "\n",
      "\tEpisode 303 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5055],\n",
      "        [0.9999, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4901],\n",
      "        [0.9997, 0.5490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53178071975708 \tStep Time:  0.005984306335449219 s \tTotal Time:  1.9367992877960205 s \n",
      "\n",
      "\n",
      "\tEpisode 304 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4529],\n",
      "        [1.0000, 0.3205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4492],\n",
      "        [1.0000, 0.4364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.604916095733643 \tStep Time:  0.006981849670410156 s \tTotal Time:  1.9437811374664307 s \n",
      "\n",
      "\n",
      "\tEpisode 305 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5567],\n",
      "        [1.0000, 0.2742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5562],\n",
      "        [0.9995, 0.5315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.69452428817749 \tStep Time:  0.005982875823974609 s \tTotal Time:  1.9497640132904053 s \n",
      "\n",
      "\n",
      "\tEpisode 306 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5262],\n",
      "        [0.9995, 0.5383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5352],\n",
      "        [0.9996, 0.5606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521886348724365 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.9557480812072754 s \n",
      "\n",
      "\n",
      "\tEpisode 307 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5453],\n",
      "        [1.0000, 0.4430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5598],\n",
      "        [0.9996, 0.5461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562080383300781 \tStep Time:  0.005985260009765625 s \tTotal Time:  1.961733341217041 s \n",
      "\n",
      "\n",
      "\tEpisode 308 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9995, 0.5534],\n",
      "        [0.9996, 0.5679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5089],\n",
      "        [0.9995, 0.5568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551623404026031 \tStep Time:  0.005983114242553711 s \tTotal Time:  1.9677164554595947 s \n",
      "\n",
      "\n",
      "\tEpisode 309 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5695],\n",
      "        [0.9997, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5317],\n",
      "        [0.9996, 0.5477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49904727935791 \tStep Time:  0.005983591079711914 s \tTotal Time:  1.9737000465393066 s \n",
      "\n",
      "\n",
      "\tEpisode 310 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6047],\n",
      "        [0.9995, 0.5548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5775],\n",
      "        [0.9996, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566858768463135 \tStep Time:  0.004987001419067383 s \tTotal Time:  1.978687047958374 s \n",
      "\n",
      "\n",
      "\tEpisode 311 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4409],\n",
      "        [0.9996, 0.5341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4149],\n",
      "        [1.0000, 0.6155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491037368774414 \tStep Time:  0.005983829498291016 s \tTotal Time:  1.984670877456665 s \n",
      "\n",
      "\n",
      "\tEpisode 312 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4753],\n",
      "        [0.9999, 0.6201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4982],\n",
      "        [0.9996, 0.5691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532588005065918 \tStep Time:  0.005984067916870117 s \tTotal Time:  1.9906549453735352 s \n",
      "\n",
      "\n",
      "\tEpisode 313 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5013],\n",
      "        [0.9998, 0.6054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6111],\n",
      "        [1.0000, 0.6487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.607529640197754 \tStep Time:  0.00897669792175293 s \tTotal Time:  1.999631643295288 s \n",
      "\n",
      "\n",
      "\tEpisode 314 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5871],\n",
      "        [0.9998, 0.5986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4969],\n",
      "        [0.9997, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515658378601074 \tStep Time:  0.005983114242553711 s \tTotal Time:  2.005614757537842 s \n",
      "\n",
      "\n",
      "\tEpisode 315 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5119],\n",
      "        [1.0000, 0.4323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5917],\n",
      "        [1.0000, 0.5974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55646562576294 \tStep Time:  0.004987001419067383 s \tTotal Time:  2.010601758956909 s \n",
      "\n",
      "\n",
      "\tEpisode 316 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5063],\n",
      "        [0.9996, 0.5541]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5644],\n",
      "        [0.9996, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568139493465424 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.0175833702087402 s \n",
      "\n",
      "\n",
      "\tEpisode 317 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5124],\n",
      "        [0.9996, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5513],\n",
      "        [0.9997, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534993827342987 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.0235671997070312 s \n",
      "\n",
      "\n",
      "\tEpisode 318 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5353],\n",
      "        [0.9996, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5338],\n",
      "        [0.9998, 0.5336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523694515228271 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.029550790786743 s \n",
      "\n",
      "\n",
      "\tEpisode 319 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5166],\n",
      "        [0.9999, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5228],\n",
      "        [0.9999, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52400016784668 \tStep Time:  0.008976221084594727 s \tTotal Time:  2.038527011871338 s \n",
      "\n",
      "\n",
      "\tEpisode 320 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5332],\n",
      "        [0.9996, 0.5126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4974],\n",
      "        [0.9996, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516480207443237 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.0455081462860107 s \n",
      "\n",
      "\n",
      "\tEpisode 321 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4994],\n",
      "        [0.9999, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5029],\n",
      "        [0.9996, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511480808258057 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.052489757537842 s \n",
      "\n",
      "\n",
      "\tEpisode 322 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3867],\n",
      "        [0.9996, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5260],\n",
      "        [1.0000, 0.4594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60508108139038 \tStep Time:  0.009008407592773438 s \tTotal Time:  2.0614981651306152 s \n",
      "\n",
      "\n",
      "\tEpisode 323 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5056],\n",
      "        [0.9999, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5159],\n",
      "        [0.9996, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498780727386475 \tStep Time:  0.004983663558959961 s \tTotal Time:  2.066481828689575 s \n",
      "\n",
      "\n",
      "\tEpisode 324 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5194],\n",
      "        [0.9998, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5205],\n",
      "        [0.9998, 0.5243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506585240364075 \tStep Time:  0.006952047348022461 s \tTotal Time:  2.0734338760375977 s \n",
      "\n",
      "\n",
      "\tEpisode 325 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5292],\n",
      "        [0.9999, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5353],\n",
      "        [0.9998, 0.5279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509972095489502 \tStep Time:  0.006024360656738281 s \tTotal Time:  2.079458236694336 s \n",
      "\n",
      "\n",
      "\tEpisode 326 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5552],\n",
      "        [0.9998, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5219],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497469425201416 \tStep Time:  0.00498652458190918 s \tTotal Time:  2.0854082107543945 s \n",
      "\n",
      "\n",
      "\tEpisode 327 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5137],\n",
      "        [1.0000, 0.5946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6222],\n",
      "        [0.9999, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528782367706299 \tStep Time:  0.009007692337036133 s \tTotal Time:  2.0944159030914307 s \n",
      "\n",
      "\n",
      "\tEpisode 328 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6112],\n",
      "        [0.9996, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4785],\n",
      "        [1.0000, 0.4807]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461524605751038 \tStep Time:  0.005992889404296875 s \tTotal Time:  2.1004087924957275 s \n",
      "\n",
      "\n",
      "\tEpisode 329 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5060],\n",
      "        [0.9996, 0.5457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6456],\n",
      "        [0.9996, 0.5288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.594545841217041 \tStep Time:  0.0070667266845703125 s \tTotal Time:  2.107475519180298 s \n",
      "\n",
      "\n",
      "\tEpisode 330 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6561],\n",
      "        [0.9998, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5856],\n",
      "        [0.9999, 0.4518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542547702789307 \tStep Time:  0.006010293960571289 s \tTotal Time:  2.113485813140869 s \n",
      "\n",
      "\n",
      "\tEpisode 331 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4418],\n",
      "        [1.0000, 0.4410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4929],\n",
      "        [0.9996, 0.4998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562295913696289 \tStep Time:  0.005951642990112305 s \tTotal Time:  2.1194374561309814 s \n",
      "\n",
      "\n",
      "\tEpisode 332 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6055],\n",
      "        [0.9998, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4575],\n",
      "        [1.0000, 0.6409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510880470275879 \tStep Time:  0.006018877029418945 s \tTotal Time:  2.1254563331604004 s \n",
      "\n",
      "\n",
      "\tEpisode 333 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5181],\n",
      "        [0.9999, 0.6022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5832],\n",
      "        [0.9999, 0.5979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5668363571167 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.1314404010772705 s \n",
      "\n",
      "\n",
      "\tEpisode 334 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5647],\n",
      "        [0.9998, 0.5658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5487],\n",
      "        [0.9996, 0.4857]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479930400848389 \tStep Time:  0.005950450897216797 s \tTotal Time:  2.1373908519744873 s \n",
      "\n",
      "\n",
      "\tEpisode 335 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5672],\n",
      "        [0.9998, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5154],\n",
      "        [0.9996, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577291905879974 \tStep Time:  0.0069806575775146484 s \tTotal Time:  2.144371509552002 s \n",
      "\n",
      "\n",
      "\tEpisode 336 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4698],\n",
      "        [0.9999, 0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5536],\n",
      "        [0.9996, 0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475220918655396 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.150355100631714 s \n",
      "\n",
      "\n",
      "\tEpisode 337 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4918],\n",
      "        [1.0000, 0.5275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4666],\n",
      "        [0.9996, 0.4694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489897727966309 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.1573362350463867 s \n",
      "\n",
      "\n",
      "\tEpisode 338 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5173],\n",
      "        [0.9996, 0.4633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4564],\n",
      "        [0.9997, 0.4832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532269477844238 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.163320302963257 s \n",
      "\n",
      "\n",
      "\tEpisode 339 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4771],\n",
      "        [0.9996, 0.4571]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4788],\n",
      "        [0.9996, 0.4584]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513096332550049 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.169304609298706 s \n",
      "\n",
      "\n",
      "\tEpisode 340 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4786],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4670],\n",
      "        [0.9996, 0.4612]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503030121326447 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.175288200378418 s \n",
      "\n",
      "\n",
      "\tEpisode 341 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4658],\n",
      "        [0.9997, 0.4786]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4607],\n",
      "        [0.9996, 0.4593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525206089019775 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.181272029876709 s \n",
      "\n",
      "\n",
      "\tEpisode 342 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4667],\n",
      "        [0.9996, 0.4630]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [0.9997, 0.4897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493412494659424 \tStep Time:  0.005984783172607422 s \tTotal Time:  2.1872568130493164 s \n",
      "\n",
      "\n",
      "\tEpisode 343 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5023],\n",
      "        [1.0000, 0.5616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4717],\n",
      "        [1.0000, 0.5580]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580806612968445 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.1932404041290283 s \n",
      "\n",
      "\n",
      "\tEpisode 344 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4702],\n",
      "        [0.9999, 0.5251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5349],\n",
      "        [0.9996, 0.4695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500243186950684 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.200221538543701 s \n",
      "\n",
      "\n",
      "\tEpisode 345 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4690],\n",
      "        [0.9998, 0.4970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5106],\n",
      "        [0.9998, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513935804367065 \tStep Time:  0.004986763000488281 s \tTotal Time:  2.2052083015441895 s \n",
      "\n",
      "\n",
      "\tEpisode 346 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5054],\n",
      "        [1.0000, 0.6119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4746],\n",
      "        [0.9999, 0.5675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477756023406982 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.2111923694610596 s \n",
      "\n",
      "\n",
      "\tEpisode 347 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4654],\n",
      "        [0.9996, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4832],\n",
      "        [1.0000, 0.3991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479111671447754 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.2171759605407715 s \n",
      "\n",
      "\n",
      "\tEpisode 348 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5025],\n",
      "        [0.9996, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4387],\n",
      "        [1.0000, 0.7545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.401074886322021 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.2231602668762207 s \n",
      "\n",
      "\n",
      "\tEpisode 349 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5353],\n",
      "        [1.0000, 0.3856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5165],\n",
      "        [0.9998, 0.5439]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444595336914062 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.2291440963745117 s \n",
      "\n",
      "\n",
      "\tEpisode 350 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3980],\n",
      "        [0.9998, 0.3980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7356],\n",
      "        [0.9999, 0.3825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.712557792663574 \tStep Time:  0.006982326507568359 s \tTotal Time:  2.23612642288208 s \n",
      "\n",
      "\n",
      "\tEpisode 351 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4171],\n",
      "        [1.0000, 0.6427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6277],\n",
      "        [0.9999, 0.3826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533193588256836 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.2421107292175293 s \n",
      "\n",
      "\n",
      "\tEpisode 352 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5227],\n",
      "        [1.0000, 0.6403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4457],\n",
      "        [0.9998, 0.4008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60936713218689 \tStep Time:  0.0069828033447265625 s \tTotal Time:  2.249093532562256 s \n",
      "\n",
      "\n",
      "\tEpisode 353 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6039],\n",
      "        [0.9996, 0.4547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5077],\n",
      "        [1.0000, 0.2673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.740952491760254 \tStep Time:  0.006979942321777344 s \tTotal Time:  2.256073474884033 s \n",
      "\n",
      "\n",
      "\tEpisode 354 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5666],\n",
      "        [0.9998, 0.4088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4933],\n",
      "        [0.9997, 0.5001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537304878234863 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.262057304382324 s \n",
      "\n",
      "\n",
      "\tEpisode 355 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4438],\n",
      "        [0.9996, 0.4817]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4705],\n",
      "        [0.9998, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56833302974701 \tStep Time:  0.005986213684082031 s \tTotal Time:  2.2680435180664062 s \n",
      "\n",
      "\n",
      "\tEpisode 356 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4793],\n",
      "        [0.9998, 0.4472]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4671],\n",
      "        [0.9999, 0.4221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487572312355042 \tStep Time:  0.0069789886474609375 s \tTotal Time:  2.275022506713867 s \n",
      "\n",
      "\n",
      "\tEpisode 357 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5280],\n",
      "        [0.9996, 0.4744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5048],\n",
      "        [1.0000, 0.3900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561591923236847 \tStep Time:  0.004987239837646484 s \tTotal Time:  2.2800097465515137 s \n",
      "\n",
      "\n",
      "\tEpisode 358 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4685],\n",
      "        [0.9996, 0.4693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4991],\n",
      "        [0.9997, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531927347183228 \tStep Time:  0.007978677749633789 s \tTotal Time:  2.2879884243011475 s \n",
      "\n",
      "\n",
      "\tEpisode 359 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4822],\n",
      "        [0.9997, 0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4821],\n",
      "        [0.9996, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527013778686523 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.2939720153808594 s \n",
      "\n",
      "\n",
      "\tEpisode 360 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4860],\n",
      "        [0.9997, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4920],\n",
      "        [1.0000, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539618492126465 \tStep Time:  0.00498652458190918 s \tTotal Time:  2.2989585399627686 s \n",
      "\n",
      "\n",
      "\tEpisode 361 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6270],\n",
      "        [0.9996, 0.4964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4995],\n",
      "        [0.9997, 0.5194]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447231769561768 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.3059401512145996 s \n",
      "\n",
      "\n",
      "\tEpisode 362 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5138],\n",
      "        [1.0000, 0.4980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5789],\n",
      "        [0.9996, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5668363571167 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.3119242191314697 s \n",
      "\n",
      "\n",
      "\tEpisode 363 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5790],\n",
      "        [0.9998, 0.5619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5830],\n",
      "        [0.9996, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491640567779541 \tStep Time:  0.004986286163330078 s \tTotal Time:  2.3169105052948 s \n",
      "\n",
      "\n",
      "\tEpisode 364 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4869],\n",
      "        [0.9997, 0.4849]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5332],\n",
      "        [0.9999, 0.4720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537634432315826 \tStep Time:  0.0069921016693115234 s \tTotal Time:  2.3239026069641113 s \n",
      "\n",
      "\n",
      "\tEpisode 365 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5058],\n",
      "        [0.9999, 0.5822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4749],\n",
      "        [0.9996, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553128719329834 \tStep Time:  0.004981040954589844 s \tTotal Time:  2.328883647918701 s \n",
      "\n",
      "\n",
      "\tEpisode 366 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5080],\n",
      "        [0.9997, 0.5350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5663],\n",
      "        [0.9998, 0.5616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530810356140137 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.334867238998413 s \n",
      "\n",
      "\n",
      "\tEpisode 367 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4858],\n",
      "        [0.9999, 0.4822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4809],\n",
      "        [0.9996, 0.4894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503489971160889 \tStep Time:  0.00598454475402832 s \tTotal Time:  2.3408517837524414 s \n",
      "\n",
      "\n",
      "\tEpisode 368 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4877],\n",
      "        [1.0000, 0.6994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5011],\n",
      "        [0.9996, 0.4851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.64705765247345 \tStep Time:  0.004986286163330078 s \tTotal Time:  2.3468353748321533 s \n",
      "\n",
      "\n",
      "\tEpisode 369 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5487],\n",
      "        [0.9996, 0.4865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4737],\n",
      "        [0.9999, 0.4739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477571487426758 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.3528196811676025 s \n",
      "\n",
      "\n",
      "\tEpisode 370 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5222],\n",
      "        [0.9998, 0.4701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4822],\n",
      "        [0.9996, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507862567901611 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.3598008155822754 s \n",
      "\n",
      "\n",
      "\tEpisode 371 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4947],\n",
      "        [1.0000, 0.5375]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4950],\n",
      "        [0.9999, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487042903900146 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.3657846450805664 s \n",
      "\n",
      "\n",
      "\tEpisode 372 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5504],\n",
      "        [0.9996, 0.4855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.7490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.387495577335358 \tStep Time:  0.004986763000488281 s \tTotal Time:  2.3707714080810547 s \n",
      "\n",
      "\n",
      "\tEpisode 373 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4715],\n",
      "        [0.9996, 0.4835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5376],\n",
      "        [1.0000, 0.4724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486879825592041 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.3777525424957275 s \n",
      "\n",
      "\n",
      "\tEpisode 374 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4562],\n",
      "        [0.9999, 0.4553]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4744],\n",
      "        [1.0000, 0.4606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504011154174805 \tStep Time:  0.004987001419067383 s \tTotal Time:  2.382739543914795 s \n",
      "\n",
      "\n",
      "\tEpisode 375 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4499],\n",
      "        [0.9998, 0.5452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4471],\n",
      "        [0.9997, 0.4527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468055129051208 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.389720916748047 s \n",
      "\n",
      "\n",
      "\tEpisode 376 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4967],\n",
      "        [0.9998, 0.5542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4503],\n",
      "        [0.9997, 0.4577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477629959583282 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.395705223083496 s \n",
      "\n",
      "\n",
      "\tEpisode 377 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5894],\n",
      "        [0.9996, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4770],\n",
      "        [1.0000, 0.6253]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.38461971282959 \tStep Time:  0.0059833526611328125 s \tTotal Time:  2.401688575744629 s \n",
      "\n",
      "\n",
      "\tEpisode 378 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4437],\n",
      "        [0.9998, 0.5730]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4570],\n",
      "        [0.9996, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474177837371826 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.407672643661499 s \n",
      "\n",
      "\n",
      "\tEpisode 379 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6780],\n",
      "        [1.0000, 0.6687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6294],\n",
      "        [1.0000, 0.4605]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463658511638641 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.41365647315979 s \n",
      "\n",
      "\n",
      "\tEpisode 380 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4540],\n",
      "        [0.9998, 0.5960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5217],\n",
      "        [0.9998, 0.6169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543611764907837 \tStep Time:  0.004987001419067383 s \tTotal Time:  2.4186434745788574 s \n",
      "\n",
      "\n",
      "\tEpisode 381 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4915],\n",
      "        [0.9996, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.5237],\n",
      "        [0.9996, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484828352928162 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.4246275424957275 s \n",
      "\n",
      "\n",
      "\tEpisode 382 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4246],\n",
      "        [0.9997, 0.4331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5275],\n",
      "        [0.9997, 0.5818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553757190704346 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.4306111335754395 s \n",
      "\n",
      "\n",
      "\tEpisode 383 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6343],\n",
      "        [0.9996, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5858],\n",
      "        [0.9996, 0.4584]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563671588897705 \tStep Time:  0.006003141403198242 s \tTotal Time:  2.4366142749786377 s \n",
      "\n",
      "\n",
      "\tEpisode 384 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9996, 0.4830],\n",
      "        [0.9998, 0.6151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5671],\n",
      "        [0.9996, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39718234539032 \tStep Time:  0.006003856658935547 s \tTotal Time:  2.443608045578003 s \n",
      "\n",
      "\n",
      "\tEpisode 385 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3700],\n",
      "        [0.9998, 0.6022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6386],\n",
      "        [0.9998, 0.3944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53937816619873 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.450589179992676 s \n",
      "\n",
      "\n",
      "\tEpisode 386 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5966],\n",
      "        [0.9996, 0.4578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6418],\n",
      "        [0.9999, 0.3605]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473144352436066 \tStep Time:  0.006948709487915039 s \tTotal Time:  2.457537889480591 s \n",
      "\n",
      "\n",
      "\tEpisode 387 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6771],\n",
      "        [0.9998, 0.3925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5286],\n",
      "        [1.0000, 0.3686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483793139457703 \tStep Time:  0.004986763000488281 s \tTotal Time:  2.462524652481079 s \n",
      "\n",
      "\n",
      "\tEpisode 388 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3841],\n",
      "        [1.0000, 0.3615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3654],\n",
      "        [0.9997, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62198543548584 \tStep Time:  0.008011341094970703 s \tTotal Time:  2.47053599357605 s \n",
      "\n",
      "\n",
      "\tEpisode 389 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3717],\n",
      "        [0.9996, 0.4844]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4080],\n",
      "        [1.0000, 0.7154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.35936975479126 \tStep Time:  0.0059850215911865234 s \tTotal Time:  2.4765210151672363 s \n",
      "\n",
      "\n",
      "\tEpisode 390 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4018],\n",
      "        [1.0000, 0.4320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4094],\n",
      "        [0.9997, 0.5518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478524208068848 \tStep Time:  0.004987478256225586 s \tTotal Time:  2.481508493423462 s \n",
      "\n",
      "\n",
      "\tEpisode 391 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.6184],\n",
      "        [0.9996, 0.4438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6915],\n",
      "        [1.0000, 0.7591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.64762407541275 \tStep Time:  0.0059506893157958984 s \tTotal Time:  2.487459182739258 s \n",
      "\n",
      "\n",
      "\tEpisode 392 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4073],\n",
      "        [0.9996, 0.4503]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7032],\n",
      "        [1.0000, 0.3995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437213897705078 \tStep Time:  0.006016731262207031 s \tTotal Time:  2.493475914001465 s \n",
      "\n",
      "\n",
      "\tEpisode 393 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3917],\n",
      "        [1.0000, 0.5641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7818],\n",
      "        [0.9999, 0.3836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459664344787598 \tStep Time:  0.0059506893157958984 s \tTotal Time:  2.4994266033172607 s \n",
      "\n",
      "\n",
      "\tEpisode 394 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3984],\n",
      "        [0.9998, 0.3901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5438],\n",
      "        [0.9997, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.686858177185059 \tStep Time:  0.005030155181884766 s \tTotal Time:  2.5044567584991455 s \n",
      "\n",
      "\n",
      "\tEpisode 395 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4046],\n",
      "        [0.9997, 0.5258]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4495],\n",
      "        [1.0000, 0.4274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591259479522705 \tStep Time:  0.005975961685180664 s \tTotal Time:  2.510432720184326 s \n",
      "\n",
      "\n",
      "\tEpisode 396 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4259],\n",
      "        [0.9999, 0.3973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4166],\n",
      "        [0.9998, 0.4102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555455207824707 \tStep Time:  0.00498199462890625 s \tTotal Time:  2.5154147148132324 s \n",
      "\n",
      "\n",
      "\tEpisode 397 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4167],\n",
      "        [0.9999, 0.6175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4784],\n",
      "        [0.9998, 0.5485]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.388914585113525 \tStep Time:  0.005986452102661133 s \tTotal Time:  2.5214011669158936 s \n",
      "\n",
      "\n",
      "\tEpisode 398 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4799],\n",
      "        [0.9999, 0.5923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6777],\n",
      "        [0.9997, 0.5344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.631762027740479 \tStep Time:  0.00498509407043457 s \tTotal Time:  2.526386260986328 s \n",
      "\n",
      "\n",
      "\tEpisode 399 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4441],\n",
      "        [0.9998, 0.4328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4410],\n",
      "        [0.9997, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528080463409424 \tStep Time:  0.0049860477447509766 s \tTotal Time:  2.531372308731079 s \n",
      "\n",
      "\n",
      "\tEpisode 400 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4303],\n",
      "        [0.9998, 0.4262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6015],\n",
      "        [1.0000, 0.6131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543729603290558 \tStep Time:  0.005965471267700195 s \tTotal Time:  2.5373377799987793 s \n",
      "\n",
      "\n",
      "\tEpisode 401 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4805],\n",
      "        [0.9997, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5500],\n",
      "        [1.0000, 0.4541]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536983489990234 \tStep Time:  0.0060040950775146484 s \tTotal Time:  2.543341875076294 s \n",
      "\n",
      "\n",
      "\tEpisode 402 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4709],\n",
      "        [0.9997, 0.4211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4260],\n",
      "        [0.9998, 0.4212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546072483062744 \tStep Time:  0.004987001419067383 s \tTotal Time:  2.5483288764953613 s \n",
      "\n",
      "\n",
      "\tEpisode 403 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4998],\n",
      "        [0.9997, 0.4316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4825],\n",
      "        [1.0000, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539620876312256 \tStep Time:  0.005991697311401367 s \tTotal Time:  2.5543205738067627 s \n",
      "\n",
      "\n",
      "\tEpisode 404 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5169],\n",
      "        [0.9998, 0.5809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6321],\n",
      "        [1.0000, 0.7916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43981647491455 \tStep Time:  0.0059740543365478516 s \tTotal Time:  2.5602946281433105 s \n",
      "\n",
      "\n",
      "\tEpisode 405 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4313],\n",
      "        [0.9999, 0.4494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6900],\n",
      "        [1.0000, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447196125984192 \tStep Time:  0.004987001419067383 s \tTotal Time:  2.565281629562378 s \n",
      "\n",
      "\n",
      "\tEpisode 406 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4234],\n",
      "        [0.9997, 0.4587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4448],\n",
      "        [0.9998, 0.4205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550290584564209 \tStep Time:  0.0059850215911865234 s \tTotal Time:  2.5712666511535645 s \n",
      "\n",
      "\n",
      "\tEpisode 407 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4647],\n",
      "        [0.9998, 0.4200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4204],\n",
      "        [0.9998, 0.4185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554683685302734 \tStep Time:  0.0069806575775146484 s \tTotal Time:  2.578247308731079 s \n",
      "\n",
      "\n",
      "\tEpisode 408 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6715],\n",
      "        [1.0000, 0.7988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6659],\n",
      "        [0.9998, 0.4205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.650262951850891 \tStep Time:  0.00499272346496582 s \tTotal Time:  2.583240032196045 s \n",
      "\n",
      "\n",
      "\tEpisode 409 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4234],\n",
      "        [0.9999, 0.6671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4570],\n",
      "        [0.9997, 0.4261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424246788024902 \tStep Time:  0.00697779655456543 s \tTotal Time:  2.5902178287506104 s \n",
      "\n",
      "\n",
      "\tEpisode 410 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4793],\n",
      "        [0.9997, 0.5323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6818],\n",
      "        [0.9999, 0.4347]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583850860595703 \tStep Time:  0.005983114242553711 s \tTotal Time:  2.596200942993164 s \n",
      "\n",
      "\n",
      "\tEpisode 411 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5469],\n",
      "        [1.0000, 0.4699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7044],\n",
      "        [0.9997, 0.4372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601972579956055 \tStep Time:  0.004991769790649414 s \tTotal Time:  2.6011927127838135 s \n",
      "\n",
      "\n",
      "\tEpisode 412 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5688],\n",
      "        [1.0000, 0.6369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4646],\n",
      "        [1.0000, 0.5537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551424980163574 \tStep Time:  0.006976604461669922 s \tTotal Time:  2.6081693172454834 s \n",
      "\n",
      "\n",
      "\tEpisode 413 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6495],\n",
      "        [0.9998, 0.5001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4488],\n",
      "        [0.9998, 0.4594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.404685497283936 \tStep Time:  0.005981922149658203 s \tTotal Time:  2.6141512393951416 s \n",
      "\n",
      "\n",
      "\tEpisode 414 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4401],\n",
      "        [0.9998, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5506],\n",
      "        [0.9999, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588701725006104 \tStep Time:  0.0059528350830078125 s \tTotal Time:  2.6201040744781494 s \n",
      "\n",
      "\n",
      "\tEpisode 415 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4438],\n",
      "        [1.0000, 0.6507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4716],\n",
      "        [0.9999, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473107814788818 \tStep Time:  0.0070133209228515625 s \tTotal Time:  2.627117395401001 s \n",
      "\n",
      "\n",
      "\tEpisode 416 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4748],\n",
      "        [0.9998, 0.4486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4532],\n",
      "        [0.9997, 0.4255]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532121181488037 \tStep Time:  0.004998207092285156 s \tTotal Time:  2.632115602493286 s \n",
      "\n",
      "\n",
      "\tEpisode 417 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4500],\n",
      "        [0.9998, 0.4403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4308],\n",
      "        [0.9999, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509981215000153 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.6390862464904785 s \n",
      "\n",
      "\n",
      "\tEpisode 418 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5709],\n",
      "        [0.9997, 0.4500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4821],\n",
      "        [0.9998, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503996849060059 \tStep Time:  0.008135557174682617 s \tTotal Time:  2.647221803665161 s \n",
      "\n",
      "\n",
      "\tEpisode 419 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6276],\n",
      "        [0.9997, 0.4294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4287],\n",
      "        [0.9998, 0.4799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599699020385742 \tStep Time:  0.00498652458190918 s \tTotal Time:  2.6522083282470703 s \n",
      "\n",
      "\n",
      "\tEpisode 420 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7536],\n",
      "        [0.9997, 0.4378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5382],\n",
      "        [0.9999, 0.4974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466960430145264 \tStep Time:  0.0079803466796875 s \tTotal Time:  2.660188674926758 s \n",
      "\n",
      "\n",
      "\tEpisode 421 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5286],\n",
      "        [0.9997, 0.4236]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6544],\n",
      "        [0.9997, 0.4274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592695713043213 \tStep Time:  0.006979465484619141 s \tTotal Time:  2.667168140411377 s \n",
      "\n",
      "\n",
      "\tEpisode 422 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4498],\n",
      "        [0.9999, 0.4535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4217],\n",
      "        [0.9997, 0.4215]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516305029392242 \tStep Time:  0.00598454475402832 s \tTotal Time:  2.6731526851654053 s \n",
      "\n",
      "\n",
      "\tEpisode 423 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5780],\n",
      "        [1.0000, 0.7034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7218],\n",
      "        [1.0000, 0.4360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.619186878204346 \tStep Time:  0.006982564926147461 s \tTotal Time:  2.6801352500915527 s \n",
      "\n",
      "\n",
      "\tEpisode 424 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4395],\n",
      "        [0.9998, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4883],\n",
      "        [1.0000, 0.5860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6073317527771 \tStep Time:  0.005982398986816406 s \tTotal Time:  2.686117649078369 s \n",
      "\n",
      "\n",
      "\tEpisode 425 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4114],\n",
      "        [1.0000, 0.5706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4209],\n",
      "        [0.9997, 0.4231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.620104312896729 \tStep Time:  0.007978677749633789 s \tTotal Time:  2.694096326828003 s \n",
      "\n",
      "\n",
      "\tEpisode 426 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4120],\n",
      "        [0.9997, 0.4124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4789],\n",
      "        [0.9997, 0.4350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503921866416931 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.701077938079834 s \n",
      "\n",
      "\n",
      "\tEpisode 427 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4411],\n",
      "        [0.9998, 0.4539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4935],\n",
      "        [0.9998, 0.4562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540783882141113 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.708059310913086 s \n",
      "\n",
      "\n",
      "\tEpisode 428 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4928],\n",
      "        [0.9998, 0.4436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5362],\n",
      "        [0.9997, 0.4204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520151615142822 \tStep Time:  0.00498652458190918 s \tTotal Time:  2.713045835494995 s \n",
      "\n",
      "\n",
      "\tEpisode 429 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4552],\n",
      "        [0.9997, 0.4213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5864],\n",
      "        [0.9998, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566942691802979 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.720027208328247 s \n",
      "\n",
      "\n",
      "\tEpisode 430 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5492],\n",
      "        [0.9999, 0.5501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4316],\n",
      "        [1.0000, 0.6155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.617117404937744 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.726011037826538 s \n",
      "\n",
      "\n",
      "\tEpisode 431 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4271],\n",
      "        [0.9997, 0.4299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4550],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570194721221924 \tStep Time:  0.004986763000488281 s \tTotal Time:  2.7309978008270264 s \n",
      "\n",
      "\n",
      "\tEpisode 432 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4349],\n",
      "        [0.9998, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6327],\n",
      "        [0.9998, 0.4304]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493781089782715 \tStep Time:  0.00797891616821289 s \tTotal Time:  2.7389767169952393 s \n",
      "\n",
      "\n",
      "\tEpisode 433 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4448],\n",
      "        [0.9997, 0.4340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4436],\n",
      "        [0.9997, 0.4617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539061486721039 \tStep Time:  0.00598907470703125 s \tTotal Time:  2.7449657917022705 s \n",
      "\n",
      "\n",
      "\tEpisode 434 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6168],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4447],\n",
      "        [0.9997, 0.4521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601229667663574 \tStep Time:  0.005985736846923828 s \tTotal Time:  2.7509515285491943 s \n",
      "\n",
      "\n",
      "\tEpisode 435 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.4889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4602],\n",
      "        [0.9997, 0.4577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537741303443909 \tStep Time:  0.006979703903198242 s \tTotal Time:  2.7579312324523926 s \n",
      "\n",
      "\n",
      "\tEpisode 436 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4387],\n",
      "        [0.9997, 0.4544]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4560],\n",
      "        [0.9997, 0.4396]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514701843261719 \tStep Time:  0.004986763000488281 s \tTotal Time:  2.762917995452881 s \n",
      "\n",
      "\n",
      "\tEpisode 437 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4494],\n",
      "        [0.9997, 0.4407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5617],\n",
      "        [0.9998, 0.4604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561764240264893 \tStep Time:  0.0059854984283447266 s \tTotal Time:  2.7689034938812256 s \n",
      "\n",
      "\n",
      "\tEpisode 438 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4562],\n",
      "        [0.9997, 0.4622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4936],\n",
      "        [0.9998, 0.4741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528133869171143 \tStep Time:  0.006979465484619141 s \tTotal Time:  2.7758829593658447 s \n",
      "\n",
      "\n",
      "\tEpisode 439 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5153],\n",
      "        [0.9997, 0.4561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5068],\n",
      "        [0.9997, 0.4469]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572433948516846 \tStep Time:  0.004987239837646484 s \tTotal Time:  2.780870199203491 s \n",
      "\n",
      "\n",
      "\tEpisode 440 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4654],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4678],\n",
      "        [0.9998, 0.4748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484596729278564 \tStep Time:  0.005988597869873047 s \tTotal Time:  2.7868587970733643 s \n",
      "\n",
      "\n",
      "\tEpisode 441 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4607],\n",
      "        [0.9999, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4546],\n",
      "        [0.9997, 0.4592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534616470336914 \tStep Time:  0.005979776382446289 s \tTotal Time:  2.793835163116455 s \n",
      "\n",
      "\n",
      "\tEpisode 442 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4703],\n",
      "        [0.9999, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4906],\n",
      "        [0.9999, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539197087287903 \tStep Time:  0.004986763000488281 s \tTotal Time:  2.7988219261169434 s \n",
      "\n",
      "\n",
      "\tEpisode 443 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4706],\n",
      "        [0.9999, 0.5209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4968],\n",
      "        [0.9998, 0.4825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513891696929932 \tStep Time:  0.005985260009765625 s \tTotal Time:  2.805804491043091 s \n",
      "\n",
      "\n",
      "\tEpisode 444 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4818],\n",
      "        [0.9999, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4878],\n",
      "        [0.9998, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501730918884277 \tStep Time:  0.005982875823974609 s \tTotal Time:  2.8117873668670654 s \n",
      "\n",
      "\n",
      "\tEpisode 445 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4710],\n",
      "        [0.9999, 0.4952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5920],\n",
      "        [0.9997, 0.4731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556478023529053 \tStep Time:  0.00498652458190918 s \tTotal Time:  2.8167738914489746 s \n",
      "\n",
      "\n",
      "\tEpisode 446 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5309],\n",
      "        [0.9998, 0.5373]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6074],\n",
      "        [0.9998, 0.4734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581991314888 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.8247525691986084 s \n",
      "\n",
      "\n",
      "\tEpisode 447 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4749],\n",
      "        [0.9999, 0.5522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4738],\n",
      "        [0.9998, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52809190750122 \tStep Time:  0.00498652458190918 s \tTotal Time:  2.8297390937805176 s \n",
      "\n",
      "\n",
      "\tEpisode 448 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5142],\n",
      "        [0.9998, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4716],\n",
      "        [0.9999, 0.4718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485346794128418 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.8357229232788086 s \n",
      "\n",
      "\n",
      "\tEpisode 449 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4718],\n",
      "        [0.9999, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5422],\n",
      "        [0.9997, 0.5034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52823007106781 \tStep Time:  0.006981372833251953 s \tTotal Time:  2.8427042961120605 s \n",
      "\n",
      "\n",
      "\tEpisode 450 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4755],\n",
      "        [0.9997, 0.4925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4666],\n",
      "        [0.9998, 0.4659]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526228904724121 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.8486883640289307 s \n",
      "\n",
      "\n",
      "\tEpisode 451 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4871],\n",
      "        [0.9998, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4708],\n",
      "        [0.9997, 0.4706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515620827674866 \tStep Time:  0.005985260009765625 s \tTotal Time:  2.8546736240386963 s \n",
      "\n",
      "\n",
      "\tEpisode 452 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5458],\n",
      "        [0.9997, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4842],\n",
      "        [0.9999, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451899528503418 \tStep Time:  0.007977485656738281 s \tTotal Time:  2.8626511096954346 s \n",
      "\n",
      "\n",
      "\tEpisode 453 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5047],\n",
      "        [0.9997, 0.4793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5121],\n",
      "        [0.9998, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488292217254639 \tStep Time:  0.00498652458190918 s \tTotal Time:  2.8676376342773438 s \n",
      "\n",
      "\n",
      "\tEpisode 454 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5015],\n",
      "        [1.0000, 0.5529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4905],\n",
      "        [0.9999, 0.5345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501728057861328 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.874619245529175 s \n",
      "\n",
      "\n",
      "\tEpisode 455 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5472],\n",
      "        [0.9998, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4824],\n",
      "        [0.9999, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525412559509277 \tStep Time:  0.00698089599609375 s \tTotal Time:  2.8816001415252686 s \n",
      "\n",
      "\n",
      "\tEpisode 456 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4807],\n",
      "        [0.9999, 0.5238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5832],\n",
      "        [0.9998, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478049278259277 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.8875844478607178 s \n",
      "\n",
      "\n",
      "\tEpisode 457 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4816],\n",
      "        [1.0000, 0.5396]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5030],\n",
      "        [0.9999, 0.5318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513808727264404 \tStep Time:  0.005984067916870117 s \tTotal Time:  2.893568515777588 s \n",
      "\n",
      "\n",
      "\tEpisode 458 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4846],\n",
      "        [0.9999, 0.5035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5902],\n",
      "        [0.9997, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467961311340332 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.899552345275879 s \n",
      "\n",
      "\n",
      "\tEpisode 459 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4785],\n",
      "        [0.9997, 0.4794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5255],\n",
      "        [0.9997, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527339458465576 \tStep Time:  0.006982326507568359 s \tTotal Time:  2.9065346717834473 s \n",
      "\n",
      "\n",
      "\tEpisode 460 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5305],\n",
      "        [0.9997, 0.4871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5157],\n",
      "        [1.0000, 0.5999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447273313999176 \tStep Time:  0.005983114242553711 s \tTotal Time:  2.912517786026001 s \n",
      "\n",
      "\n",
      "\tEpisode 461 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5084],\n",
      "        [0.9998, 0.4899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5378],\n",
      "        [0.9998, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497424125671387 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.918501615524292 s \n",
      "\n",
      "\n",
      "\tEpisode 462 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5367],\n",
      "        [0.9997, 0.4835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6276],\n",
      "        [0.9997, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469031810760498 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.924485921859741 s \n",
      "\n",
      "\n",
      "\tEpisode 463 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5634],\n",
      "        [0.9999, 0.5493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4723],\n",
      "        [1.0000, 0.5091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500487327575684 \tStep Time:  0.005984306335449219 s \tTotal Time:  2.9304702281951904 s \n",
      "\n",
      "\n",
      "\tEpisode 464 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4807],\n",
      "        [1.0000, 0.5838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4898],\n",
      "        [1.0000, 0.6536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.384498059749603 \tStep Time:  0.005983591079711914 s \tTotal Time:  2.9364538192749023 s \n",
      "\n",
      "\n",
      "\tEpisode 465 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4938],\n",
      "        [0.9997, 0.4628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6026],\n",
      "        [1.0000, 0.8289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.8008131980896 \tStep Time:  0.005983829498291016 s \tTotal Time:  2.9424376487731934 s \n",
      "\n",
      "\n",
      "\tEpisode 466 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4688],\n",
      "        [1.0000, 0.4974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4539],\n",
      "        [0.9997, 0.4548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53969955444336 \tStep Time:  0.0059854984283447266 s \tTotal Time:  2.948423147201538 s \n",
      "\n",
      "\n",
      "\tEpisode 467 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4512],\n",
      "        [0.9998, 0.4514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6682],\n",
      "        [0.9997, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.654079914093018 \tStep Time:  0.007978200912475586 s \tTotal Time:  2.9564013481140137 s \n",
      "\n",
      "\n",
      "\tEpisode 468 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6184],\n",
      "        [1.0000, 0.4671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4701],\n",
      "        [0.9997, 0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612635612487793 \tStep Time:  0.0069811344146728516 s \tTotal Time:  2.9633824825286865 s \n",
      "\n",
      "\n",
      "\tEpisode 469 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6822],\n",
      "        [0.9998, 0.4518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5276],\n",
      "        [0.9998, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481217384338379 \tStep Time:  0.006986141204833984 s \tTotal Time:  2.9703686237335205 s \n",
      "\n",
      "\n",
      "\tEpisode 470 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5155],\n",
      "        [0.9999, 0.4620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4651],\n",
      "        [0.9999, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51941043138504 \tStep Time:  0.006979227066040039 s \tTotal Time:  2.978342056274414 s \n",
      "\n",
      "\n",
      "\tEpisode 471 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4764],\n",
      "        [0.9997, 0.4567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4620],\n",
      "        [1.0000, 0.4904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518597602844238 \tStep Time:  0.006983280181884766 s \tTotal Time:  2.985325336456299 s \n",
      "\n",
      "\n",
      "\tEpisode 472 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4987],\n",
      "        [1.0000, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4596],\n",
      "        [1.0000, 0.5167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556147694587708 \tStep Time:  0.006979465484619141 s \tTotal Time:  2.992304801940918 s \n",
      "\n",
      "\n",
      "\tEpisode 473 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5127],\n",
      "        [0.9998, 0.4617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5216],\n",
      "        [0.9997, 0.4593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574082374572754 \tStep Time:  0.006981611251831055 s \tTotal Time:  2.999286413192749 s \n",
      "\n",
      "\n",
      "\tEpisode 474 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4529],\n",
      "        [0.9997, 0.4578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4902],\n",
      "        [1.0000, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503555238246918 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.006267786026001 s \n",
      "\n",
      "\n",
      "\tEpisode 475 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4459],\n",
      "        [0.9998, 0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4905],\n",
      "        [1.0000, 0.4573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536614179611206 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.012251377105713 s \n",
      "\n",
      "\n",
      "\tEpisode 476 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4531],\n",
      "        [0.9997, 0.4607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4612],\n",
      "        [0.9997, 0.4889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533170700073242 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.018235921859741 s \n",
      "\n",
      "\n",
      "\tEpisode 477 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5749],\n",
      "        [1.0000, 0.6528]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4599],\n",
      "        [0.9997, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.674177646636963 \tStep Time:  0.007977962493896484 s \tTotal Time:  3.0262138843536377 s \n",
      "\n",
      "\n",
      "\tEpisode 478 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4632],\n",
      "        [0.9998, 0.4899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4830],\n",
      "        [0.9998, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509510517120361 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.032197952270508 s \n",
      "\n",
      "\n",
      "\tEpisode 479 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4944],\n",
      "        [0.9997, 0.4665]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5562],\n",
      "        [0.9999, 0.4679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486092031002045 \tStep Time:  0.007571697235107422 s \tTotal Time:  3.0397696495056152 s \n",
      "\n",
      "\n",
      "\tEpisode 480 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4878],\n",
      "        [0.9999, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4802],\n",
      "        [1.0000, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49474173784256 \tStep Time:  0.007394313812255859 s \tTotal Time:  3.047163963317871 s \n",
      "\n",
      "\n",
      "\tEpisode 481 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4791],\n",
      "        [0.9997, 0.4692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4779],\n",
      "        [0.9998, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51488208770752 \tStep Time:  0.007977008819580078 s \tTotal Time:  3.055140972137451 s \n",
      "\n",
      "\n",
      "\tEpisode 482 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4756],\n",
      "        [0.9997, 0.4769]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4788],\n",
      "        [0.9998, 0.4785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514007568359375 \tStep Time:  0.008976221084594727 s \tTotal Time:  3.064117193222046 s \n",
      "\n",
      "\n",
      "\tEpisode 483 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4792],\n",
      "        [1.0000, 0.5298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4874],\n",
      "        [0.9997, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49094533920288 \tStep Time:  0.009974002838134766 s \tTotal Time:  3.0740911960601807 s \n",
      "\n",
      "\n",
      "\tEpisode 484 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [0.9998, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4891],\n",
      "        [0.9998, 0.4890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519739270210266 \tStep Time:  0.00698089599609375 s \tTotal Time:  3.0810720920562744 s \n",
      "\n",
      "\n",
      "\tEpisode 485 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5254],\n",
      "        [0.9997, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4861],\n",
      "        [0.9999, 0.5240]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472374200820923 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.088052988052368 s \n",
      "\n",
      "\n",
      "\tEpisode 486 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5181],\n",
      "        [1.0000, 0.4998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5059],\n",
      "        [0.9999, 0.5381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496337413787842 \tStep Time:  0.007979154586791992 s \tTotal Time:  3.09603214263916 s \n",
      "\n",
      "\n",
      "\tEpisode 487 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5310],\n",
      "        [1.0000, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5586],\n",
      "        [0.9998, 0.4902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468660354614258 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.102015733718872 s \n",
      "\n",
      "\n",
      "\tEpisode 488 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5085],\n",
      "        [0.9998, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5166],\n",
      "        [0.9997, 0.4987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51690673828125 \tStep Time:  0.007981300354003906 s \tTotal Time:  3.109997034072876 s \n",
      "\n",
      "\n",
      "\tEpisode 489 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7536],\n",
      "        [0.9998, 0.4830]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5978],\n",
      "        [0.9998, 0.4693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34542989730835 \tStep Time:  0.006978511810302734 s \tTotal Time:  3.1169755458831787 s \n",
      "\n",
      "\n",
      "\tEpisode 490 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5123],\n",
      "        [0.9998, 0.5674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4482],\n",
      "        [0.9999, 0.6034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464925289154053 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.122959613800049 s \n",
      "\n",
      "\n",
      "\tEpisode 491 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4990],\n",
      "        [0.9998, 0.5652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4022],\n",
      "        [1.0000, 0.4037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548787355422974 \tStep Time:  0.00698399543762207 s \tTotal Time:  3.1309423446655273 s \n",
      "\n",
      "\n",
      "\tEpisode 492 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7124],\n",
      "        [0.9997, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7435],\n",
      "        [1.0000, 0.6793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436441779136658 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.1369268894195557 s \n",
      "\n",
      "\n",
      "\tEpisode 493 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3902],\n",
      "        [0.9997, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5623],\n",
      "        [0.9998, 0.4386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465937614440918 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.142911195755005 s \n",
      "\n",
      "\n",
      "\tEpisode 494 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3580],\n",
      "        [0.9998, 0.5663]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3717],\n",
      "        [0.9998, 0.4334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45790719985962 \tStep Time:  0.00698089599609375 s \tTotal Time:  3.1508891582489014 s \n",
      "\n",
      "\n",
      "\tEpisode 495 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7023],\n",
      "        [0.9997, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6350],\n",
      "        [0.9997, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523195266723633 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.156873941421509 s \n",
      "\n",
      "\n",
      "\tEpisode 496 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5833],\n",
      "        [1.0000, 0.7813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3137],\n",
      "        [1.0000, 0.8299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.26861572265625 \tStep Time:  0.0069811344146728516 s \tTotal Time:  3.1638550758361816 s \n",
      "\n",
      "\n",
      "\tEpisode 497 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3877],\n",
      "        [0.9999, 0.6973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2972],\n",
      "        [1.0000, 0.7008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.932383358478546 \tStep Time:  0.007977962493896484 s \tTotal Time:  3.171833038330078 s \n",
      "\n",
      "\n",
      "\tEpisode 498 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2989],\n",
      "        [0.9998, 0.3785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4134],\n",
      "        [0.9997, 0.4417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533105790615082 \tStep Time:  0.006089448928833008 s \tTotal Time:  3.177922487258911 s \n",
      "\n",
      "\n",
      "\tEpisode 499 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.3836],\n",
      "        [0.9998, 0.4226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4029],\n",
      "        [0.9999, 0.6478]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66287088394165 \tStep Time:  0.0069811344146728516 s \tTotal Time:  3.184903621673584 s \n",
      "\n",
      "\n",
      "\tEpisode 500 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4735],\n",
      "        [0.9999, 0.3653]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4455],\n",
      "        [0.9999, 0.6217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6025972366333 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.191884994506836 s \n",
      "\n",
      "\n",
      "\tEpisode 501 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6572],\n",
      "        [0.9999, 0.3692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3480],\n",
      "        [1.0000, 0.3361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422515630722046 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.197869062423706 s \n",
      "\n",
      "\n",
      "\tEpisode 502 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4385],\n",
      "        [1.0000, 0.6349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4274],\n",
      "        [0.9998, 0.4186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.645566463470459 \tStep Time:  0.007978200912475586 s \tTotal Time:  3.2058472633361816 s \n",
      "\n",
      "\n",
      "\tEpisode 503 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5279],\n",
      "        [1.0000, 0.3585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3940],\n",
      "        [0.9998, 0.4426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521904468536377 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.2118313312530518 s \n",
      "\n",
      "\n",
      "\tEpisode 504 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4392],\n",
      "        [0.9999, 0.3932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3923],\n",
      "        [0.9999, 0.4091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506707191467285 \tStep Time:  0.0059850215911865234 s \tTotal Time:  3.2178163528442383 s \n",
      "\n",
      "\n",
      "\tEpisode 505 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5930],\n",
      "        [0.9998, 0.4462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4357],\n",
      "        [0.9998, 0.4595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458898544311523 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.2247977256774902 s \n",
      "\n",
      "\n",
      "\tEpisode 506 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4734],\n",
      "        [0.9999, 0.5874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7157],\n",
      "        [1.0000, 0.4164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477198123931885 \tStep Time:  0.005982637405395508 s \tTotal Time:  3.2307803630828857 s \n",
      "\n",
      "\n",
      "\tEpisode 507 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4234],\n",
      "        [0.9997, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4706],\n",
      "        [0.9997, 0.4899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47872257232666 \tStep Time:  0.005983829498291016 s \tTotal Time:  3.2367641925811768 s \n",
      "\n",
      "\n",
      "\tEpisode 508 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4780],\n",
      "        [0.9999, 0.6062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5552],\n",
      "        [0.9999, 0.4055]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.388604402542114 \tStep Time:  0.0069828033447265625 s \tTotal Time:  3.2437469959259033 s \n",
      "\n",
      "\n",
      "\tEpisode 509 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3910],\n",
      "        [0.9997, 0.4807]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3809],\n",
      "        [0.9998, 0.4281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566407680511475 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.2507283687591553 s \n",
      "\n",
      "\n",
      "\tEpisode 510 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4218],\n",
      "        [0.9997, 0.4921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5710],\n",
      "        [0.9998, 0.5726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483952522277832 \tStep Time:  0.008974313735961914 s \tTotal Time:  3.259702682495117 s \n",
      "\n",
      "\n",
      "\tEpisode 511 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5578],\n",
      "        [0.9997, 0.5325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6453],\n",
      "        [0.9998, 0.5610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474042892456055 \tStep Time:  0.008976221084594727 s \tTotal Time:  3.268678903579712 s \n",
      "\n",
      "\n",
      "\tEpisode 512 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3403],\n",
      "        [1.0000, 0.3435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4815],\n",
      "        [0.9997, 0.5247]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.703312873840332 \tStep Time:  0.007982015609741211 s \tTotal Time:  3.277658224105835 s \n",
      "\n",
      "\n",
      "\tEpisode 513 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6828],\n",
      "        [0.9998, 0.4415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4002],\n",
      "        [0.9998, 0.5857]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.75902795791626 \tStep Time:  0.005982160568237305 s \tTotal Time:  3.2836403846740723 s \n",
      "\n",
      "\n",
      "\tEpisode 514 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.5061],\n",
      "        [0.9999, 0.4172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6060],\n",
      "        [0.9997, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437053680419922 \tStep Time:  0.006982564926147461 s \tTotal Time:  3.2906229496002197 s \n",
      "\n",
      "\n",
      "\tEpisode 515 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4341],\n",
      "        [0.9998, 0.5512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5365],\n",
      "        [0.9997, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56703519821167 \tStep Time:  0.006980180740356445 s \tTotal Time:  3.297603130340576 s \n",
      "\n",
      "\n",
      "\tEpisode 516 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5953],\n",
      "        [0.9997, 0.5197]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5583],\n",
      "        [0.9998, 0.5506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568527042865753 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.3035874366760254 s \n",
      "\n",
      "\n",
      "\tEpisode 517 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4747],\n",
      "        [0.9999, 0.4701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4885],\n",
      "        [0.9999, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520599663257599 \tStep Time:  0.005985260009765625 s \tTotal Time:  3.309572696685791 s \n",
      "\n",
      "\n",
      "\tEpisode 518 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5277],\n",
      "        [1.0000, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4982],\n",
      "        [0.9998, 0.5543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562421798706055 \tStep Time:  0.005982398986816406 s \tTotal Time:  3.3155550956726074 s \n",
      "\n",
      "\n",
      "\tEpisode 519 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4950],\n",
      "        [0.9999, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6058],\n",
      "        [0.9999, 0.5443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562336921691895 \tStep Time:  0.0059850215911865234 s \tTotal Time:  3.321540117263794 s \n",
      "\n",
      "\n",
      "\tEpisode 520 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5181],\n",
      "        [1.0000, 0.5820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4978],\n",
      "        [0.9999, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542835235595703 \tStep Time:  0.005992889404296875 s \tTotal Time:  3.327533006668091 s \n",
      "\n",
      "\n",
      "\tEpisode 521 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6483],\n",
      "        [1.0000, 0.6091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4916],\n",
      "        [1.0000, 0.5499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521298110485077 \tStep Time:  0.005988121032714844 s \tTotal Time:  3.3335211277008057 s \n",
      "\n",
      "\n",
      "\tEpisode 522 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4865],\n",
      "        [0.9998, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4912],\n",
      "        [0.9997, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524446487426758 \tStep Time:  0.0049877166748046875 s \tTotal Time:  3.3385088443756104 s \n",
      "\n",
      "\n",
      "\tEpisode 523 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4791],\n",
      "        [0.9999, 0.4754]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5028],\n",
      "        [0.9998, 0.4845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51706838607788 \tStep Time:  0.0059833526611328125 s \tTotal Time:  3.344492197036743 s \n",
      "\n",
      "\n",
      "\tEpisode 524 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5543],\n",
      "        [0.9997, 0.4770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4942],\n",
      "        [0.9997, 0.4749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46227502822876 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.350475788116455 s \n",
      "\n",
      "\n",
      "\tEpisode 525 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5196],\n",
      "        [0.9999, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4702],\n",
      "        [0.9998, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498877048492432 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.3564600944519043 s \n",
      "\n",
      "\n",
      "\tEpisode 526 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4670],\n",
      "        [0.9998, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4995],\n",
      "        [0.9998, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521407008171082 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.3624446392059326 s \n",
      "\n",
      "\n",
      "\tEpisode 527 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5112],\n",
      "        [1.0000, 0.5729]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4812],\n",
      "        [1.0000, 0.5783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498638153076172 \tStep Time:  0.005983114242553711 s \tTotal Time:  3.3684277534484863 s \n",
      "\n",
      "\n",
      "\tEpisode 528 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9997, 0.4814],\n",
      "        [0.9998, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4800],\n",
      "        [0.9999, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520139217376709 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.3744120597839355 s \n",
      "\n",
      "\n",
      "\tEpisode 529 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5334],\n",
      "        [1.0000, 0.6028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4737],\n",
      "        [1.0000, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529982089996338 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.380396842956543 s \n",
      "\n",
      "\n",
      "\tEpisode 530 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4862],\n",
      "        [0.9999, 0.4758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4897],\n",
      "        [0.9999, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517797470092773 \tStep Time:  0.005983114242553711 s \tTotal Time:  3.3863799571990967 s \n",
      "\n",
      "\n",
      "\tEpisode 531 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4907],\n",
      "        [0.9998, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5106],\n",
      "        [0.9999, 0.4528]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533559322357178 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.392364025115967 s \n",
      "\n",
      "\n",
      "\tEpisode 532 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5337],\n",
      "        [0.9998, 0.4585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4552],\n",
      "        [0.9998, 0.4626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470461368560791 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.398348331451416 s \n",
      "\n",
      "\n",
      "\tEpisode 533 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4856],\n",
      "        [0.9998, 0.4546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4480],\n",
      "        [0.9999, 0.4295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541717529296875 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.4043328762054443 s \n",
      "\n",
      "\n",
      "\tEpisode 534 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4455],\n",
      "        [0.9999, 0.4283]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8102],\n",
      "        [0.9998, 0.4435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.756093502044678 \tStep Time:  0.005983114242553711 s \tTotal Time:  3.410315990447998 s \n",
      "\n",
      "\n",
      "\tEpisode 535 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4424],\n",
      "        [0.9998, 0.4335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5210],\n",
      "        [1.0000, 0.6248]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573943138122559 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.4163007736206055 s \n",
      "\n",
      "\n",
      "\tEpisode 536 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4292],\n",
      "        [1.0000, 0.3997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4989],\n",
      "        [0.9998, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520380973815918 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.422285318374634 s \n",
      "\n",
      "\n",
      "\tEpisode 537 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4415],\n",
      "        [1.0000, 0.6383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4523],\n",
      "        [0.9999, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.400731086730957 \tStep Time:  0.005982637405395508 s \tTotal Time:  3.4282679557800293 s \n",
      "\n",
      "\n",
      "\tEpisode 538 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4137],\n",
      "        [0.9999, 0.5271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5787],\n",
      "        [1.0000, 0.4198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.661505341529846 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.4342527389526367 s \n",
      "\n",
      "\n",
      "\tEpisode 539 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4607],\n",
      "        [1.0000, 0.6228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4648],\n",
      "        [1.0000, 0.4474]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428194999694824 \tStep Time:  0.0059833526611328125 s \tTotal Time:  3.4402360916137695 s \n",
      "\n",
      "\n",
      "\tEpisode 540 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4777],\n",
      "        [0.9998, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5077],\n",
      "        [0.9999, 0.4529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49471378326416 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.4462196826934814 s \n",
      "\n",
      "\n",
      "\tEpisode 541 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4684],\n",
      "        [1.0000, 0.5855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4665],\n",
      "        [1.0000, 0.6355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66847676038742 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.4532010555267334 s \n",
      "\n",
      "\n",
      "\tEpisode 542 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4739],\n",
      "        [0.9998, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4841],\n",
      "        [1.0000, 0.4715]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525533676147461 \tStep Time:  0.0060176849365234375 s \tTotal Time:  3.459218740463257 s \n",
      "\n",
      "\n",
      "\tEpisode 543 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4733],\n",
      "        [0.9998, 0.4782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [0.9998, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51992654800415 \tStep Time:  0.005949497222900391 s \tTotal Time:  3.4651682376861572 s \n",
      "\n",
      "\n",
      "\tEpisode 544 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4876],\n",
      "        [0.9998, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4850],\n",
      "        [1.0000, 0.4597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497526168823242 \tStep Time:  0.008976221084594727 s \tTotal Time:  3.474144458770752 s \n",
      "\n",
      "\n",
      "\tEpisode 545 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4821],\n",
      "        [0.9998, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5201],\n",
      "        [1.0000, 0.4414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470766484737396 \tStep Time:  0.004985809326171875 s \tTotal Time:  3.479130268096924 s \n",
      "\n",
      "\n",
      "\tEpisode 546 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5322],\n",
      "        [1.0000, 0.5784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4182],\n",
      "        [1.0000, 0.6019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402099013328552 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.485114336013794 s \n",
      "\n",
      "\n",
      "\tEpisode 547 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5045],\n",
      "        [0.9999, 0.4183]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4878],\n",
      "        [1.0000, 0.6109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4094939827919 \tStep Time:  0.006982326507568359 s \tTotal Time:  3.4920966625213623 s \n",
      "\n",
      "\n",
      "\tEpisode 548 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5294],\n",
      "        [1.0000, 0.3876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7381],\n",
      "        [0.9998, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508721828460693 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.498080253601074 s \n",
      "\n",
      "\n",
      "\tEpisode 549 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4792],\n",
      "        [1.0000, 0.6792]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6920],\n",
      "        [0.9998, 0.4624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536219120025635 \tStep Time:  0.004986286163330078 s \tTotal Time:  3.5030665397644043 s \n",
      "\n",
      "\n",
      "\tEpisode 550 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5096],\n",
      "        [0.9998, 0.5328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3785],\n",
      "        [0.9998, 0.5695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.622353792190552 \tStep Time:  0.006984233856201172 s \tTotal Time:  3.5100507736206055 s \n",
      "\n",
      "\n",
      "\tEpisode 551 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5406],\n",
      "        [0.9998, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5314],\n",
      "        [0.9998, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523834824562073 \tStep Time:  0.004986286163330078 s \tTotal Time:  3.5150370597839355 s \n",
      "\n",
      "\n",
      "\tEpisode 552 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5476],\n",
      "        [0.9998, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6161],\n",
      "        [0.9999, 0.6467]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.630434513092041 \tStep Time:  0.0059854984283447266 s \tTotal Time:  3.5210225582122803 s \n",
      "\n",
      "\n",
      "\tEpisode 553 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8153],\n",
      "        [0.9999, 0.4187]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5850],\n",
      "        [0.9999, 0.6304]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.406596601009369 \tStep Time:  0.0069806575775146484 s \tTotal Time:  3.528003215789795 s \n",
      "\n",
      "\n",
      "\tEpisode 554 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2801],\n",
      "        [0.9998, 0.5181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3950],\n",
      "        [0.9999, 0.4014]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.671900272369385 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.533986806869507 s \n",
      "\n",
      "\n",
      "\tEpisode 555 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3686],\n",
      "        [0.9998, 0.4322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5621],\n",
      "        [0.9998, 0.4492]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42446517944336 \tStep Time:  0.006001949310302734 s \tTotal Time:  3.5399887561798096 s \n",
      "\n",
      "\n",
      "\tEpisode 556 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4221],\n",
      "        [0.9999, 0.5651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6772],\n",
      "        [0.9998, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.388344407081604 \tStep Time:  0.005966663360595703 s \tTotal Time:  3.5459554195404053 s \n",
      "\n",
      "\n",
      "\tEpisode 557 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4454],\n",
      "        [0.9998, 0.4547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5674],\n",
      "        [0.9998, 0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592666625976562 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.5519397258758545 s \n",
      "\n",
      "\n",
      "\tEpisode 558 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4825],\n",
      "        [1.0000, 0.6240]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4737],\n",
      "        [0.9998, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.600863933563232 \tStep Time:  0.005983829498291016 s \tTotal Time:  3.5579235553741455 s \n",
      "\n",
      "\n",
      "\tEpisode 559 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5680],\n",
      "        [0.9998, 0.4806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4698],\n",
      "        [1.0000, 0.6300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504818439483643 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.5639071464538574 s \n",
      "\n",
      "\n",
      "\tEpisode 560 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6733],\n",
      "        [0.9999, 0.3626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7041],\n",
      "        [0.9998, 0.4475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51128339767456 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.5698912143707275 s \n",
      "\n",
      "\n",
      "\tEpisode 561 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2269],\n",
      "        [0.9999, 0.3689]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4593],\n",
      "        [0.9998, 0.4276]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63413143157959 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.575875759124756 s \n",
      "\n",
      "\n",
      "\tEpisode 562 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3398],\n",
      "        [0.9998, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4909],\n",
      "        [0.9999, 0.3920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419785976409912 \tStep Time:  0.005983114242553711 s \tTotal Time:  3.5828561782836914 s \n",
      "\n",
      "\n",
      "\tEpisode 563 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5413],\n",
      "        [0.9999, 0.5617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7302],\n",
      "        [0.9998, 0.4406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.409718036651611 \tStep Time:  0.004987478256225586 s \tTotal Time:  3.587843656539917 s \n",
      "\n",
      "\n",
      "\tEpisode 564 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4195],\n",
      "        [1.0000, 0.6590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3620],\n",
      "        [0.9998, 0.4961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451022624969482 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.593827247619629 s \n",
      "\n",
      "\n",
      "\tEpisode 565 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4779],\n",
      "        [0.9999, 0.5757]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4207],\n",
      "        [0.9999, 0.6040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522144794464111 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.599810838699341 s \n",
      "\n",
      "\n",
      "\tEpisode 566 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3847],\n",
      "        [1.0000, 0.3698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3605],\n",
      "        [1.0000, 0.3609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54042100906372 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.6057956218719482 s \n",
      "\n",
      "\n",
      "\tEpisode 567 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5446],\n",
      "        [0.9998, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5463],\n",
      "        [0.9999, 0.5870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597638607025146 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.61177921295166 s \n",
      "\n",
      "\n",
      "\tEpisode 568 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4488],\n",
      "        [0.9998, 0.5527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6122],\n",
      "        [0.9999, 0.5856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601198196411133 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.6177635192871094 s \n",
      "\n",
      "\n",
      "\tEpisode 569 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5609],\n",
      "        [1.0000, 0.3805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5164],\n",
      "        [0.9999, 0.5551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.636659622192383 \tStep Time:  0.006981849670410156 s \tTotal Time:  3.6247453689575195 s \n",
      "\n",
      "\n",
      "\tEpisode 570 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6583],\n",
      "        [0.9998, 0.4831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7566],\n",
      "        [1.0000, 0.4080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.653461456298828 \tStep Time:  0.005982875823974609 s \tTotal Time:  3.630728244781494 s \n",
      "\n",
      "\n",
      "\tEpisode 571 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4347],\n",
      "        [0.9998, 0.4617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4556],\n",
      "        [0.9998, 0.4739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52608346939087 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.636711835861206 s \n",
      "\n",
      "\n",
      "\tEpisode 572 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4786],\n",
      "        [0.9998, 0.4714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4633],\n",
      "        [0.9999, 0.4700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509837031364441 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.6426961421966553 s \n",
      "\n",
      "\n",
      "\tEpisode 573 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4750],\n",
      "        [0.9999, 0.4664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4711],\n",
      "        [0.9999, 0.4714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518586814403534 \tStep Time:  0.004987478256225586 s \tTotal Time:  3.6486806869506836 s \n",
      "\n",
      "\n",
      "\tEpisode 574 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4878],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4843],\n",
      "        [0.9998, 0.4766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494418203830719 \tStep Time:  0.0079803466796875 s \tTotal Time:  3.656661033630371 s \n",
      "\n",
      "\n",
      "\tEpisode 575 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4774],\n",
      "        [0.9998, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4790],\n",
      "        [0.9998, 0.4764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514141082763672 \tStep Time:  0.004985809326171875 s \tTotal Time:  3.661646842956543 s \n",
      "\n",
      "\n",
      "\tEpisode 576 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4850],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4782],\n",
      "        [0.9999, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542407035827637 \tStep Time:  0.007979154586791992 s \tTotal Time:  3.669625997543335 s \n",
      "\n",
      "\n",
      "\tEpisode 577 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4888],\n",
      "        [0.9999, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5037],\n",
      "        [0.9999, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513741374015808 \tStep Time:  0.0069806575775146484 s \tTotal Time:  3.6766066551208496 s \n",
      "\n",
      "\n",
      "\tEpisode 578 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5448],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6201],\n",
      "        [1.0000, 0.5816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.604667663574219 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.6825907230377197 s \n",
      "\n",
      "\n",
      "\tEpisode 579 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5092],\n",
      "        [0.9998, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5703],\n",
      "        [0.9999, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553777754306793 \tStep Time:  0.0059854984283447266 s \tTotal Time:  3.6885762214660645 s \n",
      "\n",
      "\n",
      "\tEpisode 580 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4844],\n",
      "        [0.9999, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5096],\n",
      "        [1.0000, 0.5279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532996654510498 \tStep Time:  0.005982398986816406 s \tTotal Time:  3.694558620452881 s \n",
      "\n",
      "\n",
      "\tEpisode 581 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4843],\n",
      "        [0.9999, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4853],\n",
      "        [0.9998, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50461721420288 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.70054292678833 s \n",
      "\n",
      "\n",
      "\tEpisode 582 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4687],\n",
      "        [0.9999, 0.4705]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4919],\n",
      "        [1.0000, 0.5641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564433574676514 \tStep Time:  0.006132364273071289 s \tTotal Time:  3.7066752910614014 s \n",
      "\n",
      "\n",
      "\tEpisode 583 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4739],\n",
      "        [0.9999, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4727],\n",
      "        [1.0000, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489944458007812 \tStep Time:  0.005835771560668945 s \tTotal Time:  3.7125110626220703 s \n",
      "\n",
      "\n",
      "\tEpisode 584 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5071],\n",
      "        [1.0000, 0.4970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4674],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538483619689941 \tStep Time:  0.004986286163330078 s \tTotal Time:  3.7174973487854004 s \n",
      "\n",
      "\n",
      "\tEpisode 585 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4847],\n",
      "        [1.0000, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4817],\n",
      "        [0.9998, 0.4689]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508678436279297 \tStep Time:  0.006991147994995117 s \tTotal Time:  3.7244884967803955 s \n",
      "\n",
      "\n",
      "\tEpisode 586 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4772],\n",
      "        [0.9999, 0.4655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4674],\n",
      "        [0.9998, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506829261779785 \tStep Time:  0.0059816837310791016 s \tTotal Time:  3.7304701805114746 s \n",
      "\n",
      "\n",
      "\tEpisode 587 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4692],\n",
      "        [1.0000, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4779],\n",
      "        [1.0000, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500246047973633 \tStep Time:  0.004986286163330078 s \tTotal Time:  3.7364532947540283 s \n",
      "\n",
      "\n",
      "\tEpisode 588 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4780],\n",
      "        [0.9999, 0.4631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4770],\n",
      "        [1.0000, 0.4790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49716854095459 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.7424373626708984 s \n",
      "\n",
      "\n",
      "\tEpisode 589 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4662],\n",
      "        [0.9998, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4742],\n",
      "        [0.9999, 0.4725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50418484210968 \tStep Time:  0.00498652458190918 s \tTotal Time:  3.7474238872528076 s \n",
      "\n",
      "\n",
      "\tEpisode 590 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4687],\n",
      "        [1.0000, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4799],\n",
      "        [0.9999, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504558086395264 \tStep Time:  0.00498652458190918 s \tTotal Time:  3.7534079551696777 s \n",
      "\n",
      "\n",
      "\tEpisode 591 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4676],\n",
      "        [0.9998, 0.4673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4839],\n",
      "        [0.9999, 0.4545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490744411945343 \tStep Time:  0.006981611251831055 s \tTotal Time:  3.760389566421509 s \n",
      "\n",
      "\n",
      "\tEpisode 592 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4956],\n",
      "        [1.0000, 0.4853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4989],\n",
      "        [0.9998, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494487404823303 \tStep Time:  0.004987001419067383 s \tTotal Time:  3.765376567840576 s \n",
      "\n",
      "\n",
      "\tEpisode 593 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4789],\n",
      "        [0.9999, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4522],\n",
      "        [0.9999, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517791748046875 \tStep Time:  0.005983829498291016 s \tTotal Time:  3.771360397338867 s \n",
      "\n",
      "\n",
      "\tEpisode 594 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4783],\n",
      "        [0.9999, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4220],\n",
      "        [0.9999, 0.5208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483784675598145 \tStep Time:  0.007978439331054688 s \tTotal Time:  3.779338836669922 s \n",
      "\n",
      "\n",
      "\tEpisode 595 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5403],\n",
      "        [0.9998, 0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5297],\n",
      "        [0.9999, 0.5258]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53310376405716 \tStep Time:  0.004986763000488281 s \tTotal Time:  3.78432559967041 s \n",
      "\n",
      "\n",
      "\tEpisode 596 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4730],\n",
      "        [1.0000, 0.4021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5591],\n",
      "        [0.9999, 0.4289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484445333480835 \tStep Time:  0.006981849670410156 s \tTotal Time:  3.7913074493408203 s \n",
      "\n",
      "\n",
      "\tEpisode 597 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6150],\n",
      "        [1.0000, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5395],\n",
      "        [0.9998, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517022609710693 \tStep Time:  0.0059833526611328125 s \tTotal Time:  3.797290802001953 s \n",
      "\n",
      "\n",
      "\tEpisode 598 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4046],\n",
      "        [0.9998, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5147],\n",
      "        [0.9999, 0.5325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564239025115967 \tStep Time:  0.005983829498291016 s \tTotal Time:  3.803274631500244 s \n",
      "\n",
      "\n",
      "\tEpisode 599 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5420],\n",
      "        [1.0000, 0.5867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5457],\n",
      "        [0.9999, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489640355110168 \tStep Time:  0.005984783172607422 s \tTotal Time:  3.8092594146728516 s \n",
      "\n",
      "\n",
      "\tEpisode 600 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5412],\n",
      "        [0.9999, 0.5442]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4180],\n",
      "        [1.0000, 0.5666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.600083589553833 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.8152430057525635 s \n",
      "\n",
      "\n",
      "\tEpisode 601 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6787],\n",
      "        [0.9998, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4800],\n",
      "        [0.9999, 0.5351]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469125270843506 \tStep Time:  0.005984306335449219 s \tTotal Time:  3.8212273120880127 s \n",
      "\n",
      "\n",
      "\tEpisode 602 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5329],\n",
      "        [1.0000, 0.5349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4955],\n",
      "        [0.9998, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50541639328003 \tStep Time:  0.00698089599609375 s \tTotal Time:  3.8282082080841064 s \n",
      "\n",
      "\n",
      "\tEpisode 603 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5071],\n",
      "        [0.9998, 0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4748],\n",
      "        [0.9998, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487634658813477 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.8341922760009766 s \n",
      "\n",
      "\n",
      "\tEpisode 604 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4817],\n",
      "        [0.9998, 0.5211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4991],\n",
      "        [0.9998, 0.5252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513902187347412 \tStep Time:  0.005983829498291016 s \tTotal Time:  3.8401761054992676 s \n",
      "\n",
      "\n",
      "\tEpisode 605 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5274],\n",
      "        [0.9999, 0.4725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5253],\n",
      "        [0.9998, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483488082885742 \tStep Time:  0.004987001419067383 s \tTotal Time:  3.845163106918335 s \n",
      "\n",
      "\n",
      "\tEpisode 606 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5133],\n",
      "        [0.9998, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4827],\n",
      "        [0.9999, 0.4613]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516432166099548 \tStep Time:  0.0059833526611328125 s \tTotal Time:  3.8521440029144287 s \n",
      "\n",
      "\n",
      "\tEpisode 607 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4711],\n",
      "        [0.9998, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4673],\n",
      "        [0.9998, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497524738311768 \tStep Time:  0.0069811344146728516 s \tTotal Time:  3.8591251373291016 s \n",
      "\n",
      "\n",
      "\tEpisode 608 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5020],\n",
      "        [0.9998, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4743],\n",
      "        [0.9999, 0.4680]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53909969329834 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.86510968208313 s \n",
      "\n",
      "\n",
      "\tEpisode 609 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6206],\n",
      "        [1.0000, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5438],\n",
      "        [0.9999, 0.5404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506453514099121 \tStep Time:  0.0059854984283447266 s \tTotal Time:  3.8710951805114746 s \n",
      "\n",
      "\n",
      "\tEpisode 610 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4946],\n",
      "        [0.9999, 0.5271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5023],\n",
      "        [0.9999, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488592624664307 \tStep Time:  0.006980180740356445 s \tTotal Time:  3.878075361251831 s \n",
      "\n",
      "\n",
      "\tEpisode 611 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5186],\n",
      "        [0.9999, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4900],\n",
      "        [0.9998, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509692668914795 \tStep Time:  0.0059850215911865234 s \tTotal Time:  3.8840603828430176 s \n",
      "\n",
      "\n",
      "\tEpisode 612 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.5592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4974],\n",
      "        [1.0000, 0.6160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483935356140137 \tStep Time:  0.006980180740356445 s \tTotal Time:  3.891040563583374 s \n",
      "\n",
      "\n",
      "\tEpisode 613 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5493],\n",
      "        [0.9998, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5116],\n",
      "        [0.9999, 0.4774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526169061660767 \tStep Time:  0.004986763000488281 s \tTotal Time:  3.8960273265838623 s \n",
      "\n",
      "\n",
      "\tEpisode 614 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4872],\n",
      "        [1.0000, 0.5291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5027],\n",
      "        [1.0000, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521443843841553 \tStep Time:  0.0069806575775146484 s \tTotal Time:  3.903007984161377 s \n",
      "\n",
      "\n",
      "\tEpisode 615 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.5587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5664],\n",
      "        [0.9998, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525100290775299 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.908992052078247 s \n",
      "\n",
      "\n",
      "\tEpisode 616 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5022],\n",
      "        [0.9998, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [0.9999, 0.4984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515337944030762 \tStep Time:  0.004986763000488281 s \tTotal Time:  3.9139788150787354 s \n",
      "\n",
      "\n",
      "\tEpisode 617 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4839],\n",
      "        [1.0000, 0.6571]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4911],\n",
      "        [0.9998, 0.4887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43005383014679 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.9199628829956055 s \n",
      "\n",
      "\n",
      "\tEpisode 618 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4902],\n",
      "        [0.9999, 0.4893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5393],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523253858089447 \tStep Time:  0.0069811344146728516 s \tTotal Time:  3.9269440174102783 s \n",
      "\n",
      "\n",
      "\tEpisode 619 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5590],\n",
      "        [0.9999, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4781],\n",
      "        [0.9999, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463769435882568 \tStep Time:  0.004987478256225586 s \tTotal Time:  3.931931495666504 s \n",
      "\n",
      "\n",
      "\tEpisode 620 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4854],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4571],\n",
      "        [0.9998, 0.4890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467665612697601 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.937915086746216 s \n",
      "\n",
      "\n",
      "\tEpisode 621 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5763],\n",
      "        [0.9999, 0.4574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4764],\n",
      "        [0.9998, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49666976928711 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.943899154663086 s \n",
      "\n",
      "\n",
      "\tEpisode 622 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4933],\n",
      "        [1.0000, 0.4334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4368],\n",
      "        [0.9999, 0.4323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540297031402588 \tStep Time:  0.004986286163330078 s \tTotal Time:  3.948885440826416 s \n",
      "\n",
      "\n",
      "\tEpisode 623 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4886],\n",
      "        [0.9999, 0.4554]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4607],\n",
      "        [1.0000, 0.6725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.407156467437744 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.955866813659668 s \n",
      "\n",
      "\n",
      "\tEpisode 624 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4167],\n",
      "        [0.9998, 0.4943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4076],\n",
      "        [0.9999, 0.5737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.399742603302002 \tStep Time:  0.004986763000488281 s \tTotal Time:  3.9608535766601562 s \n",
      "\n",
      "\n",
      "\tEpisode 625 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6054],\n",
      "        [0.9999, 0.5845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3785],\n",
      "        [1.0000, 0.7777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.347689688205719 \tStep Time:  0.005983829498291016 s \tTotal Time:  3.9668374061584473 s \n",
      "\n",
      "\n",
      "\tEpisode 626 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4408],\n",
      "        [0.9999, 0.4279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4932],\n",
      "        [0.9999, 0.4453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550191402435303 \tStep Time:  0.00598454475402832 s \tTotal Time:  3.9728219509124756 s \n",
      "\n",
      "\n",
      "\tEpisode 627 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5792],\n",
      "        [0.9998, 0.5430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4636],\n",
      "        [0.9999, 0.6030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542055606842041 \tStep Time:  0.005983591079711914 s \tTotal Time:  3.9788055419921875 s \n",
      "\n",
      "\n",
      "\tEpisode 628 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5518],\n",
      "        [1.0000, 0.7239]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5496],\n",
      "        [1.0000, 0.8306]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535037636756897 \tStep Time:  0.006981372833251953 s \tTotal Time:  3.9857869148254395 s \n",
      "\n",
      "\n",
      "\tEpisode 629 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6060],\n",
      "        [0.9998, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5527],\n",
      "        [1.0000, 0.3269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443848133087158 \tStep Time:  0.005984067916870117 s \tTotal Time:  3.9917709827423096 s \n",
      "\n",
      "\n",
      "\tEpisode 630 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4946],\n",
      "        [0.9998, 0.5245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6195],\n",
      "        [0.9998, 0.4757]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43503189086914 \tStep Time:  0.00498652458190918 s \tTotal Time:  3.9967575073242188 s \n",
      "\n",
      "\n",
      "\tEpisode 631 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6344],\n",
      "        [0.9998, 0.5258]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4532],\n",
      "        [0.9998, 0.4244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66821002960205 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.002741575241089 s \n",
      "\n",
      "\n",
      "\tEpisode 632 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5768],\n",
      "        [0.9998, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4850],\n",
      "        [0.9999, 0.5883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534163117408752 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.008725643157959 s \n",
      "\n",
      "\n",
      "\tEpisode 633 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4894],\n",
      "        [0.9998, 0.4998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3591],\n",
      "        [0.9998, 0.5302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441808223724365 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.01470947265625 s \n",
      "\n",
      "\n",
      "\tEpisode 634 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4240],\n",
      "        [1.0000, 0.2998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3979],\n",
      "        [0.9998, 0.4248]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.608940601348877 \tStep Time:  0.004986763000488281 s \tTotal Time:  4.019696235656738 s \n",
      "\n",
      "\n",
      "\tEpisode 635 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4118],\n",
      "        [0.9999, 0.3527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4685],\n",
      "        [1.0000, 0.3013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451021671295166 \tStep Time:  0.006981372833251953 s \tTotal Time:  4.02667760848999 s \n",
      "\n",
      "\n",
      "\tEpisode 636 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3650],\n",
      "        [0.9998, 0.4756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6326],\n",
      "        [0.9998, 0.5421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536832332611084 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.031664133071899 s \n",
      "\n",
      "\n",
      "\tEpisode 637 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2876],\n",
      "        [0.9999, 0.6090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4990],\n",
      "        [0.9999, 0.3739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559357166290283 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.0376482009887695 s \n",
      "\n",
      "\n",
      "\tEpisode 638 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3219],\n",
      "        [0.9998, 0.5438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5553],\n",
      "        [0.9998, 0.4548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47866439819336 \tStep Time:  0.005985260009765625 s \tTotal Time:  4.043633460998535 s \n",
      "\n",
      "\n",
      "\tEpisode 639 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4923],\n",
      "        [0.9998, 0.4624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7685],\n",
      "        [1.0000, 0.3158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63865852355957 \tStep Time:  0.005982637405395508 s \tTotal Time:  4.049616098403931 s \n",
      "\n",
      "\n",
      "\tEpisode 640 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4516],\n",
      "        [0.9999, 0.5999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3230],\n",
      "        [0.9999, 0.4118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.400941848754883 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.05560040473938 s \n",
      "\n",
      "\n",
      "\tEpisode 641 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4896],\n",
      "        [0.9998, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3085],\n",
      "        [0.9998, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.650129795074463 \tStep Time:  0.0069811344146728516 s \tTotal Time:  4.062581539154053 s \n",
      "\n",
      "\n",
      "\tEpisode 642 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6697],\n",
      "        [1.0000, 0.3148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5365],\n",
      "        [0.9998, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.729849815368652 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.068565607070923 s \n",
      "\n",
      "\n",
      "\tEpisode 643 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3998],\n",
      "        [1.0000, 0.6607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7006],\n",
      "        [1.0000, 0.7350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45058012008667 \tStep Time:  0.006982564926147461 s \tTotal Time:  4.07554817199707 s \n",
      "\n",
      "\n",
      "\tEpisode 644 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3950],\n",
      "        [0.9999, 0.5767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6343],\n",
      "        [1.0000, 0.4005]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575652599334717 \tStep Time:  0.00797724723815918 s \tTotal Time:  4.0835254192352295 s \n",
      "\n",
      "\n",
      "\tEpisode 645 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4791],\n",
      "        [0.9999, 0.4045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3777],\n",
      "        [1.0000, 0.3893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523386657238007 \tStep Time:  0.005985260009765625 s \tTotal Time:  4.089510679244995 s \n",
      "\n",
      "\n",
      "\tEpisode 646 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5697],\n",
      "        [0.9998, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6528],\n",
      "        [1.0000, 0.3961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.400633096694946 \tStep Time:  0.005982875823974609 s \tTotal Time:  4.09549355506897 s \n",
      "\n",
      "\n",
      "\tEpisode 647 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6165],\n",
      "        [1.0000, 0.4349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5983],\n",
      "        [1.0000, 0.3896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.348689019680023 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.101477384567261 s \n",
      "\n",
      "\n",
      "\tEpisode 648 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3823],\n",
      "        [0.9998, 0.5398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5057],\n",
      "        [0.9999, 0.4038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56913012266159 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.10746169090271 s \n",
      "\n",
      "\n",
      "\tEpisode 649 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5576],\n",
      "        [0.9998, 0.4878]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4133],\n",
      "        [0.9998, 0.5202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54611587524414 \tStep Time:  0.005983591079711914 s \tTotal Time:  4.113445281982422 s \n",
      "\n",
      "\n",
      "\tEpisode 650 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5216],\n",
      "        [0.9999, 0.6030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5774],\n",
      "        [1.0000, 0.3656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471110343933105 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.119429349899292 s \n",
      "\n",
      "\n",
      "\tEpisode 651 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4134],\n",
      "        [0.9999, 0.6381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5653],\n",
      "        [0.9999, 0.4415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60319209098816 \tStep Time:  0.006981372833251953 s \tTotal Time:  4.126410722732544 s \n",
      "\n",
      "\n",
      "\tEpisode 652 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5291],\n",
      "        [0.9998, 0.5276]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4086],\n",
      "        [0.9998, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468337178230286 \tStep Time:  0.004987001419067383 s \tTotal Time:  4.131397724151611 s \n",
      "\n",
      "\n",
      "\tEpisode 653 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6892],\n",
      "        [0.9998, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5723],\n",
      "        [1.0000, 0.3646]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.360949516296387 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.137381553649902 s \n",
      "\n",
      "\n",
      "\tEpisode 654 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4842],\n",
      "        [1.0000, 0.6947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5783],\n",
      "        [1.0000, 0.3515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552886962890625 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.143365383148193 s \n",
      "\n",
      "\n",
      "\tEpisode 655 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3436],\n",
      "        [0.9998, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6869],\n",
      "        [1.0000, 0.6645]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.8046875 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.1493494510650635 s \n",
      "\n",
      "\n",
      "\tEpisode 656 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4744],\n",
      "        [0.9998, 0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5518],\n",
      "        [1.0000, 0.3606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497666358947754 \tStep Time:  0.006981611251831055 s \tTotal Time:  4.1563310623168945 s \n",
      "\n",
      "\n",
      "\tEpisode 657 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7385],\n",
      "        [0.9998, 0.5194]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5898],\n",
      "        [0.9998, 0.4819]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.667848110198975 \tStep Time:  0.004986286163330078 s \tTotal Time:  4.161317348480225 s \n",
      "\n",
      "\n",
      "\tEpisode 658 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5372],\n",
      "        [1.0000, 0.3429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4278],\n",
      "        [0.9999, 0.5470]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.70134449005127 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.167301416397095 s \n",
      "\n",
      "\n",
      "\tEpisode 659 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3639],\n",
      "        [0.9998, 0.4745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5418],\n",
      "        [1.0000, 0.5658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599770545959473 \tStep Time:  0.006981849670410156 s \tTotal Time:  4.174283266067505 s \n",
      "\n",
      "\n",
      "\tEpisode 660 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4025],\n",
      "        [0.9999, 0.5305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4673],\n",
      "        [0.9998, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543994426727295 \tStep Time:  0.005983591079711914 s \tTotal Time:  4.180266857147217 s \n",
      "\n",
      "\n",
      "\tEpisode 661 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4272],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4956],\n",
      "        [0.9998, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471192717552185 \tStep Time:  0.0060079097747802734 s \tTotal Time:  4.186274766921997 s \n",
      "\n",
      "\n",
      "\tEpisode 662 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5008],\n",
      "        [1.0000, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4962],\n",
      "        [0.9999, 0.4671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549677848815918 \tStep Time:  0.005959987640380859 s \tTotal Time:  4.192234754562378 s \n",
      "\n",
      "\n",
      "\tEpisode 663 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4992],\n",
      "        [0.9998, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4881],\n",
      "        [0.9998, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522890210151672 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.197221279144287 s \n",
      "\n",
      "\n",
      "\tEpisode 664 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4692],\n",
      "        [0.9999, 0.4597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4955],\n",
      "        [0.9999, 0.4592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502427339553833 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.203205347061157 s \n",
      "\n",
      "\n",
      "\tEpisode 665 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4554],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4616],\n",
      "        [1.0000, 0.4360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503021717071533 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.2091896533966064 s \n",
      "\n",
      "\n",
      "\tEpisode 666 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5133],\n",
      "        [0.9998, 0.4949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6241],\n",
      "        [1.0000, 0.4532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448787689208984 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.2151734828948975 s \n",
      "\n",
      "\n",
      "\tEpisode 667 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4840],\n",
      "        [0.9999, 0.5241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4968],\n",
      "        [0.9998, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505380153656006 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.220160007476807 s \n",
      "\n",
      "\n",
      "\tEpisode 668 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5438],\n",
      "        [0.9998, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5501],\n",
      "        [0.9998, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46977186203003 \tStep Time:  0.006981372833251953 s \tTotal Time:  4.227141380310059 s \n",
      "\n",
      "\n",
      "\tEpisode 669 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5301],\n",
      "        [0.9999, 0.4758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5470],\n",
      "        [1.0000, 0.6401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539141654968262 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.232127904891968 s \n",
      "\n",
      "\n",
      "\tEpisode 670 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4933],\n",
      "        [0.9999, 0.4158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4591],\n",
      "        [0.9999, 0.4363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543246865272522 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.238112449645996 s \n",
      "\n",
      "\n",
      "\tEpisode 671 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3923],\n",
      "        [0.9998, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7487],\n",
      "        [1.0000, 0.4126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.421844959259033 \tStep Time:  0.005983591079711914 s \tTotal Time:  4.244096040725708 s \n",
      "\n",
      "\n",
      "\tEpisode 672 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5606],\n",
      "        [1.0000, 0.6887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4894],\n",
      "        [0.9999, 0.4168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.706995487213135 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.250080108642578 s \n",
      "\n",
      "\n",
      "\tEpisode 673 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5128],\n",
      "        [1.0000, 0.3151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5365],\n",
      "        [0.9998, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.415555477142334 \tStep Time:  0.004986763000488281 s \tTotal Time:  4.255066871643066 s \n",
      "\n",
      "\n",
      "\tEpisode 674 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5899],\n",
      "        [0.9998, 0.5279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3984],\n",
      "        [1.0000, 0.5964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460586071014404 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.261050701141357 s \n",
      "\n",
      "\n",
      "\tEpisode 675 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4506],\n",
      "        [1.0000, 0.6155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3023],\n",
      "        [0.9998, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553323745727539 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.267034530639648 s \n",
      "\n",
      "\n",
      "\tEpisode 676 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3949],\n",
      "        [0.9999, 0.5437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6330],\n",
      "        [0.9998, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451727867126465 \tStep Time:  0.004987478256225586 s \tTotal Time:  4.273019552230835 s \n",
      "\n",
      "\n",
      "\tEpisode 677 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4781],\n",
      "        [1.0000, 0.2293]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5058],\n",
      "        [1.0000, 0.3165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510221481323242 \tStep Time:  0.00698089599609375 s \tTotal Time:  4.280000448226929 s \n",
      "\n",
      "\n",
      "\tEpisode 678 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5169],\n",
      "        [0.9999, 0.4338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7066],\n",
      "        [1.0000, 0.6387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579054832458496 \tStep Time:  0.00698089599609375 s \tTotal Time:  4.2869813442230225 s \n",
      "\n",
      "\n",
      "\tEpisode 679 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2553],\n",
      "        [0.9999, 0.4417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3188],\n",
      "        [0.9999, 0.3407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502906799316406 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.292965412139893 s \n",
      "\n",
      "\n",
      "\tEpisode 680 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7112],\n",
      "        [1.0000, 0.3198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5900],\n",
      "        [0.9998, 0.4567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545559883117676 \tStep Time:  0.006981372833251953 s \tTotal Time:  4.2999467849731445 s \n",
      "\n",
      "\n",
      "\tEpisode 681 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1629],\n",
      "        [1.0000, 0.2989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3698],\n",
      "        [1.0000, 0.1891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.669947147369385 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.304933309555054 s \n",
      "\n",
      "\n",
      "\tEpisode 682 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3793],\n",
      "        [0.9999, 0.5583]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4789],\n",
      "        [0.9999, 0.5771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.667265594005585 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.310917377471924 s \n",
      "\n",
      "\n",
      "\tEpisode 683 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7229],\n",
      "        [0.9999, 0.4672]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5525],\n",
      "        [0.9998, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.703741729259491 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.316901445388794 s \n",
      "\n",
      "\n",
      "\tEpisode 684 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5458],\n",
      "        [0.9999, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5267],\n",
      "        [0.9998, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536737561225891 \tStep Time:  0.00598454475402832 s \tTotal Time:  4.322885990142822 s \n",
      "\n",
      "\n",
      "\tEpisode 685 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3988],\n",
      "        [1.0000, 0.4021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4232],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59617805480957 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.328870058059692 s \n",
      "\n",
      "\n",
      "\tEpisode 686 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5088],\n",
      "        [0.9999, 0.5117]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4630],\n",
      "        [1.0000, 0.4761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53650015592575 \tStep Time:  0.0059833526611328125 s \tTotal Time:  4.334853410720825 s \n",
      "\n",
      "\n",
      "\tEpisode 687 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4832],\n",
      "        [1.0000, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546823024749756 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.340837478637695 s \n",
      "\n",
      "\n",
      "\tEpisode 688 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5387],\n",
      "        [0.9999, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5153],\n",
      "        [0.9999, 0.5430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516310334205627 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.3458240032196045 s \n",
      "\n",
      "\n",
      "\tEpisode 689 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4994],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6093],\n",
      "        [1.0000, 0.4232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.637934684753418 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.351808309555054 s \n",
      "\n",
      "\n",
      "\tEpisode 690 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4858],\n",
      "        [0.9999, 0.5475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5371],\n",
      "        [1.0000, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540817260742188 \tStep Time:  0.00598454475402832 s \tTotal Time:  4.357792854309082 s \n",
      "\n",
      "\n",
      "\tEpisode 691 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5244],\n",
      "        [0.9998, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5112],\n",
      "        [0.9999, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523305654525757 \tStep Time:  0.005983114242553711 s \tTotal Time:  4.363775968551636 s \n",
      "\n",
      "\n",
      "\tEpisode 692 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [0.9999, 0.5336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4972],\n",
      "        [0.9999, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489946126937866 \tStep Time:  0.004986763000488281 s \tTotal Time:  4.368762731552124 s \n",
      "\n",
      "\n",
      "\tEpisode 693 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4442],\n",
      "        [0.9999, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5374],\n",
      "        [1.0000, 0.5403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475558757781982 \tStep Time:  0.00598454475402832 s \tTotal Time:  4.374747276306152 s \n",
      "\n",
      "\n",
      "\tEpisode 694 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5355],\n",
      "        [0.9999, 0.4977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5303],\n",
      "        [0.9999, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500073432922363 \tStep Time:  0.0059833526611328125 s \tTotal Time:  4.380730628967285 s \n",
      "\n",
      "\n",
      "\tEpisode 695 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3732],\n",
      "        [0.9999, 0.4441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5446],\n",
      "        [0.9998, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538686990737915 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.386714935302734 s \n",
      "\n",
      "\n",
      "\tEpisode 696 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5074],\n",
      "        [1.0000, 0.5831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5679],\n",
      "        [0.9999, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509249687194824 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.392698764801025 s \n",
      "\n",
      "\n",
      "\tEpisode 697 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3194],\n",
      "        [0.9999, 0.5718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4428],\n",
      "        [0.9998, 0.5443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.700846195220947 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.3986828327178955 s \n",
      "\n",
      "\n",
      "\tEpisode 698 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5589],\n",
      "        [1.0000, 0.6156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5247],\n",
      "        [0.9999, 0.4986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536139488220215 \tStep Time:  0.004986763000488281 s \tTotal Time:  4.403669595718384 s \n",
      "\n",
      "\n",
      "\tEpisode 699 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5165],\n",
      "        [0.9998, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5595],\n",
      "        [1.0000, 0.3726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440555393695831 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.409653425216675 s \n",
      "\n",
      "\n",
      "\tEpisode 700 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5056],\n",
      "        [1.0000, 0.5593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5561],\n",
      "        [1.0000, 0.3401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6537184715271 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.415637731552124 s \n",
      "\n",
      "\n",
      "\tEpisode 701 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5583],\n",
      "        [0.9998, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5184],\n",
      "        [1.0000, 0.5842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529750347137451 \tStep Time:  0.005985260009765625 s \tTotal Time:  4.42162299156189 s \n",
      "\n",
      "\n",
      "\tEpisode 702 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5255],\n",
      "        [0.9998, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4594],\n",
      "        [0.9998, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533370912075043 \tStep Time:  0.0059816837310791016 s \tTotal Time:  4.428602695465088 s \n",
      "\n",
      "\n",
      "\tEpisode 703 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5096],\n",
      "        [0.9998, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5047],\n",
      "        [0.9999, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514147281646729 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.434586524963379 s \n",
      "\n",
      "\n",
      "\tEpisode 704 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4267],\n",
      "        [0.9998, 0.5024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5048],\n",
      "        [0.9998, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481274604797363 \tStep Time:  0.005985260009765625 s \tTotal Time:  4.4405717849731445 s \n",
      "\n",
      "\n",
      "\tEpisode 705 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4972],\n",
      "        [0.9999, 0.5005]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5125],\n",
      "        [0.9998, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51395034790039 \tStep Time:  0.006980180740356445 s \tTotal Time:  4.447551965713501 s \n",
      "\n",
      "\n",
      "\tEpisode 706 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5135],\n",
      "        [1.0000, 0.4705]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4852],\n",
      "        [0.9999, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50508964061737 \tStep Time:  0.006988048553466797 s \tTotal Time:  4.454540014266968 s \n",
      "\n",
      "\n",
      "\tEpisode 707 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4907],\n",
      "        [0.9998, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4919],\n",
      "        [0.9999, 0.4811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505294799804688 \tStep Time:  0.007972002029418945 s \tTotal Time:  4.462512016296387 s \n",
      "\n",
      "\n",
      "\tEpisode 708 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4872],\n",
      "        [1.0000, 0.4398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4798],\n",
      "        [0.9999, 0.4692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522584915161133 \tStep Time:  0.006981372833251953 s \tTotal Time:  4.469493389129639 s \n",
      "\n",
      "\n",
      "\tEpisode 709 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4574],\n",
      "        [1.0000, 0.4089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4049],\n",
      "        [0.9999, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498146533966064 \tStep Time:  0.006985664367675781 s \tTotal Time:  4.4764790534973145 s \n",
      "\n",
      "\n",
      "\tEpisode 710 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4924],\n",
      "        [1.0000, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5127],\n",
      "        [0.9999, 0.5202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518204689025879 \tStep Time:  0.006977558135986328 s \tTotal Time:  4.483456611633301 s \n",
      "\n",
      "\n",
      "\tEpisode 711 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5067],\n",
      "        [1.0000, 0.3839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5291],\n",
      "        [0.9999, 0.4440]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493378639221191 \tStep Time:  0.005985260009765625 s \tTotal Time:  4.489441871643066 s \n",
      "\n",
      "\n",
      "\tEpisode 712 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5322],\n",
      "        [0.9999, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3619],\n",
      "        [0.9999, 0.4157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.386736392974854 \tStep Time:  0.006979942321777344 s \tTotal Time:  4.496421813964844 s \n",
      "\n",
      "\n",
      "\tEpisode 713 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5554],\n",
      "        [1.0000, 0.6369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4922],\n",
      "        [1.0000, 0.6137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481612205505371 \tStep Time:  0.005983591079711914 s \tTotal Time:  4.502405405044556 s \n",
      "\n",
      "\n",
      "\tEpisode 714 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4204],\n",
      "        [0.9999, 0.5698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5702],\n",
      "        [0.9998, 0.4749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.636836051940918 \tStep Time:  0.006981611251831055 s \tTotal Time:  4.509387016296387 s \n",
      "\n",
      "\n",
      "\tEpisode 715 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3557],\n",
      "        [0.9998, 0.4692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6204],\n",
      "        [1.0000, 0.5856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6027250289917 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.515371084213257 s \n",
      "\n",
      "\n",
      "\tEpisode 716 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5068],\n",
      "        [0.9998, 0.4887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5735],\n",
      "        [0.9998, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490286350250244 \tStep Time:  0.004986286163330078 s \tTotal Time:  4.520357370376587 s \n",
      "\n",
      "\n",
      "\tEpisode 717 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5815],\n",
      "        [0.9999, 0.4402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2765],\n",
      "        [0.9998, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.346466600894928 \tStep Time:  0.006982088088989258 s \tTotal Time:  4.527339458465576 s \n",
      "\n",
      "\n",
      "\tEpisode 718 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6862],\n",
      "        [1.0000, 0.5869]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4491],\n",
      "        [0.9999, 0.5719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.660596370697021 \tStep Time:  0.0059833526611328125 s \tTotal Time:  4.533322811126709 s \n",
      "\n",
      "\n",
      "\tEpisode 719 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5696],\n",
      "        [0.9999, 0.5492]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4116],\n",
      "        [0.9999, 0.5507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575749278068542 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.539307117462158 s \n",
      "\n",
      "\n",
      "\tEpisode 720 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5468],\n",
      "        [1.0000, 0.5882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5490],\n",
      "        [0.9999, 0.4003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573647737503052 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.545290946960449 s \n",
      "\n",
      "\n",
      "\tEpisode 721 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3294],\n",
      "        [1.0000, 0.2788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5514],\n",
      "        [0.9999, 0.5445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576912701129913 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.55127477645874 s \n",
      "\n",
      "\n",
      "\tEpisode 722 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5465],\n",
      "        [0.9999, 0.5460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4752],\n",
      "        [0.9998, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587893009185791 \tStep Time:  0.005984783172607422 s \tTotal Time:  4.557259559631348 s \n",
      "\n",
      "\n",
      "\tEpisode 723 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4090],\n",
      "        [1.0000, 0.5880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3736],\n",
      "        [0.9999, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586560249328613 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.563243627548218 s \n",
      "\n",
      "\n",
      "\tEpisode 724 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5107],\n",
      "        [0.9998, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4107],\n",
      "        [0.9998, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561325550079346 \tStep Time:  0.005983114242553711 s \tTotal Time:  4.5692267417907715 s \n",
      "\n",
      "\n",
      "\tEpisode 725 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4628],\n",
      "        [1.0000, 0.4806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5074],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534097790718079 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.575211048126221 s \n",
      "\n",
      "\n",
      "\tEpisode 726 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.4991],\n",
      "        [1.0000, 0.4001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4341],\n",
      "        [1.0000, 0.4445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5129976272583 \tStep Time:  0.005985260009765625 s \tTotal Time:  4.581196308135986 s \n",
      "\n",
      "\n",
      "\tEpisode 727 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4998],\n",
      "        [0.9999, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5079],\n",
      "        [1.0000, 0.4711]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500085771083832 \tStep Time:  0.005982398986816406 s \tTotal Time:  4.587178707122803 s \n",
      "\n",
      "\n",
      "\tEpisode 728 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5129],\n",
      "        [0.9999, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5231],\n",
      "        [0.9999, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527341783046722 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.593162775039673 s \n",
      "\n",
      "\n",
      "\tEpisode 729 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3793],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3191],\n",
      "        [0.9999, 0.4659]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587101459503174 \tStep Time:  0.004986763000488281 s \tTotal Time:  4.598149538040161 s \n",
      "\n",
      "\n",
      "\tEpisode 730 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4391],\n",
      "        [0.9999, 0.5310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5199],\n",
      "        [0.9999, 0.4539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524375915527344 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.604133367538452 s \n",
      "\n",
      "\n",
      "\tEpisode 731 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4969],\n",
      "        [0.9999, 0.5405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5521],\n",
      "        [0.9999, 0.5501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555721282958984 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.610117673873901 s \n",
      "\n",
      "\n",
      "\tEpisode 732 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5529],\n",
      "        [0.9999, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5030],\n",
      "        [0.9998, 0.5207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53049373626709 \tStep Time:  0.004986763000488281 s \tTotal Time:  4.61510443687439 s \n",
      "\n",
      "\n",
      "\tEpisode 733 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5442],\n",
      "        [0.9998, 0.5335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5159],\n",
      "        [0.9999, 0.5362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530678749084473 \tStep Time:  0.005983591079711914 s \tTotal Time:  4.621088027954102 s \n",
      "\n",
      "\n",
      "\tEpisode 734 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3575],\n",
      "        [0.9999, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5150],\n",
      "        [0.9999, 0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612841129302979 \tStep Time:  0.0059833526611328125 s \tTotal Time:  4.628069639205933 s \n",
      "\n",
      "\n",
      "\tEpisode 735 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5290],\n",
      "        [0.9999, 0.4984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4678],\n",
      "        [0.9998, 0.5145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514835834503174 \tStep Time:  0.004986286163330078 s \tTotal Time:  4.633055925369263 s \n",
      "\n",
      "\n",
      "\tEpisode 736 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5141],\n",
      "        [0.9998, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5125],\n",
      "        [0.9999, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542619228363037 \tStep Time:  0.0059854984283447266 s \tTotal Time:  4.639041423797607 s \n",
      "\n",
      "\n",
      "\tEpisode 737 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4920],\n",
      "        [0.9999, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4868],\n",
      "        [1.0000, 0.3741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452792584896088 \tStep Time:  0.005982637405395508 s \tTotal Time:  4.645024061203003 s \n",
      "\n",
      "\n",
      "\tEpisode 738 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5134],\n",
      "        [0.9999, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5122],\n",
      "        [0.9999, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515437126159668 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.650010585784912 s \n",
      "\n",
      "\n",
      "\tEpisode 739 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4419],\n",
      "        [0.9999, 0.5153]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5187],\n",
      "        [0.9999, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554343223571777 \tStep Time:  0.006981611251831055 s \tTotal Time:  4.656992197036743 s \n",
      "\n",
      "\n",
      "\tEpisode 740 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5014],\n",
      "        [0.9999, 0.5141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.3433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462213695049286 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.661978721618652 s \n",
      "\n",
      "\n",
      "\tEpisode 741 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4185],\n",
      "        [1.0000, 0.4770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5031],\n",
      "        [0.9999, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461432933807373 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.667962551116943 s \n",
      "\n",
      "\n",
      "\tEpisode 742 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4730],\n",
      "        [1.0000, 0.3586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3395],\n",
      "        [0.9999, 0.5293]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507374286651611 \tStep Time:  0.006981611251831055 s \tTotal Time:  4.674944162368774 s \n",
      "\n",
      "\n",
      "\tEpisode 743 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5315],\n",
      "        [1.0000, 0.3535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5339],\n",
      "        [1.0000, 0.3384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525084972381592 \tStep Time:  0.0069811344146728516 s \tTotal Time:  4.681925296783447 s \n",
      "\n",
      "\n",
      "\tEpisode 744 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5449],\n",
      "        [0.9999, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5431],\n",
      "        [0.9998, 0.5436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534574508666992 \tStep Time:  0.008976221084594727 s \tTotal Time:  4.690901517868042 s \n",
      "\n",
      "\n",
      "\tEpisode 745 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3980],\n",
      "        [0.9998, 0.5511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5636],\n",
      "        [0.9999, 0.5593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433954238891602 \tStep Time:  0.005982875823974609 s \tTotal Time:  4.696884393692017 s \n",
      "\n",
      "\n",
      "\tEpisode 746 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5643],\n",
      "        [1.0000, 0.5630]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5268],\n",
      "        [1.0000, 0.2743]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.663902997970581 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.701870918273926 s \n",
      "\n",
      "\n",
      "\tEpisode 747 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5847],\n",
      "        [0.9999, 0.5676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3804],\n",
      "        [0.9998, 0.5566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601495265960693 \tStep Time:  0.0069811344146728516 s \tTotal Time:  4.70984959602356 s \n",
      "\n",
      "\n",
      "\tEpisode 748 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5836],\n",
      "        [0.9999, 0.5818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5767],\n",
      "        [1.0000, 0.6134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512000560760498 \tStep Time:  0.004986286163330078 s \tTotal Time:  4.71483588218689 s \n",
      "\n",
      "\n",
      "\tEpisode 749 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3731],\n",
      "        [1.0000, 0.3673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3945],\n",
      "        [1.0000, 0.5875]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62608802318573 \tStep Time:  0.004987001419067383 s \tTotal Time:  4.719822883605957 s \n",
      "\n",
      "\n",
      "\tEpisode 750 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4959],\n",
      "        [0.9999, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5821],\n",
      "        [1.0000, 0.5845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432214736938477 \tStep Time:  0.0070154666900634766 s \tTotal Time:  4.7268383502960205 s \n",
      "\n",
      "\n",
      "\tEpisode 751 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5284],\n",
      "        [1.0000, 0.3948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4061],\n",
      "        [0.9999, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531869411468506 \tStep Time:  0.0059812068939208984 s \tTotal Time:  4.732819557189941 s \n",
      "\n",
      "\n",
      "\tEpisode 752 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5742],\n",
      "        [0.9999, 0.5846]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4292],\n",
      "        [1.0000, 0.6368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.634007453918457 \tStep Time:  0.004988908767700195 s \tTotal Time:  4.737808465957642 s \n",
      "\n",
      "\n",
      "\tEpisode 753 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4039],\n",
      "        [1.0000, 0.4230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5773],\n",
      "        [0.9999, 0.5773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529471397399902 \tStep Time:  0.005988597869873047 s \tTotal Time:  4.743797063827515 s \n",
      "\n",
      "\n",
      "\tEpisode 754 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5616],\n",
      "        [0.9998, 0.5492]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5366],\n",
      "        [1.0000, 0.4748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476058423519135 \tStep Time:  0.005976676940917969 s \tTotal Time:  4.749773740768433 s \n",
      "\n",
      "\n",
      "\tEpisode 755 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5483],\n",
      "        [0.9999, 0.5636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5459],\n",
      "        [0.9999, 0.5398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516316413879395 \tStep Time:  0.0059854984283447266 s \tTotal Time:  4.755759239196777 s \n",
      "\n",
      "\n",
      "\tEpisode 756 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5702],\n",
      "        [1.0000, 0.5504]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5409],\n",
      "        [1.0000, 0.4488]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476901531219482 \tStep Time:  0.006979942321777344 s \tTotal Time:  4.762739181518555 s \n",
      "\n",
      "\n",
      "\tEpisode 757 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5451],\n",
      "        [1.0000, 0.4831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4477],\n",
      "        [0.9999, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52194619178772 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.767725706100464 s \n",
      "\n",
      "\n",
      "\tEpisode 758 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5375],\n",
      "        [0.9999, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5217],\n",
      "        [0.9999, 0.5275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524099349975586 \tStep Time:  0.0059850215911865234 s \tTotal Time:  4.77371072769165 s \n",
      "\n",
      "\n",
      "\tEpisode 759 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5150],\n",
      "        [0.9999, 0.5180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4529],\n",
      "        [1.0000, 0.4672]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514576077461243 \tStep Time:  0.0069811344146728516 s \tTotal Time:  4.780691862106323 s \n",
      "\n",
      "\n",
      "\tEpisode 760 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4405],\n",
      "        [0.9999, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5309],\n",
      "        [0.9999, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548884391784668 \tStep Time:  0.004988431930541992 s \tTotal Time:  4.785680294036865 s \n",
      "\n",
      "\n",
      "\tEpisode 761 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5111],\n",
      "        [0.9999, 0.5282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5103],\n",
      "        [0.9999, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508450090885162 \tStep Time:  0.0059816837310791016 s \tTotal Time:  4.791661977767944 s \n",
      "\n",
      "\n",
      "\tEpisode 762 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5174],\n",
      "        [0.9999, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5129],\n",
      "        [0.9999, 0.5117]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508995532989502 \tStep Time:  0.004956722259521484 s \tTotal Time:  4.796618700027466 s \n",
      "\n",
      "\n",
      "\tEpisode 763 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5125],\n",
      "        [0.9998, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4995],\n",
      "        [1.0000, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501088500022888 \tStep Time:  0.0059816837310791016 s \tTotal Time:  4.803631782531738 s \n",
      "\n",
      "\n",
      "\tEpisode 764 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4787],\n",
      "        [0.9998, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5078],\n",
      "        [0.9999, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4869664311409 \tStep Time:  0.004985332489013672 s \tTotal Time:  4.808617115020752 s \n",
      "\n",
      "\n",
      "\tEpisode 765 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4847],\n",
      "        [0.9998, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9998, 0.5136],\n",
      "        [0.9999, 0.4797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502273082733154 \tStep Time:  0.005953550338745117 s \tTotal Time:  4.814570665359497 s \n",
      "\n",
      "\n",
      "\tEpisode 766 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5090],\n",
      "        [0.9999, 0.5164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4956],\n",
      "        [1.0000, 0.4975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51813268661499 \tStep Time:  0.006014108657836914 s \tTotal Time:  4.820584774017334 s \n",
      "\n",
      "\n",
      "\tEpisode 767 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4178],\n",
      "        [0.9999, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5159],\n",
      "        [0.9999, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559082508087158 \tStep Time:  0.00598907470703125 s \tTotal Time:  4.826573848724365 s \n",
      "\n",
      "\n",
      "\tEpisode 768 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5135],\n",
      "        [1.0000, 0.4397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5155],\n",
      "        [0.9999, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466946601867676 \tStep Time:  0.00498199462890625 s \tTotal Time:  4.8315558433532715 s \n",
      "\n",
      "\n",
      "\tEpisode 769 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4524],\n",
      "        [0.9999, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5138],\n",
      "        [0.9999, 0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531262874603271 \tStep Time:  0.005983591079711914 s \tTotal Time:  4.837539434432983 s \n",
      "\n",
      "\n",
      "\tEpisode 770 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6222],\n",
      "        [0.9999, 0.5193]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5169],\n",
      "        [0.9999, 0.4939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446428656578064 \tStep Time:  0.00598454475402832 s \tTotal Time:  4.843523979187012 s \n",
      "\n",
      "\n",
      "\tEpisode 771 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4480],\n",
      "        [0.9999, 0.5122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4664],\n",
      "        [1.0000, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482521533966064 \tStep Time:  0.00498652458190918 s \tTotal Time:  4.848510503768921 s \n",
      "\n",
      "\n",
      "\tEpisode 772 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5595],\n",
      "        [0.9999, 0.5440]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5220],\n",
      "        [0.9999, 0.5253]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510966658592224 \tStep Time:  0.005986690521240234 s \tTotal Time:  4.854497194290161 s \n",
      "\n",
      "\n",
      "\tEpisode 773 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5789],\n",
      "        [1.0000, 0.5789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5662],\n",
      "        [1.0000, 0.4388]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435407161712646 \tStep Time:  0.005980968475341797 s \tTotal Time:  4.860478162765503 s \n",
      "\n",
      "\n",
      "\tEpisode 774 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5188],\n",
      "        [1.0000, 0.5894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4201],\n",
      "        [0.9999, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425909042358398 \tStep Time:  0.005984067916870117 s \tTotal Time:  4.866462230682373 s \n",
      "\n",
      "\n",
      "\tEpisode 775 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5458],\n",
      "        [0.9999, 0.5278]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5064],\n",
      "        [1.0000, 0.6464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593376398086548 \tStep Time:  0.00695037841796875 s \tTotal Time:  4.873412609100342 s \n",
      "\n",
      "\n",
      "\tEpisode 776 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5264],\n",
      "        [0.9999, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4021],\n",
      "        [0.9999, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446842670440674 \tStep Time:  0.005983114242553711 s \tTotal Time:  4.8793957233428955 s \n",
      "\n",
      "\n",
      "\tEpisode 777 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5617],\n",
      "        [1.0000, 0.4119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5364],\n",
      "        [0.9999, 0.4316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535471439361572 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.885380029678345 s \n",
      "\n",
      "\n",
      "\tEpisode 778 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5522],\n",
      "        [0.9999, 0.4999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4540],\n",
      "        [0.9999, 0.5450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581555366516113 \tStep Time:  0.007013082504272461 s \tTotal Time:  4.892393112182617 s \n",
      "\n",
      "\n",
      "\tEpisode 779 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5882],\n",
      "        [1.0000, 0.3490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5221],\n",
      "        [0.9999, 0.5557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455322742462158 \tStep Time:  0.00498199462890625 s \tTotal Time:  4.897375106811523 s \n",
      "\n",
      "\n",
      "\tEpisode 780 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5248],\n",
      "        [1.0000, 0.5880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4293],\n",
      "        [0.9999, 0.5628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416285634040833 \tStep Time:  0.005994081497192383 s \tTotal Time:  4.903369188308716 s \n",
      "\n",
      "\n",
      "\tEpisode 781 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6375],\n",
      "        [0.9999, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4128],\n",
      "        [1.0000, 0.6486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485634803771973 \tStep Time:  0.00597834587097168 s \tTotal Time:  4.9093475341796875 s \n",
      "\n",
      "\n",
      "\tEpisode 782 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5760],\n",
      "        [0.9999, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4475],\n",
      "        [1.0000, 0.1716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.751986682415009 \tStep Time:  0.004990816116333008 s \tTotal Time:  4.9143383502960205 s \n",
      "\n",
      "\n",
      "\tEpisode 783 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4457],\n",
      "        [0.9999, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4201],\n",
      "        [0.9999, 0.4501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479686260223389 \tStep Time:  0.005979776382446289 s \tTotal Time:  4.920318126678467 s \n",
      "\n",
      "\n",
      "\tEpisode 784 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4492],\n",
      "        [0.9999, 0.4844]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3961],\n",
      "        [0.9999, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572433054447174 \tStep Time:  0.0060193538665771484 s \tTotal Time:  4.926337480545044 s \n",
      "\n",
      "\n",
      "\tEpisode 785 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4936],\n",
      "        [1.0000, 0.7056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4920],\n",
      "        [0.9999, 0.4282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.693772792816162 \tStep Time:  0.005987405776977539 s \tTotal Time:  4.9323248863220215 s \n",
      "\n",
      "\n",
      "\tEpisode 786 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3144],\n",
      "        [0.9999, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5043],\n",
      "        [0.9999, 0.4731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.615205764770508 \tStep Time:  0.004982948303222656 s \tTotal Time:  4.937307834625244 s \n",
      "\n",
      "\n",
      "\tEpisode 787 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4050],\n",
      "        [1.0000, 0.5591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3812],\n",
      "        [0.9999, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.405605018138885 \tStep Time:  0.005986452102661133 s \tTotal Time:  4.943294286727905 s \n",
      "\n",
      "\n",
      "\tEpisode 788 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5899],\n",
      "        [0.9999, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5038],\n",
      "        [1.0000, 0.5935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53757905960083 \tStep Time:  0.005003213882446289 s \tTotal Time:  4.948297500610352 s \n",
      "\n",
      "\n",
      "\tEpisode 789 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5871],\n",
      "        [0.9999, 0.5391]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5540],\n",
      "        [1.0000, 0.5900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57586145401001 \tStep Time:  0.004984140396118164 s \tTotal Time:  4.9542646408081055 s \n",
      "\n",
      "\n",
      "\tEpisode 790 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4760],\n",
      "        [1.0000, 0.4307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4586],\n",
      "        [0.9999, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555665016174316 \tStep Time:  0.005982398986816406 s \tTotal Time:  4.960247039794922 s \n",
      "\n",
      "\n",
      "\tEpisode 791 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5065],\n",
      "        [0.9999, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5164],\n",
      "        [1.0000, 0.4668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522585391998291 \tStep Time:  0.0049877166748046875 s \tTotal Time:  4.965234756469727 s \n",
      "\n",
      "\n",
      "\tEpisode 792 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5064],\n",
      "        [0.9999, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5038],\n",
      "        [0.9999, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519698023796082 \tStep Time:  0.00598454475402832 s \tTotal Time:  4.971219301223755 s \n",
      "\n",
      "\n",
      "\tEpisode 793 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [0.9999, 0.4950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4819],\n",
      "        [0.9999, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488487362861633 \tStep Time:  0.005983829498291016 s \tTotal Time:  4.977203130722046 s \n",
      "\n",
      "\n",
      "\tEpisode 794 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5006],\n",
      "        [1.0000, 0.4701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4991],\n",
      "        [1.0000, 0.4048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578632235527039 \tStep Time:  0.005984306335449219 s \tTotal Time:  4.983187437057495 s \n",
      "\n",
      "\n",
      "\tEpisode 795 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4890],\n",
      "        [1.0000, 0.4204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4568],\n",
      "        [0.9999, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540423393249512 \tStep Time:  0.005950927734375 s \tTotal Time:  4.98913836479187 s \n",
      "\n",
      "\n",
      "\tEpisode 796 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4688],\n",
      "        [0.9999, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5090],\n",
      "        [0.9999, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534846305847168 \tStep Time:  0.004987001419067383 s \tTotal Time:  4.9941253662109375 s \n",
      "\n",
      "\n",
      "\tEpisode 797 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4906],\n",
      "        [1.0000, 0.4412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5208],\n",
      "        [1.0000, 0.4447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527971744537354 \tStep Time:  0.006014347076416016 s \tTotal Time:  5.0001397132873535 s \n",
      "\n",
      "\n",
      "\tEpisode 798 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4681],\n",
      "        [0.9999, 0.5024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5320],\n",
      "        [0.9999, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546488761901855 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.006124019622803 s \n",
      "\n",
      "\n",
      "\tEpisode 799 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5171],\n",
      "        [0.9999, 0.5215]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4862],\n",
      "        [0.9999, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501807749271393 \tStep Time:  0.004987239837646484 s \tTotal Time:  5.011111259460449 s \n",
      "\n",
      "\n",
      "\tEpisode 800 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5283],\n",
      "        [0.9999, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4470],\n",
      "        [0.9999, 0.4631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577936172485352 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.017095565795898 s \n",
      "\n",
      "\n",
      "\tEpisode 801 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5110],\n",
      "        [0.9999, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5021],\n",
      "        [0.9999, 0.4999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503659725189209 \tStep Time:  0.004954814910888672 s \tTotal Time:  5.022050380706787 s \n",
      "\n",
      "\n",
      "\tEpisode 802 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5096],\n",
      "        [0.9999, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4367],\n",
      "        [1.0000, 0.4600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496199131011963 \tStep Time:  0.0060176849365234375 s \tTotal Time:  5.0280680656433105 s \n",
      "\n",
      "\n",
      "\tEpisode 803 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4989],\n",
      "        [1.0000, 0.4380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5077],\n",
      "        [0.9999, 0.4778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484089851379395 \tStep Time:  0.005983114242553711 s \tTotal Time:  5.034051179885864 s \n",
      "\n",
      "\n",
      "\tEpisode 804 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4925],\n",
      "        [0.9999, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4469],\n",
      "        [0.9999, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540549278259277 \tStep Time:  0.005982398986816406 s \tTotal Time:  5.040033578872681 s \n",
      "\n",
      "\n",
      "\tEpisode 805 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5098],\n",
      "        [0.9999, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4805],\n",
      "        [0.9999, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490253448486328 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.045020341873169 s \n",
      "\n",
      "\n",
      "\tEpisode 806 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5127],\n",
      "        [0.9999, 0.5135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5133],\n",
      "        [0.9999, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506573617458344 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.051003932952881 s \n",
      "\n",
      "\n",
      "\tEpisode 807 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5021],\n",
      "        [0.9999, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4587],\n",
      "        [0.9999, 0.5241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486239910125732 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.056988000869751 s \n",
      "\n",
      "\n",
      "\tEpisode 808 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4528],\n",
      "        [0.9999, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4982],\n",
      "        [0.9999, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523087978363037 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.061974763870239 s \n",
      "\n",
      "\n",
      "\tEpisode 809 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5247],\n",
      "        [0.9999, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5199],\n",
      "        [1.0000, 0.4418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524713516235352 \tStep Time:  0.005986690521240234 s \tTotal Time:  5.0679614543914795 s \n",
      "\n",
      "\n",
      "\tEpisode 810 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4928],\n",
      "        [0.9999, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7115],\n",
      "        [0.9999, 0.5197]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.639074444770813 \tStep Time:  0.005951642990112305 s \tTotal Time:  5.073913097381592 s \n",
      "\n",
      "\n",
      "\tEpisode 811 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5163],\n",
      "        [1.0000, 0.4466]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5149],\n",
      "        [1.0000, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473181247711182 \tStep Time:  0.005983114242553711 s \tTotal Time:  5.0798962116241455 s \n",
      "\n",
      "\n",
      "\tEpisode 812 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5076],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4953],\n",
      "        [0.9999, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504315376281738 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.085880279541016 s \n",
      "\n",
      "\n",
      "\tEpisode 813 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4959],\n",
      "        [0.9999, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5042],\n",
      "        [0.9999, 0.4763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51455044746399 \tStep Time:  0.006981611251831055 s \tTotal Time:  5.092861890792847 s \n",
      "\n",
      "\n",
      "\tEpisode 814 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4940],\n",
      "        [0.9999, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4967],\n",
      "        [1.0000, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477386832237244 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.098845720291138 s \n",
      "\n",
      "\n",
      "\tEpisode 815 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5032],\n",
      "        [1.0000, 0.4221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4640],\n",
      "        [0.9999, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566617727279663 \tStep Time:  0.006981611251831055 s \tTotal Time:  5.105827331542969 s \n",
      "\n",
      "\n",
      "\tEpisode 816 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4459],\n",
      "        [1.0000, 0.4196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4879],\n",
      "        [0.9999, 0.4680]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504502296447754 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.11181116104126 s \n",
      "\n",
      "\n",
      "\tEpisode 817 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5047],\n",
      "        [1.0000, 0.4496]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4873],\n",
      "        [0.9999, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494314312934875 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.116797685623169 s \n",
      "\n",
      "\n",
      "\tEpisode 818 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5031],\n",
      "        [0.9999, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5045],\n",
      "        [0.9999, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504380702972412 \tStep Time:  0.005985260009765625 s \tTotal Time:  5.122782945632935 s \n",
      "\n",
      "\n",
      "\tEpisode 819 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4656],\n",
      "        [1.0000, 0.4815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4600],\n",
      "        [1.0000, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503437519073486 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.128766775131226 s \n",
      "\n",
      "\n",
      "\tEpisode 820 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5073],\n",
      "        [0.9999, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5128],\n",
      "        [1.0000, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489987850189209 \tStep Time:  0.005982875823974609 s \tTotal Time:  5.1347496509552 s \n",
      "\n",
      "\n",
      "\tEpisode 821 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5023],\n",
      "        [1.0000, 0.4522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5194],\n",
      "        [1.0000, 0.4242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577730000019073 \tStep Time:  0.004987955093383789 s \tTotal Time:  5.139737606048584 s \n",
      "\n",
      "\n",
      "\tEpisode 822 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4536],\n",
      "        [0.9999, 0.5202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5063],\n",
      "        [0.9999, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530037879943848 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.145720481872559 s \n",
      "\n",
      "\n",
      "\tEpisode 823 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4580],\n",
      "        [1.0000, 0.3988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5354],\n",
      "        [1.0000, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565734207630157 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.150707244873047 s \n",
      "\n",
      "\n",
      "\tEpisode 824 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3531],\n",
      "        [1.0000, 0.2935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5082],\n",
      "        [1.0000, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493839621543884 \tStep Time:  0.005984783172607422 s \tTotal Time:  5.156692028045654 s \n",
      "\n",
      "\n",
      "\tEpisode 825 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5093],\n",
      "        [0.9999, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4934],\n",
      "        [1.0000, 0.5624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531862258911133 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.162675857543945 s \n",
      "\n",
      "\n",
      "\tEpisode 826 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4670],\n",
      "        [0.9999, 0.5369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5518],\n",
      "        [1.0000, 0.6009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519524574279785 \tStep Time:  0.0049860477447509766 s \tTotal Time:  5.167661905288696 s \n",
      "\n",
      "\n",
      "\tEpisode 827 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3897],\n",
      "        [0.9999, 0.5760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3274],\n",
      "        [0.9999, 0.5501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.323187828063965 \tStep Time:  0.006982326507568359 s \tTotal Time:  5.174644231796265 s \n",
      "\n",
      "\n",
      "\tEpisode 828 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5184],\n",
      "        [1.0000, 0.6094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5356],\n",
      "        [1.0000, 0.4152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.619902610778809 \tStep Time:  0.006980419158935547 s \tTotal Time:  5.1816246509552 s \n",
      "\n",
      "\n",
      "\tEpisode 829 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4061],\n",
      "        [1.0000, 0.3414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2456],\n",
      "        [0.9999, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471881747245789 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.186611175537109 s \n",
      "\n",
      "\n",
      "\tEpisode 830 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5257],\n",
      "        [1.0000, 0.2563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5582],\n",
      "        [0.9999, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429951190948486 \tStep Time:  0.006982326507568359 s \tTotal Time:  5.193593502044678 s \n",
      "\n",
      "\n",
      "\tEpisode 831 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5856],\n",
      "        [0.9999, 0.5504]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5092],\n",
      "        [1.0000, 0.6113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599344253540039 \tStep Time:  0.004985809326171875 s \tTotal Time:  5.19857931137085 s \n",
      "\n",
      "\n",
      "\tEpisode 832 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4997],\n",
      "        [0.9999, 0.5727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7242],\n",
      "        [1.0000, 0.3532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.696489334106445 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.204563140869141 s \n",
      "\n",
      "\n",
      "\tEpisode 833 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5047],\n",
      "        [1.0000, 0.3028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4857],\n",
      "        [1.0000, 0.6599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.352390885353088 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.210547685623169 s \n",
      "\n",
      "\n",
      "\tEpisode 834 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5895],\n",
      "        [1.0000, 0.4245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5926],\n",
      "        [1.0000, 0.6431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591838359832764 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.216531276702881 s \n",
      "\n",
      "\n",
      "\tEpisode 835 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5984],\n",
      "        [0.9999, 0.5859]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4219],\n",
      "        [0.9999, 0.5378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466447830200195 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.22151780128479 s \n",
      "\n",
      "\n",
      "\tEpisode 836 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6200],\n",
      "        [1.0000, 0.4712]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6182],\n",
      "        [0.9999, 0.5913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592140197753906 \tStep Time:  0.005984783172607422 s \tTotal Time:  5.2275025844573975 s \n",
      "\n",
      "\n",
      "\tEpisode 837 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4380],\n",
      "        [0.9999, 0.5845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5840],\n",
      "        [0.9999, 0.5813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45701676607132 \tStep Time:  0.005983114242553711 s \tTotal Time:  5.233485698699951 s \n",
      "\n",
      "\n",
      "\tEpisode 838 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5905],\n",
      "        [1.0000, 0.5790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3813],\n",
      "        [0.9999, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466718971729279 \tStep Time:  0.004987001419067383 s \tTotal Time:  5.2384727001190186 s \n",
      "\n",
      "\n",
      "\tEpisode 839 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5678],\n",
      "        [1.0000, 0.5792]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5564],\n",
      "        [1.0000, 0.5563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547348976135254 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.24445652961731 s \n",
      "\n",
      "\n",
      "\tEpisode 840 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5522],\n",
      "        [1.0000, 0.5573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5468],\n",
      "        [1.0000, 0.5405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523333609104156 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.25044059753418 s \n",
      "\n",
      "\n",
      "\tEpisode 841 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4473],\n",
      "        [1.0000, 0.5426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5252],\n",
      "        [0.9999, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46601289510727 \tStep Time:  0.00698089599609375 s \tTotal Time:  5.257421493530273 s \n",
      "\n",
      "\n",
      "\tEpisode 842 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5304],\n",
      "        [0.9999, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5252],\n",
      "        [0.9999, 0.5219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508325099945068 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.263406038284302 s \n",
      "\n",
      "\n",
      "\tEpisode 843 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5154],\n",
      "        [0.9999, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5182],\n",
      "        [0.9999, 0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50781536102295 \tStep Time:  0.007978200912475586 s \tTotal Time:  5.271384239196777 s \n",
      "\n",
      "\n",
      "\tEpisode 844 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4862],\n",
      "        [0.9999, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4034],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452759802341461 \tStep Time:  0.00797891616821289 s \tTotal Time:  5.27936315536499 s \n",
      "\n",
      "\n",
      "\tEpisode 845 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4070],\n",
      "        [1.0000, 0.3835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3059],\n",
      "        [0.9999, 0.4372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.613046407699585 \tStep Time:  0.0069811344146728516 s \tTotal Time:  5.286344289779663 s \n",
      "\n",
      "\n",
      "\tEpisode 846 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [0.9999, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5291],\n",
      "        [0.9999, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524795532226562 \tStep Time:  0.006979942321777344 s \tTotal Time:  5.29332423210144 s \n",
      "\n",
      "\n",
      "\tEpisode 847 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5297],\n",
      "        [1.0000, 0.5487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4347],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468870282173157 \tStep Time:  0.006984233856201172 s \tTotal Time:  5.300308465957642 s \n",
      "\n",
      "\n",
      "\tEpisode 848 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5294],\n",
      "        [0.9999, 0.5279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4795],\n",
      "        [0.9999, 0.4778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503131866455078 \tStep Time:  0.006980180740356445 s \tTotal Time:  5.307288646697998 s \n",
      "\n",
      "\n",
      "\tEpisode 849 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4088],\n",
      "        [1.0000, 0.3945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5110],\n",
      "        [0.9999, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52082872390747 \tStep Time:  0.005982637405395508 s \tTotal Time:  5.3132712841033936 s \n",
      "\n",
      "\n",
      "\tEpisode 850 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3710],\n",
      "        [1.0000, 0.6027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4298],\n",
      "        [0.9999, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.690478324890137 \tStep Time:  0.004986286163330078 s \tTotal Time:  5.318257570266724 s \n",
      "\n",
      "\n",
      "\tEpisode 851 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5036],\n",
      "        [0.9999, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4312],\n",
      "        [0.9999, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483130991458893 \tStep Time:  0.006982326507568359 s \tTotal Time:  5.325239896774292 s \n",
      "\n",
      "\n",
      "\tEpisode 852 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5245],\n",
      "        [0.9999, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5117],\n",
      "        [1.0000, 0.2978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.633221626281738 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.331223726272583 s \n",
      "\n",
      "\n",
      "\tEpisode 853 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5190],\n",
      "        [0.9999, 0.5095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5128],\n",
      "        [0.9999, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512995719909668 \tStep Time:  0.006016731262207031 s \tTotal Time:  5.33724045753479 s \n",
      "\n",
      "\n",
      "\tEpisode 854 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4713],\n",
      "        [1.0000, 0.4025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4974],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550678491592407 \tStep Time:  0.006980419158935547 s \tTotal Time:  5.344220876693726 s \n",
      "\n",
      "\n",
      "\tEpisode 855 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5039],\n",
      "        [1.0000, 0.4978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509401321411133 \tStep Time:  0.0059854984283447266 s \tTotal Time:  5.35020637512207 s \n",
      "\n",
      "\n",
      "\tEpisode 856 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4986],\n",
      "        [0.9999, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4761],\n",
      "        [1.0000, 0.4939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50071096420288 \tStep Time:  0.005951642990112305 s \tTotal Time:  5.356158018112183 s \n",
      "\n",
      "\n",
      "\tEpisode 857 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4986],\n",
      "        [0.9999, 0.5268]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.4815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52991008758545 \tStep Time:  0.005986928939819336 s \tTotal Time:  5.362144947052002 s \n",
      "\n",
      "\n",
      "\tEpisode 858 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5269],\n",
      "        [0.9999, 0.5310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5310],\n",
      "        [0.9999, 0.5291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504280805587769 \tStep Time:  0.004989147186279297 s \tTotal Time:  5.367134094238281 s \n",
      "\n",
      "\n",
      "\tEpisode 859 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5312],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5241],\n",
      "        [0.9999, 0.5372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507644534111023 \tStep Time:  0.005960702896118164 s \tTotal Time:  5.374126672744751 s \n",
      "\n",
      "\n",
      "\tEpisode 860 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [0.9999, 0.5382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5433],\n",
      "        [0.9999, 0.5421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513404905796051 \tStep Time:  0.005453824996948242 s \tTotal Time:  5.379580497741699 s \n",
      "\n",
      "\n",
      "\tEpisode 861 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5336],\n",
      "        [1.0000, 0.5375]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5267],\n",
      "        [0.9999, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508088111877441 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.38556432723999 s \n",
      "\n",
      "\n",
      "\tEpisode 862 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5505],\n",
      "        [1.0000, 0.5495]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5347],\n",
      "        [0.9999, 0.5257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52615737915039 \tStep Time:  0.00598597526550293 s \tTotal Time:  5.391550302505493 s \n",
      "\n",
      "\n",
      "\tEpisode 863 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5202],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5361],\n",
      "        [1.0000, 0.5314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474017143249512 \tStep Time:  0.005981922149658203 s \tTotal Time:  5.397532224655151 s \n",
      "\n",
      "\n",
      "\tEpisode 864 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4990],\n",
      "        [0.9999, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5555],\n",
      "        [1.0000, 0.5506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506925106048584 \tStep Time:  0.004991054534912109 s \tTotal Time:  5.4025232791900635 s \n",
      "\n",
      "\n",
      "\tEpisode 865 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5296],\n",
      "        [0.9999, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.4761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448236465454102 \tStep Time:  0.006976604461669922 s \tTotal Time:  5.409499883651733 s \n",
      "\n",
      "\n",
      "\tEpisode 866 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5918],\n",
      "        [0.9999, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4938],\n",
      "        [1.0000, 0.6357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520899295806885 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.414486408233643 s \n",
      "\n",
      "\n",
      "\tEpisode 867 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4389],\n",
      "        [0.9999, 0.5619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4267],\n",
      "        [1.0000, 0.6107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.360297679901123 \tStep Time:  0.005986690521240234 s \tTotal Time:  5.420473098754883 s \n",
      "\n",
      "\n",
      "\tEpisode 868 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5719],\n",
      "        [0.9999, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4907],\n",
      "        [1.0000, 0.3930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.596134662628174 \tStep Time:  0.0069468021392822266 s \tTotal Time:  5.427419900894165 s \n",
      "\n",
      "\n",
      "\tEpisode 869 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5372],\n",
      "        [1.0000, 0.4138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5170],\n",
      "        [0.9999, 0.5479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451634883880615 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.432406663894653 s \n",
      "\n",
      "\n",
      "\tEpisode 870 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5948],\n",
      "        [1.0000, 0.6171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6522],\n",
      "        [1.0000, 0.6166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504522323608398 \tStep Time:  0.006016731262207031 s \tTotal Time:  5.43842339515686 s \n",
      "\n",
      "\n",
      "\tEpisode 871 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3776],\n",
      "        [0.9999, 0.5892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5489],\n",
      "        [0.9999, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550589561462402 \tStep Time:  0.0059854984283447266 s \tTotal Time:  5.444408893585205 s \n",
      "\n",
      "\n",
      "\tEpisode 872 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3992],\n",
      "        [1.0000, 0.6386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3259],\n",
      "        [0.9999, 0.5333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.759304523468018 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.450392484664917 s \n",
      "\n",
      "\n",
      "\tEpisode 873 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4686],\n",
      "        [1.0000, 0.3583]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5575],\n",
      "        [0.9999, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548651695251465 \tStep Time:  0.005982875823974609 s \tTotal Time:  5.456375360488892 s \n",
      "\n",
      "\n",
      "\tEpisode 874 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6435],\n",
      "        [1.0000, 0.3677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4203],\n",
      "        [1.0000, 0.3936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442591190338135 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.4623589515686035 s \n",
      "\n",
      "\n",
      "\tEpisode 875 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4862],\n",
      "        [1.0000, 0.3971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5987],\n",
      "        [0.9999, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.630075931549072 \tStep Time:  0.005988121032714844 s \tTotal Time:  5.468347072601318 s \n",
      "\n",
      "\n",
      "\tEpisode 876 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4585],\n",
      "        [1.0000, 0.4236]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6025],\n",
      "        [0.9999, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.590517401695251 \tStep Time:  0.004951000213623047 s \tTotal Time:  5.473298072814941 s \n",
      "\n",
      "\n",
      "\tEpisode 877 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5015],\n",
      "        [0.9999, 0.4834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6288],\n",
      "        [1.0000, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483933448791504 \tStep Time:  0.0070133209228515625 s \tTotal Time:  5.480311393737793 s \n",
      "\n",
      "\n",
      "\tEpisode 878 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5700],\n",
      "        [0.9999, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4894],\n",
      "        [0.9999, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559091091156006 \tStep Time:  0.005952596664428711 s \tTotal Time:  5.486263990402222 s \n",
      "\n",
      "\n",
      "\tEpisode 879 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4654],\n",
      "        [1.0000, 0.5376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4524],\n",
      "        [0.9999, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572145342826843 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.492247819900513 s \n",
      "\n",
      "\n",
      "\tEpisode 880 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4556],\n",
      "        [1.0000, 0.4568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4834],\n",
      "        [1.0000, 0.4586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498672008514404 \tStep Time:  0.0059850215911865234 s \tTotal Time:  5.498232841491699 s \n",
      "\n",
      "\n",
      "\tEpisode 881 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4886],\n",
      "        [0.9999, 0.4706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4827],\n",
      "        [1.0000, 0.4725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506131649017334 \tStep Time:  0.006980419158935547 s \tTotal Time:  5.505213260650635 s \n",
      "\n",
      "\n",
      "\tEpisode 882 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4914],\n",
      "        [0.9999, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4948],\n",
      "        [1.0000, 0.4706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492250919342041 \tStep Time:  0.004986286163330078 s \tTotal Time:  5.511197090148926 s \n",
      "\n",
      "\n",
      "\tEpisode 883 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4668],\n",
      "        [0.9999, 0.4821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5074],\n",
      "        [0.9999, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533107280731201 \tStep Time:  0.006982088088989258 s \tTotal Time:  5.518179178237915 s \n",
      "\n",
      "\n",
      "\tEpisode 884 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4889],\n",
      "        [0.9999, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4872],\n",
      "        [1.0000, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494840621948242 \tStep Time:  0.006981849670410156 s \tTotal Time:  5.525161027908325 s \n",
      "\n",
      "\n",
      "\tEpisode 885 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4787],\n",
      "        [1.0000, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5098],\n",
      "        [0.9999, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477963328361511 \tStep Time:  0.005983114242553711 s \tTotal Time:  5.531144142150879 s \n",
      "\n",
      "\n",
      "\tEpisode 886 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5192],\n",
      "        [0.9999, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5184],\n",
      "        [1.0000, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492189407348633 \tStep Time:  0.006981372833251953 s \tTotal Time:  5.538125514984131 s \n",
      "\n",
      "\n",
      "\tEpisode 887 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5018],\n",
      "        [0.9999, 0.5284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5075],\n",
      "        [0.9999, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488458633422852 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.544109344482422 s \n",
      "\n",
      "\n",
      "\tEpisode 888 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5106],\n",
      "        [1.0000, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4601],\n",
      "        [0.9999, 0.4976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509531497955322 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.55009388923645 s \n",
      "\n",
      "\n",
      "\tEpisode 889 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5029],\n",
      "        [1.0000, 0.4506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4866],\n",
      "        [0.9999, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520492553710938 \tStep Time:  0.00698089599609375 s \tTotal Time:  5.557074785232544 s \n",
      "\n",
      "\n",
      "\tEpisode 890 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4446],\n",
      "        [0.9999, 0.4977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5685],\n",
      "        [1.0000, 0.4447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545553386211395 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.562061309814453 s \n",
      "\n",
      "\n",
      "\tEpisode 891 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5882],\n",
      "        [0.9999, 0.5132]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4380],\n",
      "        [0.9999, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506101608276367 \tStep Time:  0.005984783172607422 s \tTotal Time:  5.5680460929870605 s \n",
      "\n",
      "\n",
      "\tEpisode 892 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4657],\n",
      "        [1.0000, 0.4451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4566],\n",
      "        [0.9999, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520652770996094 \tStep Time:  0.0059833526611328125 s \tTotal Time:  5.574029445648193 s \n",
      "\n",
      "\n",
      "\tEpisode 893 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4817],\n",
      "        [0.9999, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4624],\n",
      "        [1.0000, 0.4663]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473062515258789 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.5800135135650635 s \n",
      "\n",
      "\n",
      "\tEpisode 894 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5330],\n",
      "        [1.0000, 0.6930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5366],\n",
      "        [0.9999, 0.4717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41096830368042 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.585000276565552 s \n",
      "\n",
      "\n",
      "\tEpisode 895 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4787],\n",
      "        [1.0000, 0.4746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6905],\n",
      "        [0.9999, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.614354729652405 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.590984106063843 s \n",
      "\n",
      "\n",
      "\tEpisode 896 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5240],\n",
      "        [0.9999, 0.5800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4915],\n",
      "        [0.9999, 0.4865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57352066040039 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.596968173980713 s \n",
      "\n",
      "\n",
      "\tEpisode 897 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5051],\n",
      "        [1.0000, 0.6446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5127],\n",
      "        [0.9999, 0.4921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583249628543854 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.601954698562622 s \n",
      "\n",
      "\n",
      "\tEpisode 898 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5160],\n",
      "        [0.9999, 0.4741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4845],\n",
      "        [1.0000, 0.5574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5371732711792 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.607938766479492 s \n",
      "\n",
      "\n",
      "\tEpisode 899 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4838],\n",
      "        [0.9999, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4905],\n",
      "        [0.9999, 0.5250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476653099060059 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.613922834396362 s \n",
      "\n",
      "\n",
      "\tEpisode 900 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4928],\n",
      "        [0.9999, 0.4904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [0.9999, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524576663970947 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.6189093589782715 s \n",
      "\n",
      "\n",
      "\tEpisode 901 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5983],\n",
      "        [0.9999, 0.5286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4561],\n",
      "        [1.0000, 0.4671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482396602630615 \tStep Time:  0.006981372833251953 s \tTotal Time:  5.625890731811523 s \n",
      "\n",
      "\n",
      "\tEpisode 902 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4604],\n",
      "        [1.0000, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4510],\n",
      "        [0.9999, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484535157680511 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.630877494812012 s \n",
      "\n",
      "\n",
      "\tEpisode 903 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4866],\n",
      "        [0.9999, 0.4649]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5556],\n",
      "        [1.0000, 0.4439]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465738594532013 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.636861324310303 s \n",
      "\n",
      "\n",
      "\tEpisode 904 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4870],\n",
      "        [1.0000, 0.5975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4907],\n",
      "        [0.9999, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467415809631348 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.642845392227173 s \n",
      "\n",
      "\n",
      "\tEpisode 905 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4941],\n",
      "        [0.9999, 0.4950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4872],\n",
      "        [1.0000, 0.5736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472262859344482 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.647832155227661 s \n",
      "\n",
      "\n",
      "\tEpisode 906 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6595],\n",
      "        [0.9999, 0.5275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4066],\n",
      "        [0.9999, 0.5365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.403759479522705 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.653815984725952 s \n",
      "\n",
      "\n",
      "\tEpisode 907 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4445],\n",
      "        [0.9999, 0.4590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3890],\n",
      "        [0.9999, 0.5411]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.431802988052368 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.659800291061401 s \n",
      "\n",
      "\n",
      "\tEpisode 908 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5204],\n",
      "        [0.9999, 0.4501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4801],\n",
      "        [0.9999, 0.4819]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470869719982147 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.665784120559692 s \n",
      "\n",
      "\n",
      "\tEpisode 909 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6080],\n",
      "        [1.0000, 0.3234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7066],\n",
      "        [0.9999, 0.4058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563025951385498 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.670770645141602 s \n",
      "\n",
      "\n",
      "\tEpisode 910 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4542],\n",
      "        [1.0000, 0.3531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6062],\n",
      "        [0.9999, 0.5676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.605157732963562 \tStep Time:  0.005984783172607422 s \tTotal Time:  5.677752733230591 s \n",
      "\n",
      "\n",
      "\tEpisode 911 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5438],\n",
      "        [0.9999, 0.5699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5144],\n",
      "        [0.9999, 0.5437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51595002412796 \tStep Time:  0.006980419158935547 s \tTotal Time:  5.684733152389526 s \n",
      "\n",
      "\n",
      "\tEpisode 912 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5925],\n",
      "        [0.9999, 0.4141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6759],\n",
      "        [0.9999, 0.6165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419848442077637 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.690717697143555 s \n",
      "\n",
      "\n",
      "\tEpisode 913 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4255],\n",
      "        [1.0000, 0.3813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2835],\n",
      "        [0.9999, 0.6066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.693867683410645 \tStep Time:  0.006981611251831055 s \tTotal Time:  5.697699308395386 s \n",
      "\n",
      "\n",
      "\tEpisode 914 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4180],\n",
      "        [0.9999, 0.4141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6142],\n",
      "        [0.9999, 0.6270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531191349029541 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.703682899475098 s \n",
      "\n",
      "\n",
      "\tEpisode 915 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4651],\n",
      "        [0.9999, 0.5170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3571],\n",
      "        [0.9999, 0.5108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597506046295166 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.709667205810547 s \n",
      "\n",
      "\n",
      "\tEpisode 916 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7607],\n",
      "        [0.9999, 0.6091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5268],\n",
      "        [1.0000, 0.8133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.404218196868896 \tStep Time:  0.0069811344146728516 s \tTotal Time:  5.71664834022522 s \n",
      "\n",
      "\n",
      "\tEpisode 917 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3916],\n",
      "        [0.9999, 0.5688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3361],\n",
      "        [1.0000, 0.6344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.319740772247314 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.722632884979248 s \n",
      "\n",
      "\n",
      "\tEpisode 918 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7308],\n",
      "        [1.0000, 0.2541]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4642],\n",
      "        [0.9999, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.375958919525146 \tStep Time:  0.006982326507568359 s \tTotal Time:  5.729615211486816 s \n",
      "\n",
      "\n",
      "\tEpisode 919 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6063],\n",
      "        [1.0000, 0.6784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2636],\n",
      "        [0.9999, 0.3971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.286262035369873 \tStep Time:  0.005982398986816406 s \tTotal Time:  5.735597610473633 s \n",
      "\n",
      "\n",
      "\tEpisode 920 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2967],\n",
      "        [0.9999, 0.4212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7348],\n",
      "        [1.0000, 0.6705]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.695191383361816 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.741581678390503 s \n",
      "\n",
      "\n",
      "\tEpisode 921 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6295],\n",
      "        [1.0000, 0.2571]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4023],\n",
      "        [0.9999, 0.5768]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.705278873443604 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.747565984725952 s \n",
      "\n",
      "\n",
      "\tEpisode 922 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3916],\n",
      "        [0.9999, 0.5033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2535],\n",
      "        [1.0000, 0.6490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.373296856880188 \tStep Time:  0.0069806575775146484 s \tTotal Time:  5.754546642303467 s \n",
      "\n",
      "\n",
      "\tEpisode 923 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5143],\n",
      "        [1.0000, 0.2888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5936],\n",
      "        [0.9999, 0.4869]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429869651794434 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.760530948638916 s \n",
      "\n",
      "\n",
      "\tEpisode 924 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3263],\n",
      "        [1.0000, 0.8155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4705],\n",
      "        [0.9999, 0.6123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.70794153213501 \tStep Time:  0.005984783172607422 s \tTotal Time:  5.766515731811523 s \n",
      "\n",
      "\n",
      "\tEpisode 925 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3662],\n",
      "        [1.0000, 0.7822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5063],\n",
      "        [0.9999, 0.4608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534642696380615 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.7724995613098145 s \n",
      "\n",
      "\n",
      "\tEpisode 926 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5323],\n",
      "        [0.9999, 0.4628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.3715],\n",
      "        [1.0000, 0.2953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569523334503174 \tStep Time:  0.005983114242553711 s \tTotal Time:  5.778482675552368 s \n",
      "\n",
      "\n",
      "\tEpisode 927 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2562],\n",
      "        [1.0000, 0.3460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3116],\n",
      "        [0.9999, 0.3825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6784086227417 \tStep Time:  0.0059854984283447266 s \tTotal Time:  5.784468173980713 s \n",
      "\n",
      "\n",
      "\tEpisode 928 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5552],\n",
      "        [1.0000, 0.7699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2774],\n",
      "        [1.0000, 0.3109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563774347305298 \tStep Time:  0.005982875823974609 s \tTotal Time:  5.7904510498046875 s \n",
      "\n",
      "\n",
      "\tEpisode 929 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4781],\n",
      "        [0.9999, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2672],\n",
      "        [0.9999, 0.6152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.380420207977295 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.796435356140137 s \n",
      "\n",
      "\n",
      "\tEpisode 930 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7132],\n",
      "        [0.9999, 0.6096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6715],\n",
      "        [1.0000, 0.6512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.609916687011719 \tStep Time:  0.005984783172607422 s \tTotal Time:  5.802420139312744 s \n",
      "\n",
      "\n",
      "\tEpisode 931 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4637],\n",
      "        [1.0000, 0.6566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3327],\n",
      "        [0.9999, 0.4423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.729777336120605 \tStep Time:  0.005982875823974609 s \tTotal Time:  5.808403015136719 s \n",
      "\n",
      "\n",
      "\tEpisode 932 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6624],\n",
      "        [1.0000, 0.3772]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5659],\n",
      "        [1.0000, 0.3915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522961139678955 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.814386606216431 s \n",
      "\n",
      "\n",
      "\tEpisode 933 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4779],\n",
      "        [1.0000, 0.3502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4494],\n",
      "        [0.9999, 0.5461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510959148406982 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.820370674133301 s \n",
      "\n",
      "\n",
      "\tEpisode 934 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3621],\n",
      "        [0.9999, 0.5897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5412],\n",
      "        [0.9999, 0.5622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476643085479736 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.82635498046875 s \n",
      "\n",
      "\n",
      "\tEpisode 935 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7996],\n",
      "        [1.0000, 0.3718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3550],\n",
      "        [0.9999, 0.4569]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447293817996979 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.832338571548462 s \n",
      "\n",
      "\n",
      "\tEpisode 936 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3633],\n",
      "        [0.9999, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3631],\n",
      "        [1.0000, 0.3254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498336791992188 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.83832311630249 s \n",
      "\n",
      "\n",
      "\tEpisode 937 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5834],\n",
      "        [1.0000, 0.3471]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3724],\n",
      "        [1.0000, 0.4151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.693240642547607 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.844306707382202 s \n",
      "\n",
      "\n",
      "\tEpisode 938 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4457],\n",
      "        [0.9999, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4665],\n",
      "        [0.9999, 0.4855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552524089813232 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.8502912521362305 s \n",
      "\n",
      "\n",
      "\tEpisode 939 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5937],\n",
      "        [0.9999, 0.4809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5374],\n",
      "        [0.9999, 0.4499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595295906066895 \tStep Time:  0.0069887638092041016 s \tTotal Time:  5.857280015945435 s \n",
      "\n",
      "\n",
      "\tEpisode 940 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4313],\n",
      "        [0.9999, 0.4545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4788],\n",
      "        [0.9999, 0.5359]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547958850860596 \tStep Time:  0.006972074508666992 s \tTotal Time:  5.864252090454102 s \n",
      "\n",
      "\n",
      "\tEpisode 941 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4471],\n",
      "        [1.0000, 0.4401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5358],\n",
      "        [1.0000, 0.4511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479974269866943 \tStep Time:  0.004986763000488281 s \tTotal Time:  5.86923885345459 s \n",
      "\n",
      "\n",
      "\tEpisode 942 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5330],\n",
      "        [1.0000, 0.4525]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5285],\n",
      "        [0.9999, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558823585510254 \tStep Time:  0.006982088088989258 s \tTotal Time:  5.876220941543579 s \n",
      "\n",
      "\n",
      "\tEpisode 943 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6713],\n",
      "        [1.0000, 0.4671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5661],\n",
      "        [1.0000, 0.4673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.377687931060791 \tStep Time:  0.004985809326171875 s \tTotal Time:  5.881206750869751 s \n",
      "\n",
      "\n",
      "\tEpisode 944 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5850],\n",
      "        [1.0000, 0.4642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6191],\n",
      "        [1.0000, 0.6611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.651989459991455 \tStep Time:  0.006982326507568359 s \tTotal Time:  5.888189077377319 s \n",
      "\n",
      "\n",
      "\tEpisode 945 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [0.9999, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6236],\n",
      "        [0.9999, 0.5998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529380679130554 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.8941731452941895 s \n",
      "\n",
      "\n",
      "\tEpisode 946 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6268],\n",
      "        [1.0000, 0.6703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5637],\n",
      "        [0.9999, 0.5185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535133361816406 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.900157451629639 s \n",
      "\n",
      "\n",
      "\tEpisode 947 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5456],\n",
      "        [0.9999, 0.5180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5450],\n",
      "        [1.0000, 0.4745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461204826831818 \tStep Time:  0.00598454475402832 s \tTotal Time:  5.906141996383667 s \n",
      "\n",
      "\n",
      "\tEpisode 948 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4630],\n",
      "        [0.9999, 0.5282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6949],\n",
      "        [0.9999, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481300234794617 \tStep Time:  0.00698089599609375 s \tTotal Time:  5.913122892379761 s \n",
      "\n",
      "\n",
      "\tEpisode 949 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5043],\n",
      "        [1.0000, 0.4540]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6514],\n",
      "        [1.0000, 0.6231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523690223693848 \tStep Time:  0.00498652458190918 s \tTotal Time:  5.91810941696167 s \n",
      "\n",
      "\n",
      "\tEpisode 950 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4369],\n",
      "        [1.0000, 0.4280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4180],\n",
      "        [1.0000, 0.4207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52742338180542 \tStep Time:  0.005999326705932617 s \tTotal Time:  5.9241087436676025 s \n",
      "\n",
      "\n",
      "\tEpisode 951 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4531],\n",
      "        [0.9999, 0.4710]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5214],\n",
      "        [1.0000, 0.4085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453146696090698 \tStep Time:  0.007963180541992188 s \tTotal Time:  5.932071924209595 s \n",
      "\n",
      "\n",
      "\tEpisode 952 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4705],\n",
      "        [0.9999, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3494],\n",
      "        [0.9999, 0.4559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554800033569336 \tStep Time:  0.004987001419067383 s \tTotal Time:  5.937058925628662 s \n",
      "\n",
      "\n",
      "\tEpisode 953 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4834],\n",
      "        [0.9999, 0.5354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4936],\n",
      "        [1.0000, 0.6449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573040008544922 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.943042755126953 s \n",
      "\n",
      "\n",
      "\tEpisode 954 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4048],\n",
      "        [0.9999, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5448],\n",
      "        [1.0000, 0.6196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447075963020325 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.949026823043823 s \n",
      "\n",
      "\n",
      "\tEpisode 955 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4222],\n",
      "        [0.9999, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4793],\n",
      "        [1.0000, 0.3811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535233497619629 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.955010890960693 s \n",
      "\n",
      "\n",
      "\tEpisode 956 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4556],\n",
      "        [0.9999, 0.5562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5753],\n",
      "        [0.9999, 0.4330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517104148864746 \tStep Time:  0.005983829498291016 s \tTotal Time:  5.960994720458984 s \n",
      "\n",
      "\n",
      "\tEpisode 957 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5226],\n",
      "        [0.9999, 0.5530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3947],\n",
      "        [0.9999, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.619767665863037 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.9669787883758545 s \n",
      "\n",
      "\n",
      "\tEpisode 958 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4823],\n",
      "        [0.9999, 0.4733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4596],\n",
      "        [0.9999, 0.5439]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561517715454102 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.972963094711304 s \n",
      "\n",
      "\n",
      "\tEpisode 959 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5528],\n",
      "        [1.0000, 0.3981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4982],\n",
      "        [0.9999, 0.4536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4161057472229 \tStep Time:  0.0069811344146728516 s \tTotal Time:  5.979944229125977 s \n",
      "\n",
      "\n",
      "\tEpisode 960 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4181],\n",
      "        [0.9999, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4437],\n",
      "        [0.9999, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532510757446289 \tStep Time:  0.005984306335449219 s \tTotal Time:  5.985928535461426 s \n",
      "\n",
      "\n",
      "\tEpisode 961 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4895],\n",
      "        [0.9999, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5122],\n",
      "        [1.0000, 0.4592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56365168094635 \tStep Time:  0.005984067916870117 s \tTotal Time:  5.991912603378296 s \n",
      "\n",
      "\n",
      "\tEpisode 962 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5657],\n",
      "        [0.9999, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4937],\n",
      "        [1.0000, 0.5611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438828766345978 \tStep Time:  0.005983591079711914 s \tTotal Time:  5.997896194458008 s \n",
      "\n",
      "\n",
      "\tEpisode 963 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5228],\n",
      "        [1.0000, 0.4543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5594],\n",
      "        [1.0000, 0.4582]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496241569519043 \tStep Time:  0.006981372833251953 s \tTotal Time:  6.00487756729126 s \n",
      "\n",
      "\n",
      "\tEpisode 964 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5532],\n",
      "        [0.9999, 0.4921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5335],\n",
      "        [0.9999, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561398148536682 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.010861873626709 s \n",
      "\n",
      "\n",
      "\tEpisode 965 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4786],\n",
      "        [0.9999, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4722],\n",
      "        [1.0000, 0.4714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521679878234863 \tStep Time:  0.004986286163330078 s \tTotal Time:  6.015848159790039 s \n",
      "\n",
      "\n",
      "\tEpisode 966 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5080],\n",
      "        [0.9999, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5252],\n",
      "        [0.9999, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52298879623413 \tStep Time:  0.006982326507568359 s \tTotal Time:  6.022830486297607 s \n",
      "\n",
      "\n",
      "\tEpisode 967 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5037],\n",
      "        [0.9999, 0.4993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5062],\n",
      "        [0.9999, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509921550750732 \tStep Time:  0.005982875823974609 s \tTotal Time:  6.028813362121582 s \n",
      "\n",
      "\n",
      "\tEpisode 968 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5050],\n",
      "        [1.0000, 0.5530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486056804656982 \tStep Time:  0.0059850215911865234 s \tTotal Time:  6.0347983837127686 s \n",
      "\n",
      "\n",
      "\tEpisode 969 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5178],\n",
      "        [1.0000, 0.5531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [0.9999, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552645206451416 \tStep Time:  0.0069811344146728516 s \tTotal Time:  6.041779518127441 s \n",
      "\n",
      "\n",
      "\tEpisode 970 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4912],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495104312896729 \tStep Time:  0.005983114242553711 s \tTotal Time:  6.047762632369995 s \n",
      "\n",
      "\n",
      "\tEpisode 971 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4864],\n",
      "        [0.9999, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4867],\n",
      "        [1.0000, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503504753112793 \tStep Time:  0.005986213684082031 s \tTotal Time:  6.053748846054077 s \n",
      "\n",
      "\n",
      "\tEpisode 972 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4836],\n",
      "        [1.0000, 0.5319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4289],\n",
      "        [0.9999, 0.4975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551121711730957 \tStep Time:  0.007976531982421875 s \tTotal Time:  6.061725378036499 s \n",
      "\n",
      "\n",
      "\tEpisode 973 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4502],\n",
      "        [0.9999, 0.5014]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [0.9999, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464022159576416 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.067709684371948 s \n",
      "\n",
      "\n",
      "\tEpisode 974 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5739],\n",
      "        [0.9999, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5161],\n",
      "        [1.0000, 0.5936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508980751037598 \tStep Time:  0.0059854984283447266 s \tTotal Time:  6.073695182800293 s \n",
      "\n",
      "\n",
      "\tEpisode 975 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4656],\n",
      "        [1.0000, 0.4286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5005],\n",
      "        [1.0000, 0.4323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522894859313965 \tStep Time:  0.006979703903198242 s \tTotal Time:  6.080674886703491 s \n",
      "\n",
      "\n",
      "\tEpisode 976 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4388],\n",
      "        [1.0000, 0.5675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5043],\n",
      "        [0.9999, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580676078796387 \tStep Time:  0.00498652458190918 s \tTotal Time:  6.0856614112854 s \n",
      "\n",
      "\n",
      "\tEpisode 977 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4884],\n",
      "        [1.0000, 0.4511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5321],\n",
      "        [0.9999, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467403411865234 \tStep Time:  0.007978439331054688 s \tTotal Time:  6.093639850616455 s \n",
      "\n",
      "\n",
      "\tEpisode 978 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4520],\n",
      "        [1.0000, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5112],\n",
      "        [0.9999, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501278400421143 \tStep Time:  0.005983114242553711 s \tTotal Time:  6.099622964859009 s \n",
      "\n",
      "\n",
      "\tEpisode 979 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4908],\n",
      "        [0.9999, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5069],\n",
      "        [1.0000, 0.4655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503339290618896 \tStep Time:  0.0069828033447265625 s \tTotal Time:  6.106605768203735 s \n",
      "\n",
      "\n",
      "\tEpisode 980 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4664],\n",
      "        [0.9999, 0.5109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4996],\n",
      "        [1.0000, 0.4605]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498266696929932 \tStep Time:  0.007977008819580078 s \tTotal Time:  6.114582777023315 s \n",
      "\n",
      "\n",
      "\tEpisode 981 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6606],\n",
      "        [0.9999, 0.5281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4530],\n",
      "        [1.0000, 0.5683]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435037612915039 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.1205668449401855 s \n",
      "\n",
      "\n",
      "\tEpisode 982 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5065],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5683],\n",
      "        [0.9999, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446974754333496 \tStep Time:  0.006981849670410156 s \tTotal Time:  6.127548694610596 s \n",
      "\n",
      "\n",
      "\tEpisode 983 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5021],\n",
      "        [1.0000, 0.4591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4882],\n",
      "        [1.0000, 0.4541]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466131091117859 \tStep Time:  0.00698089599609375 s \tTotal Time:  6.1345295906066895 s \n",
      "\n",
      "\n",
      "\tEpisode 984 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6306],\n",
      "        [0.9999, 0.5479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4547],\n",
      "        [1.0000, 0.3989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.680169105529785 \tStep Time:  0.004987239837646484 s \tTotal Time:  6.139516830444336 s \n",
      "\n",
      "\n",
      "\tEpisode 985 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4555],\n",
      "        [0.9999, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5687],\n",
      "        [0.9999, 0.5714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491506457328796 \tStep Time:  0.0070188045501708984 s \tTotal Time:  6.146535634994507 s \n",
      "\n",
      "\n",
      "\tEpisode 986 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5630],\n",
      "        [0.9999, 0.5428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7296],\n",
      "        [1.0000, 0.4269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.679225444793701 \tStep Time:  0.0059795379638671875 s \tTotal Time:  6.152515172958374 s \n",
      "\n",
      "\n",
      "\tEpisode 987 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5434],\n",
      "        [1.0000, 0.6427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4418],\n",
      "        [1.0000, 0.6100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558986604213715 \tStep Time:  0.0049915313720703125 s \tTotal Time:  6.157506704330444 s \n",
      "\n",
      "\n",
      "\tEpisode 988 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4799],\n",
      "        [1.0000, 0.3733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5393],\n",
      "        [1.0000, 0.3810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498996496200562 \tStep Time:  0.006974220275878906 s \tTotal Time:  6.164480924606323 s \n",
      "\n",
      "\n",
      "\tEpisode 989 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5932],\n",
      "        [1.0000, 0.4171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5030],\n",
      "        [0.9999, 0.4614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.407597541809082 \tStep Time:  0.00498652458190918 s \tTotal Time:  6.169467449188232 s \n",
      "\n",
      "\n",
      "\tEpisode 990 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5289],\n",
      "        [1.0000, 0.3773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4140],\n",
      "        [0.9999, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.646273612976074 \tStep Time:  0.006951808929443359 s \tTotal Time:  6.176419258117676 s \n",
      "\n",
      "\n",
      "\tEpisode 991 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6191],\n",
      "        [0.9999, 0.4555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4276],\n",
      "        [0.9999, 0.4815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.410500049591064 \tStep Time:  0.006979703903198242 s \tTotal Time:  6.183398962020874 s \n",
      "\n",
      "\n",
      "\tEpisode 992 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4530],\n",
      "        [1.0000, 0.4330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4233],\n",
      "        [0.9999, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536152839660645 \tStep Time:  0.005995750427246094 s \tTotal Time:  6.18939471244812 s \n",
      "\n",
      "\n",
      "\tEpisode 993 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4948],\n",
      "        [0.9999, 0.4835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5764],\n",
      "        [0.9999, 0.4887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465692281723022 \tStep Time:  0.006982326507568359 s \tTotal Time:  6.1963770389556885 s \n",
      "\n",
      "\n",
      "\tEpisode 994 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4948],\n",
      "        [0.9999, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4509],\n",
      "        [1.0000, 0.5309]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465155363082886 \tStep Time:  0.006005048751831055 s \tTotal Time:  6.2023820877075195 s \n",
      "\n",
      "\n",
      "\tEpisode 995 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6349],\n",
      "        [0.9999, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4916],\n",
      "        [1.0000, 0.5808]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54085111618042 \tStep Time:  0.005985736846923828 s \tTotal Time:  6.208367824554443 s \n",
      "\n",
      "\n",
      "\tEpisode 996 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4679],\n",
      "        [1.0000, 0.6033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5346],\n",
      "        [1.0000, 0.4431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.629956543445587 \tStep Time:  0.005948781967163086 s \tTotal Time:  6.2143166065216064 s \n",
      "\n",
      "\n",
      "\tEpisode 997 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5265],\n",
      "        [0.9999, 0.4887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4548],\n",
      "        [1.0000, 0.5226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561192154884338 \tStep Time:  0.007014274597167969 s \tTotal Time:  6.221330881118774 s \n",
      "\n",
      "\n",
      "\tEpisode 998 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4885],\n",
      "        [0.9999, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504087924957275 \tStep Time:  0.0069484710693359375 s \tTotal Time:  6.22827935218811 s \n",
      "\n",
      "\n",
      "\tEpisode 999 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4918],\n",
      "        [1.0000, 0.4837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4932],\n",
      "        [0.9999, 0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513742446899414 \tStep Time:  0.007978439331054688 s \tTotal Time:  6.236257791519165 s \n",
      "\n",
      "\n",
      "\tEpisode 1000 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4895],\n",
      "        [1.0000, 0.4973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4815354347229 \tStep Time:  0.00698089599609375 s \tTotal Time:  6.243238687515259 s \n",
      "\n",
      "\n",
      "\tEpisode 1001 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5143],\n",
      "        [0.9999, 0.4829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.4739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509772896766663 \tStep Time:  0.004987001419067383 s \tTotal Time:  6.248225688934326 s \n",
      "\n",
      "\n",
      "\tEpisode 1002 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4603],\n",
      "        [0.9999, 0.4724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [0.9999, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507112503051758 \tStep Time:  0.0069811344146728516 s \tTotal Time:  6.255206823348999 s \n",
      "\n",
      "\n",
      "\tEpisode 1003 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5584],\n",
      "        [0.9999, 0.4976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4431],\n",
      "        [0.9999, 0.4448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477866172790527 \tStep Time:  0.0069811344146728516 s \tTotal Time:  6.262187957763672 s \n",
      "\n",
      "\n",
      "\tEpisode 1004 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4913],\n",
      "        [1.0000, 0.5534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5147],\n",
      "        [0.9999, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544338703155518 \tStep Time:  0.00598454475402832 s \tTotal Time:  6.2681725025177 s \n",
      "\n",
      "\n",
      "\tEpisode 1005 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5087],\n",
      "        [0.9999, 0.4573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6040],\n",
      "        [0.9999, 0.5181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582011759281158 \tStep Time:  0.006981372833251953 s \tTotal Time:  6.275153875350952 s \n",
      "\n",
      "\n",
      "\tEpisode 1006 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4168],\n",
      "        [0.9999, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4132],\n",
      "        [0.9999, 0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510905623435974 \tStep Time:  0.0069806575775146484 s \tTotal Time:  6.282134532928467 s \n",
      "\n",
      "\n",
      "\tEpisode 1007 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5597],\n",
      "        [1.0000, 0.5737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4443],\n",
      "        [1.0000, 0.5452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467317581176758 \tStep Time:  0.005986213684082031 s \tTotal Time:  6.288120746612549 s \n",
      "\n",
      "\n",
      "\tEpisode 1008 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3904],\n",
      "        [1.0000, 0.3936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4358],\n",
      "        [0.9999, 0.5193]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47544527053833 \tStep Time:  0.007977962493896484 s \tTotal Time:  6.296098709106445 s \n",
      "\n",
      "\n",
      "\tEpisode 1009 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4498],\n",
      "        [0.9999, 0.4566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5411],\n",
      "        [1.0000, 0.5695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519612669944763 \tStep Time:  0.00498652458190918 s \tTotal Time:  6.3010852336883545 s \n",
      "\n",
      "\n",
      "\tEpisode 1010 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4957],\n",
      "        [0.9999, 0.4373]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4651],\n",
      "        [0.9999, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53536593914032 \tStep Time:  0.008976221084594727 s \tTotal Time:  6.310061454772949 s \n",
      "\n",
      "\n",
      "\tEpisode 1011 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5568],\n",
      "        [1.0000, 0.3920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5261],\n",
      "        [1.0000, 0.5722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577317595481873 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.316045522689819 s \n",
      "\n",
      "\n",
      "\tEpisode 1012 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4960],\n",
      "        [0.9999, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5656],\n",
      "        [1.0000, 0.5775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495876789093018 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.32202935218811 s \n",
      "\n",
      "\n",
      "\tEpisode 1013 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5628],\n",
      "        [1.0000, 0.4384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5475],\n",
      "        [1.0000, 0.5501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45076036453247 \tStep Time:  0.008974552154541016 s \tTotal Time:  6.331003904342651 s \n",
      "\n",
      "\n",
      "\tEpisode 1014 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5495],\n",
      "        [0.9999, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6224],\n",
      "        [1.0000, 0.6019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592280864715576 \tStep Time:  0.00498652458190918 s \tTotal Time:  6.3359904289245605 s \n",
      "\n",
      "\n",
      "\tEpisode 1015 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4381],\n",
      "        [0.9999, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5105],\n",
      "        [0.9999, 0.5252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468575954437256 \tStep Time:  0.006982088088989258 s \tTotal Time:  6.34297251701355 s \n",
      "\n",
      "\n",
      "\tEpisode 1016 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4989],\n",
      "        [1.0000, 0.4291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4319],\n",
      "        [1.0000, 0.5438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426164507865906 \tStep Time:  0.0069806575775146484 s \tTotal Time:  6.3499531745910645 s \n",
      "\n",
      "\n",
      "\tEpisode 1017 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5730],\n",
      "        [0.9999, 0.5233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4563],\n",
      "        [0.9999, 0.5337]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52708888053894 \tStep Time:  0.004986763000488281 s \tTotal Time:  6.354939937591553 s \n",
      "\n",
      "\n",
      "\tEpisode 1018 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6061],\n",
      "        [1.0000, 0.3985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5492],\n",
      "        [1.0000, 0.4059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.703356266021729 \tStep Time:  0.006981372833251953 s \tTotal Time:  6.361921310424805 s \n",
      "\n",
      "\n",
      "\tEpisode 1019 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5275],\n",
      "        [0.9999, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4905],\n",
      "        [0.9999, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52151733636856 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.367905378341675 s \n",
      "\n",
      "\n",
      "\tEpisode 1020 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4961],\n",
      "        [1.0000, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4540],\n",
      "        [0.9999, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527568340301514 \tStep Time:  0.0059854984283447266 s \tTotal Time:  6.3738908767700195 s \n",
      "\n",
      "\n",
      "\tEpisode 1021 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4766],\n",
      "        [0.9999, 0.4973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5010],\n",
      "        [0.9999, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517965316772461 \tStep Time:  0.00797724723815918 s \tTotal Time:  6.381868124008179 s \n",
      "\n",
      "\n",
      "\tEpisode 1022 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5089],\n",
      "        [1.0000, 0.4845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5545],\n",
      "        [1.0000, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532984733581543 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.38785195350647 s \n",
      "\n",
      "\n",
      "\tEpisode 1023 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5003],\n",
      "        [1.0000, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4758],\n",
      "        [0.9999, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497617721557617 \tStep Time:  0.005984783172607422 s \tTotal Time:  6.393836736679077 s \n",
      "\n",
      "\n",
      "\tEpisode 1024 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4833],\n",
      "        [1.0000, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4857],\n",
      "        [1.0000, 0.4772]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50404167175293 \tStep Time:  0.0069806575775146484 s \tTotal Time:  6.400817394256592 s \n",
      "\n",
      "\n",
      "\tEpisode 1025 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6192],\n",
      "        [0.9999, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [0.9999, 0.4976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450693130493164 \tStep Time:  0.00498652458190918 s \tTotal Time:  6.405803918838501 s \n",
      "\n",
      "\n",
      "\tEpisode 1026 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4004],\n",
      "        [0.9999, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5015],\n",
      "        [1.0000, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54997205734253 \tStep Time:  0.006982326507568359 s \tTotal Time:  6.412786245346069 s \n",
      "\n",
      "\n",
      "\tEpisode 1027 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4622],\n",
      "        [0.9999, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4889],\n",
      "        [0.9999, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492793560028076 \tStep Time:  0.006980180740356445 s \tTotal Time:  6.419766426086426 s \n",
      "\n",
      "\n",
      "\tEpisode 1028 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4989],\n",
      "        [0.9999, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4775],\n",
      "        [1.0000, 0.4538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492865562438965 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.425750732421875 s \n",
      "\n",
      "\n",
      "\tEpisode 1029 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4935],\n",
      "        [1.0000, 0.5829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [0.9999, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558281362056732 \tStep Time:  0.006982088088989258 s \tTotal Time:  6.432732820510864 s \n",
      "\n",
      "\n",
      "\tEpisode 1030 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4908],\n",
      "        [1.0000, 0.5455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.4535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501782417297363 \tStep Time:  0.005983114242553711 s \tTotal Time:  6.438715934753418 s \n",
      "\n",
      "\n",
      "\tEpisode 1031 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4945],\n",
      "        [1.0000, 0.4809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4942],\n",
      "        [1.0000, 0.4767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50739049911499 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.444700002670288 s \n",
      "\n",
      "\n",
      "\tEpisode 1032 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4801],\n",
      "        [1.0000, 0.4217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4696],\n",
      "        [0.9999, 0.4787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5394287109375 \tStep Time:  0.00598454475402832 s \tTotal Time:  6.450684547424316 s \n",
      "\n",
      "\n",
      "\tEpisode 1033 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [0.9999, 0.4797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4891],\n",
      "        [0.9999, 0.4870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500786304473877 \tStep Time:  0.006983280181884766 s \tTotal Time:  6.457667827606201 s \n",
      "\n",
      "\n",
      "\tEpisode 1034 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4801],\n",
      "        [0.9999, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5120],\n",
      "        [1.0000, 0.5033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494822978973389 \tStep Time:  0.0059814453125 s \tTotal Time:  6.463649272918701 s \n",
      "\n",
      "\n",
      "\tEpisode 1035 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4482],\n",
      "        [1.0000, 0.4478]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5193],\n",
      "        [0.9999, 0.5141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505885601043701 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.469633340835571 s \n",
      "\n",
      "\n",
      "\tEpisode 1036 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4700],\n",
      "        [0.9999, 0.4761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5410],\n",
      "        [1.0000, 0.4430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453910768032074 \tStep Time:  0.006981372833251953 s \tTotal Time:  6.476614713668823 s \n",
      "\n",
      "\n",
      "\tEpisode 1037 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5505],\n",
      "        [1.0000, 0.4272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4552],\n",
      "        [1.0000, 0.5617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5087229013443 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.482598543167114 s \n",
      "\n",
      "\n",
      "\tEpisode 1038 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5546],\n",
      "        [0.9999, 0.5541]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5642],\n",
      "        [1.0000, 0.4164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58581829071045 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.4885828495025635 s \n",
      "\n",
      "\n",
      "\tEpisode 1039 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4034],\n",
      "        [1.0000, 0.6054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4590],\n",
      "        [0.9999, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437137961387634 \tStep Time:  0.0069828033447265625 s \tTotal Time:  6.49556565284729 s \n",
      "\n",
      "\n",
      "\tEpisode 1040 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5676],\n",
      "        [0.9999, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5716],\n",
      "        [0.9999, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587052822113037 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.501549959182739 s \n",
      "\n",
      "\n",
      "\tEpisode 1041 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4518],\n",
      "        [1.0000, 0.6675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5752],\n",
      "        [0.9999, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.605282008647919 \tStep Time:  0.00800013542175293 s \tTotal Time:  6.509550094604492 s \n",
      "\n",
      "\n",
      "\tEpisode 1042 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5780],\n",
      "        [1.0000, 0.5748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5397],\n",
      "        [0.9999, 0.5546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510194301605225 \tStep Time:  0.00795602798461914 s \tTotal Time:  6.517506122589111 s \n",
      "\n",
      "\n",
      "\tEpisode 1043 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3877],\n",
      "        [1.0000, 0.5970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6134],\n",
      "        [0.9999, 0.4581]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563274383544922 \tStep Time:  0.009008407592773438 s \tTotal Time:  6.526514530181885 s \n",
      "\n",
      "\n",
      "\tEpisode 1044 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4026],\n",
      "        [1.0000, 0.4055]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5470],\n",
      "        [0.9999, 0.5167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504556655883789 \tStep Time:  0.00598454475402832 s \tTotal Time:  6.532499074935913 s \n",
      "\n",
      "\n",
      "\tEpisode 1045 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5467],\n",
      "        [1.0000, 0.4104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4248],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512587547302246 \tStep Time:  0.004989147186279297 s \tTotal Time:  6.537488222122192 s \n",
      "\n",
      "\n",
      "\tEpisode 1046 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4312],\n",
      "        [0.9999, 0.4531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5256],\n",
      "        [0.9999, 0.5043]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52949434518814 \tStep Time:  0.006976604461669922 s \tTotal Time:  6.544464826583862 s \n",
      "\n",
      "\n",
      "\tEpisode 1047 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4358],\n",
      "        [0.9999, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5036],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479732990264893 \tStep Time:  0.004986763000488281 s \tTotal Time:  6.549451589584351 s \n",
      "\n",
      "\n",
      "\tEpisode 1048 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5029],\n",
      "        [0.9999, 0.5117]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4862],\n",
      "        [1.0000, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519990921020508 \tStep Time:  0.0059528350830078125 s \tTotal Time:  6.555404424667358 s \n",
      "\n",
      "\n",
      "\tEpisode 1049 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4784],\n",
      "        [0.9999, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4864],\n",
      "        [1.0000, 0.4846]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506669998168945 \tStep Time:  0.007978439331054688 s \tTotal Time:  6.563382863998413 s \n",
      "\n",
      "\n",
      "\tEpisode 1050 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4740],\n",
      "        [0.9999, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [0.9999, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526514530181885 \tStep Time:  0.004987001419067383 s \tTotal Time:  6.5683698654174805 s \n",
      "\n",
      "\n",
      "\tEpisode 1051 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4890],\n",
      "        [0.9999, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4916],\n",
      "        [0.9999, 0.4993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506284236907959 \tStep Time:  0.0069849491119384766 s \tTotal Time:  6.575354814529419 s \n",
      "\n",
      "\n",
      "\tEpisode 1052 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4011],\n",
      "        [1.0000, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4314],\n",
      "        [1.0000, 0.4767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508026599884033 \tStep Time:  0.0060117244720458984 s \tTotal Time:  6.581366539001465 s \n",
      "\n",
      "\n",
      "\tEpisode 1053 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5948],\n",
      "        [1.0000, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5010],\n",
      "        [0.9999, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447718620300293 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.587350368499756 s \n",
      "\n",
      "\n",
      "\tEpisode 1054 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4170],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4774],\n",
      "        [1.0000, 0.4611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479368686676025 \tStep Time:  0.006949901580810547 s \tTotal Time:  6.594300270080566 s \n",
      "\n",
      "\n",
      "\tEpisode 1055 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5478],\n",
      "        [1.0000, 0.4546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4020],\n",
      "        [1.0000, 0.3477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53989315032959 \tStep Time:  0.00598454475402832 s \tTotal Time:  6.600284814834595 s \n",
      "\n",
      "\n",
      "\tEpisode 1056 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4768],\n",
      "        [0.9999, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5223],\n",
      "        [1.0000, 0.4605]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499766826629639 \tStep Time:  0.005024433135986328 s \tTotal Time:  6.605309247970581 s \n",
      "\n",
      "\n",
      "\tEpisode 1057 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5473],\n",
      "        [0.9999, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4741],\n",
      "        [1.0000, 0.6483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418113470077515 \tStep Time:  0.0069768428802490234 s \tTotal Time:  6.61228609085083 s \n",
      "\n",
      "\n",
      "\tEpisode 1058 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5179],\n",
      "        [0.9999, 0.5357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3878],\n",
      "        [0.9999, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435145854949951 \tStep Time:  0.005981922149658203 s \tTotal Time:  6.618268013000488 s \n",
      "\n",
      "\n",
      "\tEpisode 1059 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4362],\n",
      "        [1.0000, 0.5584]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5648],\n",
      "        [1.0000, 0.4581]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530503273010254 \tStep Time:  0.0059528350830078125 s \tTotal Time:  6.624220848083496 s \n",
      "\n",
      "\n",
      "\tEpisode 1060 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5403],\n",
      "        [1.0000, 0.6033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6057],\n",
      "        [1.0000, 0.3635]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.373612880706787 \tStep Time:  0.006016254425048828 s \tTotal Time:  6.630237102508545 s \n",
      "\n",
      "\n",
      "\tEpisode 1061 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4166],\n",
      "        [1.0000, 0.7443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5418],\n",
      "        [0.9999, 0.5622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.714471817016602 \tStep Time:  0.005984783172607422 s \tTotal Time:  6.636221885681152 s \n",
      "\n",
      "\n",
      "\tEpisode 1062 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4397],\n",
      "        [0.9999, 0.5319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4792],\n",
      "        [1.0000, 0.4154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558318614959717 \tStep Time:  0.0059816837310791016 s \tTotal Time:  6.6422035694122314 s \n",
      "\n",
      "\n",
      "\tEpisode 1063 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5190],\n",
      "        [0.9999, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4818],\n",
      "        [0.9999, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542776584625244 \tStep Time:  0.005983591079711914 s \tTotal Time:  6.648187160491943 s \n",
      "\n",
      "\n",
      "\tEpisode 1064 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5878],\n",
      "        [1.0000, 0.5798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4894],\n",
      "        [1.0000, 0.4155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561089038848877 \tStep Time:  0.005993366241455078 s \tTotal Time:  6.654180526733398 s \n",
      "\n",
      "\n",
      "\tEpisode 1065 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5198],\n",
      "        [1.0000, 0.3717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5173],\n",
      "        [0.9999, 0.5313]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449819445610046 \tStep Time:  0.005977153778076172 s \tTotal Time:  6.660157680511475 s \n",
      "\n",
      "\n",
      "\tEpisode 1066 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4918],\n",
      "        [1.0000, 0.5674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2885],\n",
      "        [1.0000, 0.4500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.380055904388428 \tStep Time:  0.0059816837310791016 s \tTotal Time:  6.666139364242554 s \n",
      "\n",
      "\n",
      "\tEpisode 1067 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5515],\n",
      "        [0.9999, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5698],\n",
      "        [0.9999, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53444766998291 \tStep Time:  0.004988908767700195 s \tTotal Time:  6.671128273010254 s \n",
      "\n",
      "\n",
      "\tEpisode 1068 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4776],\n",
      "        [0.9999, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5139],\n",
      "        [0.9999, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541260719299316 \tStep Time:  0.005950212478637695 s \tTotal Time:  6.677078485488892 s \n",
      "\n",
      "\n",
      "\tEpisode 1069 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5884],\n",
      "        [0.9999, 0.5331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4324],\n",
      "        [0.9999, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.622405529022217 \tStep Time:  0.005985736846923828 s \tTotal Time:  6.684093475341797 s \n",
      "\n",
      "\n",
      "\tEpisode 1070 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4443],\n",
      "        [0.9999, 0.5247]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5092],\n",
      "        [0.9999, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488373756408691 \tStep Time:  0.004986763000488281 s \tTotal Time:  6.689080238342285 s \n",
      "\n",
      "\n",
      "\tEpisode 1071 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4136],\n",
      "        [1.0000, 0.4444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4712],\n",
      "        [0.9999, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455140590667725 \tStep Time:  0.005985260009765625 s \tTotal Time:  6.695065498352051 s \n",
      "\n",
      "\n",
      "\tEpisode 1072 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5297],\n",
      "        [1.0000, 0.5515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4606],\n",
      "        [1.0000, 0.6728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439353942871094 \tStep Time:  0.006948232650756836 s \tTotal Time:  6.702013731002808 s \n",
      "\n",
      "\n",
      "\tEpisode 1073 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4681],\n",
      "        [0.9999, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5292],\n",
      "        [1.0000, 0.5405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508109092712402 \tStep Time:  0.0059833526611328125 s \tTotal Time:  6.70799708366394 s \n",
      "\n",
      "\n",
      "\tEpisode 1074 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4907],\n",
      "        [0.9999, 0.5351]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7139],\n",
      "        [1.0000, 0.5510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432664453983307 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.71398138999939 s \n",
      "\n",
      "\n",
      "\tEpisode 1075 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7015],\n",
      "        [0.9999, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5657],\n",
      "        [1.0000, 0.6100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46734368801117 \tStep Time:  0.006981372833251953 s \tTotal Time:  6.720962762832642 s \n",
      "\n",
      "\n",
      "\tEpisode 1076 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4860],\n",
      "        [1.0000, 0.5897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6687],\n",
      "        [1.0000, 0.3580]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.332723140716553 \tStep Time:  0.006981372833251953 s \tTotal Time:  6.7279441356658936 s \n",
      "\n",
      "\n",
      "\tEpisode 1077 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6238],\n",
      "        [1.0000, 0.2855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5090],\n",
      "        [1.0000, 0.6334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.319524347782135 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.733927965164185 s \n",
      "\n",
      "\n",
      "\tEpisode 1078 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4836],\n",
      "        [0.9999, 0.4643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5904],\n",
      "        [1.0000, 0.6260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53286361694336 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.739912033081055 s \n",
      "\n",
      "\n",
      "\tEpisode 1079 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7409],\n",
      "        [1.0000, 0.3260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3410],\n",
      "        [0.9999, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.850212097167969 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.745895862579346 s \n",
      "\n",
      "\n",
      "\tEpisode 1080 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4991],\n",
      "        [0.9999, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5017],\n",
      "        [0.9999, 0.4802]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545839309692383 \tStep Time:  0.004986763000488281 s \tTotal Time:  6.750882625579834 s \n",
      "\n",
      "\n",
      "\tEpisode 1081 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4410],\n",
      "        [0.9999, 0.4656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4278],\n",
      "        [0.9999, 0.5197]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.589120864868164 \tStep Time:  0.0059909820556640625 s \tTotal Time:  6.756873607635498 s \n",
      "\n",
      "\n",
      "\tEpisode 1082 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5368],\n",
      "        [1.0000, 0.6727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4307],\n",
      "        [1.0000, 0.6472]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517200946807861 \tStep Time:  0.005977153778076172 s \tTotal Time:  6.762850761413574 s \n",
      "\n",
      "\n",
      "\tEpisode 1083 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4681],\n",
      "        [1.0000, 0.3996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4572],\n",
      "        [1.0000, 0.4034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537283420562744 \tStep Time:  0.004986763000488281 s \tTotal Time:  6.7678375244140625 s \n",
      "\n",
      "\n",
      "\tEpisode 1084 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5577],\n",
      "        [1.0000, 0.3722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5332],\n",
      "        [1.0000, 0.5515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440598487854004 \tStep Time:  0.006982326507568359 s \tTotal Time:  6.774819850921631 s \n",
      "\n",
      "\n",
      "\tEpisode 1085 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2845],\n",
      "        [1.0000, 0.3590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4613],\n",
      "        [0.9999, 0.4624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597463130950928 \tStep Time:  0.005982875823974609 s \tTotal Time:  6.7808027267456055 s \n",
      "\n",
      "\n",
      "\tEpisode 1086 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3939],\n",
      "        [0.9999, 0.4641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5277],\n",
      "        [1.0000, 0.6106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.619346022605896 \tStep Time:  0.004986763000488281 s \tTotal Time:  6.785789489746094 s \n",
      "\n",
      "\n",
      "\tEpisode 1087 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4693],\n",
      "        [0.9999, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3752],\n",
      "        [1.0000, 0.6720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59410810470581 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.791773796081543 s \n",
      "\n",
      "\n",
      "\tEpisode 1088 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5396],\n",
      "        [1.0000, 0.3143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9445],\n",
      "        [0.9999, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.352539539337158 \tStep Time:  0.004986286163330078 s \tTotal Time:  6.796760082244873 s \n",
      "\n",
      "\n",
      "\tEpisode 1089 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4938],\n",
      "        [1.0000, 0.5451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5881],\n",
      "        [0.9999, 0.4548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485279440879822 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.802743911743164 s \n",
      "\n",
      "\n",
      "\tEpisode 1090 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6538],\n",
      "        [1.0000, 0.4036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1444],\n",
      "        [0.9999, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.694782733917236 \tStep Time:  0.0059850215911865234 s \tTotal Time:  6.808728933334351 s \n",
      "\n",
      "\n",
      "\tEpisode 1091 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5129],\n",
      "        [0.9999, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4818],\n",
      "        [1.0000, 0.5655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49198293685913 \tStep Time:  0.005982875823974609 s \tTotal Time:  6.814711809158325 s \n",
      "\n",
      "\n",
      "\tEpisode 1092 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6120],\n",
      "        [1.0000, 0.3829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4303],\n",
      "        [1.0000, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.644271969795227 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.820696115493774 s \n",
      "\n",
      "\n",
      "\tEpisode 1093 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4727],\n",
      "        [1.0000, 0.5965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4714],\n",
      "        [0.9999, 0.4747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.621933937072754 \tStep Time:  0.00598454475402832 s \tTotal Time:  6.826680660247803 s \n",
      "\n",
      "\n",
      "\tEpisode 1094 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3869],\n",
      "        [0.9999, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6224],\n",
      "        [1.0000, 0.4178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463731288909912 \tStep Time:  0.005983114242553711 s \tTotal Time:  6.8326637744903564 s \n",
      "\n",
      "\n",
      "\tEpisode 1095 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4822],\n",
      "        [0.9999, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4953],\n",
      "        [0.9999, 0.4853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528568625450134 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.838648080825806 s \n",
      "\n",
      "\n",
      "\tEpisode 1096 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4001],\n",
      "        [1.0000, 0.4411]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6291],\n",
      "        [0.9999, 0.4705]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447142601013184 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.844632387161255 s \n",
      "\n",
      "\n",
      "\tEpisode 1097 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5056],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4968],\n",
      "        [0.9999, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504790306091309 \tStep Time:  0.004986286163330078 s \tTotal Time:  6.849618673324585 s \n",
      "\n",
      "\n",
      "\tEpisode 1098 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6061],\n",
      "        [1.0000, 0.3407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4722],\n",
      "        [0.9999, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539809703826904 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.855602741241455 s \n",
      "\n",
      "\n",
      "\tEpisode 1099 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4729],\n",
      "        [0.9999, 0.4745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4119],\n",
      "        [0.9999, 0.4677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565803050994873 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.861586570739746 s \n",
      "\n",
      "\n",
      "\tEpisode 1100 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5016],\n",
      "        [0.9999, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4412],\n",
      "        [1.0000, 0.3003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478718340396881 \tStep Time:  0.004986763000488281 s \tTotal Time:  6.866573333740234 s \n",
      "\n",
      "\n",
      "\tEpisode 1101 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4557],\n",
      "        [0.9999, 0.4750]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3848],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464961647987366 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.8725574016571045 s \n",
      "\n",
      "\n",
      "\tEpisode 1102 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4755],\n",
      "        [1.0000, 0.3741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3202],\n",
      "        [0.9999, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554237842559814 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.878541707992554 s \n",
      "\n",
      "\n",
      "\tEpisode 1103 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4879],\n",
      "        [1.0000, 0.2121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6197],\n",
      "        [0.9999, 0.5210]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.74659013748169 \tStep Time:  0.005983591079711914 s \tTotal Time:  6.884525299072266 s \n",
      "\n",
      "\n",
      "\tEpisode 1104 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4663],\n",
      "        [1.0000, 0.6145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4198],\n",
      "        [0.9999, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59823751449585 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.890509605407715 s \n",
      "\n",
      "\n",
      "\tEpisode 1105 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5055],\n",
      "        [0.9999, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5150],\n",
      "        [0.9999, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587899208068848 \tStep Time:  0.004986286163330078 s \tTotal Time:  6.895495891571045 s \n",
      "\n",
      "\n",
      "\tEpisode 1106 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4478],\n",
      "        [0.9999, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5128],\n",
      "        [1.0000, 0.5689]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463629245758057 \tStep Time:  0.006981372833251953 s \tTotal Time:  6.902477264404297 s \n",
      "\n",
      "\n",
      "\tEpisode 1107 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5077],\n",
      "        [1.0000, 0.4543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5111],\n",
      "        [1.0000, 0.4996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501967430114746 \tStep Time:  0.005984067916870117 s \tTotal Time:  6.908461332321167 s \n",
      "\n",
      "\n",
      "\tEpisode 1108 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4798],\n",
      "        [1.0000, 0.5927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5309],\n",
      "        [1.0000, 0.3655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463772773742676 \tStep Time:  0.0059850215911865234 s \tTotal Time:  6.9144463539123535 s \n",
      "\n",
      "\n",
      "\tEpisode 1109 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5228],\n",
      "        [1.0000, 0.4572]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5245],\n",
      "        [0.9999, 0.5299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545917510986328 \tStep Time:  0.0069806575775146484 s \tTotal Time:  6.921427011489868 s \n",
      "\n",
      "\n",
      "\tEpisode 1110 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5227],\n",
      "        [0.9999, 0.5166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5268],\n",
      "        [1.0000, 0.4812]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530988991260529 \tStep Time:  0.005983591079711914 s \tTotal Time:  6.92741060256958 s \n",
      "\n",
      "\n",
      "\tEpisode 1111 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5016],\n",
      "        [0.9999, 0.5347]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4128],\n",
      "        [1.0000, 0.5751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.430215418338776 \tStep Time:  0.006983518600463867 s \tTotal Time:  6.934394121170044 s \n",
      "\n",
      "\n",
      "\tEpisode 1112 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4206],\n",
      "        [1.0000, 0.5856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5284],\n",
      "        [1.0000, 0.5556]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.616124629974365 \tStep Time:  0.005982160568237305 s \tTotal Time:  6.940376281738281 s \n",
      "\n",
      "\n",
      "\tEpisode 1113 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4076],\n",
      "        [1.0000, 0.2622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5285],\n",
      "        [0.9999, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61369413137436 \tStep Time:  0.005983829498291016 s \tTotal Time:  6.946360111236572 s \n",
      "\n",
      "\n",
      "\tEpisode 1114 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5271],\n",
      "        [1.0000, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4366],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540863037109375 \tStep Time:  0.005985736846923828 s \tTotal Time:  6.952345848083496 s \n",
      "\n",
      "\n",
      "\tEpisode 1115 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5067],\n",
      "        [0.9999, 0.5308]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5242],\n",
      "        [0.9999, 0.5263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499958038330078 \tStep Time:  0.0069806575775146484 s \tTotal Time:  6.959326505661011 s \n",
      "\n",
      "\n",
      "\tEpisode 1116 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5148],\n",
      "        [0.9999, 0.5251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5024],\n",
      "        [1.0000, 0.5410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534432590007782 \tStep Time:  0.005982875823974609 s \tTotal Time:  6.965309381484985 s \n",
      "\n",
      "\n",
      "\tEpisode 1117 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5247],\n",
      "        [0.9999, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5216],\n",
      "        [1.0000, 0.4416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4712393283844 \tStep Time:  0.005984783172607422 s \tTotal Time:  6.971294164657593 s \n",
      "\n",
      "\n",
      "\tEpisode 1118 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5254],\n",
      "        [0.9999, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5324],\n",
      "        [1.0000, 0.5249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51395559310913 \tStep Time:  0.0069806575775146484 s \tTotal Time:  6.978274822235107 s \n",
      "\n",
      "\n",
      "\tEpisode 1119 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5389],\n",
      "        [0.9999, 0.5426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5588],\n",
      "        [1.0000, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498906552791595 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.984259128570557 s \n",
      "\n",
      "\n",
      "\tEpisode 1120 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5378],\n",
      "        [1.0000, 0.5720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5358],\n",
      "        [0.9999, 0.5236]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51977789402008 \tStep Time:  0.004986286163330078 s \tTotal Time:  6.989245414733887 s \n",
      "\n",
      "\n",
      "\tEpisode 1121 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5218],\n",
      "        [0.9999, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5124],\n",
      "        [1.0000, 0.4534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547054290771484 \tStep Time:  0.005984306335449219 s \tTotal Time:  6.995229721069336 s \n",
      "\n",
      "\n",
      "\tEpisode 1122 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5097],\n",
      "        [1.0000, 0.4778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5092],\n",
      "        [1.0000, 0.4980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514859676361084 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.001213788986206 s \n",
      "\n",
      "\n",
      "\tEpisode 1123 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4933],\n",
      "        [1.0000, 0.4957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5471],\n",
      "        [0.9999, 0.5045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487838745117188 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.008194923400879 s \n",
      "\n",
      "\n",
      "\tEpisode 1124 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5022],\n",
      "        [1.0000, 0.4498]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [0.9999, 0.5261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545271873474121 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.01417875289917 s \n",
      "\n",
      "\n",
      "\tEpisode 1125 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5300],\n",
      "        [0.9999, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5308],\n",
      "        [1.0000, 0.5230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48869001865387 \tStep Time:  0.004986763000488281 s \tTotal Time:  7.019165515899658 s \n",
      "\n",
      "\n",
      "\tEpisode 1126 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4922],\n",
      "        [1.0000, 0.5183]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [0.9999, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514643669128418 \tStep Time:  0.0069828033447265625 s \tTotal Time:  7.026148319244385 s \n",
      "\n",
      "\n",
      "\tEpisode 1127 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5135],\n",
      "        [1.0000, 0.4793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5158],\n",
      "        [0.9999, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515910387039185 \tStep Time:  0.005982160568237305 s \tTotal Time:  7.032130479812622 s \n",
      "\n",
      "\n",
      "\tEpisode 1128 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5028],\n",
      "        [1.0000, 0.5539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5215],\n",
      "        [0.9999, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529809951782227 \tStep Time:  0.004986763000488281 s \tTotal Time:  7.03711724281311 s \n",
      "\n",
      "\n",
      "\tEpisode 1129 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4900],\n",
      "        [0.9999, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4973],\n",
      "        [1.0000, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486001312732697 \tStep Time:  0.00598454475402832 s \tTotal Time:  7.043101787567139 s \n",
      "\n",
      "\n",
      "\tEpisode 1130 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5159],\n",
      "        [1.0000, 0.4463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5335],\n",
      "        [1.0000, 0.5363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453605651855469 \tStep Time:  0.005992412567138672 s \tTotal Time:  7.049094200134277 s \n",
      "\n",
      "\n",
      "\tEpisode 1131 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4593],\n",
      "        [1.0000, 0.4177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4698],\n",
      "        [0.9999, 0.5303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455825805664062 \tStep Time:  0.00498652458190918 s \tTotal Time:  7.0540807247161865 s \n",
      "\n",
      "\n",
      "\tEpisode 1132 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5046],\n",
      "        [1.0000, 0.4048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5641],\n",
      "        [1.0000, 0.4109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.640008926391602 \tStep Time:  0.0069887638092041016 s \tTotal Time:  7.061069488525391 s \n",
      "\n",
      "\n",
      "\tEpisode 1133 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3911],\n",
      "        [0.9999, 0.4238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3872],\n",
      "        [0.9999, 0.4319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525473475456238 \tStep Time:  0.004984140396118164 s \tTotal Time:  7.066053628921509 s \n",
      "\n",
      "\n",
      "\tEpisode 1134 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4251],\n",
      "        [0.9999, 0.5564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6029],\n",
      "        [1.0000, 0.4013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51746940612793 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.072037696838379 s \n",
      "\n",
      "\n",
      "\tEpisode 1135 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4230],\n",
      "        [1.0000, 0.6109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4027],\n",
      "        [0.9999, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484949588775635 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.078021764755249 s \n",
      "\n",
      "\n",
      "\tEpisode 1136 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3742],\n",
      "        [0.9999, 0.4275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5844],\n",
      "        [0.9999, 0.4669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48853349685669 \tStep Time:  0.005983591079711914 s \tTotal Time:  7.084005355834961 s \n",
      "\n",
      "\n",
      "\tEpisode 1137 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5118],\n",
      "        [0.9999, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4148],\n",
      "        [0.9999, 0.4811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44638442993164 \tStep Time:  0.004987239837646484 s \tTotal Time:  7.088992595672607 s \n",
      "\n",
      "\n",
      "\tEpisode 1138 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6830],\n",
      "        [1.0000, 0.6454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4747],\n",
      "        [1.0000, 0.4188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55140209197998 \tStep Time:  0.006982088088989258 s \tTotal Time:  7.095974683761597 s \n",
      "\n",
      "\n",
      "\tEpisode 1139 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3736],\n",
      "        [0.9999, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6243],\n",
      "        [0.9999, 0.5645]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49341344833374 \tStep Time:  0.005982875823974609 s \tTotal Time:  7.101957559585571 s \n",
      "\n",
      "\n",
      "\tEpisode 1140 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5711],\n",
      "        [0.9999, 0.5377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6314],\n",
      "        [0.9999, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465978145599365 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.1079418659210205 s \n",
      "\n",
      "\n",
      "\tEpisode 1141 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4114],\n",
      "        [1.0000, 0.3494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6286],\n",
      "        [0.9999, 0.4761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.644046306610107 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.114923000335693 s \n",
      "\n",
      "\n",
      "\tEpisode 1142 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3539],\n",
      "        [0.9999, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6623],\n",
      "        [0.9999, 0.5797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.656483173370361 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.1209070682525635 s \n",
      "\n",
      "\n",
      "\tEpisode 1143 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4713],\n",
      "        [0.9999, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6578],\n",
      "        [0.9999, 0.5500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.654988765716553 \tStep Time:  0.006981611251831055 s \tTotal Time:  7.1278886795043945 s \n",
      "\n",
      "\n",
      "\tEpisode 1144 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6312],\n",
      "        [1.0000, 0.6289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4701],\n",
      "        [0.9999, 0.5523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495010375976562 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.134869813919067 s \n",
      "\n",
      "\n",
      "\tEpisode 1145 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4615],\n",
      "        [0.9999, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5642],\n",
      "        [1.0000, 0.5927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520277976989746 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.140854120254517 s \n",
      "\n",
      "\n",
      "\tEpisode 1146 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6142],\n",
      "        [0.9999, 0.4992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4801],\n",
      "        [1.0000, 0.5616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560294151306152 \tStep Time:  0.0059833526611328125 s \tTotal Time:  7.146837472915649 s \n",
      "\n",
      "\n",
      "\tEpisode 1147 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5715],\n",
      "        [1.0000, 0.5514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5454],\n",
      "        [1.0000, 0.4335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576173663139343 \tStep Time:  0.006981611251831055 s \tTotal Time:  7.1538190841674805 s \n",
      "\n",
      "\n",
      "\tEpisode 1148 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4675],\n",
      "        [0.9999, 0.5164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4540],\n",
      "        [0.9999, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492576122283936 \tStep Time:  0.005984783172607422 s \tTotal Time:  7.159803867340088 s \n",
      "\n",
      "\n",
      "\tEpisode 1149 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5234],\n",
      "        [1.0000, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4573],\n",
      "        [0.9999, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593302726745605 \tStep Time:  0.0059833526611328125 s \tTotal Time:  7.165787220001221 s \n",
      "\n",
      "\n",
      "\tEpisode 1150 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5098],\n",
      "        [0.9999, 0.4882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4983],\n",
      "        [0.9999, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531909942626953 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.171771287918091 s \n",
      "\n",
      "\n",
      "\tEpisode 1151 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4903],\n",
      "        [1.0000, 0.4832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5189],\n",
      "        [1.0000, 0.4882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48809814453125 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.178752422332764 s \n",
      "\n",
      "\n",
      "\tEpisode 1152 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.4340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [1.0000, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556326866149902 \tStep Time:  0.004987239837646484 s \tTotal Time:  7.18373966217041 s \n",
      "\n",
      "\n",
      "\tEpisode 1153 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5400],\n",
      "        [1.0000, 0.5170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4868],\n",
      "        [0.9999, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52721357345581 \tStep Time:  0.0059833526611328125 s \tTotal Time:  7.189723014831543 s \n",
      "\n",
      "\n",
      "\tEpisode 1154 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4903],\n",
      "        [0.9999, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4457],\n",
      "        [0.9999, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482243537902832 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.195706844329834 s \n",
      "\n",
      "\n",
      "\tEpisode 1155 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5029],\n",
      "        [0.9999, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5587],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47042179107666 \tStep Time:  0.004987001419067383 s \tTotal Time:  7.200693845748901 s \n",
      "\n",
      "\n",
      "\tEpisode 1156 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5497],\n",
      "        [1.0000, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4789],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481442630290985 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.206677675247192 s \n",
      "\n",
      "\n",
      "\tEpisode 1157 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5066],\n",
      "        [1.0000, 0.4499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3065],\n",
      "        [1.0000, 0.5638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.380461692810059 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.2126617431640625 s \n",
      "\n",
      "\n",
      "\tEpisode 1158 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5754],\n",
      "        [0.9999, 0.4694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4934],\n",
      "        [0.9999, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432987689971924 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.2186455726623535 s \n",
      "\n",
      "\n",
      "\tEpisode 1159 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5173],\n",
      "        [1.0000, 0.3741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5180],\n",
      "        [0.9999, 0.4652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56119680404663 \tStep Time:  0.005984783172607422 s \tTotal Time:  7.224630355834961 s \n",
      "\n",
      "\n",
      "\tEpisode 1160 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4606],\n",
      "        [0.9999, 0.4523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4415],\n",
      "        [0.9999, 0.5220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487271308898926 \tStep Time:  0.005983114242553711 s \tTotal Time:  7.230613470077515 s \n",
      "\n",
      "\n",
      "\tEpisode 1161 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4624],\n",
      "        [0.9999, 0.4684]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6351],\n",
      "        [0.9999, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470711708068848 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.236597537994385 s \n",
      "\n",
      "\n",
      "\tEpisode 1162 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4734],\n",
      "        [1.0000, 0.6417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8660],\n",
      "        [1.0000, 0.4329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.297501981258392 \tStep Time:  0.005984783172607422 s \tTotal Time:  7.242582321166992 s \n",
      "\n",
      "\n",
      "\tEpisode 1163 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3356],\n",
      "        [1.0000, 0.2989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2416],\n",
      "        [0.9999, 0.4693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.620140075683594 \tStep Time:  0.004985809326171875 s \tTotal Time:  7.247568130493164 s \n",
      "\n",
      "\n",
      "\tEpisode 1164 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5301],\n",
      "        [1.0000, 0.2842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4490],\n",
      "        [0.9999, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45226240158081 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.253552198410034 s \n",
      "\n",
      "\n",
      "\tEpisode 1165 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6673],\n",
      "        [1.0000, 0.3557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3583],\n",
      "        [1.0000, 0.9182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525100708007812 \tStep Time:  0.006979942321777344 s \tTotal Time:  7.2605321407318115 s \n",
      "\n",
      "\n",
      "\tEpisode 1166 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6856],\n",
      "        [1.0000, 0.6873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3010],\n",
      "        [1.0000, 0.6839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.413163185119629 \tStep Time:  0.004986763000488281 s \tTotal Time:  7.2655189037323 s \n",
      "\n",
      "\n",
      "\tEpisode 1167 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4620],\n",
      "        [0.9999, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3232],\n",
      "        [1.0000, 0.7342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.391357123851776 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.271502733230591 s \n",
      "\n",
      "\n",
      "\tEpisode 1168 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4136],\n",
      "        [1.0000, 0.1566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7133],\n",
      "        [1.0000, 0.2675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.839537620544434 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.277486801147461 s \n",
      "\n",
      "\n",
      "\tEpisode 1169 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4455],\n",
      "        [1.0000, 0.2529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6665],\n",
      "        [1.0000, 0.3751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.415874481201172 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.283470869064331 s \n",
      "\n",
      "\n",
      "\tEpisode 1170 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4596],\n",
      "        [1.0000, 0.8479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4351],\n",
      "        [0.9999, 0.5643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446296215057373 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.289454936981201 s \n",
      "\n",
      "\n",
      "\tEpisode 1171 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3740],\n",
      "        [0.9999, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5439],\n",
      "        [0.9999, 0.5372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.634635925292969 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.295438766479492 s \n",
      "\n",
      "\n",
      "\tEpisode 1172 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3946],\n",
      "        [1.0000, 0.2651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4444],\n",
      "        [1.0000, 0.2307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.609921932220459 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.301422834396362 s \n",
      "\n",
      "\n",
      "\tEpisode 1173 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6895],\n",
      "        [1.0000, 0.4058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3817],\n",
      "        [1.0000, 0.7729]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.277508914470673 \tStep Time:  0.006982564926147461 s \tTotal Time:  7.30840539932251 s \n",
      "\n",
      "\n",
      "\tEpisode 1174 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6815],\n",
      "        [0.9999, 0.5349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4295],\n",
      "        [1.0000, 0.7792]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.698846817016602 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.314389705657959 s \n",
      "\n",
      "\n",
      "\tEpisode 1175 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4280],\n",
      "        [0.9999, 0.4802]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4459],\n",
      "        [1.0000, 0.6601]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422473430633545 \tStep Time:  0.0059850215911865234 s \tTotal Time:  7.3203747272491455 s \n",
      "\n",
      "\n",
      "\tEpisode 1176 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.6073],\n",
      "        [1.0000, 0.6928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5260],\n",
      "        [1.0000, 0.6713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.685863018035889 \tStep Time:  0.006982088088989258 s \tTotal Time:  7.327356815338135 s \n",
      "\n",
      "\n",
      "\tEpisode 1177 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5387],\n",
      "        [0.9999, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6702],\n",
      "        [0.9999, 0.5269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489722490310669 \tStep Time:  0.007977008819580078 s \tTotal Time:  7.335333824157715 s \n",
      "\n",
      "\n",
      "\tEpisode 1178 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3502],\n",
      "        [1.0000, 0.2100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5115],\n",
      "        [0.9999, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.365695476531982 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.341317892074585 s \n",
      "\n",
      "\n",
      "\tEpisode 1179 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6643],\n",
      "        [1.0000, 0.6486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5581],\n",
      "        [1.0000, 0.3378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.69188642501831 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.348299026489258 s \n",
      "\n",
      "\n",
      "\tEpisode 1180 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3904],\n",
      "        [1.0000, 0.3650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5760],\n",
      "        [0.9999, 0.5618]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556204438209534 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.354283094406128 s \n",
      "\n",
      "\n",
      "\tEpisode 1181 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7828],\n",
      "        [0.9999, 0.4415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6862],\n",
      "        [1.0000, 0.3360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.70762586593628 \tStep Time:  0.00498652458190918 s \tTotal Time:  7.359269618988037 s \n",
      "\n",
      "\n",
      "\tEpisode 1182 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4554],\n",
      "        [1.0000, 0.5844]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3112],\n",
      "        [0.9999, 0.4406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564759254455566 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.365253925323486 s \n",
      "\n",
      "\n",
      "\tEpisode 1183 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5869],\n",
      "        [0.9999, 0.5473]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4495],\n",
      "        [1.0000, 0.4136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.682755947113037 \tStep Time:  0.005983591079711914 s \tTotal Time:  7.371237516403198 s \n",
      "\n",
      "\n",
      "\tEpisode 1184 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4961],\n",
      "        [1.0000, 0.3103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3708],\n",
      "        [0.9999, 0.4790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.407426476478577 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.3772218227386475 s \n",
      "\n",
      "\n",
      "\tEpisode 1185 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4704],\n",
      "        [1.0000, 0.4305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5409],\n",
      "        [0.9999, 0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50402569770813 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.3832056522369385 s \n",
      "\n",
      "\n",
      "\tEpisode 1186 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5585],\n",
      "        [1.0000, 0.8180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5749],\n",
      "        [1.0000, 0.3996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.391298770904541 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.389189720153809 s \n",
      "\n",
      "\n",
      "\tEpisode 1187 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5117],\n",
      "        [1.0000, 0.5781]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3845],\n",
      "        [1.0000, 0.7088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426498055458069 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.3951735496521 s \n",
      "\n",
      "\n",
      "\tEpisode 1188 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6139],\n",
      "        [1.0000, 0.5492]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5643],\n",
      "        [0.9999, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520473003387451 \tStep Time:  0.004987239837646484 s \tTotal Time:  7.400160789489746 s \n",
      "\n",
      "\n",
      "\tEpisode 1189 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5295],\n",
      "        [0.9999, 0.4555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3779],\n",
      "        [1.0000, 0.4321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44104290008545 \tStep Time:  0.0059833526611328125 s \tTotal Time:  7.406144142150879 s \n",
      "\n",
      "\n",
      "\tEpisode 1190 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5074],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4924],\n",
      "        [0.9999, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490481853485107 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.412128210067749 s \n",
      "\n",
      "\n",
      "\tEpisode 1191 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5056],\n",
      "        [1.0000, 0.5899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4675],\n",
      "        [1.0000, 0.3600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534780502319336 \tStep Time:  0.0059850215911865234 s \tTotal Time:  7.4181132316589355 s \n",
      "\n",
      "\n",
      "\tEpisode 1192 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6089],\n",
      "        [1.0000, 0.3865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4449],\n",
      "        [0.9999, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425842761993408 \tStep Time:  0.0059833526611328125 s \tTotal Time:  7.424096584320068 s \n",
      "\n",
      "\n",
      "\tEpisode 1193 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2763],\n",
      "        [1.0000, 0.3889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3519],\n",
      "        [0.9999, 0.4575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55447006225586 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.431077718734741 s \n",
      "\n",
      "\n",
      "\tEpisode 1194 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3832],\n",
      "        [1.0000, 0.3545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5765],\n",
      "        [0.9999, 0.5488]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569446086883545 \tStep Time:  0.004987001419067383 s \tTotal Time:  7.436064720153809 s \n",
      "\n",
      "\n",
      "\tEpisode 1195 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3710],\n",
      "        [1.0000, 0.5811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5562],\n",
      "        [1.0000, 0.3494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539784908294678 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.4420485496521 s \n",
      "\n",
      "\n",
      "\tEpisode 1196 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4383],\n",
      "        [1.0000, 0.3408]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5037],\n",
      "        [0.9999, 0.4719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426391124725342 \tStep Time:  0.005983591079711914 s \tTotal Time:  7.4480321407318115 s \n",
      "\n",
      "\n",
      "\tEpisode 1197 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4602],\n",
      "        [1.0000, 0.4333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4337],\n",
      "        [0.9999, 0.5620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465454578399658 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.454016447067261 s \n",
      "\n",
      "\n",
      "\tEpisode 1198 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4972],\n",
      "        [1.0000, 0.4474]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7224],\n",
      "        [1.0000, 0.8490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562505722045898 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.460000514984131 s \n",
      "\n",
      "\n",
      "\tEpisode 1199 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5601],\n",
      "        [1.0000, 0.6424]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6372],\n",
      "        [0.9999, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510473251342773 \tStep Time:  0.005983591079711914 s \tTotal Time:  7.465984106063843 s \n",
      "\n",
      "\n",
      "\tEpisode 1200 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3864],\n",
      "        [1.0000, 0.3643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5217],\n",
      "        [1.0000, 0.6951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46955156326294 \tStep Time:  0.004987478256225586 s \tTotal Time:  7.470971584320068 s \n",
      "\n",
      "\n",
      "\tEpisode 1201 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5539],\n",
      "        [1.0000, 0.3642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6254],\n",
      "        [0.9999, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.658116340637207 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.477952718734741 s \n",
      "\n",
      "\n",
      "\tEpisode 1202 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5631],\n",
      "        [1.0000, 0.4619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5069],\n",
      "        [1.0000, 0.5646]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545269787311554 \tStep Time:  0.005983591079711914 s \tTotal Time:  7.483936309814453 s \n",
      "\n",
      "\n",
      "\tEpisode 1203 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4161],\n",
      "        [1.0000, 0.5798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5359],\n",
      "        [0.9999, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459298610687256 \tStep Time:  0.004986763000488281 s \tTotal Time:  7.488923072814941 s \n",
      "\n",
      "\n",
      "\tEpisode 1204 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4849],\n",
      "        [0.9999, 0.5320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5082],\n",
      "        [1.0000, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465752124786377 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.494906902313232 s \n",
      "\n",
      "\n",
      "\tEpisode 1205 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5364],\n",
      "        [0.9999, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4668],\n",
      "        [1.0000, 0.5620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592460572719574 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.5008909702301025 s \n",
      "\n",
      "\n",
      "\tEpisode 1206 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4387],\n",
      "        [1.0000, 0.4565]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4973],\n",
      "        [1.0000, 0.4306]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541004121303558 \tStep Time:  0.00498652458190918 s \tTotal Time:  7.505877494812012 s \n",
      "\n",
      "\n",
      "\tEpisode 1207 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5368],\n",
      "        [1.0000, 0.4385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4588],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49387776851654 \tStep Time:  0.006981372833251953 s \tTotal Time:  7.512858867645264 s \n",
      "\n",
      "\n",
      "\tEpisode 1208 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4884],\n",
      "        [0.9999, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4920],\n",
      "        [1.0000, 0.4513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493252277374268 \tStep Time:  0.004987001419067383 s \tTotal Time:  7.517845869064331 s \n",
      "\n",
      "\n",
      "\tEpisode 1209 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4273],\n",
      "        [0.9999, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5631],\n",
      "        [0.9999, 0.4851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447473526000977 \tStep Time:  0.006989002227783203 s \tTotal Time:  7.524834871292114 s \n",
      "\n",
      "\n",
      "\tEpisode 1210 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5064],\n",
      "        [0.9999, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4747],\n",
      "        [0.9999, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513056874275208 \tStep Time:  0.006973743438720703 s \tTotal Time:  7.531808614730835 s \n",
      "\n",
      "\n",
      "\tEpisode 1211 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5409],\n",
      "        [0.9999, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4808],\n",
      "        [0.9999, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49951696395874 \tStep Time:  0.00498652458190918 s \tTotal Time:  7.536795139312744 s \n",
      "\n",
      "\n",
      "\tEpisode 1212 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4630],\n",
      "        [1.0000, 0.5269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4008],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542667388916016 \tStep Time:  0.005984783172607422 s \tTotal Time:  7.542779922485352 s \n",
      "\n",
      "\n",
      "\tEpisode 1213 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4789],\n",
      "        [1.0000, 0.5181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3679],\n",
      "        [0.9999, 0.4822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58784008026123 \tStep Time:  0.006980419158935547 s \tTotal Time:  7.549760341644287 s \n",
      "\n",
      "\n",
      "\tEpisode 1214 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4996],\n",
      "        [1.0000, 0.4711]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5120],\n",
      "        [0.9999, 0.4799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497226238250732 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.555744171142578 s \n",
      "\n",
      "\n",
      "\tEpisode 1215 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5106],\n",
      "        [1.0000, 0.4642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4826],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482256889343262 \tStep Time:  0.006982326507568359 s \tTotal Time:  7.5627264976501465 s \n",
      "\n",
      "\n",
      "\tEpisode 1216 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5452],\n",
      "        [1.0000, 0.4673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4819],\n",
      "        [0.9999, 0.4899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461669147014618 \tStep Time:  0.005983114242553711 s \tTotal Time:  7.5687096118927 s \n",
      "\n",
      "\n",
      "\tEpisode 1217 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [0.9999, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5450],\n",
      "        [0.9999, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49979305267334 \tStep Time:  0.00498652458190918 s \tTotal Time:  7.573696136474609 s \n",
      "\n",
      "\n",
      "\tEpisode 1218 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5672],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5421],\n",
      "        [0.9999, 0.5030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527773380279541 \tStep Time:  0.0069828033447265625 s \tTotal Time:  7.580678939819336 s \n",
      "\n",
      "\n",
      "\tEpisode 1219 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5043],\n",
      "        [0.9999, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6036],\n",
      "        [1.0000, 0.7311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47269755601883 \tStep Time:  0.005982637405395508 s \tTotal Time:  7.5866615772247314 s \n",
      "\n",
      "\n",
      "\tEpisode 1220 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5028],\n",
      "        [1.0000, 0.5594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6611],\n",
      "        [0.9999, 0.5523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565723299980164 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.592645645141602 s \n",
      "\n",
      "\n",
      "\tEpisode 1221 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5857],\n",
      "        [0.9999, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4936],\n",
      "        [1.0000, 0.5658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58650827407837 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.598629951477051 s \n",
      "\n",
      "\n",
      "\tEpisode 1222 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5359],\n",
      "        [0.9999, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5561],\n",
      "        [1.0000, 0.4853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551423072814941 \tStep Time:  0.004986286163330078 s \tTotal Time:  7.603616237640381 s \n",
      "\n",
      "\n",
      "\tEpisode 1223 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [0.9999, 0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4698],\n",
      "        [1.0000, 0.4850]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489678859710693 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.60960054397583 s \n",
      "\n",
      "\n",
      "\tEpisode 1224 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4639],\n",
      "        [1.0000, 0.4638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4790],\n",
      "        [1.0000, 0.4686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502497434616089 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.6155846118927 s \n",
      "\n",
      "\n",
      "\tEpisode 1225 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5192],\n",
      "        [0.9999, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5022],\n",
      "        [0.9999, 0.4964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511149406433105 \tStep Time:  0.005983591079711914 s \tTotal Time:  7.621568202972412 s \n",
      "\n",
      "\n",
      "\tEpisode 1226 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4710],\n",
      "        [1.0000, 0.4693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4792],\n",
      "        [0.9999, 0.4776]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50669914484024 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.627552270889282 s \n",
      "\n",
      "\n",
      "\tEpisode 1227 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4780],\n",
      "        [1.0000, 0.4643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5350],\n",
      "        [1.0000, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490663528442383 \tStep Time:  0.00598454475402832 s \tTotal Time:  7.6335368156433105 s \n",
      "\n",
      "\n",
      "\tEpisode 1228 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4779],\n",
      "        [1.0000, 0.5465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5272],\n",
      "        [0.9999, 0.4720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518879413604736 \tStep Time:  0.0059833526611328125 s \tTotal Time:  7.639520168304443 s \n",
      "\n",
      "\n",
      "\tEpisode 1229 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4688],\n",
      "        [1.0000, 0.7246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4743],\n",
      "        [1.0000, 0.4586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.3964262008667 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.645504474639893 s \n",
      "\n",
      "\n",
      "\tEpisode 1230 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5018],\n",
      "        [0.9999, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5611],\n",
      "        [1.0000, 0.5183]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54607629776001 \tStep Time:  0.0059850215911865234 s \tTotal Time:  7.651489496231079 s \n",
      "\n",
      "\n",
      "\tEpisode 1231 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6197],\n",
      "        [1.0000, 0.7463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4832],\n",
      "        [1.0000, 0.4589]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.33068323135376 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.658470153808594 s \n",
      "\n",
      "\n",
      "\tEpisode 1232 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4541],\n",
      "        [1.0000, 0.6049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6475],\n",
      "        [0.9999, 0.4596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.702757835388184 \tStep Time:  0.0059833526611328125 s \tTotal Time:  7.664453506469727 s \n",
      "\n",
      "\n",
      "\tEpisode 1233 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4526],\n",
      "        [0.9999, 0.4628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5274],\n",
      "        [0.9999, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491081237792969 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.670437812805176 s \n",
      "\n",
      "\n",
      "\tEpisode 1234 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4042],\n",
      "        [1.0000, 0.4425]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3976],\n",
      "        [1.0000, 0.3903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513532876968384 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.676421880722046 s \n",
      "\n",
      "\n",
      "\tEpisode 1235 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4496],\n",
      "        [1.0000, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4602],\n",
      "        [0.9999, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470177173614502 \tStep Time:  0.00598454475402832 s \tTotal Time:  7.682406425476074 s \n",
      "\n",
      "\n",
      "\tEpisode 1236 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4687],\n",
      "        [0.9999, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4676],\n",
      "        [0.9999, 0.4731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527797996997833 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.689387083053589 s \n",
      "\n",
      "\n",
      "\tEpisode 1237 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4471],\n",
      "        [1.0000, 0.4325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4559],\n",
      "        [0.9999, 0.4675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530059456825256 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.69537091255188 s \n",
      "\n",
      "\n",
      "\tEpisode 1238 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4324],\n",
      "        [0.9999, 0.4655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4664],\n",
      "        [0.9999, 0.4987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509846866130829 \tStep Time:  0.006981372833251953 s \tTotal Time:  7.702352285385132 s \n",
      "\n",
      "\n",
      "\tEpisode 1239 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4996],\n",
      "        [0.9999, 0.4761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4055],\n",
      "        [0.9999, 0.4763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492730259895325 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.708336353302002 s \n",
      "\n",
      "\n",
      "\tEpisode 1240 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5895],\n",
      "        [1.0000, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4008],\n",
      "        [1.0000, 0.3953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561624526977539 \tStep Time:  0.006981611251831055 s \tTotal Time:  7.715317964553833 s \n",
      "\n",
      "\n",
      "\tEpisode 1241 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3981],\n",
      "        [0.9999, 0.4929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5097],\n",
      "        [0.9999, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555146932601929 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.722299098968506 s \n",
      "\n",
      "\n",
      "\tEpisode 1242 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7617],\n",
      "        [1.0000, 0.4575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4980],\n",
      "        [0.9999, 0.4939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426008224487305 \tStep Time:  0.006981611251831055 s \tTotal Time:  7.729280710220337 s \n",
      "\n",
      "\n",
      "\tEpisode 1243 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5010],\n",
      "        [0.9999, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4920],\n",
      "        [1.0000, 0.4549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4946209192276 \tStep Time:  0.007006406784057617 s \tTotal Time:  7.7362871170043945 s \n",
      "\n",
      "\n",
      "\tEpisode 1244 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4552],\n",
      "        [1.0000, 0.5371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531126141548157 \tStep Time:  0.008950471878051758 s \tTotal Time:  7.745237588882446 s \n",
      "\n",
      "\n",
      "\tEpisode 1245 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.5014]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5085],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52848768234253 \tStep Time:  0.004985809326171875 s \tTotal Time:  7.750223398208618 s \n",
      "\n",
      "\n",
      "\tEpisode 1246 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [0.9999, 0.4974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502583503723145 \tStep Time:  0.007994651794433594 s \tTotal Time:  7.758218050003052 s \n",
      "\n",
      "\n",
      "\tEpisode 1247 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [1.0000, 0.4829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5062],\n",
      "        [0.9999, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52558422088623 \tStep Time:  0.006967067718505859 s \tTotal Time:  7.765185117721558 s \n",
      "\n",
      "\n",
      "\tEpisode 1248 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5011],\n",
      "        [1.0000, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5020],\n",
      "        [1.0000, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510886907577515 \tStep Time:  0.006979465484619141 s \tTotal Time:  7.772164583206177 s \n",
      "\n",
      "\n",
      "\tEpisode 1249 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4975],\n",
      "        [0.9999, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [0.9999, 0.4968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51046335697174 \tStep Time:  0.006982088088989258 s \tTotal Time:  7.779146671295166 s \n",
      "\n",
      "\n",
      "\tEpisode 1250 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4954],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.4964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502418518066406 \tStep Time:  0.0069811344146728516 s \tTotal Time:  7.786127805709839 s \n",
      "\n",
      "\n",
      "\tEpisode 1251 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4934],\n",
      "        [0.9999, 0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4954],\n",
      "        [0.9999, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505243301391602 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.792111873626709 s \n",
      "\n",
      "\n",
      "\tEpisode 1252 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5074],\n",
      "        [0.9999, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4997],\n",
      "        [1.0000, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48450517654419 \tStep Time:  0.00698089599609375 s \tTotal Time:  7.799092769622803 s \n",
      "\n",
      "\n",
      "\tEpisode 1253 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4950],\n",
      "        [0.9999, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4840],\n",
      "        [1.0000, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514511585235596 \tStep Time:  0.004986286163330078 s \tTotal Time:  7.804079055786133 s \n",
      "\n",
      "\n",
      "\tEpisode 1254 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4919],\n",
      "        [0.9999, 0.4987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4807],\n",
      "        [0.9999, 0.4815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507449626922607 \tStep Time:  0.0059854984283447266 s \tTotal Time:  7.8100645542144775 s \n",
      "\n",
      "\n",
      "\tEpisode 1255 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [0.9999, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [0.9999, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512459754943848 \tStep Time:  0.005982637405395508 s \tTotal Time:  7.816047191619873 s \n",
      "\n",
      "\n",
      "\tEpisode 1256 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5219],\n",
      "        [0.9999, 0.4845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4836],\n",
      "        [1.0000, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50827407836914 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.822031021118164 s \n",
      "\n",
      "\n",
      "\tEpisode 1257 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5295],\n",
      "        [1.0000, 0.4680]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5025],\n",
      "        [0.9999, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50739336013794 \tStep Time:  0.0059931278228759766 s \tTotal Time:  7.82802414894104 s \n",
      "\n",
      "\n",
      "\tEpisode 1258 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4875],\n",
      "        [1.0000, 0.5630]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4916],\n",
      "        [0.9999, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468156337738037 \tStep Time:  0.007011890411376953 s \tTotal Time:  7.835036039352417 s \n",
      "\n",
      "\n",
      "\tEpisode 1259 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4838],\n",
      "        [1.0000, 0.4711]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4766],\n",
      "        [1.0000, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502676963806152 \tStep Time:  0.004988193511962891 s \tTotal Time:  7.84002423286438 s \n",
      "\n",
      "\n",
      "\tEpisode 1260 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [0.9999, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5708],\n",
      "        [0.9999, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536037802696228 \tStep Time:  0.005953550338745117 s \tTotal Time:  7.845977783203125 s \n",
      "\n",
      "\n",
      "\tEpisode 1261 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5047],\n",
      "        [1.0000, 0.5636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5336],\n",
      "        [1.0000, 0.4614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472649574279785 \tStep Time:  0.005986213684082031 s \tTotal Time:  7.851963996887207 s \n",
      "\n",
      "\n",
      "\tEpisode 1262 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4625],\n",
      "        [1.0000, 0.5949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5769],\n",
      "        [0.9999, 0.4780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497606337070465 \tStep Time:  0.004986763000488281 s \tTotal Time:  7.856950759887695 s \n",
      "\n",
      "\n",
      "\tEpisode 1263 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5273],\n",
      "        [1.0000, 0.5660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5137],\n",
      "        [1.0000, 0.5695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502279937267303 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.862934827804565 s \n",
      "\n",
      "\n",
      "\tEpisode 1264 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6112],\n",
      "        [1.0000, 0.5809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4442],\n",
      "        [0.9999, 0.4930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555187106132507 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.868919134140015 s \n",
      "\n",
      "\n",
      "\tEpisode 1265 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4915],\n",
      "        [1.0000, 0.6076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4178],\n",
      "        [1.0000, 0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.385374069213867 \tStep Time:  0.005985260009765625 s \tTotal Time:  7.87490439414978 s \n",
      "\n",
      "\n",
      "\tEpisode 1266 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4774],\n",
      "        [0.9999, 0.4334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4862],\n",
      "        [0.9999, 0.4414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515937328338623 \tStep Time:  0.005982398986816406 s \tTotal Time:  7.880886793136597 s \n",
      "\n",
      "\n",
      "\tEpisode 1267 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4765],\n",
      "        [1.0000, 0.4028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3826],\n",
      "        [0.9999, 0.4718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51078987121582 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.886870861053467 s \n",
      "\n",
      "\n",
      "\tEpisode 1268 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4063],\n",
      "        [1.0000, 0.4231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6917],\n",
      "        [0.9999, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445929825305939 \tStep Time:  0.006017446517944336 s \tTotal Time:  7.892888307571411 s \n",
      "\n",
      "\n",
      "\tEpisode 1269 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4539],\n",
      "        [0.9999, 0.4868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4485],\n",
      "        [1.0000, 0.6801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440810203552246 \tStep Time:  0.0050051212310791016 s \tTotal Time:  7.89789342880249 s \n",
      "\n",
      "\n",
      "\tEpisode 1270 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5848],\n",
      "        [0.9999, 0.4574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4910],\n",
      "        [1.0000, 0.5682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528422355651855 \tStep Time:  0.004986763000488281 s \tTotal Time:  7.903856992721558 s \n",
      "\n",
      "\n",
      "\tEpisode 1271 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3739],\n",
      "        [1.0000, 0.6376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7167],\n",
      "        [0.9999, 0.4372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540901064872742 \tStep Time:  0.0060007572174072266 s \tTotal Time:  7.909857749938965 s \n",
      "\n",
      "\n",
      "\tEpisode 1272 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5709],\n",
      "        [1.0000, 0.5845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [0.9999, 0.4336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568512916564941 \tStep Time:  0.006934404373168945 s \tTotal Time:  7.916792154312134 s \n",
      "\n",
      "\n",
      "\tEpisode 1273 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3897],\n",
      "        [0.9999, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3462],\n",
      "        [1.0000, 0.5446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.687941789627075 \tStep Time:  0.004986286163330078 s \tTotal Time:  7.921778440475464 s \n",
      "\n",
      "\n",
      "\tEpisode 1274 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6565],\n",
      "        [1.0000, 0.8140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4200],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.847561836242676 \tStep Time:  0.007978677749633789 s \tTotal Time:  7.929757118225098 s \n",
      "\n",
      "\n",
      "\tEpisode 1275 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4457],\n",
      "        [0.9999, 0.4393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5252],\n",
      "        [1.0000, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518550395965576 \tStep Time:  0.004986763000488281 s \tTotal Time:  7.934743881225586 s \n",
      "\n",
      "\n",
      "\tEpisode 1276 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4827],\n",
      "        [0.9999, 0.4941]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4414],\n",
      "        [1.0000, 0.5606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503542423248291 \tStep Time:  0.00598454475402832 s \tTotal Time:  7.940728425979614 s \n",
      "\n",
      "\n",
      "\tEpisode 1277 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5232],\n",
      "        [1.0000, 0.4109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4406],\n",
      "        [1.0000, 0.4924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440563917160034 \tStep Time:  0.006981372833251953 s \tTotal Time:  7.947709798812866 s \n",
      "\n",
      "\n",
      "\tEpisode 1278 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5112],\n",
      "        [0.9999, 0.4759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5326],\n",
      "        [0.9999, 0.4444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.585657835006714 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.953693866729736 s \n",
      "\n",
      "\n",
      "\tEpisode 1279 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4884],\n",
      "        [1.0000, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4321],\n",
      "        [1.0000, 0.4974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560316801071167 \tStep Time:  0.006981611251831055 s \tTotal Time:  7.960675477981567 s \n",
      "\n",
      "\n",
      "\tEpisode 1280 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4544],\n",
      "        [1.0000, 0.4430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [0.9999, 0.4598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499845802783966 \tStep Time:  0.005987405776977539 s \tTotal Time:  7.966662883758545 s \n",
      "\n",
      "\n",
      "\tEpisode 1281 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4795],\n",
      "        [1.0000, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4549],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475126802921295 \tStep Time:  0.005984306335449219 s \tTotal Time:  7.972647190093994 s \n",
      "\n",
      "\n",
      "\tEpisode 1282 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5219],\n",
      "        [0.9999, 0.4773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4749],\n",
      "        [1.0000, 0.4559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522921919822693 \tStep Time:  0.006981611251831055 s \tTotal Time:  7.979628801345825 s \n",
      "\n",
      "\n",
      "\tEpisode 1283 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4571],\n",
      "        [0.9999, 0.4614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4552],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538290977478027 \tStep Time:  0.005983829498291016 s \tTotal Time:  7.985612630844116 s \n",
      "\n",
      "\n",
      "\tEpisode 1284 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4546],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4466],\n",
      "        [0.9999, 0.4651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520166873931885 \tStep Time:  0.005984067916870117 s \tTotal Time:  7.991596698760986 s \n",
      "\n",
      "\n",
      "\tEpisode 1285 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4625],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4704],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565834164619446 \tStep Time:  0.005993843078613281 s \tTotal Time:  7.9975905418396 s \n",
      "\n",
      "\n",
      "\tEpisode 1286 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4693],\n",
      "        [1.0000, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489596962928772 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.00357460975647 s \n",
      "\n",
      "\n",
      "\tEpisode 1287 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.5033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4873],\n",
      "        [0.9999, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500263214111328 \tStep Time:  0.006982326507568359 s \tTotal Time:  8.010556936264038 s \n",
      "\n",
      "\n",
      "\tEpisode 1288 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4862],\n",
      "        [1.0000, 0.5026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5106],\n",
      "        [1.0000, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515855312347412 \tStep Time:  0.005982875823974609 s \tTotal Time:  8.016539812088013 s \n",
      "\n",
      "\n",
      "\tEpisode 1289 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4982],\n",
      "        [0.9999, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50628411769867 \tStep Time:  0.004986763000488281 s \tTotal Time:  8.021526575088501 s \n",
      "\n",
      "\n",
      "\tEpisode 1290 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4981],\n",
      "        [0.9999, 0.5026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4978],\n",
      "        [1.0000, 0.4890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51424765586853 \tStep Time:  0.006982564926147461 s \tTotal Time:  8.028509140014648 s \n",
      "\n",
      "\n",
      "\tEpisode 1291 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4654],\n",
      "        [1.0000, 0.5459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5306],\n",
      "        [1.0000, 0.4850]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51294755935669 \tStep Time:  0.005982875823974609 s \tTotal Time:  8.034492015838623 s \n",
      "\n",
      "\n",
      "\tEpisode 1292 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5028],\n",
      "        [1.0000, 0.4670]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5474],\n",
      "        [1.0000, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453165054321289 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.040475845336914 s \n",
      "\n",
      "\n",
      "\tEpisode 1293 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5463],\n",
      "        [0.9999, 0.5180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5122],\n",
      "        [1.0000, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511441767215729 \tStep Time:  0.0059850215911865234 s \tTotal Time:  8.0464608669281 s \n",
      "\n",
      "\n",
      "\tEpisode 1294 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5406],\n",
      "        [0.9999, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5846],\n",
      "        [1.0000, 0.6512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428726196289062 \tStep Time:  0.005983114242553711 s \tTotal Time:  8.052443981170654 s \n",
      "\n",
      "\n",
      "\tEpisode 1295 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5914],\n",
      "        [0.9999, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4368],\n",
      "        [1.0000, 0.3889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463126122951508 \tStep Time:  0.00598454475402832 s \tTotal Time:  8.058428525924683 s \n",
      "\n",
      "\n",
      "\tEpisode 1296 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3873],\n",
      "        [0.9999, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5971],\n",
      "        [1.0000, 0.8810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.397375762462616 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.065409898757935 s \n",
      "\n",
      "\n",
      "\tEpisode 1297 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4291],\n",
      "        [0.9999, 0.4686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4854],\n",
      "        [0.9999, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512702465057373 \tStep Time:  0.005983114242553711 s \tTotal Time:  8.071393013000488 s \n",
      "\n",
      "\n",
      "\tEpisode 1298 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5080],\n",
      "        [1.0000, 0.3918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3382],\n",
      "        [1.0000, 0.3226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612675666809082 \tStep Time:  0.005985260009765625 s \tTotal Time:  8.077378273010254 s \n",
      "\n",
      "\n",
      "\tEpisode 1299 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3774],\n",
      "        [1.0000, 0.6286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5007],\n",
      "        [0.9999, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42855179309845 \tStep Time:  0.006980180740356445 s \tTotal Time:  8.08435845375061 s \n",
      "\n",
      "\n",
      "\tEpisode 1300 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5776],\n",
      "        [1.0000, 0.3534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3577],\n",
      "        [0.9999, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36192137002945 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.090342283248901 s \n",
      "\n",
      "\n",
      "\tEpisode 1301 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8251],\n",
      "        [1.0000, 0.6116]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4893],\n",
      "        [1.0000, 0.6192]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420044302940369 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.096326351165771 s \n",
      "\n",
      "\n",
      "\tEpisode 1302 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4094],\n",
      "        [1.0000, 0.8231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5927],\n",
      "        [1.0000, 0.2682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.795687675476074 \tStep Time:  0.005985260009765625 s \tTotal Time:  8.102311611175537 s \n",
      "\n",
      "\n",
      "\tEpisode 1303 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2644],\n",
      "        [1.0000, 0.3006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3465],\n",
      "        [1.0000, 0.4201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.632108211517334 \tStep Time:  0.00698089599609375 s \tTotal Time:  8.10929250717163 s \n",
      "\n",
      "\n",
      "\tEpisode 1304 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3690],\n",
      "        [1.0000, 0.3834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6432],\n",
      "        [0.9999, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504438400268555 \tStep Time:  0.005983591079711914 s \tTotal Time:  8.115276098251343 s \n",
      "\n",
      "\n",
      "\tEpisode 1305 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4951],\n",
      "        [0.9999, 0.4783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4049],\n",
      "        [0.9999, 0.5572]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569230556488037 \tStep Time:  0.0069811344146728516 s \tTotal Time:  8.122257232666016 s \n",
      "\n",
      "\n",
      "\tEpisode 1306 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5131],\n",
      "        [1.0000, 0.3724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6315],\n",
      "        [1.0000, 0.4405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443509101867676 \tStep Time:  0.005985736846923828 s \tTotal Time:  8.12824296951294 s \n",
      "\n",
      "\n",
      "\tEpisode 1307 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4060],\n",
      "        [1.0000, 0.6767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3700],\n",
      "        [0.9999, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.347631931304932 \tStep Time:  0.005982637405395508 s \tTotal Time:  8.135222434997559 s \n",
      "\n",
      "\n",
      "\tEpisode 1308 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6425],\n",
      "        [0.9999, 0.4924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6528],\n",
      "        [1.0000, 0.4624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.737539231777191 \tStep Time:  0.006981611251831055 s \tTotal Time:  8.14220404624939 s \n",
      "\n",
      "\n",
      "\tEpisode 1309 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9492],\n",
      "        [0.9999, 0.5521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5084],\n",
      "        [1.0000, 0.7079]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34738302230835 \tStep Time:  0.005984783172607422 s \tTotal Time:  8.148188829421997 s \n",
      "\n",
      "\n",
      "\tEpisode 1310 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7038],\n",
      "        [1.0000, 0.4146]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6531],\n",
      "        [0.9999, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587746143341064 \tStep Time:  0.0069789886474609375 s \tTotal Time:  8.155167818069458 s \n",
      "\n",
      "\n",
      "\tEpisode 1311 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4835],\n",
      "        [0.9999, 0.5109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5040],\n",
      "        [1.0000, 0.3590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486420452594757 \tStep Time:  0.0069844722747802734 s \tTotal Time:  8.162152290344238 s \n",
      "\n",
      "\n",
      "\tEpisode 1312 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4128],\n",
      "        [0.9999, 0.5450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7291],\n",
      "        [0.9999, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588158190250397 \tStep Time:  0.006979465484619141 s \tTotal Time:  8.169131755828857 s \n",
      "\n",
      "\n",
      "\tEpisode 1313 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4864],\n",
      "        [0.9999, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6619],\n",
      "        [0.9999, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495647370815277 \tStep Time:  0.005985736846923828 s \tTotal Time:  8.175117492675781 s \n",
      "\n",
      "\n",
      "\tEpisode 1314 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6373],\n",
      "        [1.0000, 0.4163]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5100],\n",
      "        [0.9999, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569148540496826 \tStep Time:  0.006979703903198242 s \tTotal Time:  8.18209719657898 s \n",
      "\n",
      "\n",
      "\tEpisode 1315 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3108],\n",
      "        [1.0000, 0.3477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3563],\n",
      "        [1.0000, 0.3836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52812671661377 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.18808102607727 s \n",
      "\n",
      "\n",
      "\tEpisode 1316 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3973],\n",
      "        [1.0000, 0.3650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4216],\n",
      "        [0.9999, 0.4623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54553210735321 \tStep Time:  0.0069959163665771484 s \tTotal Time:  8.195076942443848 s \n",
      "\n",
      "\n",
      "\tEpisode 1317 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5118],\n",
      "        [1.0000, 0.6454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5315],\n",
      "        [0.9999, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462457776069641 \tStep Time:  0.005979776382446289 s \tTotal Time:  8.201056718826294 s \n",
      "\n",
      "\n",
      "\tEpisode 1318 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3634],\n",
      "        [0.9999, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5550],\n",
      "        [0.9999, 0.5126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467352092266083 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.207040548324585 s \n",
      "\n",
      "\n",
      "\tEpisode 1319 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8965],\n",
      "        [1.0000, 0.7063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4976],\n",
      "        [1.0000, 0.4499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.943141460418701 \tStep Time:  0.005988359451293945 s \tTotal Time:  8.213028907775879 s \n",
      "\n",
      "\n",
      "\tEpisode 1320 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5620],\n",
      "        [1.0000, 0.3714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4717],\n",
      "        [1.0000, 0.7283]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.33983689546585 \tStep Time:  0.0059888362884521484 s \tTotal Time:  8.219017744064331 s \n",
      "\n",
      "\n",
      "\tEpisode 1321 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4914],\n",
      "        [1.0000, 0.3663]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5572],\n",
      "        [0.9999, 0.5427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.646657943725586 \tStep Time:  0.005982875823974609 s \tTotal Time:  8.225000619888306 s \n",
      "\n",
      "\n",
      "\tEpisode 1322 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3972],\n",
      "        [1.0000, 0.3902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3801],\n",
      "        [1.0000, 0.3872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5492422580719 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.231981992721558 s \n",
      "\n",
      "\n",
      "\tEpisode 1323 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4514],\n",
      "        [1.0000, 0.5850]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4834],\n",
      "        [1.0000, 0.6140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53836965560913 \tStep Time:  0.004986286163330078 s \tTotal Time:  8.236968278884888 s \n",
      "\n",
      "\n",
      "\tEpisode 1324 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6118],\n",
      "        [0.9999, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3730],\n",
      "        [1.0000, 0.4520]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.404765605926514 \tStep Time:  0.006981849670410156 s \tTotal Time:  8.243950128555298 s \n",
      "\n",
      "\n",
      "\tEpisode 1325 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4130],\n",
      "        [0.9999, 0.5371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4217],\n",
      "        [1.0000, 0.6059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570636749267578 \tStep Time:  0.0059833526611328125 s \tTotal Time:  8.24993348121643 s \n",
      "\n",
      "\n",
      "\tEpisode 1326 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7291],\n",
      "        [0.9999, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4188],\n",
      "        [0.9999, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.709157049655914 \tStep Time:  0.005984306335449219 s \tTotal Time:  8.25591778755188 s \n",
      "\n",
      "\n",
      "\tEpisode 1327 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3919],\n",
      "        [1.0000, 0.4743]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4841],\n",
      "        [1.0000, 0.3994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545669078826904 \tStep Time:  0.005984306335449219 s \tTotal Time:  8.261902093887329 s \n",
      "\n",
      "\n",
      "\tEpisode 1328 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4348],\n",
      "        [0.9999, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4390],\n",
      "        [1.0000, 0.5855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503522515296936 \tStep Time:  0.0059833526611328125 s \tTotal Time:  8.267885446548462 s \n",
      "\n",
      "\n",
      "\tEpisode 1329 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5053],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.4341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477458000183105 \tStep Time:  0.004987478256225586 s \tTotal Time:  8.272872924804688 s \n",
      "\n",
      "\n",
      "\tEpisode 1330 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4398],\n",
      "        [0.9999, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5181],\n",
      "        [0.9999, 0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5598726272583 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.278856754302979 s \n",
      "\n",
      "\n",
      "\tEpisode 1331 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4405],\n",
      "        [0.9999, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4560],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467770636081696 \tStep Time:  0.007012844085693359 s \tTotal Time:  8.285869598388672 s \n",
      "\n",
      "\n",
      "\tEpisode 1332 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5065],\n",
      "        [0.9999, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5208],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509601593017578 \tStep Time:  0.005953311920166016 s \tTotal Time:  8.291822910308838 s \n",
      "\n",
      "\n",
      "\tEpisode 1333 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4736],\n",
      "        [1.0000, 0.4399]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5193],\n",
      "        [0.9999, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489096641540527 \tStep Time:  0.005983591079711914 s \tTotal Time:  8.29780650138855 s \n",
      "\n",
      "\n",
      "\tEpisode 1334 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4947],\n",
      "        [1.0000, 0.4211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5180],\n",
      "        [0.9999, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470867276191711 \tStep Time:  0.005982637405395508 s \tTotal Time:  8.303789138793945 s \n",
      "\n",
      "\n",
      "\tEpisode 1335 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5047],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5252],\n",
      "        [1.0000, 0.4344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541844844818115 \tStep Time:  0.00701451301574707 s \tTotal Time:  8.310803651809692 s \n",
      "\n",
      "\n",
      "\tEpisode 1336 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4242],\n",
      "        [0.9999, 0.5035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5249],\n",
      "        [1.0000, 0.5282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445281505584717 \tStep Time:  0.006949663162231445 s \tTotal Time:  8.317753314971924 s \n",
      "\n",
      "\n",
      "\tEpisode 1337 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4971],\n",
      "        [1.0000, 0.5348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3976],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42662000656128 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.323737382888794 s \n",
      "\n",
      "\n",
      "\tEpisode 1338 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5459],\n",
      "        [0.9999, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3972],\n",
      "        [1.0000, 0.3687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514647722244263 \tStep Time:  0.006981849670410156 s \tTotal Time:  8.330719232559204 s \n",
      "\n",
      "\n",
      "\tEpisode 1339 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5731],\n",
      "        [1.0000, 0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5038],\n",
      "        [1.0000, 0.4632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493690013885498 \tStep Time:  0.00698089599609375 s \tTotal Time:  8.337700128555298 s \n",
      "\n",
      "\n",
      "\tEpisode 1340 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5366],\n",
      "        [0.9999, 0.5145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4904],\n",
      "        [0.9999, 0.5459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487922191619873 \tStep Time:  0.005984306335449219 s \tTotal Time:  8.343684434890747 s \n",
      "\n",
      "\n",
      "\tEpisode 1341 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5222],\n",
      "        [1.0000, 0.4106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4942],\n",
      "        [1.0000, 0.5959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.617741107940674 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.349668264389038 s \n",
      "\n",
      "\n",
      "\tEpisode 1342 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2838],\n",
      "        [1.0000, 0.6193]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4873],\n",
      "        [1.0000, 0.3207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501984119415283 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.355652332305908 s \n",
      "\n",
      "\n",
      "\tEpisode 1343 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5065],\n",
      "        [1.0000, 0.4739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5889],\n",
      "        [0.9999, 0.5721]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601295948028564 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.36263370513916 s \n",
      "\n",
      "\n",
      "\tEpisode 1344 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5836],\n",
      "        [1.0000, 0.6201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5509],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511991500854492 \tStep Time:  0.00698089599609375 s \tTotal Time:  8.369614601135254 s \n",
      "\n",
      "\n",
      "\tEpisode 1345 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5933],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4794],\n",
      "        [0.9999, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48046725988388 \tStep Time:  0.005984783172607422 s \tTotal Time:  8.375599384307861 s \n",
      "\n",
      "\n",
      "\tEpisode 1346 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4540],\n",
      "        [0.9999, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6673],\n",
      "        [1.0000, 0.5882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532052159309387 \tStep Time:  0.0059833526611328125 s \tTotal Time:  8.381582736968994 s \n",
      "\n",
      "\n",
      "\tEpisode 1347 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7323],\n",
      "        [0.9999, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.414364337921143 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.387566804885864 s \n",
      "\n",
      "\n",
      "\tEpisode 1348 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5151],\n",
      "        [0.9999, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4511],\n",
      "        [0.9999, 0.5455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466740608215332 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.393550872802734 s \n",
      "\n",
      "\n",
      "\tEpisode 1349 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5741],\n",
      "        [1.0000, 0.6056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5414],\n",
      "        [0.9999, 0.5427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50548130273819 \tStep Time:  0.004991292953491211 s \tTotal Time:  8.398542165756226 s \n",
      "\n",
      "\n",
      "\tEpisode 1350 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4560],\n",
      "        [1.0000, 0.4676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5489],\n",
      "        [0.9999, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457131385803223 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.404525995254517 s \n",
      "\n",
      "\n",
      "\tEpisode 1351 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4567],\n",
      "        [1.0000, 0.3668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5362],\n",
      "        [0.9999, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540838241577148 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.410510063171387 s \n",
      "\n",
      "\n",
      "\tEpisode 1352 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6382],\n",
      "        [1.0000, 0.4291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5630],\n",
      "        [1.0000, 0.3666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542667806148529 \tStep Time:  0.0059926509857177734 s \tTotal Time:  8.416502714157104 s \n",
      "\n",
      "\n",
      "\tEpisode 1353 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5520],\n",
      "        [1.0000, 0.5666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3287],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469655156135559 \tStep Time:  0.004987001419067383 s \tTotal Time:  8.421489715576172 s \n",
      "\n",
      "\n",
      "\tEpisode 1354 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3226],\n",
      "        [0.9999, 0.4961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4427],\n",
      "        [1.0000, 0.6314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.40947151184082 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.428471088409424 s \n",
      "\n",
      "\n",
      "\tEpisode 1355 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5628],\n",
      "        [0.9999, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4562],\n",
      "        [1.0000, 0.4024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5313321352005 \tStep Time:  0.005983591079711914 s \tTotal Time:  8.434454679489136 s \n",
      "\n",
      "\n",
      "\tEpisode 1356 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5593],\n",
      "        [1.0000, 0.6177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4624],\n",
      "        [0.9999, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.629528999328613 \tStep Time:  0.004986763000488281 s \tTotal Time:  8.439441442489624 s \n",
      "\n",
      "\n",
      "\tEpisode 1357 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4692],\n",
      "        [1.0000, 0.3657]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5048],\n",
      "        [1.0000, 0.5552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553300857543945 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.446422815322876 s \n",
      "\n",
      "\n",
      "\tEpisode 1358 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6532],\n",
      "        [1.0000, 0.5331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4822],\n",
      "        [0.9999, 0.5199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451409935951233 \tStep Time:  0.004986763000488281 s \tTotal Time:  8.451409578323364 s \n",
      "\n",
      "\n",
      "\tEpisode 1359 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4269],\n",
      "        [0.9999, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3224],\n",
      "        [1.0000, 0.5508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553874969482422 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.457393407821655 s \n",
      "\n",
      "\n",
      "\tEpisode 1360 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3866],\n",
      "        [1.0000, 0.2880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4711],\n",
      "        [0.9999, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582545220851898 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.464374780654907 s \n",
      "\n",
      "\n",
      "\tEpisode 1361 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4413],\n",
      "        [1.0000, 0.4499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4209],\n",
      "        [0.9999, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472240626811981 \tStep Time:  0.00498652458190918 s \tTotal Time:  8.469361305236816 s \n",
      "\n",
      "\n",
      "\tEpisode 1362 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4546],\n",
      "        [1.0000, 0.3580]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5580],\n",
      "        [1.0000, 0.6377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34402847290039 \tStep Time:  0.005985260009765625 s \tTotal Time:  8.475346565246582 s \n",
      "\n",
      "\n",
      "\tEpisode 1363 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5463],\n",
      "        [1.0000, 0.4467]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5132],\n",
      "        [1.0000, 0.5650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439494609832764 \tStep Time:  0.006980180740356445 s \tTotal Time:  8.482326745986938 s \n",
      "\n",
      "\n",
      "\tEpisode 1364 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5152],\n",
      "        [1.0000, 0.4060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6345],\n",
      "        [1.0000, 0.7044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52951955795288 \tStep Time:  0.005984306335449219 s \tTotal Time:  8.488311052322388 s \n",
      "\n",
      "\n",
      "\tEpisode 1365 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3257],\n",
      "        [1.0000, 0.3593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4394],\n",
      "        [1.0000, 0.3761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60920763015747 \tStep Time:  0.005984306335449219 s \tTotal Time:  8.494295358657837 s \n",
      "\n",
      "\n",
      "\tEpisode 1366 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5633],\n",
      "        [1.0000, 0.6356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5923],\n",
      "        [0.9999, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498941898345947 \tStep Time:  0.005983591079711914 s \tTotal Time:  8.500278949737549 s \n",
      "\n",
      "\n",
      "\tEpisode 1367 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5837],\n",
      "        [1.0000, 0.6491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5107],\n",
      "        [1.0000, 0.4294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530359268188477 \tStep Time:  0.00498652458190918 s \tTotal Time:  8.506263017654419 s \n",
      "\n",
      "\n",
      "\tEpisode 1368 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4346],\n",
      "        [1.0000, 0.3009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7192],\n",
      "        [1.0000, 0.6153]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443925857543945 \tStep Time:  0.006982088088989258 s \tTotal Time:  8.513245105743408 s \n",
      "\n",
      "\n",
      "\tEpisode 1369 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4815],\n",
      "        [1.0000, 0.4177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3373],\n",
      "        [1.0000, 0.6882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.342442393302917 \tStep Time:  0.005983114242553711 s \tTotal Time:  8.519228219985962 s \n",
      "\n",
      "\n",
      "\tEpisode 1370 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6891],\n",
      "        [1.0000, 0.4168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4146],\n",
      "        [1.0000, 0.3613]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.389413356781006 \tStep Time:  0.005984783172607422 s \tTotal Time:  8.52521300315857 s \n",
      "\n",
      "\n",
      "\tEpisode 1371 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6285],\n",
      "        [0.9999, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4599],\n",
      "        [0.9999, 0.5894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551096439361572 \tStep Time:  0.007978439331054688 s \tTotal Time:  8.534188270568848 s \n",
      "\n",
      "\n",
      "\tEpisode 1372 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6514],\n",
      "        [1.0000, 0.6028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5194],\n",
      "        [1.0000, 0.6205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482045650482178 \tStep Time:  0.009972333908081055 s \tTotal Time:  8.544160604476929 s \n",
      "\n",
      "\n",
      "\tEpisode 1373 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.7529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3588],\n",
      "        [1.0000, 0.3114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.75708532333374 \tStep Time:  0.00997304916381836 s \tTotal Time:  8.554133653640747 s \n",
      "\n",
      "\n",
      "\tEpisode 1374 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4754],\n",
      "        [1.0000, 0.4101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2920],\n",
      "        [1.0000, 0.3632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562601566314697 \tStep Time:  0.007979154586791992 s \tTotal Time:  8.563110589981079 s \n",
      "\n",
      "\n",
      "\tEpisode 1375 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2587],\n",
      "        [1.0000, 0.6833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6136],\n",
      "        [1.0000, 0.3570]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.680794715881348 \tStep Time:  0.009972572326660156 s \tTotal Time:  8.57308316230774 s \n",
      "\n",
      "\n",
      "\tEpisode 1376 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5545],\n",
      "        [1.0000, 0.6097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5658],\n",
      "        [1.0000, 0.6460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550657272338867 \tStep Time:  0.011970043182373047 s \tTotal Time:  8.585053205490112 s \n",
      "\n",
      "\n",
      "\tEpisode 1377 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6080],\n",
      "        [1.0000, 0.7325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6766],\n",
      "        [1.0000, 0.3807]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.715927600860596 \tStep Time:  0.0069789886474609375 s \tTotal Time:  8.592032194137573 s \n",
      "\n",
      "\n",
      "\tEpisode 1378 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3826],\n",
      "        [1.0000, 0.8437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4037],\n",
      "        [0.9999, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416714668273926 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.598016262054443 s \n",
      "\n",
      "\n",
      "\tEpisode 1379 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3857],\n",
      "        [1.0000, 0.4147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4179],\n",
      "        [1.0000, 0.4098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525167465209961 \tStep Time:  0.005984783172607422 s \tTotal Time:  8.60400104522705 s \n",
      "\n",
      "\n",
      "\tEpisode 1380 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5068],\n",
      "        [1.0000, 0.4139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5331],\n",
      "        [1.0000, 0.5299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492063283920288 \tStep Time:  0.006981611251831055 s \tTotal Time:  8.610982656478882 s \n",
      "\n",
      "\n",
      "\tEpisode 1381 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4177],\n",
      "        [1.0000, 0.4353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4273],\n",
      "        [1.0000, 0.4368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51729965209961 \tStep Time:  0.005806446075439453 s \tTotal Time:  8.616966247558594 s \n",
      "\n",
      "\n",
      "\tEpisode 1382 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4865],\n",
      "        [1.0000, 0.5486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4798],\n",
      "        [1.0000, 0.5045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540585041046143 \tStep Time:  0.0059833526611328125 s \tTotal Time:  8.622949600219727 s \n",
      "\n",
      "\n",
      "\tEpisode 1383 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4855],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4953],\n",
      "        [0.9999, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519818305969238 \tStep Time:  0.005984783172607422 s \tTotal Time:  8.628934383392334 s \n",
      "\n",
      "\n",
      "\tEpisode 1384 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4678],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.5241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514410614967346 \tStep Time:  0.006125688552856445 s \tTotal Time:  8.63506007194519 s \n",
      "\n",
      "\n",
      "\tEpisode 1385 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [0.9999, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [1.0000, 0.4692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505065441131592 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.64104413986206 s \n",
      "\n",
      "\n",
      "\tEpisode 1386 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4582],\n",
      "        [1.0000, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [0.9999, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495564937591553 \tStep Time:  0.0059871673583984375 s \tTotal Time:  8.647031307220459 s \n",
      "\n",
      "\n",
      "\tEpisode 1387 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4898],\n",
      "        [1.0000, 0.4953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4873],\n",
      "        [1.0000, 0.4780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504359245300293 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.65301513671875 s \n",
      "\n",
      "\n",
      "\tEpisode 1388 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [1.0000, 0.4694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4619],\n",
      "        [1.0000, 0.4772]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510782182216644 \tStep Time:  0.005985260009765625 s \tTotal Time:  8.659000396728516 s \n",
      "\n",
      "\n",
      "\tEpisode 1389 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4562],\n",
      "        [1.0000, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4548],\n",
      "        [1.0000, 0.5170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517107486724854 \tStep Time:  0.0059833526611328125 s \tTotal Time:  8.664983749389648 s \n",
      "\n",
      "\n",
      "\tEpisode 1390 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4964],\n",
      "        [1.0000, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4637],\n",
      "        [0.9999, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508892059326172 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.670967817306519 s \n",
      "\n",
      "\n",
      "\tEpisode 1391 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4643],\n",
      "        [1.0000, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4982],\n",
      "        [1.0000, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480301260948181 \tStep Time:  0.0069811344146728516 s \tTotal Time:  8.677948951721191 s \n",
      "\n",
      "\n",
      "\tEpisode 1392 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4749],\n",
      "        [1.0000, 0.4213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [0.9999, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46536111831665 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.683933019638062 s \n",
      "\n",
      "\n",
      "\tEpisode 1393 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4594],\n",
      "        [1.0000, 0.4118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [0.9999, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447953224182129 \tStep Time:  0.005983591079711914 s \tTotal Time:  8.689916610717773 s \n",
      "\n",
      "\n",
      "\tEpisode 1394 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.4713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [0.9999, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475962162017822 \tStep Time:  0.006981611251831055 s \tTotal Time:  8.696898221969604 s \n",
      "\n",
      "\n",
      "\tEpisode 1395 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3621],\n",
      "        [0.9999, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3136],\n",
      "        [0.9999, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.679208278656006 \tStep Time:  0.004986286163330078 s \tTotal Time:  8.701884508132935 s \n",
      "\n",
      "\n",
      "\tEpisode 1396 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4692],\n",
      "        [1.0000, 0.4632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4931],\n",
      "        [0.9999, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515687465667725 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.707868337631226 s \n",
      "\n",
      "\n",
      "\tEpisode 1397 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4338],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4865],\n",
      "        [1.0000, 0.5166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543649673461914 \tStep Time:  0.006981611251831055 s \tTotal Time:  8.714849948883057 s \n",
      "\n",
      "\n",
      "\tEpisode 1398 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5042],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3846],\n",
      "        [0.9999, 0.5302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456798553466797 \tStep Time:  0.004987239837646484 s \tTotal Time:  8.719837188720703 s \n",
      "\n",
      "\n",
      "\tEpisode 1399 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5071],\n",
      "        [1.0000, 0.5224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5363],\n",
      "        [1.0000, 0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493757247924805 \tStep Time:  0.007978677749633789 s \tTotal Time:  8.727815866470337 s \n",
      "\n",
      "\n",
      "\tEpisode 1400 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5433],\n",
      "        [1.0000, 0.3999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4001],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53529179096222 \tStep Time:  0.005983591079711914 s \tTotal Time:  8.733799457550049 s \n",
      "\n",
      "\n",
      "\tEpisode 1401 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5446],\n",
      "        [1.0000, 0.3323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5542],\n",
      "        [0.9999, 0.5320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44283539056778 \tStep Time:  0.0059833526611328125 s \tTotal Time:  8.739782810211182 s \n",
      "\n",
      "\n",
      "\tEpisode 1402 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5618],\n",
      "        [1.0000, 0.4275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4737],\n",
      "        [1.0000, 0.4023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48461389541626 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.746764183044434 s \n",
      "\n",
      "\n",
      "\tEpisode 1403 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5514],\n",
      "        [1.0000, 0.4667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5446],\n",
      "        [1.0000, 0.5701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461370468139648 \tStep Time:  0.006005525588989258 s \tTotal Time:  8.752769708633423 s \n",
      "\n",
      "\n",
      "\tEpisode 1404 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6100],\n",
      "        [0.9999, 0.4941]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5548],\n",
      "        [1.0000, 0.5967]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54263162612915 \tStep Time:  0.006960868835449219 s \tTotal Time:  8.759730577468872 s \n",
      "\n",
      "\n",
      "\tEpisode 1405 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5632],\n",
      "        [1.0000, 0.5895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5801],\n",
      "        [1.0000, 0.5741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522245407104492 \tStep Time:  0.006980419158935547 s \tTotal Time:  8.766710996627808 s \n",
      "\n",
      "\n",
      "\tEpisode 1406 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5929],\n",
      "        [1.0000, 0.2139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6325],\n",
      "        [1.0000, 0.5879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.403036117553711 \tStep Time:  0.006982088088989258 s \tTotal Time:  8.773693084716797 s \n",
      "\n",
      "\n",
      "\tEpisode 1407 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6124],\n",
      "        [0.9999, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7085],\n",
      "        [0.9999, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580569744110107 \tStep Time:  0.006982088088989258 s \tTotal Time:  8.780675172805786 s \n",
      "\n",
      "\n",
      "\tEpisode 1408 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6186],\n",
      "        [1.0000, 0.3851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3989],\n",
      "        [1.0000, 0.6211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.310448169708252 \tStep Time:  0.006980180740356445 s \tTotal Time:  8.787655353546143 s \n",
      "\n",
      "\n",
      "\tEpisode 1409 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4495],\n",
      "        [1.0000, 0.2521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4517],\n",
      "        [1.0000, 0.2568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577860832214355 \tStep Time:  0.007999658584594727 s \tTotal Time:  8.795655012130737 s \n",
      "\n",
      "\n",
      "\tEpisode 1410 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5360],\n",
      "        [1.0000, 0.6591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6123],\n",
      "        [0.9999, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56389307975769 \tStep Time:  0.006960868835449219 s \tTotal Time:  8.802615880966187 s \n",
      "\n",
      "\n",
      "\tEpisode 1411 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6116],\n",
      "        [1.0000, 0.2735]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4567],\n",
      "        [1.0000, 0.4438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.386171698570251 \tStep Time:  0.008005619049072266 s \tTotal Time:  8.810621500015259 s \n",
      "\n",
      "\n",
      "\tEpisode 1412 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6144],\n",
      "        [0.9999, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6235],\n",
      "        [1.0000, 0.6316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50026798248291 \tStep Time:  0.007015705108642578 s \tTotal Time:  8.817637205123901 s \n",
      "\n",
      "\n",
      "\tEpisode 1413 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5832],\n",
      "        [1.0000, 0.6128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3931],\n",
      "        [1.0000, 0.6354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.682576179504395 \tStep Time:  0.005953073501586914 s \tTotal Time:  8.823590278625488 s \n",
      "\n",
      "\n",
      "\tEpisode 1414 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1994],\n",
      "        [1.0000, 0.5609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5921],\n",
      "        [1.0000, 0.5965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.412012100219727 \tStep Time:  0.007980108261108398 s \tTotal Time:  8.831570386886597 s \n",
      "\n",
      "\n",
      "\tEpisode 1415 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2622],\n",
      "        [1.0000, 0.5841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2900],\n",
      "        [1.0000, 0.2290]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469173431396484 \tStep Time:  0.007015228271484375 s \tTotal Time:  8.838585615158081 s \n",
      "\n",
      "\n",
      "\tEpisode 1416 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5621],\n",
      "        [1.0000, 0.5678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5768],\n",
      "        [1.0000, 0.3953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.621135711669922 \tStep Time:  0.005982637405395508 s \tTotal Time:  8.844568252563477 s \n",
      "\n",
      "\n",
      "\tEpisode 1417 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5522],\n",
      "        [0.9999, 0.4874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5550],\n",
      "        [0.9999, 0.4847]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518041133880615 \tStep Time:  0.00599360466003418 s \tTotal Time:  8.85056185722351 s \n",
      "\n",
      "\n",
      "\tEpisode 1418 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5316],\n",
      "        [1.0000, 0.5464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5035],\n",
      "        [0.9999, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52441120147705 \tStep Time:  0.007936239242553711 s \tTotal Time:  8.858498096466064 s \n",
      "\n",
      "\n",
      "\tEpisode 1419 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5464],\n",
      "        [1.0000, 0.4461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4910],\n",
      "        [1.0000, 0.5490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439320683479309 \tStep Time:  0.007978200912475586 s \tTotal Time:  8.86647629737854 s \n",
      "\n",
      "\n",
      "\tEpisode 1420 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3804],\n",
      "        [1.0000, 0.5321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5684],\n",
      "        [0.9999, 0.5261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4867525100708 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.87246036529541 s \n",
      "\n",
      "\n",
      "\tEpisode 1421 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4951],\n",
      "        [1.0000, 0.5441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4546],\n",
      "        [1.0000, 0.2185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462949275970459 \tStep Time:  0.007979869842529297 s \tTotal Time:  8.88044023513794 s \n",
      "\n",
      "\n",
      "\tEpisode 1422 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5355],\n",
      "        [1.0000, 0.3729]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4215],\n",
      "        [1.0000, 0.5753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586502075195312 \tStep Time:  0.0059871673583984375 s \tTotal Time:  8.886427402496338 s \n",
      "\n",
      "\n",
      "\tEpisode 1423 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2286],\n",
      "        [1.0000, 0.5777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5572],\n",
      "        [1.0000, 0.5599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496635437011719 \tStep Time:  0.007980823516845703 s \tTotal Time:  8.894408226013184 s \n",
      "\n",
      "\n",
      "\tEpisode 1424 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4782],\n",
      "        [1.0000, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2833],\n",
      "        [1.0000, 0.4428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424957752227783 \tStep Time:  0.007045269012451172 s \tTotal Time:  8.901453495025635 s \n",
      "\n",
      "\n",
      "\tEpisode 1425 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5692],\n",
      "        [1.0000, 0.4360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5828],\n",
      "        [1.0000, 0.5824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.658863008022308 \tStep Time:  0.0059931278228759766 s \tTotal Time:  8.90744662284851 s \n",
      "\n",
      "\n",
      "\tEpisode 1426 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2734],\n",
      "        [1.0000, 0.5914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1725],\n",
      "        [1.0000, 0.5853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66562271118164 \tStep Time:  0.006972074508666992 s \tTotal Time:  8.914418697357178 s \n",
      "\n",
      "\n",
      "\tEpisode 1427 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5749],\n",
      "        [1.0000, 0.2368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6208],\n",
      "        [1.0000, 0.5937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.780986309051514 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.920402765274048 s \n",
      "\n",
      "\n",
      "\tEpisode 1428 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5760],\n",
      "        [1.0000, 0.5921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5930],\n",
      "        [1.0000, 0.5951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584217548370361 \tStep Time:  0.007949590682983398 s \tTotal Time:  8.928352355957031 s \n",
      "\n",
      "\n",
      "\tEpisode 1429 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4075],\n",
      "        [1.0000, 0.5852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3138],\n",
      "        [1.0000, 0.4296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.687317073345184 \tStep Time:  0.005982637405395508 s \tTotal Time:  8.934334993362427 s \n",
      "\n",
      "\n",
      "\tEpisode 1430 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4512],\n",
      "        [1.0000, 0.5753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5497],\n",
      "        [0.9999, 0.5456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586969375610352 \tStep Time:  0.0059850215911865234 s \tTotal Time:  8.940320014953613 s \n",
      "\n",
      "\n",
      "\tEpisode 1431 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4855],\n",
      "        [1.0000, 0.5615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4961],\n",
      "        [1.0000, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560051441192627 \tStep Time:  0.008975028991699219 s \tTotal Time:  8.949295043945312 s \n",
      "\n",
      "\n",
      "\tEpisode 1432 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.5515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5231],\n",
      "        [1.0000, 0.4328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551686763763428 \tStep Time:  0.005984067916870117 s \tTotal Time:  8.955279111862183 s \n",
      "\n",
      "\n",
      "\tEpisode 1433 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4935],\n",
      "        [1.0000, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5363],\n",
      "        [1.0000, 0.5225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509543776512146 \tStep Time:  0.006983518600463867 s \tTotal Time:  8.962262630462646 s \n",
      "\n",
      "\n",
      "\tEpisode 1434 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4627],\n",
      "        [1.0000, 0.4625]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4402],\n",
      "        [1.0000, 0.5313]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562780380249023 \tStep Time:  0.006979465484619141 s \tTotal Time:  8.969242095947266 s \n",
      "\n",
      "\n",
      "\tEpisode 1435 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5417],\n",
      "        [1.0000, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4694],\n",
      "        [1.0000, 0.5255]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559074878692627 \tStep Time:  0.0059893131256103516 s \tTotal Time:  8.975231409072876 s \n",
      "\n",
      "\n",
      "\tEpisode 1436 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5231],\n",
      "        [1.0000, 0.5397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5459],\n",
      "        [1.0000, 0.5458]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522315979003906 \tStep Time:  0.00797891616821289 s \tTotal Time:  8.983210325241089 s \n",
      "\n",
      "\n",
      "\tEpisode 1437 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4277],\n",
      "        [0.9999, 0.5457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5377],\n",
      "        [1.0000, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467034816741943 \tStep Time:  0.005983829498291016 s \tTotal Time:  8.98919415473938 s \n",
      "\n",
      "\n",
      "\tEpisode 1438 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5455],\n",
      "        [0.9999, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4152],\n",
      "        [1.0000, 0.4037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510295391082764 \tStep Time:  0.006981372833251953 s \tTotal Time:  8.996175527572632 s \n",
      "\n",
      "\n",
      "\tEpisode 1439 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5416],\n",
      "        [1.0000, 0.5497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4410],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457602977752686 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.003156900405884 s \n",
      "\n",
      "\n",
      "\tEpisode 1440 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4688],\n",
      "        [1.0000, 0.5111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5358],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489782333374023 \tStep Time:  0.006983518600463867 s \tTotal Time:  9.010140419006348 s \n",
      "\n",
      "\n",
      "\tEpisode 1441 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3902],\n",
      "        [0.9999, 0.5437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5630],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471680283546448 \tStep Time:  0.006979227066040039 s \tTotal Time:  9.017119646072388 s \n",
      "\n",
      "\n",
      "\tEpisode 1442 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4089],\n",
      "        [1.0000, 0.4899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4343],\n",
      "        [1.0000, 0.5298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420917809009552 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.02410101890564 s \n",
      "\n",
      "\n",
      "\tEpisode 1443 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4613],\n",
      "        [1.0000, 0.4951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6255],\n",
      "        [1.0000, 0.5728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471819043159485 \tStep Time:  0.007978677749633789 s \tTotal Time:  9.032079696655273 s \n",
      "\n",
      "\n",
      "\tEpisode 1444 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5755],\n",
      "        [1.0000, 0.5755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.3719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588522911071777 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.039061069488525 s \n",
      "\n",
      "\n",
      "\tEpisode 1445 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3756],\n",
      "        [1.0000, 0.5674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3356],\n",
      "        [1.0000, 0.4243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.678425312042236 \tStep Time:  0.007979869842529297 s \tTotal Time:  9.047040939331055 s \n",
      "\n",
      "\n",
      "\tEpisode 1446 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4117],\n",
      "        [1.0000, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5872],\n",
      "        [0.9999, 0.5413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579521656036377 \tStep Time:  0.006978273391723633 s \tTotal Time:  9.054019212722778 s \n",
      "\n",
      "\n",
      "\tEpisode 1447 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3076],\n",
      "        [0.9999, 0.5518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5489],\n",
      "        [1.0000, 0.4393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.361087799072266 \tStep Time:  0.008975982666015625 s \tTotal Time:  9.062995195388794 s \n",
      "\n",
      "\n",
      "\tEpisode 1448 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5740],\n",
      "        [0.9999, 0.5460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5538],\n",
      "        [1.0000, 0.4479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581350326538086 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.068978786468506 s \n",
      "\n",
      "\n",
      "\tEpisode 1449 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4538],\n",
      "        [1.0000, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5399],\n",
      "        [1.0000, 0.5425]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541785717010498 \tStep Time:  0.00798177719116211 s \tTotal Time:  9.076960563659668 s \n",
      "\n",
      "\n",
      "\tEpisode 1450 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5335],\n",
      "        [0.9999, 0.5422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5420],\n",
      "        [1.0000, 0.5219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52219295501709 \tStep Time:  0.007977008819580078 s \tTotal Time:  9.084937572479248 s \n",
      "\n",
      "\n",
      "\tEpisode 1451 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5439],\n",
      "        [1.0000, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4106],\n",
      "        [0.9999, 0.5383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474379181861877 \tStep Time:  0.005017280578613281 s \tTotal Time:  9.089954853057861 s \n",
      "\n",
      "\n",
      "\tEpisode 1452 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5008],\n",
      "        [0.9999, 0.5389]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4799],\n",
      "        [1.0000, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552315831184387 \tStep Time:  0.006981611251831055 s \tTotal Time:  9.096936464309692 s \n",
      "\n",
      "\n",
      "\tEpisode 1453 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [0.9999, 0.5358]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5270],\n",
      "        [1.0000, 0.5426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486932754516602 \tStep Time:  0.0059528350830078125 s \tTotal Time:  9.1028892993927 s \n",
      "\n",
      "\n",
      "\tEpisode 1454 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4713],\n",
      "        [0.9999, 0.5325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5221],\n",
      "        [1.0000, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509798049926758 \tStep Time:  0.005017518997192383 s \tTotal Time:  9.107906818389893 s \n",
      "\n",
      "\n",
      "\tEpisode 1455 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4722],\n",
      "        [1.0000, 0.4427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [0.9999, 0.5219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537171363830566 \tStep Time:  0.007016181945800781 s \tTotal Time:  9.114923000335693 s \n",
      "\n",
      "\n",
      "\tEpisode 1456 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4172],\n",
      "        [0.9999, 0.5284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5217],\n",
      "        [0.9999, 0.5320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5589280128479 \tStep Time:  0.00498652458190918 s \tTotal Time:  9.119909524917603 s \n",
      "\n",
      "\n",
      "\tEpisode 1457 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4185],\n",
      "        [1.0000, 0.4532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4884],\n",
      "        [1.0000, 0.5444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462498664855957 \tStep Time:  0.005953311920166016 s \tTotal Time:  9.125862836837769 s \n",
      "\n",
      "\n",
      "\tEpisode 1458 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4660],\n",
      "        [1.0000, 0.5375]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5546],\n",
      "        [1.0000, 0.5538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478935837745667 \tStep Time:  0.007979869842529297 s \tTotal Time:  9.133842706680298 s \n",
      "\n",
      "\n",
      "\tEpisode 1459 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4793],\n",
      "        [1.0000, 0.4226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4951],\n",
      "        [1.0000, 0.4013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529413223266602 \tStep Time:  0.0059833526611328125 s \tTotal Time:  9.13982605934143 s \n",
      "\n",
      "\n",
      "\tEpisode 1460 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5601],\n",
      "        [1.0000, 0.4126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511043548583984 \tStep Time:  0.007979154586791992 s \tTotal Time:  9.147805213928223 s \n",
      "\n",
      "\n",
      "\tEpisode 1461 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.4057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4366],\n",
      "        [1.0000, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488327324390411 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.153789281845093 s \n",
      "\n",
      "\n",
      "\tEpisode 1462 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5721],\n",
      "        [1.0000, 0.4404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5679],\n",
      "        [1.0000, 0.5627]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449149250984192 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.159773588180542 s \n",
      "\n",
      "\n",
      "\tEpisode 1463 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [1.0000, 0.5761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5674],\n",
      "        [1.0000, 0.5790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477497100830078 \tStep Time:  0.0059833526611328125 s \tTotal Time:  9.165756940841675 s \n",
      "\n",
      "\n",
      "\tEpisode 1464 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5793],\n",
      "        [1.0000, 0.4205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4731],\n",
      "        [1.0000, 0.3649]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549921989440918 \tStep Time:  0.006981849670410156 s \tTotal Time:  9.172738790512085 s \n",
      "\n",
      "\n",
      "\tEpisode 1465 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4764],\n",
      "        [1.0000, 0.5800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4250],\n",
      "        [1.0000, 0.5889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495355606079102 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.179720163345337 s \n",
      "\n",
      "\n",
      "\tEpisode 1466 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5838],\n",
      "        [1.0000, 0.5878]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5793],\n",
      "        [1.0000, 0.5827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531129360198975 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.185703754425049 s \n",
      "\n",
      "\n",
      "\tEpisode 1467 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5802],\n",
      "        [1.0000, 0.4030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5279],\n",
      "        [1.0000, 0.4047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.675481498241425 \tStep Time:  0.00498652458190918 s \tTotal Time:  9.190690279006958 s \n",
      "\n",
      "\n",
      "\tEpisode 1468 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3761],\n",
      "        [1.0000, 0.4226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.4988],\n",
      "        [1.0000, 0.5573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57353687286377 \tStep Time:  0.008011341094970703 s \tTotal Time:  9.198701620101929 s \n",
      "\n",
      "\n",
      "\tEpisode 1469 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5564],\n",
      "        [1.0000, 0.5565]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5509],\n",
      "        [1.0000, 0.5493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513927936553955 \tStep Time:  0.0049860477447509766 s \tTotal Time:  9.20368766784668 s \n",
      "\n",
      "\n",
      "\tEpisode 1470 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5153],\n",
      "        [0.9999, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4405],\n",
      "        [1.0000, 0.4642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502825438976288 \tStep Time:  0.005982875823974609 s \tTotal Time:  9.209670543670654 s \n",
      "\n",
      "\n",
      "\tEpisode 1471 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4608],\n",
      "        [1.0000, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5244],\n",
      "        [0.9999, 0.5207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48898959159851 \tStep Time:  0.006989240646362305 s \tTotal Time:  9.216659784317017 s \n",
      "\n",
      "\n",
      "\tEpisode 1472 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.4934]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5343],\n",
      "        [1.0000, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546738147735596 \tStep Time:  0.0059778690338134766 s \tTotal Time:  9.22263765335083 s \n",
      "\n",
      "\n",
      "\tEpisode 1473 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5137],\n",
      "        [1.0000, 0.5012]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5183],\n",
      "        [1.0000, 0.4332]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546495139598846 \tStep Time:  0.006979703903198242 s \tTotal Time:  9.229617357254028 s \n",
      "\n",
      "\n",
      "\tEpisode 1474 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5246],\n",
      "        [1.0000, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506383419036865 \tStep Time:  0.005985736846923828 s \tTotal Time:  9.235603094100952 s \n",
      "\n",
      "\n",
      "\tEpisode 1475 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4457],\n",
      "        [1.0000, 0.4679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4860],\n",
      "        [1.0000, 0.4610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507590293884277 \tStep Time:  0.004987478256225586 s \tTotal Time:  9.240590572357178 s \n",
      "\n",
      "\n",
      "\tEpisode 1476 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4603],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3677],\n",
      "        [1.0000, 0.4384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52337396144867 \tStep Time:  0.00795888900756836 s \tTotal Time:  9.248549461364746 s \n",
      "\n",
      "\n",
      "\tEpisode 1477 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4150],\n",
      "        [1.0000, 0.4939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4016],\n",
      "        [1.0000, 0.3761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58341646194458 \tStep Time:  0.005968809127807617 s \tTotal Time:  9.254518270492554 s \n",
      "\n",
      "\n",
      "\tEpisode 1478 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4687],\n",
      "        [1.0000, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4950],\n",
      "        [1.0000, 0.4126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500640392303467 \tStep Time:  0.008977174758911133 s \tTotal Time:  9.263495445251465 s \n",
      "\n",
      "\n",
      "\tEpisode 1479 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4905],\n",
      "        [1.0000, 0.4433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483815968036652 \tStep Time:  0.0059814453125 s \tTotal Time:  9.27047610282898 s \n",
      "\n",
      "\n",
      "\tEpisode 1480 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5451],\n",
      "        [1.0000, 0.4612]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4377],\n",
      "        [1.0000, 0.3732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441150188446045 \tStep Time:  0.005985260009765625 s \tTotal Time:  9.276461362838745 s \n",
      "\n",
      "\n",
      "\tEpisode 1481 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5366],\n",
      "        [0.9999, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5638],\n",
      "        [1.0000, 0.5490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540841102600098 \tStep Time:  0.007977008819580078 s \tTotal Time:  9.284438371658325 s \n",
      "\n",
      "\n",
      "\tEpisode 1482 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5520],\n",
      "        [1.0000, 0.5534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5338],\n",
      "        [1.0000, 0.5334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520930290222168 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.290422439575195 s \n",
      "\n",
      "\n",
      "\tEpisode 1483 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5532],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5395],\n",
      "        [1.0000, 0.5546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48438549041748 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.297403812408447 s \n",
      "\n",
      "\n",
      "\tEpisode 1484 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5385],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5277],\n",
      "        [1.0000, 0.5594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528237342834473 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.303387641906738 s \n",
      "\n",
      "\n",
      "\tEpisode 1485 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5349],\n",
      "        [1.0000, 0.5567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4513],\n",
      "        [1.0000, 0.4929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538175880908966 \tStep Time:  0.005984783172607422 s \tTotal Time:  9.309372425079346 s \n",
      "\n",
      "\n",
      "\tEpisode 1486 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5160],\n",
      "        [0.9999, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5554],\n",
      "        [1.0000, 0.4809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469466805458069 \tStep Time:  0.00698089599609375 s \tTotal Time:  9.31635332107544 s \n",
      "\n",
      "\n",
      "\tEpisode 1487 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4212],\n",
      "        [0.9999, 0.5194]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3705],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416436731815338 \tStep Time:  0.006014585494995117 s \tTotal Time:  9.322367906570435 s \n",
      "\n",
      "\n",
      "\tEpisode 1488 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3736],\n",
      "        [1.0000, 0.4424]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4901],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544628739356995 \tStep Time:  0.0069828033447265625 s \tTotal Time:  9.329350709915161 s \n",
      "\n",
      "\n",
      "\tEpisode 1489 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[0.9999, 0.5175],\n",
      "        [1.0000, 0.5163]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4713],\n",
      "        [1.0000, 0.3207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595030307769775 \tStep Time:  0.00794839859008789 s \tTotal Time:  9.337299108505249 s \n",
      "\n",
      "\n",
      "\tEpisode 1490 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5498],\n",
      "        [1.0000, 0.4772]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4998],\n",
      "        [1.0000, 0.5563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499851703643799 \tStep Time:  0.006982564926147461 s \tTotal Time:  9.344281673431396 s \n",
      "\n",
      "\n",
      "\tEpisode 1491 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5619],\n",
      "        [1.0000, 0.5311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4510],\n",
      "        [1.0000, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504989504814148 \tStep Time:  0.006979465484619141 s \tTotal Time:  9.351261138916016 s \n",
      "\n",
      "\n",
      "\tEpisode 1492 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.4046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4712],\n",
      "        [1.0000, 0.5497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429296970367432 \tStep Time:  0.0069811344146728516 s \tTotal Time:  9.358242273330688 s \n",
      "\n",
      "\n",
      "\tEpisode 1493 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.4156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.5597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466966152191162 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.36522364616394 s \n",
      "\n",
      "\n",
      "\tEpisode 1494 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5575],\n",
      "        [1.0000, 0.5257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4868],\n",
      "        [1.0000, 0.5661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497409343719482 \tStep Time:  0.006983041763305664 s \tTotal Time:  9.372206687927246 s \n",
      "\n",
      "\n",
      "\tEpisode 1495 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.5352]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4048],\n",
      "        [1.0000, 0.5611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446967720985413 \tStep Time:  0.007977008819580078 s \tTotal Time:  9.380183696746826 s \n",
      "\n",
      "\n",
      "\tEpisode 1496 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5440],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5219],\n",
      "        [1.0000, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544994831085205 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.386167764663696 s \n",
      "\n",
      "\n",
      "\tEpisode 1497 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4488],\n",
      "        [1.0000, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4343],\n",
      "        [1.0000, 0.5362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43270206451416 \tStep Time:  0.006982326507568359 s \tTotal Time:  9.393150091171265 s \n",
      "\n",
      "\n",
      "\tEpisode 1498 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5382],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5100],\n",
      "        [1.0000, 0.5608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521840572357178 \tStep Time:  0.006980180740356445 s \tTotal Time:  9.400130271911621 s \n",
      "\n",
      "\n",
      "\tEpisode 1499 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.5331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5555],\n",
      "        [1.0000, 0.4314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558664321899414 \tStep Time:  0.004987001419067383 s \tTotal Time:  9.405117273330688 s \n",
      "\n",
      "\n",
      "\tEpisode 1500 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4406],\n",
      "        [1.0000, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.4893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561118364334106 \tStep Time:  0.006981849670410156 s \tTotal Time:  9.412099123001099 s \n",
      "\n",
      "\n",
      "\tEpisode 1501 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3161],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5502],\n",
      "        [1.0000, 0.5531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.628794193267822 \tStep Time:  0.0059833526611328125 s \tTotal Time:  9.418082475662231 s \n",
      "\n",
      "\n",
      "\tEpisode 1502 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4410],\n",
      "        [1.0000, 0.5512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5260],\n",
      "        [1.0000, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502517223358154 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.424066305160522 s \n",
      "\n",
      "\n",
      "\tEpisode 1503 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5331],\n",
      "        [1.0000, 0.5334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5475],\n",
      "        [1.0000, 0.5230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547414779663086 \tStep Time:  0.0069887638092041016 s \tTotal Time:  9.431055068969727 s \n",
      "\n",
      "\n",
      "\tEpisode 1504 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4180],\n",
      "        [1.0000, 0.4675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480241775512695 \tStep Time:  0.005983114242553711 s \tTotal Time:  9.43703818321228 s \n",
      "\n",
      "\n",
      "\tEpisode 1505 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2304],\n",
      "        [1.0000, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5743],\n",
      "        [1.0000, 0.4232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.336364269256592 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.44302225112915 s \n",
      "\n",
      "\n",
      "\tEpisode 1506 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5902],\n",
      "        [1.0000, 0.5903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6035],\n",
      "        [1.0000, 0.3769]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.651789665222168 \tStep Time:  0.0069811344146728516 s \tTotal Time:  9.450003385543823 s \n",
      "\n",
      "\n",
      "\tEpisode 1507 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4381],\n",
      "        [1.0000, 0.4695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3966],\n",
      "        [1.0000, 0.3892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541624069213867 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.455987453460693 s \n",
      "\n",
      "\n",
      "\tEpisode 1508 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5748],\n",
      "        [1.0000, 0.3464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4323],\n",
      "        [1.0000, 0.5397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552411556243896 \tStep Time:  0.006983757019042969 s \tTotal Time:  9.462971210479736 s \n",
      "\n",
      "\n",
      "\tEpisode 1509 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3497],\n",
      "        [1.0000, 0.5950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6049],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610631465911865 \tStep Time:  0.0059816837310791016 s \tTotal Time:  9.468952894210815 s \n",
      "\n",
      "\n",
      "\tEpisode 1510 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.5676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6115],\n",
      "        [1.0000, 0.6251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572314739227295 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.474936723709106 s \n",
      "\n",
      "\n",
      "\tEpisode 1511 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6353],\n",
      "        [1.0000, 0.6176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6372],\n",
      "        [1.0000, 0.4170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428016185760498 \tStep Time:  0.006983280181884766 s \tTotal Time:  9.481920003890991 s \n",
      "\n",
      "\n",
      "\tEpisode 1512 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6081],\n",
      "        [1.0000, 0.4310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2960],\n",
      "        [1.0000, 0.4426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.70458459854126 \tStep Time:  0.005982160568237305 s \tTotal Time:  9.487902164459229 s \n",
      "\n",
      "\n",
      "\tEpisode 1513 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5583],\n",
      "        [1.0000, 0.6192]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3466],\n",
      "        [1.0000, 0.4682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445116579532623 \tStep Time:  0.005988121032714844 s \tTotal Time:  9.493890285491943 s \n",
      "\n",
      "\n",
      "\tEpisode 1514 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3812],\n",
      "        [1.0000, 0.6588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5370],\n",
      "        [1.0000, 0.5757]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42845618724823 \tStep Time:  0.0059812068939208984 s \tTotal Time:  9.499871492385864 s \n",
      "\n",
      "\n",
      "\tEpisode 1515 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5437],\n",
      "        [1.0000, 0.6451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6874],\n",
      "        [1.0000, 0.6463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.642139911651611 \tStep Time:  0.005982637405395508 s \tTotal Time:  9.50585412979126 s \n",
      "\n",
      "\n",
      "\tEpisode 1516 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5052],\n",
      "        [1.0000, 0.4929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5923],\n",
      "        [1.0000, 0.5766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612571716308594 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.512835502624512 s \n",
      "\n",
      "\n",
      "\tEpisode 1517 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.3026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6172],\n",
      "        [1.0000, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.40766716003418 \tStep Time:  0.00598454475402832 s \tTotal Time:  9.51882004737854 s \n",
      "\n",
      "\n",
      "\tEpisode 1518 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5645],\n",
      "        [1.0000, 0.5870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5964],\n",
      "        [1.0000, 0.3639]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.407174587249756 \tStep Time:  0.0059833526611328125 s \tTotal Time:  9.524803400039673 s \n",
      "\n",
      "\n",
      "\tEpisode 1519 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6027],\n",
      "        [1.0000, 0.5955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6098],\n",
      "        [1.0000, 0.3367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.406504034996033 \tStep Time:  0.00598454475402832 s \tTotal Time:  9.530787944793701 s \n",
      "\n",
      "\n",
      "\tEpisode 1520 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3589],\n",
      "        [1.0000, 0.5491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6094],\n",
      "        [1.0000, 0.5199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584335327148438 \tStep Time:  0.007978200912475586 s \tTotal Time:  9.538766145706177 s \n",
      "\n",
      "\n",
      "\tEpisode 1521 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5978],\n",
      "        [1.0000, 0.4609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5842],\n",
      "        [1.0000, 0.5622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566102981567383 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.544750213623047 s \n",
      "\n",
      "\n",
      "\tEpisode 1522 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5420],\n",
      "        [1.0000, 0.4409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6204],\n",
      "        [1.0000, 0.5920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.594082415103912 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.550734043121338 s \n",
      "\n",
      "\n",
      "\tEpisode 1523 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4302],\n",
      "        [1.0000, 0.2327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3698],\n",
      "        [1.0000, 0.3841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469044208526611 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.556718349456787 s \n",
      "\n",
      "\n",
      "\tEpisode 1524 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5428],\n",
      "        [1.0000, 0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5182],\n",
      "        [1.0000, 0.5528]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550878047943115 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.562702417373657 s \n",
      "\n",
      "\n",
      "\tEpisode 1525 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5082],\n",
      "        [1.0000, 0.5820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5338],\n",
      "        [1.0000, 0.5892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45606279373169 \tStep Time:  0.007978439331054688 s \tTotal Time:  9.570680856704712 s \n",
      "\n",
      "\n",
      "\tEpisode 1526 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5398],\n",
      "        [1.0000, 0.3528]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5789],\n",
      "        [1.0000, 0.4636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.679244995117188 \tStep Time:  0.0059854984283447266 s \tTotal Time:  9.576666355133057 s \n",
      "\n",
      "\n",
      "\tEpisode 1527 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3674],\n",
      "        [1.0000, 0.2966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5338],\n",
      "        [1.0000, 0.4186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451284050941467 \tStep Time:  0.00698089599609375 s \tTotal Time:  9.58364725112915 s \n",
      "\n",
      "\n",
      "\tEpisode 1528 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5212],\n",
      "        [1.0000, 0.5434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5354],\n",
      "        [1.0000, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503327369689941 \tStep Time:  0.006980419158935547 s \tTotal Time:  9.590627670288086 s \n",
      "\n",
      "\n",
      "\tEpisode 1529 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4243],\n",
      "        [1.0000, 0.5443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4632],\n",
      "        [1.0000, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573360443115234 \tStep Time:  0.0059850215911865234 s \tTotal Time:  9.596612691879272 s \n",
      "\n",
      "\n",
      "\tEpisode 1530 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5401],\n",
      "        [1.0000, 0.5347]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4947],\n",
      "        [1.0000, 0.3586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449923992156982 \tStep Time:  0.005982875823974609 s \tTotal Time:  9.602595567703247 s \n",
      "\n",
      "\n",
      "\tEpisode 1531 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5324],\n",
      "        [1.0000, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3871],\n",
      "        [1.0000, 0.5111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.607141017913818 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.608579874038696 s \n",
      "\n",
      "\n",
      "\tEpisode 1532 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5196],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5263],\n",
      "        [1.0000, 0.3876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450095653533936 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.615561246871948 s \n",
      "\n",
      "\n",
      "\tEpisode 1533 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5418],\n",
      "        [1.0000, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5066],\n",
      "        [1.0000, 0.4402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515705585479736 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.62154483795166 s \n",
      "\n",
      "\n",
      "\tEpisode 1534 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4637],\n",
      "        [1.0000, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3793],\n",
      "        [1.0000, 0.4004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425673007965088 \tStep Time:  0.005985736846923828 s \tTotal Time:  9.627530574798584 s \n",
      "\n",
      "\n",
      "\tEpisode 1535 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.4573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4939],\n",
      "        [1.0000, 0.5419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472756385803223 \tStep Time:  0.006979465484619141 s \tTotal Time:  9.634510040283203 s \n",
      "\n",
      "\n",
      "\tEpisode 1536 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5712],\n",
      "        [1.0000, 0.3649]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.5217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460448741912842 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.640494108200073 s \n",
      "\n",
      "\n",
      "\tEpisode 1537 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3868],\n",
      "        [1.0000, 0.4930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5982],\n",
      "        [1.0000, 0.5604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.383017539978027 \tStep Time:  0.006982326507568359 s \tTotal Time:  9.647476434707642 s \n",
      "\n",
      "\n",
      "\tEpisode 1538 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.5513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6026],\n",
      "        [1.0000, 0.3164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433897018432617 \tStep Time:  0.0049855709075927734 s \tTotal Time:  9.652462005615234 s \n",
      "\n",
      "\n",
      "\tEpisode 1539 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5728],\n",
      "        [1.0000, 0.4765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5782],\n",
      "        [1.0000, 0.5345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581610798835754 \tStep Time:  0.00498652458190918 s \tTotal Time:  9.658446073532104 s \n",
      "\n",
      "\n",
      "\tEpisode 1540 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3325],\n",
      "        [1.0000, 0.3538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3935],\n",
      "        [1.0000, 0.6530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.431114435195923 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.665427446365356 s \n",
      "\n",
      "\n",
      "\tEpisode 1541 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5629],\n",
      "        [1.0000, 0.5856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3993],\n",
      "        [1.0000, 0.6057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59161376953125 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.671411514282227 s \n",
      "\n",
      "\n",
      "\tEpisode 1542 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5654],\n",
      "        [1.0000, 0.6864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3057],\n",
      "        [1.0000, 0.5253]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.594224452972412 \tStep Time:  0.005984783172607422 s \tTotal Time:  9.677396297454834 s \n",
      "\n",
      "\n",
      "\tEpisode 1543 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4237],\n",
      "        [1.0000, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6287],\n",
      "        [1.0000, 0.4768]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483256101608276 \tStep Time:  0.0059833526611328125 s \tTotal Time:  9.683379650115967 s \n",
      "\n",
      "\n",
      "\tEpisode 1544 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5869],\n",
      "        [1.0000, 0.4235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4065],\n",
      "        [1.0000, 0.6053]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522867202758789 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.689363479614258 s \n",
      "\n",
      "\n",
      "\tEpisode 1545 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5790],\n",
      "        [1.0000, 0.5738]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536645889282227 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.695347785949707 s \n",
      "\n",
      "\n",
      "\tEpisode 1546 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4495],\n",
      "        [1.0000, 0.5177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5397],\n",
      "        [1.0000, 0.3440]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46743631362915 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.701331377029419 s \n",
      "\n",
      "\n",
      "\tEpisode 1547 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4881],\n",
      "        [1.0000, 0.6740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3464],\n",
      "        [1.0000, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517221450805664 \tStep Time:  0.004987001419067383 s \tTotal Time:  9.706318378448486 s \n",
      "\n",
      "\n",
      "\tEpisode 1548 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3222],\n",
      "        [1.0000, 0.3920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5003],\n",
      "        [1.0000, 0.5645]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.707320213317871 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.712301969528198 s \n",
      "\n",
      "\n",
      "\tEpisode 1549 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7411],\n",
      "        [1.0000, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6249],\n",
      "        [1.0000, 0.3819]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455735206604004 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.718286275863647 s \n",
      "\n",
      "\n",
      "\tEpisode 1550 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6008],\n",
      "        [1.0000, 0.5409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.5579]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46478545665741 \tStep Time:  0.004986763000488281 s \tTotal Time:  9.723273038864136 s \n",
      "\n",
      "\n",
      "\tEpisode 1551 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5376],\n",
      "        [1.0000, 0.4968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4145],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470493793487549 \tStep Time:  0.006982564926147461 s \tTotal Time:  9.730255603790283 s \n",
      "\n",
      "\n",
      "\tEpisode 1552 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5191],\n",
      "        [1.0000, 0.6705]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5820],\n",
      "        [1.0000, 0.5808]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52214765548706 \tStep Time:  0.0068225860595703125 s \tTotal Time:  9.737078189849854 s \n",
      "\n",
      "\n",
      "\tEpisode 1553 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7140],\n",
      "        [1.0000, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.5442]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.454283237457275 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.743062496185303 s \n",
      "\n",
      "\n",
      "\tEpisode 1554 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5814],\n",
      "        [1.0000, 0.5387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5062],\n",
      "        [1.0000, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49237608909607 \tStep Time:  0.005953073501586914 s \tTotal Time:  9.74901556968689 s \n",
      "\n",
      "\n",
      "\tEpisode 1555 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6319],\n",
      "        [1.0000, 0.4785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6361],\n",
      "        [1.0000, 0.7788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.72043514251709 \tStep Time:  0.005982875823974609 s \tTotal Time:  9.754998445510864 s \n",
      "\n",
      "\n",
      "\tEpisode 1556 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5866],\n",
      "        [1.0000, 0.6114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5230],\n",
      "        [1.0000, 0.4978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436573505401611 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.761979818344116 s \n",
      "\n",
      "\n",
      "\tEpisode 1557 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5326],\n",
      "        [1.0000, 0.4482]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5387],\n",
      "        [1.0000, 0.5498]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459496021270752 \tStep Time:  0.004986763000488281 s \tTotal Time:  9.766966581344604 s \n",
      "\n",
      "\n",
      "\tEpisode 1558 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4655],\n",
      "        [1.0000, 0.4640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4379],\n",
      "        [1.0000, 0.4521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521934509277344 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.772950887680054 s \n",
      "\n",
      "\n",
      "\tEpisode 1559 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.4554]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5169],\n",
      "        [1.0000, 0.6895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595465660095215 \tStep Time:  0.009974241256713867 s \tTotal Time:  9.782925128936768 s \n",
      "\n",
      "\n",
      "\tEpisode 1560 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3792],\n",
      "        [1.0000, 0.3549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4629],\n",
      "        [1.0000, 0.3707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50243055820465 \tStep Time:  0.007977724075317383 s \tTotal Time:  9.790902853012085 s \n",
      "\n",
      "\n",
      "\tEpisode 1561 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4362],\n",
      "        [1.0000, 0.5224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5213],\n",
      "        [1.0000, 0.4407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42931354045868 \tStep Time:  0.0069811344146728516 s \tTotal Time:  9.797883987426758 s \n",
      "\n",
      "\n",
      "\tEpisode 1562 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3855],\n",
      "        [1.0000, 0.4403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6598],\n",
      "        [1.0000, 0.4457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.609031915664673 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.803868293762207 s \n",
      "\n",
      "\n",
      "\tEpisode 1563 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4611],\n",
      "        [1.0000, 0.6243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4264],\n",
      "        [1.0000, 0.5350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456573486328125 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.809851884841919 s \n",
      "\n",
      "\n",
      "\tEpisode 1564 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5448],\n",
      "        [1.0000, 0.4234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.4044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490580558776855 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.815836191177368 s \n",
      "\n",
      "\n",
      "\tEpisode 1565 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5055],\n",
      "        [1.0000, 0.5609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5247],\n",
      "        [1.0000, 0.3164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.658761978149414 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.821820259094238 s \n",
      "\n",
      "\n",
      "\tEpisode 1566 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.5632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4932],\n",
      "        [1.0000, 0.5395]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438010394573212 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.827804327011108 s \n",
      "\n",
      "\n",
      "\tEpisode 1567 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4400],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5995],\n",
      "        [1.0000, 0.6342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.374125003814697 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.83478569984436 s \n",
      "\n",
      "\n",
      "\tEpisode 1568 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4452],\n",
      "        [1.0000, 0.4384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5279],\n",
      "        [1.0000, 0.5543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49707019329071 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.840769290924072 s \n",
      "\n",
      "\n",
      "\tEpisode 1569 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5277],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5692],\n",
      "        [1.0000, 0.3725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471774101257324 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.847750663757324 s \n",
      "\n",
      "\n",
      "\tEpisode 1570 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4719],\n",
      "        [1.0000, 0.5281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4093],\n",
      "        [1.0000, 0.5667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565994203090668 \tStep Time:  0.00498652458190918 s \tTotal Time:  9.852737188339233 s \n",
      "\n",
      "\n",
      "\tEpisode 1571 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5650],\n",
      "        [1.0000, 0.6060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5023],\n",
      "        [1.0000, 0.4365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505736589431763 \tStep Time:  0.005984306335449219 s \tTotal Time:  9.858721494674683 s \n",
      "\n",
      "\n",
      "\tEpisode 1572 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6457],\n",
      "        [1.0000, 0.4227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4375],\n",
      "        [1.0000, 0.4625]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.663623809814453 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.865702629089355 s \n",
      "\n",
      "\n",
      "\tEpisode 1573 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5785],\n",
      "        [1.0000, 0.3405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4499],\n",
      "        [1.0000, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546138286590576 \tStep Time:  0.004987001419067383 s \tTotal Time:  9.870689630508423 s \n",
      "\n",
      "\n",
      "\tEpisode 1574 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4631],\n",
      "        [1.0000, 0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6178],\n",
      "        [1.0000, 0.6228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532607972621918 \tStep Time:  0.0069811344146728516 s \tTotal Time:  9.877670764923096 s \n",
      "\n",
      "\n",
      "\tEpisode 1575 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3696],\n",
      "        [1.0000, 0.3321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4693],\n",
      "        [1.0000, 0.5846]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470964312553406 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.883654832839966 s \n",
      "\n",
      "\n",
      "\tEpisode 1576 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3834],\n",
      "        [1.0000, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5658],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447288036346436 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.889638900756836 s \n",
      "\n",
      "\n",
      "\tEpisode 1577 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5361],\n",
      "        [1.0000, 0.5346]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4413],\n",
      "        [1.0000, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487169742584229 \tStep Time:  0.005984067916870117 s \tTotal Time:  9.895622968673706 s \n",
      "\n",
      "\n",
      "\tEpisode 1578 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3983],\n",
      "        [1.0000, 0.4569]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4431],\n",
      "        [1.0000, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469416379928589 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.901606559753418 s \n",
      "\n",
      "\n",
      "\tEpisode 1579 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4175],\n",
      "        [1.0000, 0.4549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6222],\n",
      "        [1.0000, 0.4496]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458353996276855 \tStep Time:  0.004986763000488281 s \tTotal Time:  9.906593322753906 s \n",
      "\n",
      "\n",
      "\tEpisode 1580 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4486],\n",
      "        [1.0000, 0.4387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4736],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524429321289062 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.913574695587158 s \n",
      "\n",
      "\n",
      "\tEpisode 1581 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4357],\n",
      "        [1.0000, 0.4369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5717],\n",
      "        [1.0000, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.400162696838379 \tStep Time:  0.004986763000488281 s \tTotal Time:  9.918561458587646 s \n",
      "\n",
      "\n",
      "\tEpisode 1582 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6148],\n",
      "        [1.0000, 0.4014]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7084],\n",
      "        [1.0000, 0.4477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520857810974121 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.924545288085938 s \n",
      "\n",
      "\n",
      "\tEpisode 1583 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3468],\n",
      "        [1.0000, 0.6118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5112],\n",
      "        [1.0000, 0.4378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.357769966125488 \tStep Time:  0.006981372833251953 s \tTotal Time:  9.93152666091919 s \n",
      "\n",
      "\n",
      "\tEpisode 1584 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6272],\n",
      "        [1.0000, 0.4260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.423447072505951 \tStep Time:  0.005994319915771484 s \tTotal Time:  9.937520980834961 s \n",
      "\n",
      "\n",
      "\tEpisode 1585 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5673],\n",
      "        [1.0000, 0.4984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.4346]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449618816375732 \tStep Time:  0.006971836090087891 s \tTotal Time:  9.944492816925049 s \n",
      "\n",
      "\n",
      "\tEpisode 1586 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3335],\n",
      "        [1.0000, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4214],\n",
      "        [1.0000, 0.3957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.629735946655273 \tStep Time:  0.005983829498291016 s \tTotal Time:  9.95047664642334 s \n",
      "\n",
      "\n",
      "\tEpisode 1587 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6737],\n",
      "        [1.0000, 0.5863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4987],\n",
      "        [1.0000, 0.6578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50475549697876 \tStep Time:  0.005983591079711914 s \tTotal Time:  9.956460237503052 s \n",
      "\n",
      "\n",
      "\tEpisode 1588 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6231],\n",
      "        [1.0000, 0.4113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7093],\n",
      "        [1.0000, 0.7189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.673611640930176 \tStep Time:  0.006982326507568359 s \tTotal Time:  9.96344256401062 s \n",
      "\n",
      "\n",
      "\tEpisode 1589 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2575],\n",
      "        [1.0000, 0.6546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6289],\n",
      "        [1.0000, 0.7045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.430144309997559 \tStep Time:  0.0059833526611328125 s \tTotal Time:  9.969425916671753 s \n",
      "\n",
      "\n",
      "\tEpisode 1590 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4959],\n",
      "        [1.0000, 0.7624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.5944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.728975176811218 \tStep Time:  0.006018400192260742 s \tTotal Time:  9.975444316864014 s \n",
      "\n",
      "\n",
      "\tEpisode 1591 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6137],\n",
      "        [1.0000, 0.5595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3932],\n",
      "        [1.0000, 0.5722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.650383114814758 \tStep Time:  0.006947755813598633 s \tTotal Time:  9.982392072677612 s \n",
      "\n",
      "\n",
      "\tEpisode 1592 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5571],\n",
      "        [1.0000, 0.3017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3794],\n",
      "        [1.0000, 0.3681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503710269927979 \tStep Time:  0.005982875823974609 s \tTotal Time:  9.988374948501587 s \n",
      "\n",
      "\n",
      "\tEpisode 1593 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3999],\n",
      "        [1.0000, 0.5737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5011],\n",
      "        [1.0000, 0.3577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.598485946655273 \tStep Time:  0.00629425048828125 s \tTotal Time:  9.994669198989868 s \n",
      "\n",
      "\n",
      "\tEpisode 1594 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4807],\n",
      "        [1.0000, 0.3816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4391],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457947015762329 \tStep Time:  0.005674600601196289 s \tTotal Time:  10.000343799591064 s \n",
      "\n",
      "\n",
      "\tEpisode 1595 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4000],\n",
      "        [1.0000, 0.5957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3419],\n",
      "        [1.0000, 0.4619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506530344486237 \tStep Time:  0.005982875823974609 s \tTotal Time:  10.006326675415039 s \n",
      "\n",
      "\n",
      "\tEpisode 1596 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.4770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4125],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57352066040039 \tStep Time:  0.006982088088989258 s \tTotal Time:  10.013308763504028 s \n",
      "\n",
      "\n",
      "\tEpisode 1597 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5396],\n",
      "        [1.0000, 0.6803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5106],\n",
      "        [1.0000, 0.5841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511134624481201 \tStep Time:  0.006018400192260742 s \tTotal Time:  10.019327163696289 s \n",
      "\n",
      "\n",
      "\tEpisode 1598 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4512],\n",
      "        [1.0000, 0.4030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6500],\n",
      "        [1.0000, 0.3724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.383888065814972 \tStep Time:  0.0059490203857421875 s \tTotal Time:  10.025276184082031 s \n",
      "\n",
      "\n",
      "\tEpisode 1599 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4342],\n",
      "        [1.0000, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4528],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555965423583984 \tStep Time:  0.0063588619232177734 s \tTotal Time:  10.031635046005249 s \n",
      "\n",
      "\n",
      "\tEpisode 1600 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4495],\n",
      "        [1.0000, 0.4146]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4210],\n",
      "        [1.0000, 0.4170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510967135429382 \tStep Time:  0.005985736846923828 s \tTotal Time:  10.037620782852173 s \n",
      "\n",
      "\n",
      "\tEpisode 1601 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5524],\n",
      "        [1.0000, 0.4561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4758],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496385097503662 \tStep Time:  0.004987239837646484 s \tTotal Time:  10.04260802268982 s \n",
      "\n",
      "\n",
      "\tEpisode 1602 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5549],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4619],\n",
      "        [1.0000, 0.4176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570764303207397 \tStep Time:  0.005985736846923828 s \tTotal Time:  10.048593759536743 s \n",
      "\n",
      "\n",
      "\tEpisode 1603 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5146],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4720],\n",
      "        [1.0000, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560950934886932 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.054577827453613 s \n",
      "\n",
      "\n",
      "\tEpisode 1604 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6764],\n",
      "        [1.0000, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445779800415039 \tStep Time:  0.006076812744140625 s \tTotal Time:  10.060654640197754 s \n",
      "\n",
      "\n",
      "\tEpisode 1605 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5449],\n",
      "        [1.0000, 0.6545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5681],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52166485786438 \tStep Time:  0.005929470062255859 s \tTotal Time:  10.06658411026001 s \n",
      "\n",
      "\n",
      "\tEpisode 1606 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5643],\n",
      "        [1.0000, 0.5635]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4886],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513674259185791 \tStep Time:  0.004949092864990234 s \tTotal Time:  10.071533203125 s \n",
      "\n",
      "\n",
      "\tEpisode 1607 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4625],\n",
      "        [1.0000, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5086],\n",
      "        [1.0000, 0.5964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57220458984375 \tStep Time:  0.0069806575775146484 s \tTotal Time:  10.078513860702515 s \n",
      "\n",
      "\n",
      "\tEpisode 1608 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.5550]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4971],\n",
      "        [1.0000, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467808723449707 \tStep Time:  0.00601506233215332 s \tTotal Time:  10.084528923034668 s \n",
      "\n",
      "\n",
      "\tEpisode 1609 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5177],\n",
      "        [1.0000, 0.4970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4532],\n",
      "        [1.0000, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545156002044678 \tStep Time:  0.004987001419067383 s \tTotal Time:  10.089515924453735 s \n",
      "\n",
      "\n",
      "\tEpisode 1610 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5201],\n",
      "        [1.0000, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4956],\n",
      "        [1.0000, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502289772033691 \tStep Time:  0.005952358245849609 s \tTotal Time:  10.095468282699585 s \n",
      "\n",
      "\n",
      "\tEpisode 1611 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4903],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4914],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488941669464111 \tStep Time:  0.006016969680786133 s \tTotal Time:  10.101485252380371 s \n",
      "\n",
      "\n",
      "\tEpisode 1612 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5021],\n",
      "        [1.0000, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522966623306274 \tStep Time:  0.004987001419067383 s \tTotal Time:  10.106472253799438 s \n",
      "\n",
      "\n",
      "\tEpisode 1613 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5072],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.5259]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51175606250763 \tStep Time:  0.006947994232177734 s \tTotal Time:  10.113420248031616 s \n",
      "\n",
      "\n",
      "\tEpisode 1614 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4947],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6168],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568589210510254 \tStep Time:  0.004987239837646484 s \tTotal Time:  10.118407487869263 s \n",
      "\n",
      "\n",
      "\tEpisode 1615 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5224],\n",
      "        [1.0000, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515345871448517 \tStep Time:  0.005983591079711914 s \tTotal Time:  10.124391078948975 s \n",
      "\n",
      "\n",
      "\tEpisode 1616 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5181],\n",
      "        [1.0000, 0.4868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5205],\n",
      "        [1.0000, 0.4847]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545430183410645 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.130374908447266 s \n",
      "\n",
      "\n",
      "\tEpisode 1617 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.4734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.4734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500290870666504 \tStep Time:  0.006016254425048828 s \tTotal Time:  10.136391162872314 s \n",
      "\n",
      "\n",
      "\tEpisode 1618 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.4711]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4721],\n",
      "        [1.0000, 0.4597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523364067077637 \tStep Time:  0.005952358245849609 s \tTotal Time:  10.142343521118164 s \n",
      "\n",
      "\n",
      "\tEpisode 1619 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5294],\n",
      "        [1.0000, 0.4538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6232],\n",
      "        [1.0000, 0.4829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48218858242035 \tStep Time:  0.0059967041015625 s \tTotal Time:  10.149357318878174 s \n",
      "\n",
      "\n",
      "\tEpisode 1620 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4825],\n",
      "        [1.0000, 0.4775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4657],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4999098777771 \tStep Time:  0.005952596664428711 s \tTotal Time:  10.155309915542603 s \n",
      "\n",
      "\n",
      "\tEpisode 1621 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4785],\n",
      "        [1.0000, 0.4902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.5813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576302587985992 \tStep Time:  0.007977008819580078 s \tTotal Time:  10.163286924362183 s \n",
      "\n",
      "\n",
      "\tEpisode 1622 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5335],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504772186279297 \tStep Time:  0.006981849670410156 s \tTotal Time:  10.170268774032593 s \n",
      "\n",
      "\n",
      "\tEpisode 1623 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4793],\n",
      "        [1.0000, 0.4778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4744],\n",
      "        [1.0000, 0.4724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506800532341003 \tStep Time:  0.00698089599609375 s \tTotal Time:  10.177249670028687 s \n",
      "\n",
      "\n",
      "\tEpisode 1624 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4749],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4858],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518774390220642 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.184231519699097 s \n",
      "\n",
      "\n",
      "\tEpisode 1625 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5002],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4929],\n",
      "        [1.0000, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501650512218475 \tStep Time:  0.007978200912475586 s \tTotal Time:  10.192209720611572 s \n",
      "\n",
      "\n",
      "\tEpisode 1626 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4127],\n",
      "        [1.0000, 0.4804]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544906735420227 \tStep Time:  0.0069811344146728516 s \tTotal Time:  10.199190855026245 s \n",
      "\n",
      "\n",
      "\tEpisode 1627 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4693],\n",
      "        [1.0000, 0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4777],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520382404327393 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.205174684524536 s \n",
      "\n",
      "\n",
      "\tEpisode 1628 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4885],\n",
      "        [1.0000, 0.4768]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4927],\n",
      "        [1.0000, 0.4797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510393142700195 \tStep Time:  0.006981372833251953 s \tTotal Time:  10.212156057357788 s \n",
      "\n",
      "\n",
      "\tEpisode 1629 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4748],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4885],\n",
      "        [1.0000, 0.4924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49289321899414 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.218140125274658 s \n",
      "\n",
      "\n",
      "\tEpisode 1630 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5004],\n",
      "        [1.0000, 0.4815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4899],\n",
      "        [1.0000, 0.4894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511293411254883 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.224124431610107 s \n",
      "\n",
      "\n",
      "\tEpisode 1631 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4484],\n",
      "        [1.0000, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511332035064697 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.230108261108398 s \n",
      "\n",
      "\n",
      "\tEpisode 1632 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5010],\n",
      "        [1.0000, 0.5005]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.4831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496690273284912 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.23609209060669 s \n",
      "\n",
      "\n",
      "\tEpisode 1633 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4873],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5183],\n",
      "        [1.0000, 0.7229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.652507305145264 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.24207615852356 s \n",
      "\n",
      "\n",
      "\tEpisode 1634 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5118],\n",
      "        [1.0000, 0.4956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5108],\n",
      "        [1.0000, 0.4893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52116572856903 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.24806022644043 s \n",
      "\n",
      "\n",
      "\tEpisode 1635 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5098],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5050],\n",
      "        [1.0000, 0.4622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481640338897705 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.25404405593872 s \n",
      "\n",
      "\n",
      "\tEpisode 1636 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4668],\n",
      "        [1.0000, 0.4956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490932941436768 \tStep Time:  0.006577253341674805 s \tTotal Time:  10.260621309280396 s \n",
      "\n",
      "\n",
      "\tEpisode 1637 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [1.0000, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [1.0000, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510152816772461 \tStep Time:  0.00498652458190918 s \tTotal Time:  10.266012191772461 s \n",
      "\n",
      "\n",
      "\tEpisode 1638 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4968],\n",
      "        [1.0000, 0.4860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49237585067749 \tStep Time:  0.00598454475402832 s \tTotal Time:  10.27199673652649 s \n",
      "\n",
      "\n",
      "\tEpisode 1639 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4600],\n",
      "        [1.0000, 0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5022],\n",
      "        [1.0000, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515219449996948 \tStep Time:  0.00598454475402832 s \tTotal Time:  10.278978109359741 s \n",
      "\n",
      "\n",
      "\tEpisode 1640 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4146],\n",
      "        [1.0000, 0.4818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5048],\n",
      "        [1.0000, 0.5224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571450233459473 \tStep Time:  0.0059833526611328125 s \tTotal Time:  10.284961462020874 s \n",
      "\n",
      "\n",
      "\tEpisode 1641 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.5677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473158359527588 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.290945529937744 s \n",
      "\n",
      "\n",
      "\tEpisode 1642 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4981],\n",
      "        [1.0000, 0.5400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.4985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52370834350586 \tStep Time:  0.006981372833251953 s \tTotal Time:  10.297926902770996 s \n",
      "\n",
      "\n",
      "\tEpisode 1643 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4998],\n",
      "        [1.0000, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.4435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529712557792664 \tStep Time:  0.00498652458190918 s \tTotal Time:  10.302913427352905 s \n",
      "\n",
      "\n",
      "\tEpisode 1644 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.4731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520946025848389 \tStep Time:  0.005983591079711914 s \tTotal Time:  10.308897018432617 s \n",
      "\n",
      "\n",
      "\tEpisode 1645 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4723],\n",
      "        [1.0000, 0.4902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5016],\n",
      "        [1.0000, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48470163345337 \tStep Time:  0.006981849670410156 s \tTotal Time:  10.315878868103027 s \n",
      "\n",
      "\n",
      "\tEpisode 1646 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4476],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50673246383667 \tStep Time:  0.004986763000488281 s \tTotal Time:  10.320865631103516 s \n",
      "\n",
      "\n",
      "\tEpisode 1647 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5279],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4623],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48297268152237 \tStep Time:  0.006981372833251953 s \tTotal Time:  10.327847003936768 s \n",
      "\n",
      "\n",
      "\tEpisode 1648 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5338],\n",
      "        [1.0000, 0.4733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530278205871582 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.333830833435059 s \n",
      "\n",
      "\n",
      "\tEpisode 1649 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4405],\n",
      "        [1.0000, 0.4444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4974],\n",
      "        [1.0000, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502954959869385 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.339814901351929 s \n",
      "\n",
      "\n",
      "\tEpisode 1650 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.4434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4891],\n",
      "        [1.0000, 0.4959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458416938781738 \tStep Time:  0.00701451301574707 s \tTotal Time:  10.346829414367676 s \n",
      "\n",
      "\n",
      "\tEpisode 1651 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4532],\n",
      "        [1.0000, 0.4338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529009103775024 \tStep Time:  0.005962848663330078 s \tTotal Time:  10.352792263031006 s \n",
      "\n",
      "\n",
      "\tEpisode 1652 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5386],\n",
      "        [1.0000, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4325],\n",
      "        [1.0000, 0.5310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575473368167877 \tStep Time:  0.006007194519042969 s \tTotal Time:  10.358799457550049 s \n",
      "\n",
      "\n",
      "\tEpisode 1653 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4997],\n",
      "        [1.0000, 0.3982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4057],\n",
      "        [1.0000, 0.4467]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443634152412415 \tStep Time:  0.006946563720703125 s \tTotal Time:  10.365746021270752 s \n",
      "\n",
      "\n",
      "\tEpisode 1654 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4042],\n",
      "        [1.0000, 0.5211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449374735355377 \tStep Time:  0.005019187927246094 s \tTotal Time:  10.370765209197998 s \n",
      "\n",
      "\n",
      "\tEpisode 1655 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5268],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483599781990051 \tStep Time:  0.006647825241088867 s \tTotal Time:  10.377413034439087 s \n",
      "\n",
      "\n",
      "\tEpisode 1656 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5218],\n",
      "        [1.0000, 0.4976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5155],\n",
      "        [1.0000, 0.5045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499432563781738 \tStep Time:  0.006014823913574219 s \tTotal Time:  10.383729696273804 s \n",
      "\n",
      "\n",
      "\tEpisode 1657 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5256],\n",
      "        [1.0000, 0.5288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3389],\n",
      "        [1.0000, 0.5095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62109661102295 \tStep Time:  0.005983114242553711 s \tTotal Time:  10.389712810516357 s \n",
      "\n",
      "\n",
      "\tEpisode 1658 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4010],\n",
      "        [1.0000, 0.3525]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5161],\n",
      "        [1.0000, 0.4870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529110670089722 \tStep Time:  0.00499415397644043 s \tTotal Time:  10.394706964492798 s \n",
      "\n",
      "\n",
      "\tEpisode 1659 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [1.0000, 0.4939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5334],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55920934677124 \tStep Time:  0.004987239837646484 s \tTotal Time:  10.400684356689453 s \n",
      "\n",
      "\n",
      "\tEpisode 1660 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5372],\n",
      "        [1.0000, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5322],\n",
      "        [1.0000, 0.5161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508707046508789 \tStep Time:  0.005952358245849609 s \tTotal Time:  10.406636714935303 s \n",
      "\n",
      "\n",
      "\tEpisode 1661 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5282],\n",
      "        [1.0000, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5279],\n",
      "        [1.0000, 0.5208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503202676773071 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.412620782852173 s \n",
      "\n",
      "\n",
      "\tEpisode 1662 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5062],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515795230865479 \tStep Time:  0.006015300750732422 s \tTotal Time:  10.418636083602905 s \n",
      "\n",
      "\n",
      "\tEpisode 1663 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5268],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5236],\n",
      "        [1.0000, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499346256256104 \tStep Time:  0.005994081497192383 s \tTotal Time:  10.424630165100098 s \n",
      "\n",
      "\n",
      "\tEpisode 1664 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.4036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.4129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526586055755615 \tStep Time:  0.005942583084106445 s \tTotal Time:  10.430572748184204 s \n",
      "\n",
      "\n",
      "\tEpisode 1665 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5308],\n",
      "        [1.0000, 0.4731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5227],\n",
      "        [1.0000, 0.5284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5362788438797 \tStep Time:  0.00601649284362793 s \tTotal Time:  10.436589241027832 s \n",
      "\n",
      "\n",
      "\tEpisode 1666 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4558],\n",
      "        [1.0000, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4889],\n",
      "        [1.0000, 0.4640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478822708129883 \tStep Time:  0.005983114242553711 s \tTotal Time:  10.442572355270386 s \n",
      "\n",
      "\n",
      "\tEpisode 1667 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5482],\n",
      "        [1.0000, 0.5339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5094],\n",
      "        [1.0000, 0.5344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524396300315857 \tStep Time:  0.005983114242553711 s \tTotal Time:  10.44855546951294 s \n",
      "\n",
      "\n",
      "\tEpisode 1668 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4999],\n",
      "        [1.0000, 0.4501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.5476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550692081451416 \tStep Time:  0.005991935729980469 s \tTotal Time:  10.45454740524292 s \n",
      "\n",
      "\n",
      "\tEpisode 1669 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4433],\n",
      "        [1.0000, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5294],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531059265136719 \tStep Time:  0.005944728851318359 s \tTotal Time:  10.460492134094238 s \n",
      "\n",
      "\n",
      "\tEpisode 1670 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5128],\n",
      "        [1.0000, 0.4998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5275],\n",
      "        [1.0000, 0.5135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504138708114624 \tStep Time:  0.007980108261108398 s \tTotal Time:  10.468472242355347 s \n",
      "\n",
      "\n",
      "\tEpisode 1671 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5217],\n",
      "        [1.0000, 0.3864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436222672462463 \tStep Time:  0.007980585098266602 s \tTotal Time:  10.477450132369995 s \n",
      "\n",
      "\n",
      "\tEpisode 1672 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4890],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3805],\n",
      "        [1.0000, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579441547393799 \tStep Time:  0.006986379623413086 s \tTotal Time:  10.484436511993408 s \n",
      "\n",
      "\n",
      "\tEpisode 1673 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4667],\n",
      "        [1.0000, 0.4507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4475],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543584823608398 \tStep Time:  0.005977153778076172 s \tTotal Time:  10.490413665771484 s \n",
      "\n",
      "\n",
      "\tEpisode 1674 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5361],\n",
      "        [1.0000, 0.5166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5321],\n",
      "        [1.0000, 0.5306]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520601749420166 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.496397733688354 s \n",
      "\n",
      "\n",
      "\tEpisode 1675 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4544],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5405],\n",
      "        [1.0000, 0.4631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512451767921448 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.502382040023804 s \n",
      "\n",
      "\n",
      "\tEpisode 1676 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5237],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4723],\n",
      "        [1.0000, 0.4392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492990970611572 \tStep Time:  0.005983591079711914 s \tTotal Time:  10.508365631103516 s \n",
      "\n",
      "\n",
      "\tEpisode 1677 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5255],\n",
      "        [1.0000, 0.4686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5210],\n",
      "        [1.0000, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505946636199951 \tStep Time:  0.0059893131256103516 s \tTotal Time:  10.514354944229126 s \n",
      "\n",
      "\n",
      "\tEpisode 1678 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5183],\n",
      "        [1.0000, 0.5282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514803230762482 \tStep Time:  0.005982875823974609 s \tTotal Time:  10.5203378200531 s \n",
      "\n",
      "\n",
      "\tEpisode 1679 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4730],\n",
      "        [1.0000, 0.4775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.4699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517179489135742 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.52632188796997 s \n",
      "\n",
      "\n",
      "\tEpisode 1680 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.4851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5195],\n",
      "        [1.0000, 0.5178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520679354667664 \tStep Time:  0.005989551544189453 s \tTotal Time:  10.53231143951416 s \n",
      "\n",
      "\n",
      "\tEpisode 1681 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5164],\n",
      "        [1.0000, 0.5207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5060],\n",
      "        [1.0000, 0.4530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540715396404266 \tStep Time:  0.0059833526611328125 s \tTotal Time:  10.538294792175293 s \n",
      "\n",
      "\n",
      "\tEpisode 1682 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4059],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5200],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567463397979736 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.544278621673584 s \n",
      "\n",
      "\n",
      "\tEpisode 1683 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.5373]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5384],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51039981842041 \tStep Time:  0.007017612457275391 s \tTotal Time:  10.55129623413086 s \n",
      "\n",
      "\n",
      "\tEpisode 1684 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5465],\n",
      "        [1.0000, 0.4136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4696],\n",
      "        [1.0000, 0.5560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479844570159912 \tStep Time:  0.004949331283569336 s \tTotal Time:  10.556245565414429 s \n",
      "\n",
      "\n",
      "\tEpisode 1685 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4843],\n",
      "        [1.0000, 0.5239]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5242],\n",
      "        [1.0000, 0.5432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51388144493103 \tStep Time:  0.0070095062255859375 s \tTotal Time:  10.563255071640015 s \n",
      "\n",
      "\n",
      "\tEpisode 1686 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5378],\n",
      "        [1.0000, 0.6255]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61692762374878 \tStep Time:  0.005955219268798828 s \tTotal Time:  10.569210290908813 s \n",
      "\n",
      "\n",
      "\tEpisode 1687 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5457],\n",
      "        [1.0000, 0.5482]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4400],\n",
      "        [1.0000, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538200378417969 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.575194358825684 s \n",
      "\n",
      "\n",
      "\tEpisode 1688 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4107],\n",
      "        [1.0000, 0.5534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5369],\n",
      "        [1.0000, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586293756961823 \tStep Time:  0.0069811344146728516 s \tTotal Time:  10.582175493240356 s \n",
      "\n",
      "\n",
      "\tEpisode 1689 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5291],\n",
      "        [1.0000, 0.5091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50628411769867 \tStep Time:  0.0070154666900634766 s \tTotal Time:  10.58919095993042 s \n",
      "\n",
      "\n",
      "\tEpisode 1690 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5076],\n",
      "        [1.0000, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492724001407623 \tStep Time:  0.006947994232177734 s \tTotal Time:  10.596138954162598 s \n",
      "\n",
      "\n",
      "\tEpisode 1691 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5112],\n",
      "        [1.0000, 0.4951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507473945617676 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.602123022079468 s \n",
      "\n",
      "\n",
      "\tEpisode 1692 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5112],\n",
      "        [1.0000, 0.4693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537625551223755 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.608106851577759 s \n",
      "\n",
      "\n",
      "\tEpisode 1693 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.4051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5255],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506040573120117 \tStep Time:  0.006020069122314453 s \tTotal Time:  10.614126920700073 s \n",
      "\n",
      "\n",
      "\tEpisode 1694 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500584840774536 \tStep Time:  0.005979061126708984 s \tTotal Time:  10.620105981826782 s \n",
      "\n",
      "\n",
      "\tEpisode 1695 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5268],\n",
      "        [1.0000, 0.5314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4464],\n",
      "        [1.0000, 0.4263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51965218782425 \tStep Time:  0.00498652458190918 s \tTotal Time:  10.625092506408691 s \n",
      "\n",
      "\n",
      "\tEpisode 1696 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4891],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.3675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474568367004395 \tStep Time:  0.006949901580810547 s \tTotal Time:  10.632042407989502 s \n",
      "\n",
      "\n",
      "\tEpisode 1697 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.5371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3080],\n",
      "        [1.0000, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.630077838897705 \tStep Time:  0.0060198307037353516 s \tTotal Time:  10.638062238693237 s \n",
      "\n",
      "\n",
      "\tEpisode 1698 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5132],\n",
      "        [1.0000, 0.4279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5218],\n",
      "        [1.0000, 0.4220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49945205450058 \tStep Time:  0.004950761795043945 s \tTotal Time:  10.643013000488281 s \n",
      "\n",
      "\n",
      "\tEpisode 1699 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4981],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4957],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489133358001709 \tStep Time:  0.0060155391693115234 s \tTotal Time:  10.649028539657593 s \n",
      "\n",
      "\n",
      "\tEpisode 1700 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4977],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4796],\n",
      "        [1.0000, 0.4993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51413631439209 \tStep Time:  0.004987001419067383 s \tTotal Time:  10.655012607574463 s \n",
      "\n",
      "\n",
      "\tEpisode 1701 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5003],\n",
      "        [1.0000, 0.4727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4654],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503765106201172 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.660996675491333 s \n",
      "\n",
      "\n",
      "\tEpisode 1702 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4790],\n",
      "        [1.0000, 0.4808]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501387238502502 \tStep Time:  0.00598597526550293 s \tTotal Time:  10.666982650756836 s \n",
      "\n",
      "\n",
      "\tEpisode 1703 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3214],\n",
      "        [1.0000, 0.3745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4344],\n",
      "        [1.0000, 0.4547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436574459075928 \tStep Time:  0.00498652458190918 s \tTotal Time:  10.671969175338745 s \n",
      "\n",
      "\n",
      "\tEpisode 1704 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5151],\n",
      "        [1.0000, 0.4768]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5357],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55010461807251 \tStep Time:  0.005951404571533203 s \tTotal Time:  10.677920579910278 s \n",
      "\n",
      "\n",
      "\tEpisode 1705 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5060],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5581],\n",
      "        [1.0000, 0.4228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504687786102295 \tStep Time:  0.0069811344146728516 s \tTotal Time:  10.684901714324951 s \n",
      "\n",
      "\n",
      "\tEpisode 1706 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3589],\n",
      "        [1.0000, 0.2896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2902],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.641021251678467 \tStep Time:  0.0059833526611328125 s \tTotal Time:  10.690885066986084 s \n",
      "\n",
      "\n",
      "\tEpisode 1707 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5830],\n",
      "        [1.0000, 0.4686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5876],\n",
      "        [1.0000, 0.5551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497087478637695 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.696869134902954 s \n",
      "\n",
      "\n",
      "\tEpisode 1708 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5896],\n",
      "        [1.0000, 0.5811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4203],\n",
      "        [1.0000, 0.3536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492077827453613 \tStep Time:  0.005988359451293945 s \tTotal Time:  10.702857494354248 s \n",
      "\n",
      "\n",
      "\tEpisode 1709 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5829],\n",
      "        [1.0000, 0.4088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6005],\n",
      "        [1.0000, 0.2501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.672431588172913 \tStep Time:  0.0049822330474853516 s \tTotal Time:  10.707839727401733 s \n",
      "\n",
      "\n",
      "\tEpisode 1710 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6025],\n",
      "        [1.0000, 0.4063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5955],\n",
      "        [1.0000, 0.5890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422918796539307 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.713824033737183 s \n",
      "\n",
      "\n",
      "\tEpisode 1711 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3125],\n",
      "        [1.0000, 0.6008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3255],\n",
      "        [1.0000, 0.5521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.305381298065186 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.719808101654053 s \n",
      "\n",
      "\n",
      "\tEpisode 1712 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5974],\n",
      "        [1.0000, 0.4280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5987],\n",
      "        [1.0000, 0.3247]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.596847534179688 \tStep Time:  0.00498652458190918 s \tTotal Time:  10.724794626235962 s \n",
      "\n",
      "\n",
      "\tEpisode 1713 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5539],\n",
      "        [1.0000, 0.6566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2475],\n",
      "        [1.0000, 0.6366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.725941181182861 \tStep Time:  0.006981849670410156 s \tTotal Time:  10.731776475906372 s \n",
      "\n",
      "\n",
      "\tEpisode 1714 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5950],\n",
      "        [1.0000, 0.6479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6055],\n",
      "        [1.0000, 0.6114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571780681610107 \tStep Time:  0.0059833526611328125 s \tTotal Time:  10.737759828567505 s \n",
      "\n",
      "\n",
      "\tEpisode 1715 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2678],\n",
      "        [1.0000, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6098],\n",
      "        [1.0000, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521250188350677 \tStep Time:  0.004986763000488281 s \tTotal Time:  10.742746591567993 s \n",
      "\n",
      "\n",
      "\tEpisode 1716 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6060],\n",
      "        [1.0000, 0.4475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7029],\n",
      "        [1.0000, 0.4307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.342212677001953 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.748730659484863 s \n",
      "\n",
      "\n",
      "\tEpisode 1717 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5410],\n",
      "        [1.0000, 0.5831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4519],\n",
      "        [1.0000, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.629939079284668 \tStep Time:  0.006981372833251953 s \tTotal Time:  10.755712032318115 s \n",
      "\n",
      "\n",
      "\tEpisode 1718 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4136],\n",
      "        [1.0000, 0.4643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2962],\n",
      "        [1.0000, 0.2232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.763393878936768 \tStep Time:  0.005984067916870117 s \tTotal Time:  10.761696100234985 s \n",
      "\n",
      "\n",
      "\tEpisode 1719 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5512],\n",
      "        [1.0000, 0.4695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5505],\n",
      "        [1.0000, 0.6680]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564798831939697 \tStep Time:  0.006982564926147461 s \tTotal Time:  10.768678665161133 s \n",
      "\n",
      "\n",
      "\tEpisode 1720 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6006],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5444],\n",
      "        [1.0000, 0.3959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478829860687256 \tStep Time:  0.004986286163330078 s \tTotal Time:  10.773664951324463 s \n",
      "\n",
      "\n",
      "\tEpisode 1721 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6147],\n",
      "        [1.0000, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6471],\n",
      "        [1.0000, 0.3801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60939610004425 \tStep Time:  0.005983114242553711 s \tTotal Time:  10.780645847320557 s \n",
      "\n",
      "\n",
      "\tEpisode 1722 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3377],\n",
      "        [1.0000, 0.3397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5998],\n",
      "        [1.0000, 0.3810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.68202555179596 \tStep Time:  0.006982088088989258 s \tTotal Time:  10.787627935409546 s \n",
      "\n",
      "\n",
      "\tEpisode 1723 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5749],\n",
      "        [1.0000, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5663],\n",
      "        [1.0000, 0.5164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52124547958374 \tStep Time:  0.006980419158935547 s \tTotal Time:  10.794608354568481 s \n",
      "\n",
      "\n",
      "\tEpisode 1724 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5189],\n",
      "        [1.0000, 0.4823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5770],\n",
      "        [1.0000, 0.5715]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464898109436035 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.800592184066772 s \n",
      "\n",
      "\n",
      "\tEpisode 1725 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5877],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5756],\n",
      "        [1.0000, 0.5400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528147459030151 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.806576490402222 s \n",
      "\n",
      "\n",
      "\tEpisode 1726 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5779],\n",
      "        [1.0000, 0.5523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5719],\n",
      "        [1.0000, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571782052516937 \tStep Time:  0.00698089599609375 s \tTotal Time:  10.813557386398315 s \n",
      "\n",
      "\n",
      "\tEpisode 1727 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5454],\n",
      "        [1.0000, 0.5591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.5360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511531352996826 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.819541215896606 s \n",
      "\n",
      "\n",
      "\tEpisode 1728 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5520],\n",
      "        [1.0000, 0.5588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5567],\n",
      "        [1.0000, 0.5514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518939018249512 \tStep Time:  0.004987001419067383 s \tTotal Time:  10.824528217315674 s \n",
      "\n",
      "\n",
      "\tEpisode 1729 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5399],\n",
      "        [1.0000, 0.5546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524716854095459 \tStep Time:  0.00697016716003418 s \tTotal Time:  10.831498384475708 s \n",
      "\n",
      "\n",
      "\tEpisode 1730 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5406],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.5323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502467274665833 \tStep Time:  0.006021022796630859 s \tTotal Time:  10.837519407272339 s \n",
      "\n",
      "\n",
      "\tEpisode 1731 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5264],\n",
      "        [1.0000, 0.5288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515476107597351 \tStep Time:  0.005950450897216797 s \tTotal Time:  10.843469858169556 s \n",
      "\n",
      "\n",
      "\tEpisode 1732 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5230],\n",
      "        [1.0000, 0.5130]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506357192993164 \tStep Time:  0.006064414978027344 s \tTotal Time:  10.849534273147583 s \n",
      "\n",
      "\n",
      "\tEpisode 1733 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5182],\n",
      "        [1.0000, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504910469055176 \tStep Time:  0.0050199031829833984 s \tTotal Time:  10.854554176330566 s \n",
      "\n",
      "\n",
      "\tEpisode 1734 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4862],\n",
      "        [1.0000, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5479],\n",
      "        [1.0000, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480028629302979 \tStep Time:  0.005985260009765625 s \tTotal Time:  10.860539436340332 s \n",
      "\n",
      "\n",
      "\tEpisode 1735 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5410],\n",
      "        [1.0000, 0.4235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.5036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49668025970459 \tStep Time:  0.0059511661529541016 s \tTotal Time:  10.866490602493286 s \n",
      "\n",
      "\n",
      "\tEpisode 1736 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4761],\n",
      "        [1.0000, 0.4559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.4870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524457335472107 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.872474908828735 s \n",
      "\n",
      "\n",
      "\tEpisode 1737 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4623],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4638],\n",
      "        [1.0000, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549744963645935 \tStep Time:  0.006016254425048828 s \tTotal Time:  10.878491163253784 s \n",
      "\n",
      "\n",
      "\tEpisode 1738 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.3982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [1.0000, 0.4866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571783304214478 \tStep Time:  0.005951642990112305 s \tTotal Time:  10.884442806243896 s \n",
      "\n",
      "\n",
      "\tEpisode 1739 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4735],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4786],\n",
      "        [1.0000, 0.4806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4984250664711 \tStep Time:  0.006015300750732422 s \tTotal Time:  10.890458106994629 s \n",
      "\n",
      "\n",
      "\tEpisode 1740 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4897],\n",
      "        [1.0000, 0.4548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4347],\n",
      "        [1.0000, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525374412536621 \tStep Time:  0.005986928939819336 s \tTotal Time:  10.896445035934448 s \n",
      "\n",
      "\n",
      "\tEpisode 1741 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4728],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.4496]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488810539245605 \tStep Time:  0.0059812068939208984 s \tTotal Time:  10.90242624282837 s \n",
      "\n",
      "\n",
      "\tEpisode 1742 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.4604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.4509]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508934020996094 \tStep Time:  0.0049896240234375 s \tTotal Time:  10.907415866851807 s \n",
      "\n",
      "\n",
      "\tEpisode 1743 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4298],\n",
      "        [1.0000, 0.4785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.4453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461056232452393 \tStep Time:  0.005980968475341797 s \tTotal Time:  10.913396835327148 s \n",
      "\n",
      "\n",
      "\tEpisode 1744 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.4844]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4884],\n",
      "        [1.0000, 0.4356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472204208374023 \tStep Time:  0.005983829498291016 s \tTotal Time:  10.91938066482544 s \n",
      "\n",
      "\n",
      "\tEpisode 1745 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5099],\n",
      "        [1.0000, 0.4822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4961],\n",
      "        [1.0000, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498866558074951 \tStep Time:  0.005986213684082031 s \tTotal Time:  10.925366878509521 s \n",
      "\n",
      "\n",
      "\tEpisode 1746 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4670],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4794],\n",
      "        [1.0000, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51267957687378 \tStep Time:  0.0059511661529541016 s \tTotal Time:  10.931318044662476 s \n",
      "\n",
      "\n",
      "\tEpisode 1747 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4676],\n",
      "        [1.0000, 0.3894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5300],\n",
      "        [1.0000, 0.3888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484387397766113 \tStep Time:  0.005983591079711914 s \tTotal Time:  10.937301635742188 s \n",
      "\n",
      "\n",
      "\tEpisode 1748 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5385],\n",
      "        [1.0000, 0.4821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4801],\n",
      "        [1.0000, 0.4666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539737701416016 \tStep Time:  0.005984783172607422 s \tTotal Time:  10.943286418914795 s \n",
      "\n",
      "\n",
      "\tEpisode 1749 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5484],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.5548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44473671913147 \tStep Time:  0.005017518997192383 s \tTotal Time:  10.948303937911987 s \n",
      "\n",
      "\n",
      "\tEpisode 1750 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3864],\n",
      "        [1.0000, 0.4493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5795],\n",
      "        [1.0000, 0.5591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495028674602509 \tStep Time:  0.006949663162231445 s \tTotal Time:  10.955253601074219 s \n",
      "\n",
      "\n",
      "\tEpisode 1751 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2304],\n",
      "        [1.0000, 0.4817]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2507],\n",
      "        [1.0000, 0.5846]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.282815396785736 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.961237907409668 s \n",
      "\n",
      "\n",
      "\tEpisode 1752 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4561],\n",
      "        [1.0000, 0.3820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.2431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.721027851104736 \tStep Time:  0.005983591079711914 s \tTotal Time:  10.96722149848938 s \n",
      "\n",
      "\n",
      "\tEpisode 1753 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5037],\n",
      "        [1.0000, 0.4264]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3679],\n",
      "        [1.0000, 0.4809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.624309062957764 \tStep Time:  0.006981372833251953 s \tTotal Time:  10.974202871322632 s \n",
      "\n",
      "\n",
      "\tEpisode 1754 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3683],\n",
      "        [1.0000, 0.6177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6053],\n",
      "        [1.0000, 0.4231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517167866230011 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.980187177658081 s \n",
      "\n",
      "\n",
      "\tEpisode 1755 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4779],\n",
      "        [1.0000, 0.6453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3454],\n",
      "        [1.0000, 0.6213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610731482505798 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.98617148399353 s \n",
      "\n",
      "\n",
      "\tEpisode 1756 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.5529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6557],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419585704803467 \tStep Time:  0.005984306335449219 s \tTotal Time:  10.99215579032898 s \n",
      "\n",
      "\n",
      "\tEpisode 1757 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4933],\n",
      "        [1.0000, 0.5456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3798],\n",
      "        [1.0000, 0.4205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.650358200073242 \tStep Time:  0.010972261428833008 s \tTotal Time:  11.003128051757812 s \n",
      "\n",
      "\n",
      "\tEpisode 1758 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6355],\n",
      "        [1.0000, 0.6133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6232],\n",
      "        [1.0000, 0.6479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528323650360107 \tStep Time:  0.006013154983520508 s \tTotal Time:  11.009141206741333 s \n",
      "\n",
      "\n",
      "\tEpisode 1759 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6057],\n",
      "        [1.0000, 0.4249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5815],\n",
      "        [1.0000, 0.4860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39809513092041 \tStep Time:  0.006983041763305664 s \tTotal Time:  11.016124248504639 s \n",
      "\n",
      "\n",
      "\tEpisode 1760 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6255],\n",
      "        [1.0000, 0.6029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5174],\n",
      "        [1.0000, 0.4998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531496286392212 \tStep Time:  0.004986286163330078 s \tTotal Time:  11.021110534667969 s \n",
      "\n",
      "\n",
      "\tEpisode 1761 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4168],\n",
      "        [1.0000, 0.5647]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6189],\n",
      "        [1.0000, 0.4142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560540199279785 \tStep Time:  0.004992008209228516 s \tTotal Time:  11.026102542877197 s \n",
      "\n",
      "\n",
      "\tEpisode 1762 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6173],\n",
      "        [1.0000, 0.6477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4705],\n",
      "        [1.0000, 0.5382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488174438476562 \tStep Time:  0.007346630096435547 s \tTotal Time:  11.033449172973633 s \n",
      "\n",
      "\n",
      "\tEpisode 1763 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5849],\n",
      "        [1.0000, 0.6354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5186],\n",
      "        [1.0000, 0.6633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593969702720642 \tStep Time:  0.005014896392822266 s \tTotal Time:  11.038464069366455 s \n",
      "\n",
      "\n",
      "\tEpisode 1764 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6241],\n",
      "        [1.0000, 0.5632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5732],\n",
      "        [1.0000, 0.4144]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432930946350098 \tStep Time:  0.005987405776977539 s \tTotal Time:  11.044451475143433 s \n",
      "\n",
      "\n",
      "\tEpisode 1765 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5725],\n",
      "        [1.0000, 0.3266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.6241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.665349006652832 \tStep Time:  0.00501704216003418 s \tTotal Time:  11.049468517303467 s \n",
      "\n",
      "\n",
      "\tEpisode 1766 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5602],\n",
      "        [1.0000, 0.5980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5523],\n",
      "        [1.0000, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481278896331787 \tStep Time:  0.005019426345825195 s \tTotal Time:  11.055488109588623 s \n",
      "\n",
      "\n",
      "\tEpisode 1767 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5129],\n",
      "        [1.0000, 0.5993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4714],\n",
      "        [1.0000, 0.4725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561497211456299 \tStep Time:  0.005952596664428711 s \tTotal Time:  11.061440706253052 s \n",
      "\n",
      "\n",
      "\tEpisode 1768 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4224],\n",
      "        [1.0000, 0.6182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519689559936523 \tStep Time:  0.004987001419067383 s \tTotal Time:  11.06642770767212 s \n",
      "\n",
      "\n",
      "\tEpisode 1769 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4768],\n",
      "        [1.0000, 0.5527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5506],\n",
      "        [1.0000, 0.5301]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495055198669434 \tStep Time:  0.005019187927246094 s \tTotal Time:  11.071446895599365 s \n",
      "\n",
      "\n",
      "\tEpisode 1770 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4487],\n",
      "        [1.0000, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5340],\n",
      "        [1.0000, 0.4185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443201541900635 \tStep Time:  0.005951881408691406 s \tTotal Time:  11.077398777008057 s \n",
      "\n",
      "\n",
      "\tEpisode 1771 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5241],\n",
      "        [1.0000, 0.5248]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5482],\n",
      "        [1.0000, 0.4356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573124408721924 \tStep Time:  0.006055116653442383 s \tTotal Time:  11.083453893661499 s \n",
      "\n",
      "\n",
      "\tEpisode 1772 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5276],\n",
      "        [1.0000, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3782],\n",
      "        [1.0000, 0.4428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.406042098999023 \tStep Time:  0.004985332489013672 s \tTotal Time:  11.088439226150513 s \n",
      "\n",
      "\n",
      "\tEpisode 1773 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5405],\n",
      "        [1.0000, 0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4311],\n",
      "        [1.0000, 0.3412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477445840835571 \tStep Time:  0.005951881408691406 s \tTotal Time:  11.094391107559204 s \n",
      "\n",
      "\n",
      "\tEpisode 1774 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [1.0000, 0.4597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5359],\n",
      "        [1.0000, 0.4114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47878497838974 \tStep Time:  0.006014823913574219 s \tTotal Time:  11.100405931472778 s \n",
      "\n",
      "\n",
      "\tEpisode 1775 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4212],\n",
      "        [1.0000, 0.3560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5443],\n",
      "        [1.0000, 0.5362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.674161434173584 \tStep Time:  0.0049860477447509766 s \tTotal Time:  11.10539197921753 s \n",
      "\n",
      "\n",
      "\tEpisode 1776 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3007],\n",
      "        [1.0000, 0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5183],\n",
      "        [1.0000, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.405367374420166 \tStep Time:  0.00495600700378418 s \tTotal Time:  11.110347986221313 s \n",
      "\n",
      "\n",
      "\tEpisode 1777 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5206],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5235],\n",
      "        [1.0000, 0.4373]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567864418029785 \tStep Time:  0.0060155391693115234 s \tTotal Time:  11.116363525390625 s \n",
      "\n",
      "\n",
      "\tEpisode 1778 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5357],\n",
      "        [1.0000, 0.4414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3426],\n",
      "        [1.0000, 0.4528]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.622174263000488 \tStep Time:  0.004986286163330078 s \tTotal Time:  11.121349811553955 s \n",
      "\n",
      "\n",
      "\tEpisode 1779 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5060],\n",
      "        [1.0000, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500333547592163 \tStep Time:  0.004988431930541992 s \tTotal Time:  11.126338243484497 s \n",
      "\n",
      "\n",
      "\tEpisode 1780 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5083],\n",
      "        [1.0000, 0.3077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4511],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.388280034065247 \tStep Time:  0.006948232650756836 s \tTotal Time:  11.133286476135254 s \n",
      "\n",
      "\n",
      "\tEpisode 1781 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5296],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5445],\n",
      "        [1.0000, 0.3527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436338901519775 \tStep Time:  0.00498652458190918 s \tTotal Time:  11.138273000717163 s \n",
      "\n",
      "\n",
      "\tEpisode 1782 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4801],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5097],\n",
      "        [1.0000, 0.3922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569544017314911 \tStep Time:  0.004987001419067383 s \tTotal Time:  11.14326000213623 s \n",
      "\n",
      "\n",
      "\tEpisode 1783 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5007],\n",
      "        [1.0000, 0.4337]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5812],\n",
      "        [1.0000, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493412494659424 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.149243831634521 s \n",
      "\n",
      "\n",
      "\tEpisode 1784 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4482],\n",
      "        [1.0000, 0.4401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.2824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.648925304412842 \tStep Time:  0.00598454475402832 s \tTotal Time:  11.15522837638855 s \n",
      "\n",
      "\n",
      "\tEpisode 1785 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3799],\n",
      "        [1.0000, 0.4575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6070],\n",
      "        [1.0000, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59421682357788 \tStep Time:  0.005982398986816406 s \tTotal Time:  11.161210775375366 s \n",
      "\n",
      "\n",
      "\tEpisode 1786 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3763],\n",
      "        [1.0000, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3409],\n",
      "        [1.0000, 0.5755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592556476593018 \tStep Time:  0.0059850215911865234 s \tTotal Time:  11.167195796966553 s \n",
      "\n",
      "\n",
      "\tEpisode 1787 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4741],\n",
      "        [1.0000, 0.4039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4752],\n",
      "        [1.0000, 0.5731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4293212890625 \tStep Time:  0.00598454475402832 s \tTotal Time:  11.173180341720581 s \n",
      "\n",
      "\n",
      "\tEpisode 1788 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6233],\n",
      "        [1.0000, 0.4952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5368],\n",
      "        [1.0000, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5523042678833 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.180161476135254 s \n",
      "\n",
      "\n",
      "\tEpisode 1789 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4476],\n",
      "        [1.0000, 0.5761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5795],\n",
      "        [1.0000, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402488768100739 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.186145305633545 s \n",
      "\n",
      "\n",
      "\tEpisode 1790 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4778],\n",
      "        [1.0000, 0.5759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5935],\n",
      "        [1.0000, 0.5671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455101013183594 \tStep Time:  0.00598454475402832 s \tTotal Time:  11.192129850387573 s \n",
      "\n",
      "\n",
      "\tEpisode 1791 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.4353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5842],\n",
      "        [1.0000, 0.4557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564534664154053 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.199110984802246 s \n",
      "\n",
      "\n",
      "\tEpisode 1792 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5484],\n",
      "        [1.0000, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5995],\n",
      "        [1.0000, 0.5709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55063545703888 \tStep Time:  0.00698089599609375 s \tTotal Time:  11.20609188079834 s \n",
      "\n",
      "\n",
      "\tEpisode 1793 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5714],\n",
      "        [1.0000, 0.6252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4737],\n",
      "        [1.0000, 0.5251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515638649463654 \tStep Time:  0.00615239143371582 s \tTotal Time:  11.212244272232056 s \n",
      "\n",
      "\n",
      "\tEpisode 1794 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6073],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5850],\n",
      "        [1.0000, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438105165958405 \tStep Time:  0.005815744400024414 s \tTotal Time:  11.21806001663208 s \n",
      "\n",
      "\n",
      "\tEpisode 1795 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5455],\n",
      "        [1.0000, 0.5132]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6717],\n",
      "        [1.0000, 0.6109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514837563037872 \tStep Time:  0.005983591079711914 s \tTotal Time:  11.224043607711792 s \n",
      "\n",
      "\n",
      "\tEpisode 1796 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5152],\n",
      "        [1.0000, 0.4762]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4886],\n",
      "        [1.0000, 0.5508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529956340789795 \tStep Time:  0.006981849670410156 s \tTotal Time:  11.231025457382202 s \n",
      "\n",
      "\n",
      "\tEpisode 1797 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5182],\n",
      "        [1.0000, 0.5713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5857],\n",
      "        [1.0000, 0.5720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479251265525818 \tStep Time:  0.0059833526611328125 s \tTotal Time:  11.237008810043335 s \n",
      "\n",
      "\n",
      "\tEpisode 1798 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4734],\n",
      "        [1.0000, 0.5680]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4646],\n",
      "        [1.0000, 0.5595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.606901168823242 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.242992877960205 s \n",
      "\n",
      "\n",
      "\tEpisode 1799 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6125],\n",
      "        [1.0000, 0.4985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5784],\n",
      "        [1.0000, 0.5657]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535265445709229 \tStep Time:  0.005986928939819336 s \tTotal Time:  11.248979806900024 s \n",
      "\n",
      "\n",
      "\tEpisode 1800 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4566],\n",
      "        [1.0000, 0.5552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5588],\n",
      "        [1.0000, 0.4577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.608003497123718 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.254964113235474 s \n",
      "\n",
      "\n",
      "\tEpisode 1801 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4446],\n",
      "        [1.0000, 0.5482]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5209],\n",
      "        [1.0000, 0.4516]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523997783660889 \tStep Time:  0.006688833236694336 s \tTotal Time:  11.261652946472168 s \n",
      "\n",
      "\n",
      "\tEpisode 1802 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5556],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5943],\n",
      "        [1.0000, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432754516601562 \tStep Time:  0.005277872085571289 s \tTotal Time:  11.26693081855774 s \n",
      "\n",
      "\n",
      "\tEpisode 1803 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5044],\n",
      "        [1.0000, 0.5378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5547],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519872188568115 \tStep Time:  0.005984783172607422 s \tTotal Time:  11.272915601730347 s \n",
      "\n",
      "\n",
      "\tEpisode 1804 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4463],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5329],\n",
      "        [1.0000, 0.4448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44329309463501 \tStep Time:  0.009973764419555664 s \tTotal Time:  11.282889366149902 s \n",
      "\n",
      "\n",
      "\tEpisode 1805 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.5659]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4714],\n",
      "        [1.0000, 0.4361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429613590240479 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.289870500564575 s \n",
      "\n",
      "\n",
      "\tEpisode 1806 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4077],\n",
      "        [1.0000, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4514],\n",
      "        [1.0000, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536064147949219 \tStep Time:  0.011968135833740234 s \tTotal Time:  11.301838636398315 s \n",
      "\n",
      "\n",
      "\tEpisode 1807 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4599],\n",
      "        [1.0000, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4372],\n",
      "        [1.0000, 0.4615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479794025421143 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.308819770812988 s \n",
      "\n",
      "\n",
      "\tEpisode 1808 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6664],\n",
      "        [1.0000, 0.4032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7148],\n",
      "        [1.0000, 0.5499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.643576622009277 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.315800905227661 s \n",
      "\n",
      "\n",
      "\tEpisode 1809 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5097],\n",
      "        [1.0000, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5674],\n",
      "        [1.0000, 0.5504]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43466567993164 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.321784734725952 s \n",
      "\n",
      "\n",
      "\tEpisode 1810 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5279],\n",
      "        [1.0000, 0.4422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3905],\n",
      "        [1.0000, 0.6062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504376888275146 \tStep Time:  0.005985736846923828 s \tTotal Time:  11.327770471572876 s \n",
      "\n",
      "\n",
      "\tEpisode 1811 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5384],\n",
      "        [1.0000, 0.6215]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4646],\n",
      "        [1.0000, 0.4086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586376965045929 \tStep Time:  0.005735158920288086 s \tTotal Time:  11.33378553390503 s \n",
      "\n",
      "\n",
      "\tEpisode 1812 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4327],\n",
      "        [1.0000, 0.4059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5041],\n",
      "        [1.0000, 0.4781]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440410137176514 \tStep Time:  0.004990577697753906 s \tTotal Time:  11.338776111602783 s \n",
      "\n",
      "\n",
      "\tEpisode 1813 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5354],\n",
      "        [1.0000, 0.4056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4103],\n",
      "        [1.0000, 0.4420]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436302661895752 \tStep Time:  0.004950761795043945 s \tTotal Time:  11.343726873397827 s \n",
      "\n",
      "\n",
      "\tEpisode 1814 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6662],\n",
      "        [1.0000, 0.5232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5209],\n",
      "        [1.0000, 0.4234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.401287078857422 \tStep Time:  0.007012128829956055 s \tTotal Time:  11.350739002227783 s \n",
      "\n",
      "\n",
      "\tEpisode 1815 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4087],\n",
      "        [1.0000, 0.6262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7501],\n",
      "        [1.0000, 0.4266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.284266829490662 \tStep Time:  0.0049860477447509766 s \tTotal Time:  11.355725049972534 s \n",
      "\n",
      "\n",
      "\tEpisode 1816 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.4119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3419],\n",
      "        [1.0000, 0.5476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471838414669037 \tStep Time:  0.005953073501586914 s \tTotal Time:  11.361678123474121 s \n",
      "\n",
      "\n",
      "\tEpisode 1817 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3525],\n",
      "        [1.0000, 0.4334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5326],\n",
      "        [1.0000, 0.6110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446309566497803 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.367662191390991 s \n",
      "\n",
      "\n",
      "\tEpisode 1818 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3969],\n",
      "        [1.0000, 0.3699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5908],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.357842922210693 \tStep Time:  0.006982088088989258 s \tTotal Time:  11.37464427947998 s \n",
      "\n",
      "\n",
      "\tEpisode 1819 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8287],\n",
      "        [1.0000, 0.2256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3817],\n",
      "        [1.0000, 0.3526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459650039672852 \tStep Time:  0.007978200912475586 s \tTotal Time:  11.382622480392456 s \n",
      "\n",
      "\n",
      "\tEpisode 1820 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.6480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6177],\n",
      "        [1.0000, 0.3021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.29805076122284 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.388606309890747 s \n",
      "\n",
      "\n",
      "\tEpisode 1821 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6442],\n",
      "        [1.0000, 0.3032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3304],\n",
      "        [1.0000, 0.3032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416415691375732 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.394590616226196 s \n",
      "\n",
      "\n",
      "\tEpisode 1822 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4758],\n",
      "        [1.0000, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7388],\n",
      "        [1.0000, 0.6500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493154048919678 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.40157175064087 s \n",
      "\n",
      "\n",
      "\tEpisode 1823 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4144],\n",
      "        [1.0000, 0.3442]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4088],\n",
      "        [1.0000, 0.8650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.334228873252869 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.408552885055542 s \n",
      "\n",
      "\n",
      "\tEpisode 1824 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4102],\n",
      "        [1.0000, 0.5208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5765],\n",
      "        [1.0000, 0.5940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458123207092285 \tStep Time:  0.006981372833251953 s \tTotal Time:  11.415534257888794 s \n",
      "\n",
      "\n",
      "\tEpisode 1825 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2729],\n",
      "        [1.0000, 0.3735]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3446],\n",
      "        [1.0000, 0.5803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.630740642547607 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.421518325805664 s \n",
      "\n",
      "\n",
      "\tEpisode 1826 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5441],\n",
      "        [1.0000, 0.4175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3898],\n",
      "        [1.0000, 0.4561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47044324874878 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.427502393722534 s \n",
      "\n",
      "\n",
      "\tEpisode 1827 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2730],\n",
      "        [1.0000, 0.2666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6791],\n",
      "        [1.0000, 0.8376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  14.162207126617432 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.434483528137207 s \n",
      "\n",
      "\n",
      "\tEpisode 1828 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4841],\n",
      "        [1.0000, 0.5155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7708],\n",
      "        [1.0000, 0.4382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.683722496032715 \tStep Time:  0.00498652458190918 s \tTotal Time:  11.440467596054077 s \n",
      "\n",
      "\n",
      "\tEpisode 1829 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7118],\n",
      "        [1.0000, 0.2867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3682],\n",
      "        [1.0000, 0.3669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.389846444129944 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.44744873046875 s \n",
      "\n",
      "\n",
      "\tEpisode 1830 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8205],\n",
      "        [1.0000, 0.3302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8748],\n",
      "        [1.0000, 0.6109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510990142822266 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.4534330368042 s \n",
      "\n",
      "\n",
      "\tEpisode 1831 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4211],\n",
      "        [1.0000, 0.6338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3135],\n",
      "        [1.0000, 0.7825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581241607666016 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.460414171218872 s \n",
      "\n",
      "\n",
      "\tEpisode 1832 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3892],\n",
      "        [1.0000, 0.4129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3344],\n",
      "        [1.0000, 0.4664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562644481658936 \tStep Time:  0.006981372833251953 s \tTotal Time:  11.467395544052124 s \n",
      "\n",
      "\n",
      "\tEpisode 1833 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4281],\n",
      "        [1.0000, 0.3471]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6051],\n",
      "        [1.0000, 0.8786]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451893329620361 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.473379611968994 s \n",
      "\n",
      "\n",
      "\tEpisode 1834 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4540],\n",
      "        [1.0000, 0.5539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6214],\n",
      "        [1.0000, 0.3462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577975749969482 \tStep Time:  0.006981372833251953 s \tTotal Time:  11.480360984802246 s \n",
      "\n",
      "\n",
      "\tEpisode 1835 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3833],\n",
      "        [1.0000, 0.4181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3734],\n",
      "        [1.0000, 0.3681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53558349609375 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.486344814300537 s \n",
      "\n",
      "\n",
      "\tEpisode 1836 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4603],\n",
      "        [1.0000, 0.3812]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4234],\n",
      "        [1.0000, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552887439727783 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.492328882217407 s \n",
      "\n",
      "\n",
      "\tEpisode 1837 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7512],\n",
      "        [1.0000, 0.4284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4334],\n",
      "        [1.0000, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433764457702637 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.49931001663208 s \n",
      "\n",
      "\n",
      "\tEpisode 1838 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6734],\n",
      "        [1.0000, 0.4859]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5036],\n",
      "        [1.0000, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469713687896729 \tStep Time:  0.004987001419067383 s \tTotal Time:  11.504297018051147 s \n",
      "\n",
      "\n",
      "\tEpisode 1839 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5917],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6696],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5092533826828 \tStep Time:  0.005983591079711914 s \tTotal Time:  11.51028060913086 s \n",
      "\n",
      "\n",
      "\tEpisode 1840 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5254],\n",
      "        [1.0000, 0.5834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6339],\n",
      "        [1.0000, 0.6493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560387134552002 \tStep Time:  0.006981372833251953 s \tTotal Time:  11.517261981964111 s \n",
      "\n",
      "\n",
      "\tEpisode 1841 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6240],\n",
      "        [1.0000, 0.4956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5999],\n",
      "        [1.0000, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418324947357178 \tStep Time:  0.004987001419067383 s \tTotal Time:  11.522248983383179 s \n",
      "\n",
      "\n",
      "\tEpisode 1842 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4945],\n",
      "        [1.0000, 0.5632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7686],\n",
      "        [1.0000, 0.6369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.397067546844482 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.52923035621643 s \n",
      "\n",
      "\n",
      "\tEpisode 1843 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5120],\n",
      "        [1.0000, 0.6177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4685],\n",
      "        [1.0000, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439107894897461 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.5352144241333 s \n",
      "\n",
      "\n",
      "\tEpisode 1844 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4982],\n",
      "        [1.0000, 0.5919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4604],\n",
      "        [1.0000, 0.4421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57779347896576 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.541198253631592 s \n",
      "\n",
      "\n",
      "\tEpisode 1845 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4650],\n",
      "        [1.0000, 0.6015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7314],\n",
      "        [1.0000, 0.6370]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561468362808228 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.548179388046265 s \n",
      "\n",
      "\n",
      "\tEpisode 1846 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7437],\n",
      "        [1.0000, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4597],\n",
      "        [1.0000, 0.4524]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.668551921844482 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.554163694381714 s \n",
      "\n",
      "\n",
      "\tEpisode 1847 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5794],\n",
      "        [1.0000, 0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6730],\n",
      "        [1.0000, 0.7111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51186865568161 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.560147762298584 s \n",
      "\n",
      "\n",
      "\tEpisode 1848 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6286],\n",
      "        [1.0000, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4555],\n",
      "        [1.0000, 0.4579]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63611900806427 \tStep Time:  0.00698089599609375 s \tTotal Time:  11.567128658294678 s \n",
      "\n",
      "\n",
      "\tEpisode 1849 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4170],\n",
      "        [1.0000, 0.6788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5969],\n",
      "        [1.0000, 0.4294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512689113616943 \tStep Time:  0.004986763000488281 s \tTotal Time:  11.572115421295166 s \n",
      "\n",
      "\n",
      "\tEpisode 1850 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4537],\n",
      "        [1.0000, 0.4409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4769],\n",
      "        [1.0000, 0.4283]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579586029052734 \tStep Time:  0.007980108261108398 s \tTotal Time:  11.580095529556274 s \n",
      "\n",
      "\n",
      "\tEpisode 1851 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4434],\n",
      "        [1.0000, 0.5808]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4353],\n",
      "        [1.0000, 0.4263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47573435306549 \tStep Time:  0.005982875823974609 s \tTotal Time:  11.586078405380249 s \n",
      "\n",
      "\n",
      "\tEpisode 1852 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4312],\n",
      "        [1.0000, 0.4600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4185],\n",
      "        [1.0000, 0.4302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559848308563232 \tStep Time:  0.004986286163330078 s \tTotal Time:  11.59206247329712 s \n",
      "\n",
      "\n",
      "\tEpisode 1853 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5410],\n",
      "        [1.0000, 0.4746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4472],\n",
      "        [1.0000, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512795448303223 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.599043607711792 s \n",
      "\n",
      "\n",
      "\tEpisode 1854 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4351],\n",
      "        [1.0000, 0.4229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5152],\n",
      "        [1.0000, 0.7134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.73530626296997 \tStep Time:  0.00598454475402832 s \tTotal Time:  11.60502815246582 s \n",
      "\n",
      "\n",
      "\tEpisode 1855 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5030],\n",
      "        [1.0000, 0.5364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4601],\n",
      "        [1.0000, 0.4473]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541848599910736 \tStep Time:  0.011001825332641602 s \tTotal Time:  11.616029977798462 s \n",
      "\n",
      "\n",
      "\tEpisode 1856 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4394],\n",
      "        [1.0000, 0.4405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4531],\n",
      "        [1.0000, 0.4929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510567963123322 \tStep Time:  0.0059833526611328125 s \tTotal Time:  11.622013330459595 s \n",
      "\n",
      "\n",
      "\tEpisode 1857 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4575],\n",
      "        [1.0000, 0.4315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4583],\n",
      "        [1.0000, 0.5475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.606035709381104 \tStep Time:  0.0059833526611328125 s \tTotal Time:  11.627996683120728 s \n",
      "\n",
      "\n",
      "\tEpisode 1858 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4507],\n",
      "        [1.0000, 0.4452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4364],\n",
      "        [1.0000, 0.4666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537480533123016 \tStep Time:  0.006951093673706055 s \tTotal Time:  11.634947776794434 s \n",
      "\n",
      "\n",
      "\tEpisode 1859 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5013],\n",
      "        [1.0000, 0.4543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4670],\n",
      "        [1.0000, 0.5234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57566499710083 \tStep Time:  0.00498652458190918 s \tTotal Time:  11.639934301376343 s \n",
      "\n",
      "\n",
      "\tEpisode 1860 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4688],\n",
      "        [1.0000, 0.4555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4788],\n",
      "        [1.0000, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559153199195862 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.645918607711792 s \n",
      "\n",
      "\n",
      "\tEpisode 1861 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.4745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4693],\n",
      "        [1.0000, 0.4532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481811702251434 \tStep Time:  0.006091117858886719 s \tTotal Time:  11.652009725570679 s \n",
      "\n",
      "\n",
      "\tEpisode 1862 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.4591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4583],\n",
      "        [1.0000, 0.5238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.606096744537354 \tStep Time:  0.005986928939819336 s \tTotal Time:  11.657996654510498 s \n",
      "\n",
      "\n",
      "\tEpisode 1863 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [1.0000, 0.4794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4605],\n",
      "        [1.0000, 0.5603]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492766380310059 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.663980960845947 s \n",
      "\n",
      "\n",
      "\tEpisode 1864 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4574],\n",
      "        [1.0000, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4761],\n",
      "        [1.0000, 0.5430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5607750415802 \tStep Time:  0.0050220489501953125 s \tTotal Time:  11.669003009796143 s \n",
      "\n",
      "\n",
      "\tEpisode 1865 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5180],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5123],\n",
      "        [1.0000, 0.6232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495213985443115 \tStep Time:  0.006016969680786133 s \tTotal Time:  11.675019979476929 s \n",
      "\n",
      "\n",
      "\tEpisode 1866 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4628],\n",
      "        [1.0000, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5236],\n",
      "        [1.0000, 0.4775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494826316833496 \tStep Time:  0.005950927734375 s \tTotal Time:  11.680970907211304 s \n",
      "\n",
      "\n",
      "\tEpisode 1867 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6990],\n",
      "        [1.0000, 0.4651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4681],\n",
      "        [1.0000, 0.4812]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4194016456604 \tStep Time:  0.00598454475402832 s \tTotal Time:  11.686955451965332 s \n",
      "\n",
      "\n",
      "\tEpisode 1868 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5028],\n",
      "        [1.0000, 0.5664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4591],\n",
      "        [1.0000, 0.5539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455452620983124 \tStep Time:  0.006014823913574219 s \tTotal Time:  11.692970275878906 s \n",
      "\n",
      "\n",
      "\tEpisode 1869 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.4509]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4695],\n",
      "        [1.0000, 0.5664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572648525238037 \tStep Time:  0.00598597526550293 s \tTotal Time:  11.69895625114441 s \n",
      "\n",
      "\n",
      "\tEpisode 1870 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4583],\n",
      "        [1.0000, 0.4563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4589],\n",
      "        [1.0000, 0.5577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486487865447998 \tStep Time:  0.0059506893157958984 s \tTotal Time:  11.704906940460205 s \n",
      "\n",
      "\n",
      "\tEpisode 1871 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5130],\n",
      "        [1.0000, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4856],\n",
      "        [1.0000, 0.4576]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49823784828186 \tStep Time:  0.0059833526611328125 s \tTotal Time:  11.710890293121338 s \n",
      "\n",
      "\n",
      "\tEpisode 1872 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5657],\n",
      "        [1.0000, 0.6166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5592],\n",
      "        [1.0000, 0.4610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547772407531738 \tStep Time:  0.0050182342529296875 s \tTotal Time:  11.715908527374268 s \n",
      "\n",
      "\n",
      "\tEpisode 1873 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5055],\n",
      "        [1.0000, 0.6982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4550],\n",
      "        [1.0000, 0.4526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445013999938965 \tStep Time:  0.004987001419067383 s \tTotal Time:  11.721893072128296 s \n",
      "\n",
      "\n",
      "\tEpisode 1874 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4592],\n",
      "        [1.0000, 0.6161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5576],\n",
      "        [1.0000, 0.5449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502695083618164 \tStep Time:  0.0059528350830078125 s \tTotal Time:  11.727845907211304 s \n",
      "\n",
      "\n",
      "\tEpisode 1875 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4460],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4546],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518572330474854 \tStep Time:  0.006016969680786133 s \tTotal Time:  11.73386287689209 s \n",
      "\n",
      "\n",
      "\tEpisode 1876 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5605],\n",
      "        [1.0000, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4570],\n",
      "        [1.0000, 0.5679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490679025650024 \tStep Time:  0.005982637405395508 s \tTotal Time:  11.739845514297485 s \n",
      "\n",
      "\n",
      "\tEpisode 1877 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4564],\n",
      "        [1.0000, 0.6900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5171],\n",
      "        [1.0000, 0.4577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442042350769043 \tStep Time:  0.006095409393310547 s \tTotal Time:  11.745940923690796 s \n",
      "\n",
      "\n",
      "\tEpisode 1878 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5808],\n",
      "        [1.0000, 0.4544]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.4938]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4530668258667 \tStep Time:  0.0058727264404296875 s \tTotal Time:  11.751813650131226 s \n",
      "\n",
      "\n",
      "\tEpisode 1879 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5916],\n",
      "        [1.0000, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4524],\n",
      "        [1.0000, 0.4625]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592312812805176 \tStep Time:  0.0049855709075927734 s \tTotal Time:  11.756799221038818 s \n",
      "\n",
      "\n",
      "\tEpisode 1880 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.5036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4539],\n",
      "        [1.0000, 0.4895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493327140808105 \tStep Time:  0.005989551544189453 s \tTotal Time:  11.762788772583008 s \n",
      "\n",
      "\n",
      "\tEpisode 1881 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4682],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4595],\n",
      "        [1.0000, 0.6091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570911407470703 \tStep Time:  0.005952596664428711 s \tTotal Time:  11.769733190536499 s \n",
      "\n",
      "\n",
      "\tEpisode 1882 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5757],\n",
      "        [1.0000, 0.5295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4720],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577821731567383 \tStep Time:  0.00598454475402832 s \tTotal Time:  11.775717735290527 s \n",
      "\n",
      "\n",
      "\tEpisode 1883 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.4508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4532],\n",
      "        [1.0000, 0.5598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437969863414764 \tStep Time:  0.006981611251831055 s \tTotal Time:  11.782699346542358 s \n",
      "\n",
      "\n",
      "\tEpisode 1884 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6773],\n",
      "        [1.0000, 0.4515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.4480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.658843517303467 \tStep Time:  0.005983591079711914 s \tTotal Time:  11.78868293762207 s \n",
      "\n",
      "\n",
      "\tEpisode 1885 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.5319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517396450042725 \tStep Time:  0.0059854984283447266 s \tTotal Time:  11.794668436050415 s \n",
      "\n",
      "\n",
      "\tEpisode 1886 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4529],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4534],\n",
      "        [1.0000, 0.5830]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429212987422943 \tStep Time:  0.0070111751556396484 s \tTotal Time:  11.801679611206055 s \n",
      "\n",
      "\n",
      "\tEpisode 1887 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5590],\n",
      "        [1.0000, 0.5461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4564],\n",
      "        [1.0000, 0.6576]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525554656982422 \tStep Time:  0.00498652458190918 s \tTotal Time:  11.806666135787964 s \n",
      "\n",
      "\n",
      "\tEpisode 1888 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4463],\n",
      "        [1.0000, 0.4420]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4289],\n",
      "        [1.0000, 0.4303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535623550415039 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.812649965286255 s \n",
      "\n",
      "\n",
      "\tEpisode 1889 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5036],\n",
      "        [1.0000, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5104],\n",
      "        [1.0000, 0.4195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566929817199707 \tStep Time:  0.005986928939819336 s \tTotal Time:  11.818636894226074 s \n",
      "\n",
      "\n",
      "\tEpisode 1890 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5981],\n",
      "        [1.0000, 0.4208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6173],\n",
      "        [1.0000, 0.5576]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.608456134796143 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.824621200561523 s \n",
      "\n",
      "\n",
      "\tEpisode 1891 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4735],\n",
      "        [1.0000, 0.5244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4288],\n",
      "        [1.0000, 0.5978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418116092681885 \tStep Time:  0.00694727897644043 s \tTotal Time:  11.831568479537964 s \n",
      "\n",
      "\n",
      "\tEpisode 1892 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4576],\n",
      "        [1.0000, 0.4276]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5388],\n",
      "        [1.0000, 0.4244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566210746765137 \tStep Time:  0.0060176849365234375 s \tTotal Time:  11.837586164474487 s \n",
      "\n",
      "\n",
      "\tEpisode 1893 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4374],\n",
      "        [1.0000, 0.4235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4314],\n",
      "        [1.0000, 0.4295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529048442840576 \tStep Time:  0.005950212478637695 s \tTotal Time:  11.843536376953125 s \n",
      "\n",
      "\n",
      "\tEpisode 1894 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4317],\n",
      "        [1.0000, 0.5711]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6211],\n",
      "        [1.0000, 0.4365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555913925170898 \tStep Time:  0.006014823913574219 s \tTotal Time:  11.8495512008667 s \n",
      "\n",
      "\n",
      "\tEpisode 1895 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4337],\n",
      "        [1.0000, 0.4340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4368],\n",
      "        [1.0000, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547764778137207 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.855535507202148 s \n",
      "\n",
      "\n",
      "\tEpisode 1896 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4408],\n",
      "        [1.0000, 0.6037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4378],\n",
      "        [1.0000, 0.4622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595237731933594 \tStep Time:  0.00495457649230957 s \tTotal Time:  11.860490083694458 s \n",
      "\n",
      "\n",
      "\tEpisode 1897 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4427],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4482],\n",
      "        [1.0000, 0.5645]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479530811309814 \tStep Time:  0.007013559341430664 s \tTotal Time:  11.867503643035889 s \n",
      "\n",
      "\n",
      "\tEpisode 1898 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.4502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4911],\n",
      "        [1.0000, 0.4437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49416446685791 \tStep Time:  0.0049896240234375 s \tTotal Time:  11.872493267059326 s \n",
      "\n",
      "\n",
      "\tEpisode 1899 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6645],\n",
      "        [1.0000, 0.4436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.4384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.619266986846924 \tStep Time:  0.006947517395019531 s \tTotal Time:  11.879440784454346 s \n",
      "\n",
      "\n",
      "\tEpisode 1900 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4465],\n",
      "        [1.0000, 0.5661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6147],\n",
      "        [1.0000, 0.4412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542503833770752 \tStep Time:  0.006017208099365234 s \tTotal Time:  11.885457992553711 s \n",
      "\n",
      "\n",
      "\tEpisode 1901 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4449],\n",
      "        [1.0000, 0.4464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4526],\n",
      "        [1.0000, 0.6919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.414514064788818 \tStep Time:  0.006947517395019531 s \tTotal Time:  11.89240550994873 s \n",
      "\n",
      "\n",
      "\tEpisode 1902 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4937],\n",
      "        [1.0000, 0.4416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.4421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555389642715454 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.899386644363403 s \n",
      "\n",
      "\n",
      "\tEpisode 1903 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4497],\n",
      "        [1.0000, 0.4500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5884],\n",
      "        [1.0000, 0.4699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.590154647827148 \tStep Time:  0.004986763000488281 s \tTotal Time:  11.904373407363892 s \n",
      "\n",
      "\n",
      "\tEpisode 1904 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4705],\n",
      "        [1.0000, 0.4668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526822984218597 \tStep Time:  0.005983829498291016 s \tTotal Time:  11.910357236862183 s \n",
      "\n",
      "\n",
      "\tEpisode 1905 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.5161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6318],\n",
      "        [1.0000, 0.4704]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588295757770538 \tStep Time:  0.006981372833251953 s \tTotal Time:  11.917338609695435 s \n",
      "\n",
      "\n",
      "\tEpisode 1906 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.4738]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4953],\n",
      "        [1.0000, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50141817331314 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.923322916030884 s \n",
      "\n",
      "\n",
      "\tEpisode 1907 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4857],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5421],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539287686347961 \tStep Time:  0.005983114242553711 s \tTotal Time:  11.929306030273438 s \n",
      "\n",
      "\n",
      "\tEpisode 1908 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4770],\n",
      "        [1.0000, 0.4753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4745],\n",
      "        [1.0000, 0.4733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504190802574158 \tStep Time:  0.005987405776977539 s \tTotal Time:  11.935293436050415 s \n",
      "\n",
      "\n",
      "\tEpisode 1909 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4655],\n",
      "        [1.0000, 0.4676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4544],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48094654083252 \tStep Time:  0.00598454475402832 s \tTotal Time:  11.941277980804443 s \n",
      "\n",
      "\n",
      "\tEpisode 1910 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5254],\n",
      "        [1.0000, 0.4865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4903],\n",
      "        [1.0000, 0.4689]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512226104736328 \tStep Time:  0.0059833526611328125 s \tTotal Time:  11.947261333465576 s \n",
      "\n",
      "\n",
      "\tEpisode 1911 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4916],\n",
      "        [1.0000, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7145],\n",
      "        [1.0000, 0.4122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.387216567993164 \tStep Time:  0.005988597869873047 s \tTotal Time:  11.95324993133545 s \n",
      "\n",
      "\n",
      "\tEpisode 1912 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6303],\n",
      "        [1.0000, 0.5444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5705],\n",
      "        [1.0000, 0.4704]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521752417087555 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.95923399925232 s \n",
      "\n",
      "\n",
      "\tEpisode 1913 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4626],\n",
      "        [1.0000, 0.4665]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4289],\n",
      "        [1.0000, 0.5455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572505354881287 \tStep Time:  0.005986928939819336 s \tTotal Time:  11.965220928192139 s \n",
      "\n",
      "\n",
      "\tEpisode 1914 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4982],\n",
      "        [1.0000, 0.6514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5329],\n",
      "        [1.0000, 0.4674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559810161590576 \tStep Time:  0.005983591079711914 s \tTotal Time:  11.97120451927185 s \n",
      "\n",
      "\n",
      "\tEpisode 1915 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3496],\n",
      "        [1.0000, 0.5783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3745],\n",
      "        [1.0000, 0.3481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437780857086182 \tStep Time:  0.005984306335449219 s \tTotal Time:  11.9771888256073 s \n",
      "\n",
      "\n",
      "\tEpisode 1916 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6633],\n",
      "        [1.0000, 0.4502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5464],\n",
      "        [1.0000, 0.3978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.614147663116455 \tStep Time:  0.006981611251831055 s \tTotal Time:  11.98417043685913 s \n",
      "\n",
      "\n",
      "\tEpisode 1917 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4580],\n",
      "        [1.0000, 0.4674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5752],\n",
      "        [1.0000, 0.4090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597042560577393 \tStep Time:  0.005984067916870117 s \tTotal Time:  11.990154504776001 s \n",
      "\n",
      "\n",
      "\tEpisode 1918 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5275],\n",
      "        [1.0000, 0.4445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3726],\n",
      "        [1.0000, 0.4547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.590434074401855 \tStep Time:  0.0069811344146728516 s \tTotal Time:  11.997135639190674 s \n",
      "\n",
      "\n",
      "\tEpisode 1919 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5300],\n",
      "        [1.0000, 0.5160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4482],\n",
      "        [1.0000, 0.4084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416990280151367 \tStep Time:  0.005984783172607422 s \tTotal Time:  12.003120422363281 s \n",
      "\n",
      "\n",
      "\tEpisode 1920 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4841],\n",
      "        [1.0000, 0.3453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5325],\n",
      "        [1.0000, 0.7614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.374916553497314 \tStep Time:  0.006981611251831055 s \tTotal Time:  12.010102033615112 s \n",
      "\n",
      "\n",
      "\tEpisode 1921 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5446],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3892],\n",
      "        [1.0000, 0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489336013793945 \tStep Time:  0.005984306335449219 s \tTotal Time:  12.016086339950562 s \n",
      "\n",
      "\n",
      "\tEpisode 1922 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.4265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4632],\n",
      "        [1.0000, 0.4019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461716651916504 \tStep Time:  0.006980180740356445 s \tTotal Time:  12.023066520690918 s \n",
      "\n",
      "\n",
      "\tEpisode 1923 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6672],\n",
      "        [1.0000, 0.5557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3876],\n",
      "        [1.0000, 0.4626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.627684116363525 \tStep Time:  0.005985260009765625 s \tTotal Time:  12.029051780700684 s \n",
      "\n",
      "\n",
      "\tEpisode 1924 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3194],\n",
      "        [1.0000, 0.5500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.4610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502772808074951 \tStep Time:  0.006979703903198242 s \tTotal Time:  12.036031484603882 s \n",
      "\n",
      "\n",
      "\tEpisode 1925 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2601],\n",
      "        [1.0000, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5839],\n",
      "        [1.0000, 0.5398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.37815809249878 \tStep Time:  0.0050258636474609375 s \tTotal Time:  12.041057348251343 s \n",
      "\n",
      "\n",
      "\tEpisode 1926 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4139],\n",
      "        [1.0000, 0.4517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5261],\n",
      "        [1.0000, 0.5960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469264030456543 \tStep Time:  0.005945444107055664 s \tTotal Time:  12.047002792358398 s \n",
      "\n",
      "\n",
      "\tEpisode 1927 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4774],\n",
      "        [1.0000, 0.4707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5630],\n",
      "        [1.0000, 0.4121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44861125946045 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.05298638343811 s \n",
      "\n",
      "\n",
      "\tEpisode 1928 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3712],\n",
      "        [1.0000, 0.4919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5800],\n",
      "        [1.0000, 0.5412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.655222415924072 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.05897045135498 s \n",
      "\n",
      "\n",
      "\tEpisode 1929 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4295],\n",
      "        [1.0000, 0.4977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5624],\n",
      "        [1.0000, 0.5215]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537742853164673 \tStep Time:  0.006014585494995117 s \tTotal Time:  12.064985036849976 s \n",
      "\n",
      "\n",
      "\tEpisode 1930 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7877],\n",
      "        [1.0000, 0.3636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4359],\n",
      "        [1.0000, 0.4444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.369009494781494 \tStep Time:  0.005986928939819336 s \tTotal Time:  12.070971965789795 s \n",
      "\n",
      "\n",
      "\tEpisode 1931 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5568],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7910],\n",
      "        [1.0000, 0.5437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41954755783081 \tStep Time:  0.0059816837310791016 s \tTotal Time:  12.076953649520874 s \n",
      "\n",
      "\n",
      "\tEpisode 1932 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5744],\n",
      "        [1.0000, 0.5587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548499464988708 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.082937240600586 s \n",
      "\n",
      "\n",
      "\tEpisode 1933 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4855],\n",
      "        [1.0000, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6149],\n",
      "        [1.0000, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458451807498932 \tStep Time:  0.005986928939819336 s \tTotal Time:  12.088924169540405 s \n",
      "\n",
      "\n",
      "\tEpisode 1934 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7167],\n",
      "        [1.0000, 0.3230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [1.0000, 0.4804]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.344285011291504 \tStep Time:  0.005986690521240234 s \tTotal Time:  12.094910860061646 s \n",
      "\n",
      "\n",
      "\tEpisode 1935 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5021],\n",
      "        [1.0000, 0.5775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3322],\n",
      "        [1.0000, 0.3621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591744422912598 \tStep Time:  0.005948543548583984 s \tTotal Time:  12.10085940361023 s \n",
      "\n",
      "\n",
      "\tEpisode 1936 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5881],\n",
      "        [1.0000, 0.5010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3473],\n",
      "        [1.0000, 0.2475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562606632709503 \tStep Time:  0.006014108657836914 s \tTotal Time:  12.106873512268066 s \n",
      "\n",
      "\n",
      "\tEpisode 1937 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3466],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.4321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47453260421753 \tStep Time:  0.00598597526550293 s \tTotal Time:  12.11285948753357 s \n",
      "\n",
      "\n",
      "\tEpisode 1938 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5272],\n",
      "        [1.0000, 0.4170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4309],\n",
      "        [1.0000, 0.4087]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452971935272217 \tStep Time:  0.005951642990112305 s \tTotal Time:  12.118811130523682 s \n",
      "\n",
      "\n",
      "\tEpisode 1939 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6054],\n",
      "        [1.0000, 0.3778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6504],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34309196472168 \tStep Time:  0.006014347076416016 s \tTotal Time:  12.124825477600098 s \n",
      "\n",
      "\n",
      "\tEpisode 1940 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4876],\n",
      "        [1.0000, 0.2835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5400],\n",
      "        [1.0000, 0.3122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512261867523193 \tStep Time:  0.005953073501586914 s \tTotal Time:  12.130778551101685 s \n",
      "\n",
      "\n",
      "\tEpisode 1941 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4929],\n",
      "        [1.0000, 0.5324]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7046],\n",
      "        [1.0000, 0.3640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.353185415267944 \tStep Time:  0.006015300750732422 s \tTotal Time:  12.136793851852417 s \n",
      "\n",
      "\n",
      "\tEpisode 1942 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8246],\n",
      "        [1.0000, 0.7072]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4950],\n",
      "        [1.0000, 0.5648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.822401523590088 \tStep Time:  0.005986452102661133 s \tTotal Time:  12.142780303955078 s \n",
      "\n",
      "\n",
      "\tEpisode 1943 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6059],\n",
      "        [1.0000, 0.8044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4298],\n",
      "        [1.0000, 0.6180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5797278881073 \tStep Time:  0.0059795379638671875 s \tTotal Time:  12.148759841918945 s \n",
      "\n",
      "\n",
      "\tEpisode 1944 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4641],\n",
      "        [1.0000, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5021],\n",
      "        [1.0000, 0.4310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459777295589447 \tStep Time:  0.004957675933837891 s \tTotal Time:  12.153717517852783 s \n",
      "\n",
      "\n",
      "\tEpisode 1945 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5594],\n",
      "        [1.0000, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505889177322388 \tStep Time:  0.005983114242553711 s \tTotal Time:  12.159700632095337 s \n",
      "\n",
      "\n",
      "\tEpisode 1946 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4553],\n",
      "        [1.0000, 0.5756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4249],\n",
      "        [1.0000, 0.6043]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.3836350440979 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.165684700012207 s \n",
      "\n",
      "\n",
      "\tEpisode 1947 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3261],\n",
      "        [1.0000, 0.4527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6099],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.363391876220703 \tStep Time:  0.006981611251831055 s \tTotal Time:  12.172666311264038 s \n",
      "\n",
      "\n",
      "\tEpisode 1948 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2653],\n",
      "        [1.0000, 0.3727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2820],\n",
      "        [1.0000, 0.7445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.802573680877686 \tStep Time:  0.005984783172607422 s \tTotal Time:  12.178651094436646 s \n",
      "\n",
      "\n",
      "\tEpisode 1949 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5238],\n",
      "        [1.0000, 0.7983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6209],\n",
      "        [1.0000, 0.3529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.326333999633789 \tStep Time:  0.0069811344146728516 s \tTotal Time:  12.185632228851318 s \n",
      "\n",
      "\n",
      "\tEpisode 1950 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5718],\n",
      "        [1.0000, 0.5764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526564598083496 \tStep Time:  0.0059833526611328125 s \tTotal Time:  12.191615581512451 s \n",
      "\n",
      "\n",
      "\tEpisode 1951 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.4370]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3123],\n",
      "        [1.0000, 0.3798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.618523597717285 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.197599411010742 s \n",
      "\n",
      "\n",
      "\tEpisode 1952 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5140],\n",
      "        [1.0000, 0.6434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.4029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.398765087127686 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.203583478927612 s \n",
      "\n",
      "\n",
      "\tEpisode 1953 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4525],\n",
      "        [1.0000, 0.4568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5411],\n",
      "        [1.0000, 0.4607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577281951904297 \tStep Time:  0.009975671768188477 s \tTotal Time:  12.2135591506958 s \n",
      "\n",
      "\n",
      "\tEpisode 1954 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6744],\n",
      "        [1.0000, 0.5511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5795],\n",
      "        [1.0000, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469611644744873 \tStep Time:  0.006986856460571289 s \tTotal Time:  12.220546007156372 s \n",
      "\n",
      "\n",
      "\tEpisode 1955 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3526],\n",
      "        [1.0000, 0.7656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3653],\n",
      "        [1.0000, 0.3705]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.376791954040527 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.226530075073242 s \n",
      "\n",
      "\n",
      "\tEpisode 1956 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [1.0000, 0.5130]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6176],\n",
      "        [1.0000, 0.6560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542198896408081 \tStep Time:  0.006981372833251953 s \tTotal Time:  12.233511447906494 s \n",
      "\n",
      "\n",
      "\tEpisode 1957 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6055],\n",
      "        [1.0000, 0.5531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5147],\n",
      "        [1.0000, 0.3080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.37468147277832 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.239495277404785 s \n",
      "\n",
      "\n",
      "\tEpisode 1958 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5686],\n",
      "        [1.0000, 0.2898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4062],\n",
      "        [1.0000, 0.3660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443095862865448 \tStep Time:  0.0059850215911865234 s \tTotal Time:  12.245480298995972 s \n",
      "\n",
      "\n",
      "\tEpisode 1959 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6275],\n",
      "        [1.0000, 0.3003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7123],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535521507263184 \tStep Time:  0.005992412567138672 s \tTotal Time:  12.25147271156311 s \n",
      "\n",
      "\n",
      "\tEpisode 1960 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7891],\n",
      "        [1.0000, 0.2908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6093],\n",
      "        [1.0000, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.398216724395752 \tStep Time:  0.00498652458190918 s \tTotal Time:  12.25645923614502 s \n",
      "\n",
      "\n",
      "\tEpisode 1961 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3845],\n",
      "        [1.0000, 0.5966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4231],\n",
      "        [1.0000, 0.3056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.392600297927856 \tStep Time:  0.006233692169189453 s \tTotal Time:  12.262692928314209 s \n",
      "\n",
      "\n",
      "\tEpisode 1962 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7882],\n",
      "        [1.0000, 0.2725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7300],\n",
      "        [1.0000, 0.3580]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63628101348877 \tStep Time:  0.0057392120361328125 s \tTotal Time:  12.268432140350342 s \n",
      "\n",
      "\n",
      "\tEpisode 1963 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7599],\n",
      "        [1.0000, 0.2254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4296],\n",
      "        [1.0000, 0.4102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.877946972846985 \tStep Time:  0.0059833526611328125 s \tTotal Time:  12.274415493011475 s \n",
      "\n",
      "\n",
      "\tEpisode 1964 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4242],\n",
      "        [1.0000, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2246],\n",
      "        [1.0000, 0.8169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593957901000977 \tStep Time:  0.005984306335449219 s \tTotal Time:  12.280399799346924 s \n",
      "\n",
      "\n",
      "\tEpisode 1965 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7343],\n",
      "        [1.0000, 0.6047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9049],\n",
      "        [1.0000, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.670947074890137 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.286383628845215 s \n",
      "\n",
      "\n",
      "\tEpisode 1966 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4651],\n",
      "        [1.0000, 0.2221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4855],\n",
      "        [1.0000, 0.3521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.655073642730713 \tStep Time:  0.00598454475402832 s \tTotal Time:  12.292368173599243 s \n",
      "\n",
      "\n",
      "\tEpisode 1967 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4157],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4393],\n",
      "        [1.0000, 0.3369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544589519500732 \tStep Time:  0.0069811344146728516 s \tTotal Time:  12.299349308013916 s \n",
      "\n",
      "\n",
      "\tEpisode 1968 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4042],\n",
      "        [1.0000, 0.2688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6878],\n",
      "        [1.0000, 0.6059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47365802526474 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.305332899093628 s \n",
      "\n",
      "\n",
      "\tEpisode 1969 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4714],\n",
      "        [1.0000, 0.6027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5011],\n",
      "        [1.0000, 0.6810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583107948303223 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.311316728591919 s \n",
      "\n",
      "\n",
      "\tEpisode 1970 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4866],\n",
      "        [1.0000, 0.6426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4487],\n",
      "        [1.0000, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601533889770508 \tStep Time:  0.005984783172607422 s \tTotal Time:  12.317301511764526 s \n",
      "\n",
      "\n",
      "\tEpisode 1971 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4595],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5438],\n",
      "        [1.0000, 0.5506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465710163116455 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.323285102844238 s \n",
      "\n",
      "\n",
      "\tEpisode 1972 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4927],\n",
      "        [1.0000, 0.4666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3954],\n",
      "        [1.0000, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.606284976005554 \tStep Time:  0.0059850215911865234 s \tTotal Time:  12.329270124435425 s \n",
      "\n",
      "\n",
      "\tEpisode 1973 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5340],\n",
      "        [1.0000, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50471019744873 \tStep Time:  0.005982875823974609 s \tTotal Time:  12.3352530002594 s \n",
      "\n",
      "\n",
      "\tEpisode 1974 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.5005]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4978],\n",
      "        [1.0000, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531932055950165 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.34123706817627 s \n",
      "\n",
      "\n",
      "\tEpisode 1975 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.4834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5211],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481890201568604 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.34722113609314 s \n",
      "\n",
      "\n",
      "\tEpisode 1976 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [1.0000, 0.4833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4600],\n",
      "        [1.0000, 0.4728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502846717834473 \tStep Time:  0.00598454475402832 s \tTotal Time:  12.353205680847168 s \n",
      "\n",
      "\n",
      "\tEpisode 1977 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5444],\n",
      "        [1.0000, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571930944919586 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.35918927192688 s \n",
      "\n",
      "\n",
      "\tEpisode 1978 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4931],\n",
      "        [1.0000, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.5862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537272453308105 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.365173101425171 s \n",
      "\n",
      "\n",
      "\tEpisode 1979 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4964],\n",
      "        [1.0000, 0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4941],\n",
      "        [1.0000, 0.4817]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497237503528595 \tStep Time:  0.007013082504272461 s \tTotal Time:  12.372186183929443 s \n",
      "\n",
      "\n",
      "\tEpisode 1980 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.4799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4987],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514490604400635 \tStep Time:  0.005951881408691406 s \tTotal Time:  12.378138065338135 s \n",
      "\n",
      "\n",
      "\tEpisode 1981 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4901],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4857],\n",
      "        [1.0000, 0.4817]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504310607910156 \tStep Time:  0.006980180740356445 s \tTotal Time:  12.385118246078491 s \n",
      "\n",
      "\n",
      "\tEpisode 1982 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4713],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53774380683899 \tStep Time:  0.0069811344146728516 s \tTotal Time:  12.392099380493164 s \n",
      "\n",
      "\n",
      "\tEpisode 1983 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4815],\n",
      "        [1.0000, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4934],\n",
      "        [1.0000, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518221378326416 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.398083209991455 s \n",
      "\n",
      "\n",
      "\tEpisode 1984 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503711223602295 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.404067277908325 s \n",
      "\n",
      "\n",
      "\tEpisode 1985 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.4919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503720879554749 \tStep Time:  0.006981372833251953 s \tTotal Time:  12.411048650741577 s \n",
      "\n",
      "\n",
      "\tEpisode 1986 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4988],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.4738]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514844834804535 \tStep Time:  0.006981849670410156 s \tTotal Time:  12.418030500411987 s \n",
      "\n",
      "\n",
      "\tEpisode 1987 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4803],\n",
      "        [1.0000, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514230251312256 \tStep Time:  0.007012367248535156 s \tTotal Time:  12.425042867660522 s \n",
      "\n",
      "\n",
      "\tEpisode 1988 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4819],\n",
      "        [1.0000, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4679],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511946558952332 \tStep Time:  0.005953550338745117 s \tTotal Time:  12.430996417999268 s \n",
      "\n",
      "\n",
      "\tEpisode 1989 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.4519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4482],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504224300384521 \tStep Time:  0.006014347076416016 s \tTotal Time:  12.437010765075684 s \n",
      "\n",
      "\n",
      "\tEpisode 1990 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4426],\n",
      "        [1.0000, 0.4710]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4805],\n",
      "        [1.0000, 0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530982673168182 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.442994594573975 s \n",
      "\n",
      "\n",
      "\tEpisode 1991 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.4182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529655575752258 \tStep Time:  0.0059583187103271484 s \tTotal Time:  12.448952913284302 s \n",
      "\n",
      "\n",
      "\tEpisode 1992 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.4919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4554],\n",
      "        [1.0000, 0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517493724822998 \tStep Time:  0.005982875823974609 s \tTotal Time:  12.454935789108276 s \n",
      "\n",
      "\n",
      "\tEpisode 1993 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4756],\n",
      "        [1.0000, 0.4703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511850833892822 \tStep Time:  0.006016254425048828 s \tTotal Time:  12.460952043533325 s \n",
      "\n",
      "\n",
      "\tEpisode 1994 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4928],\n",
      "        [1.0000, 0.5108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499717831611633 \tStep Time:  0.005955934524536133 s \tTotal Time:  12.466907978057861 s \n",
      "\n",
      "\n",
      "\tEpisode 1995 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5122],\n",
      "        [1.0000, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506661176681519 \tStep Time:  0.005020856857299805 s \tTotal Time:  12.471928834915161 s \n",
      "\n",
      "\n",
      "\tEpisode 1996 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5104],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5223],\n",
      "        [1.0000, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530489444732666 \tStep Time:  0.005949497222900391 s \tTotal Time:  12.477878332138062 s \n",
      "\n",
      "\n",
      "\tEpisode 1997 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.5439]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5475],\n",
      "        [1.0000, 0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482669234275818 \tStep Time:  0.007012605667114258 s \tTotal Time:  12.484890937805176 s \n",
      "\n",
      "\n",
      "\tEpisode 1998 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4540],\n",
      "        [1.0000, 0.4536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5196],\n",
      "        [1.0000, 0.4959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51543116569519 \tStep Time:  0.004988908767700195 s \tTotal Time:  12.489879846572876 s \n",
      "\n",
      "\n",
      "\tEpisode 1999 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5604],\n",
      "        [1.0000, 0.5323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5918],\n",
      "        [1.0000, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533881187438965 \tStep Time:  0.006623744964599609 s \tTotal Time:  12.496503591537476 s \n",
      "\n",
      "\n",
      "\tEpisode 2000 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5309],\n",
      "        [1.0000, 0.5696]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4592],\n",
      "        [1.0000, 0.5702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433241367340088 \tStep Time:  0.0053446292877197266 s \tTotal Time:  12.501848220825195 s \n",
      "\n",
      "\n",
      "\tEpisode 2001 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5140],\n",
      "        [1.0000, 0.5467]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5464],\n",
      "        [1.0000, 0.4618]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477742671966553 \tStep Time:  0.005986452102661133 s \tTotal Time:  12.507834672927856 s \n",
      "\n",
      "\n",
      "\tEpisode 2002 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4413],\n",
      "        [1.0000, 0.4780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5321],\n",
      "        [1.0000, 0.4938]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465531527996063 \tStep Time:  0.0059893131256103516 s \tTotal Time:  12.513823986053467 s \n",
      "\n",
      "\n",
      "\tEpisode 2003 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5935],\n",
      "        [1.0000, 0.5708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.4205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.397860527038574 \tStep Time:  0.005976438522338867 s \tTotal Time:  12.519800424575806 s \n",
      "\n",
      "\n",
      "\tEpisode 2004 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4722],\n",
      "        [1.0000, 0.4101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3699],\n",
      "        [1.0000, 0.4446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549665927886963 \tStep Time:  0.0059528350830078125 s \tTotal Time:  12.525753259658813 s \n",
      "\n",
      "\n",
      "\tEpisode 2005 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5154],\n",
      "        [1.0000, 0.4214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7525],\n",
      "        [1.0000, 0.5755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581856727600098 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.531737089157104 s \n",
      "\n",
      "\n",
      "\tEpisode 2006 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4200],\n",
      "        [1.0000, 0.4459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4149],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542556524276733 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.537720918655396 s \n",
      "\n",
      "\n",
      "\tEpisode 2007 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6008],\n",
      "        [1.0000, 0.4100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6201],\n",
      "        [1.0000, 0.6572]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468806803226471 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.543704986572266 s \n",
      "\n",
      "\n",
      "\tEpisode 2008 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5314],\n",
      "        [1.0000, 0.5626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6893],\n",
      "        [1.0000, 0.3234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504588603973389 \tStep Time:  0.006016969680786133 s \tTotal Time:  12.549721956253052 s \n",
      "\n",
      "\n",
      "\tEpisode 2009 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6297],\n",
      "        [1.0000, 0.6915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4156],\n",
      "        [1.0000, 0.6001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.390498161315918 \tStep Time:  0.0059854984283447266 s \tTotal Time:  12.555707454681396 s \n",
      "\n",
      "\n",
      "\tEpisode 2010 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5066],\n",
      "        [1.0000, 0.7552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3764],\n",
      "        [1.0000, 0.7504]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5246262550354 \tStep Time:  0.005949974060058594 s \tTotal Time:  12.561657428741455 s \n",
      "\n",
      "\n",
      "\tEpisode 2011 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3260],\n",
      "        [1.0000, 0.6376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6423],\n",
      "        [1.0000, 0.3047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.239329814910889 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.567641496658325 s \n",
      "\n",
      "\n",
      "\tEpisode 2012 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5476],\n",
      "        [1.0000, 0.6155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3838],\n",
      "        [1.0000, 0.6185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446436882019043 \tStep Time:  0.006014347076416016 s \tTotal Time:  12.573655843734741 s \n",
      "\n",
      "\n",
      "\tEpisode 2013 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6028],\n",
      "        [1.0000, 0.3221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4046],\n",
      "        [1.0000, 0.6095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502131938934326 \tStep Time:  0.004956245422363281 s \tTotal Time:  12.579609632492065 s \n",
      "\n",
      "\n",
      "\tEpisode 2014 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5054],\n",
      "        [1.0000, 0.5465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5595],\n",
      "        [1.0000, 0.6325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547284126281738 \tStep Time:  0.0069811344146728516 s \tTotal Time:  12.586590766906738 s \n",
      "\n",
      "\n",
      "\tEpisode 2015 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6025],\n",
      "        [1.0000, 0.2598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1781],\n",
      "        [1.0000, 0.1543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50074952840805 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.59257459640503 s \n",
      "\n",
      "\n",
      "\tEpisode 2016 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4404],\n",
      "        [1.0000, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1749],\n",
      "        [1.0000, 0.4639]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.390653133392334 \tStep Time:  0.006981611251831055 s \tTotal Time:  12.59955620765686 s \n",
      "\n",
      "\n",
      "\tEpisode 2017 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6115],\n",
      "        [1.0000, 0.1883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5259],\n",
      "        [1.0000, 0.1888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.239596366882324 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.605540037155151 s \n",
      "\n",
      "\n",
      "\tEpisode 2018 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6446],\n",
      "        [1.0000, 0.6484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555315017700195 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.611524105072021 s \n",
      "\n",
      "\n",
      "\tEpisode 2019 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4144],\n",
      "        [1.0000, 0.2687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6495],\n",
      "        [1.0000, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.69651710987091 \tStep Time:  0.006981611251831055 s \tTotal Time:  12.618505716323853 s \n",
      "\n",
      "\n",
      "\tEpisode 2020 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.7056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6796],\n",
      "        [1.0000, 0.6326]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517079830169678 \tStep Time:  0.0059833526611328125 s \tTotal Time:  12.624489068984985 s \n",
      "\n",
      "\n",
      "\tEpisode 2021 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5271],\n",
      "        [1.0000, 0.2071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3298],\n",
      "        [1.0000, 0.2019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53149938583374 \tStep Time:  0.006981372833251953 s \tTotal Time:  12.631470441818237 s \n",
      "\n",
      "\n",
      "\tEpisode 2022 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6378],\n",
      "        [1.0000, 0.6233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4193],\n",
      "        [1.0000, 0.6317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.427953958511353 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.63745403289795 s \n",
      "\n",
      "\n",
      "\tEpisode 2023 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4411],\n",
      "        [1.0000, 0.4735]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6660],\n",
      "        [1.0000, 0.6299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576027750968933 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.64343810081482 s \n",
      "\n",
      "\n",
      "\tEpisode 2024 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6272],\n",
      "        [1.0000, 0.5848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2827],\n",
      "        [1.0000, 0.6233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.710746169090271 \tStep Time:  0.00598597526550293 s \tTotal Time:  12.649424076080322 s \n",
      "\n",
      "\n",
      "\tEpisode 2025 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3805],\n",
      "        [1.0000, 0.2232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4754],\n",
      "        [1.0000, 0.5487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.357879638671875 \tStep Time:  0.005981922149658203 s \tTotal Time:  12.65540599822998 s \n",
      "\n",
      "\n",
      "\tEpisode 2026 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5747],\n",
      "        [1.0000, 0.5497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3641],\n",
      "        [1.0000, 0.5909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.417605817317963 \tStep Time:  0.0059850215911865234 s \tTotal Time:  12.661391019821167 s \n",
      "\n",
      "\n",
      "\tEpisode 2027 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4135],\n",
      "        [1.0000, 0.5321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5780],\n",
      "        [1.0000, 0.5477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.434378147125244 \tStep Time:  0.005986213684082031 s \tTotal Time:  12.667377233505249 s \n",
      "\n",
      "\n",
      "\tEpisode 2028 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6106],\n",
      "        [1.0000, 0.3560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.5823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49360466003418 \tStep Time:  0.006017446517944336 s \tTotal Time:  12.673394680023193 s \n",
      "\n",
      "\n",
      "\tEpisode 2029 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4513],\n",
      "        [1.0000, 0.1527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5803],\n",
      "        [1.0000, 0.3514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559320449829102 \tStep Time:  0.0059528350830078125 s \tTotal Time:  12.679347515106201 s \n",
      "\n",
      "\n",
      "\tEpisode 2030 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5090],\n",
      "        [1.0000, 0.5754]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5548],\n",
      "        [1.0000, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574862003326416 \tStep Time:  0.005986213684082031 s \tTotal Time:  12.685333728790283 s \n",
      "\n",
      "\n",
      "\tEpisode 2031 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4485],\n",
      "        [1.0000, 0.2473]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.3815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528162181377411 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.691317796707153 s \n",
      "\n",
      "\n",
      "\tEpisode 2032 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.5704]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3095],\n",
      "        [1.0000, 0.5756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424635410308838 \tStep Time:  0.006981372833251953 s \tTotal Time:  12.698299169540405 s \n",
      "\n",
      "\n",
      "\tEpisode 2033 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5766],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3890],\n",
      "        [1.0000, 0.5763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59742021560669 \tStep Time:  0.004986286163330078 s \tTotal Time:  12.703285455703735 s \n",
      "\n",
      "\n",
      "\tEpisode 2034 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5493],\n",
      "        [1.0000, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4832],\n",
      "        [1.0000, 0.3434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490965366363525 \tStep Time:  0.0060100555419921875 s \tTotal Time:  12.709295511245728 s \n",
      "\n",
      "\n",
      "\tEpisode 2035 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5757],\n",
      "        [1.0000, 0.5627]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489420354366302 \tStep Time:  0.0069561004638671875 s \tTotal Time:  12.716251611709595 s \n",
      "\n",
      "\n",
      "\tEpisode 2036 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5536],\n",
      "        [1.0000, 0.5522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5690],\n",
      "        [1.0000, 0.4256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480853080749512 \tStep Time:  0.0069806575775146484 s \tTotal Time:  12.72323226928711 s \n",
      "\n",
      "\n",
      "\tEpisode 2037 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3832],\n",
      "        [1.0000, 0.5755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4090],\n",
      "        [1.0000, 0.2791]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.709429264068604 \tStep Time:  0.0059850215911865234 s \tTotal Time:  12.729217290878296 s \n",
      "\n",
      "\n",
      "\tEpisode 2038 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4192],\n",
      "        [1.0000, 0.2278]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5715],\n",
      "        [1.0000, 0.6043]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.645736694335938 \tStep Time:  0.006980419158935547 s \tTotal Time:  12.736197710037231 s \n",
      "\n",
      "\n",
      "\tEpisode 2039 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5949],\n",
      "        [1.0000, 0.6039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5115],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445110321044922 \tStep Time:  0.00498652458190918 s \tTotal Time:  12.74118423461914 s \n",
      "\n",
      "\n",
      "\tEpisode 2040 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6127],\n",
      "        [1.0000, 0.6007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6019],\n",
      "        [1.0000, 0.4650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.603105068206787 \tStep Time:  0.007013559341430664 s \tTotal Time:  12.748197793960571 s \n",
      "\n",
      "\n",
      "\tEpisode 2041 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3688],\n",
      "        [1.0000, 0.6041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6073],\n",
      "        [1.0000, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36127519607544 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.754181861877441 s \n",
      "\n",
      "\n",
      "\tEpisode 2042 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6054],\n",
      "        [1.0000, 0.3787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3533],\n",
      "        [1.0000, 0.6142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559864044189453 \tStep Time:  0.00495457649230957 s \tTotal Time:  12.759136438369751 s \n",
      "\n",
      "\n",
      "\tEpisode 2043 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6447],\n",
      "        [1.0000, 0.6164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3948],\n",
      "        [1.0000, 0.3930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553624749183655 \tStep Time:  0.00601506233215332 s \tTotal Time:  12.766149044036865 s \n",
      "\n",
      "\n",
      "\tEpisode 2044 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3456],\n",
      "        [1.0000, 0.5898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6041],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.379388809204102 \tStep Time:  0.0059528350830078125 s \tTotal Time:  12.772101879119873 s \n",
      "\n",
      "\n",
      "\tEpisode 2045 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5989],\n",
      "        [1.0000, 0.3443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3557],\n",
      "        [1.0000, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575846672058105 \tStep Time:  0.0069828033447265625 s \tTotal Time:  12.7790846824646 s \n",
      "\n",
      "\n",
      "\tEpisode 2046 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5940],\n",
      "        [1.0000, 0.3464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3546],\n",
      "        [1.0000, 0.3956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4387845993042 \tStep Time:  0.005982398986816406 s \tTotal Time:  12.785067081451416 s \n",
      "\n",
      "\n",
      "\tEpisode 2047 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5176],\n",
      "        [1.0000, 0.4151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5612],\n",
      "        [1.0000, 0.5890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.406957626342773 \tStep Time:  0.006981372833251953 s \tTotal Time:  12.792048454284668 s \n",
      "\n",
      "\n",
      "\tEpisode 2048 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.4462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6077],\n",
      "        [1.0000, 0.5998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551848769187927 \tStep Time:  0.00598454475402832 s \tTotal Time:  12.798032999038696 s \n",
      "\n",
      "\n",
      "\tEpisode 2049 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4324],\n",
      "        [1.0000, 0.6054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5795],\n",
      "        [1.0000, 0.6067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452059388160706 \tStep Time:  0.005983829498291016 s \tTotal Time:  12.804016828536987 s \n",
      "\n",
      "\n",
      "\tEpisode 2050 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5686],\n",
      "        [1.0000, 0.5739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6311],\n",
      "        [1.0000, 0.5547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540906429290771 \tStep Time:  0.005984783172607422 s \tTotal Time:  12.810001611709595 s \n",
      "\n",
      "\n",
      "\tEpisode 2051 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4916],\n",
      "        [1.0000, 0.6325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3877],\n",
      "        [1.0000, 0.6257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.719410240650177 \tStep Time:  0.006980419158935547 s \tTotal Time:  12.81698203086853 s \n",
      "\n",
      "\n",
      "\tEpisode 2052 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6197],\n",
      "        [1.0000, 0.6102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3828],\n",
      "        [1.0000, 0.3502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558710634708405 \tStep Time:  0.0110015869140625 s \tTotal Time:  12.827983617782593 s \n",
      "\n",
      "\n",
      "\tEpisode 2053 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4257],\n",
      "        [1.0000, 0.6033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5636],\n",
      "        [1.0000, 0.5420]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57845687866211 \tStep Time:  0.006975889205932617 s \tTotal Time:  12.834959506988525 s \n",
      "\n",
      "\n",
      "\tEpisode 2054 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5227],\n",
      "        [1.0000, 0.4612]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3984],\n",
      "        [1.0000, 0.3838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.626053810119629 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.840943098068237 s \n",
      "\n",
      "\n",
      "\tEpisode 2055 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5259],\n",
      "        [1.0000, 0.5525]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544983863830566 \tStep Time:  0.004961252212524414 s \tTotal Time:  12.845904350280762 s \n",
      "\n",
      "\n",
      "\tEpisode 2056 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5255],\n",
      "        [1.0000, 0.5597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5426],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535941123962402 \tStep Time:  0.006015777587890625 s \tTotal Time:  12.851920127868652 s \n",
      "\n",
      "\n",
      "\tEpisode 2057 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5355],\n",
      "        [1.0000, 0.4970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4160],\n",
      "        [1.0000, 0.4541]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597320079803467 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.857903718948364 s \n",
      "\n",
      "\n",
      "\tEpisode 2058 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4897],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5206],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489763736724854 \tStep Time:  0.005986213684082031 s \tTotal Time:  12.863889932632446 s \n",
      "\n",
      "\n",
      "\tEpisode 2059 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4565],\n",
      "        [1.0000, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5154],\n",
      "        [1.0000, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550470471382141 \tStep Time:  0.006024360656738281 s \tTotal Time:  12.869914293289185 s \n",
      "\n",
      "\n",
      "\tEpisode 2060 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4229],\n",
      "        [1.0000, 0.4578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4988],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47586441040039 \tStep Time:  0.004987001419067383 s \tTotal Time:  12.874901294708252 s \n",
      "\n",
      "\n",
      "\tEpisode 2061 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5074],\n",
      "        [1.0000, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4921],\n",
      "        [1.0000, 0.4868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525167465209961 \tStep Time:  0.005991935729980469 s \tTotal Time:  12.881890296936035 s \n",
      "\n",
      "\n",
      "\tEpisode 2062 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4131],\n",
      "        [1.0000, 0.4050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5222],\n",
      "        [1.0000, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.622700691223145 \tStep Time:  0.005982637405395508 s \tTotal Time:  12.88787293434143 s \n",
      "\n",
      "\n",
      "\tEpisode 2063 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5218],\n",
      "        [1.0000, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5185],\n",
      "        [1.0000, 0.5180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513644695281982 \tStep Time:  0.004987001419067383 s \tTotal Time:  12.892859935760498 s \n",
      "\n",
      "\n",
      "\tEpisode 2064 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4821],\n",
      "        [1.0000, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5698],\n",
      "        [1.0000, 0.5163]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515721321105957 \tStep Time:  0.005984067916870117 s \tTotal Time:  12.898844003677368 s \n",
      "\n",
      "\n",
      "\tEpisode 2065 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5104],\n",
      "        [1.0000, 0.5230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5073],\n",
      "        [1.0000, 0.4396]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554247379302979 \tStep Time:  0.005983591079711914 s \tTotal Time:  12.90482759475708 s \n",
      "\n",
      "\n",
      "\tEpisode 2066 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5207],\n",
      "        [1.0000, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5153],\n",
      "        [1.0000, 0.5278]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510607719421387 \tStep Time:  0.006015300750732422 s \tTotal Time:  12.910842895507812 s \n",
      "\n",
      "\n",
      "\tEpisode 2067 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5201],\n",
      "        [1.0000, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5227],\n",
      "        [1.0000, 0.4986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489631533622742 \tStep Time:  0.0059528350830078125 s \tTotal Time:  12.91679573059082 s \n",
      "\n",
      "\n",
      "\tEpisode 2068 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5209],\n",
      "        [1.0000, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5525],\n",
      "        [1.0000, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498848497867584 \tStep Time:  0.006017923355102539 s \tTotal Time:  12.922813653945923 s \n",
      "\n",
      "\n",
      "\tEpisode 2069 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [1.0000, 0.5426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4745],\n",
      "        [1.0000, 0.4565]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56953477859497 \tStep Time:  0.004987478256225586 s \tTotal Time:  12.927801132202148 s \n",
      "\n",
      "\n",
      "\tEpisode 2070 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [1.0000, 0.5526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478074312210083 \tStep Time:  0.0059833526611328125 s \tTotal Time:  12.93474793434143 s \n",
      "\n",
      "\n",
      "\tEpisode 2071 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5898],\n",
      "        [1.0000, 0.5280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5014],\n",
      "        [1.0000, 0.4763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462653517723083 \tStep Time:  0.006025791168212891 s \tTotal Time:  12.940773725509644 s \n",
      "\n",
      "\n",
      "\tEpisode 2072 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5393],\n",
      "        [1.0000, 0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5723],\n",
      "        [1.0000, 0.4627]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432946860790253 \tStep Time:  0.005341291427612305 s \tTotal Time:  12.946115016937256 s \n",
      "\n",
      "\n",
      "\tEpisode 2073 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5027],\n",
      "        [1.0000, 0.6045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5065],\n",
      "        [1.0000, 0.5911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517621517181396 \tStep Time:  0.004986763000488281 s \tTotal Time:  12.95170259475708 s \n",
      "\n",
      "\n",
      "\tEpisode 2074 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.5373]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5332],\n",
      "        [1.0000, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555037021636963 \tStep Time:  0.0059850215911865234 s \tTotal Time:  12.957687616348267 s \n",
      "\n",
      "\n",
      "\tEpisode 2075 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4603],\n",
      "        [1.0000, 0.6161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4926],\n",
      "        [1.0000, 0.6569]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560569763183594 \tStep Time:  0.006014585494995117 s \tTotal Time:  12.963702201843262 s \n",
      "\n",
      "\n",
      "\tEpisode 2076 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4560],\n",
      "        [1.0000, 0.5743]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6180],\n",
      "        [1.0000, 0.6354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63490104675293 \tStep Time:  0.00598454475402832 s \tTotal Time:  12.96968674659729 s \n",
      "\n",
      "\n",
      "\tEpisode 2077 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5981],\n",
      "        [1.0000, 0.6003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5986],\n",
      "        [1.0000, 0.5791]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531628012657166 \tStep Time:  0.00598597526550293 s \tTotal Time:  12.975672721862793 s \n",
      "\n",
      "\n",
      "\tEpisode 2078 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4868],\n",
      "        [1.0000, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5899],\n",
      "        [1.0000, 0.5918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.404606342315674 \tStep Time:  0.006947755813598633 s \tTotal Time:  12.982620477676392 s \n",
      "\n",
      "\n",
      "\tEpisode 2079 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.5886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4519],\n",
      "        [1.0000, 0.5682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500316500663757 \tStep Time:  0.005983114242553711 s \tTotal Time:  12.988603591918945 s \n",
      "\n",
      "\n",
      "\tEpisode 2080 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4384],\n",
      "        [1.0000, 0.4575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5779],\n",
      "        [1.0000, 0.4763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591532230377197 \tStep Time:  0.0070018768310546875 s \tTotal Time:  12.99560546875 s \n",
      "\n",
      "\n",
      "\tEpisode 2081 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4707],\n",
      "        [1.0000, 0.4730]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5619],\n",
      "        [1.0000, 0.5416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51996660232544 \tStep Time:  0.0059642791748046875 s \tTotal Time:  13.001569747924805 s \n",
      "\n",
      "\n",
      "\tEpisode 2082 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5600],\n",
      "        [1.0000, 0.4343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5559],\n",
      "        [1.0000, 0.4570]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518861293792725 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.007553577423096 s \n",
      "\n",
      "\n",
      "\tEpisode 2083 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4416],\n",
      "        [1.0000, 0.4766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4577],\n",
      "        [1.0000, 0.5772]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455883026123047 \tStep Time:  0.005984783172607422 s \tTotal Time:  13.013538360595703 s \n",
      "\n",
      "\n",
      "\tEpisode 2084 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5459],\n",
      "        [1.0000, 0.5352]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4367],\n",
      "        [1.0000, 0.4531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513575553894043 \tStep Time:  0.005982875823974609 s \tTotal Time:  13.019521236419678 s \n",
      "\n",
      "\n",
      "\tEpisode 2085 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5420],\n",
      "        [1.0000, 0.4247]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4242],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.430224895477295 \tStep Time:  0.006981611251831055 s \tTotal Time:  13.026502847671509 s \n",
      "\n",
      "\n",
      "\tEpisode 2086 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4252],\n",
      "        [1.0000, 0.4502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4352],\n",
      "        [1.0000, 0.4686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487976133823395 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.032486915588379 s \n",
      "\n",
      "\n",
      "\tEpisode 2087 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5406],\n",
      "        [1.0000, 0.5573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524857878684998 \tStep Time:  0.006981372833251953 s \tTotal Time:  13.03946828842163 s \n",
      "\n",
      "\n",
      "\tEpisode 2088 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4681],\n",
      "        [1.0000, 0.4288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5980],\n",
      "        [1.0000, 0.4772]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47199010848999 \tStep Time:  0.0049860477447509766 s \tTotal Time:  13.044454336166382 s \n",
      "\n",
      "\n",
      "\tEpisode 2089 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4627],\n",
      "        [1.0000, 0.5661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.6611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562334060668945 \tStep Time:  0.006982088088989258 s \tTotal Time:  13.051436424255371 s \n",
      "\n",
      "\n",
      "\tEpisode 2090 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5600],\n",
      "        [1.0000, 0.5788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.5499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572953224182129 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.057420253753662 s \n",
      "\n",
      "\n",
      "\tEpisode 2091 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4558],\n",
      "        [1.0000, 0.4938]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4743],\n",
      "        [1.0000, 0.5371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539053440093994 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.063404083251953 s \n",
      "\n",
      "\n",
      "\tEpisode 2092 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4932],\n",
      "        [1.0000, 0.4893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.4582]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535458087921143 \tStep Time:  0.006024599075317383 s \tTotal Time:  13.06942868232727 s \n",
      "\n",
      "\n",
      "\tEpisode 2093 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4542],\n",
      "        [1.0000, 0.6457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4965],\n",
      "        [1.0000, 0.4655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.626805305480957 \tStep Time:  0.005020618438720703 s \tTotal Time:  13.074449300765991 s \n",
      "\n",
      "\n",
      "\tEpisode 2094 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5144],\n",
      "        [1.0000, 0.4949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4580],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533923149108887 \tStep Time:  0.005950450897216797 s \tTotal Time:  13.080399751663208 s \n",
      "\n",
      "\n",
      "\tEpisode 2095 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4757],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4582],\n",
      "        [1.0000, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512511372566223 \tStep Time:  0.006985902786254883 s \tTotal Time:  13.087385654449463 s \n",
      "\n",
      "\n",
      "\tEpisode 2096 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4621],\n",
      "        [1.0000, 0.4767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5082],\n",
      "        [1.0000, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493110179901123 \tStep Time:  0.0049855709075927734 s \tTotal Time:  13.092371225357056 s \n",
      "\n",
      "\n",
      "\tEpisode 2097 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4610],\n",
      "        [1.0000, 0.4603]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4555],\n",
      "        [1.0000, 0.4720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517509818077087 \tStep Time:  0.0060155391693115234 s \tTotal Time:  13.098386764526367 s \n",
      "\n",
      "\n",
      "\tEpisode 2098 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4907],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4376],\n",
      "        [1.0000, 0.4793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476822197437286 \tStep Time:  0.005984783172607422 s \tTotal Time:  13.104371547698975 s \n",
      "\n",
      "\n",
      "\tEpisode 2099 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4564],\n",
      "        [1.0000, 0.5536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5380],\n",
      "        [1.0000, 0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559816360473633 \tStep Time:  0.005982637405395508 s \tTotal Time:  13.11035418510437 s \n",
      "\n",
      "\n",
      "\tEpisode 2100 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4725],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4546],\n",
      "        [1.0000, 0.5624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460614204406738 \tStep Time:  0.0059528350830078125 s \tTotal Time:  13.116307020187378 s \n",
      "\n",
      "\n",
      "\tEpisode 2101 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4552],\n",
      "        [1.0000, 0.4340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4507],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504725098609924 \tStep Time:  0.0060160160064697266 s \tTotal Time:  13.122323036193848 s \n",
      "\n",
      "\n",
      "\tEpisode 2102 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4745],\n",
      "        [1.0000, 0.3696]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5616],\n",
      "        [1.0000, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595324695110321 \tStep Time:  0.005952119827270508 s \tTotal Time:  13.128275156021118 s \n",
      "\n",
      "\n",
      "\tEpisode 2103 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5555],\n",
      "        [1.0000, 0.4350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4627],\n",
      "        [1.0000, 0.4442]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459509253501892 \tStep Time:  0.00598454475402832 s \tTotal Time:  13.134259700775146 s \n",
      "\n",
      "\n",
      "\tEpisode 2104 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5182],\n",
      "        [1.0000, 0.3545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5139],\n",
      "        [1.0000, 0.4257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.643527328968048 \tStep Time:  0.0059833526611328125 s \tTotal Time:  13.14024305343628 s \n",
      "\n",
      "\n",
      "\tEpisode 2105 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5042],\n",
      "        [1.0000, 0.3773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4556],\n",
      "        [1.0000, 0.4921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54615831375122 \tStep Time:  0.006074666976928711 s \tTotal Time:  13.146317720413208 s \n",
      "\n",
      "\n",
      "\tEpisode 2106 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5686],\n",
      "        [1.0000, 0.4956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4978],\n",
      "        [1.0000, 0.5201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469547927379608 \tStep Time:  0.005924701690673828 s \tTotal Time:  13.152242422103882 s \n",
      "\n",
      "\n",
      "\tEpisode 2107 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5003],\n",
      "        [1.0000, 0.6631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5654],\n",
      "        [1.0000, 0.5250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586158275604248 \tStep Time:  0.00498652458190918 s \tTotal Time:  13.157228946685791 s \n",
      "\n",
      "\n",
      "\tEpisode 2108 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5021],\n",
      "        [1.0000, 0.6097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4904],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556684017181396 \tStep Time:  0.005953073501586914 s \tTotal Time:  13.163182020187378 s \n",
      "\n",
      "\n",
      "\tEpisode 2109 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5042],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5078],\n",
      "        [1.0000, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516284942626953 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.169165849685669 s \n",
      "\n",
      "\n",
      "\tEpisode 2110 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5060],\n",
      "        [1.0000, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5231],\n",
      "        [1.0000, 0.5437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497303068637848 \tStep Time:  0.006020784378051758 s \tTotal Time:  13.17518663406372 s \n",
      "\n",
      "\n",
      "\tEpisode 2111 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5244],\n",
      "        [1.0000, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497882843017578 \tStep Time:  0.0069446563720703125 s \tTotal Time:  13.182131290435791 s \n",
      "\n",
      "\n",
      "\tEpisode 2112 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6849],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5074],\n",
      "        [1.0000, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.614195823669434 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.188115119934082 s \n",
      "\n",
      "\n",
      "\tEpisode 2113 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4905],\n",
      "        [1.0000, 0.5095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [1.0000, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520516395568848 \tStep Time:  0.005984783172607422 s \tTotal Time:  13.19409990310669 s \n",
      "\n",
      "\n",
      "\tEpisode 2114 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5065],\n",
      "        [1.0000, 0.5233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.4720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555019855499268 \tStep Time:  0.00698089599609375 s \tTotal Time:  13.201080799102783 s \n",
      "\n",
      "\n",
      "\tEpisode 2115 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4824],\n",
      "        [1.0000, 0.5688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5502],\n",
      "        [1.0000, 0.4565]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41917097568512 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.207064628601074 s \n",
      "\n",
      "\n",
      "\tEpisode 2116 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5529],\n",
      "        [1.0000, 0.5505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4511],\n",
      "        [1.0000, 0.4399]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.409182071685791 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.213048458099365 s \n",
      "\n",
      "\n",
      "\tEpisode 2117 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5847],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4137],\n",
      "        [1.0000, 0.4979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524925231933594 \tStep Time:  0.0060155391693115234 s \tTotal Time:  13.219063997268677 s \n",
      "\n",
      "\n",
      "\tEpisode 2118 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5689],\n",
      "        [1.0000, 0.4011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.5414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58021593093872 \tStep Time:  0.005951881408691406 s \tTotal Time:  13.225015878677368 s \n",
      "\n",
      "\n",
      "\tEpisode 2119 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4694],\n",
      "        [1.0000, 0.6063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3997],\n",
      "        [1.0000, 0.4734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549731731414795 \tStep Time:  0.006982088088989258 s \tTotal Time:  13.231997966766357 s \n",
      "\n",
      "\n",
      "\tEpisode 2120 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4848],\n",
      "        [1.0000, 0.3771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6792],\n",
      "        [1.0000, 0.6529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61055850982666 \tStep Time:  0.0070149898529052734 s \tTotal Time:  13.239012956619263 s \n",
      "\n",
      "\n",
      "\tEpisode 2121 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4898],\n",
      "        [1.0000, 0.5405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4872],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532681941986084 \tStep Time:  0.005982160568237305 s \tTotal Time:  13.2449951171875 s \n",
      "\n",
      "\n",
      "\tEpisode 2122 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5923],\n",
      "        [1.0000, 0.3742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5749],\n",
      "        [1.0000, 0.5315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.392284393310547 \tStep Time:  0.0069811344146728516 s \tTotal Time:  13.251976251602173 s \n",
      "\n",
      "\n",
      "\tEpisode 2123 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3675],\n",
      "        [1.0000, 0.4305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4519],\n",
      "        [1.0000, 0.5624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496726512908936 \tStep Time:  0.006949663162231445 s \tTotal Time:  13.258925914764404 s \n",
      "\n",
      "\n",
      "\tEpisode 2124 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4700],\n",
      "        [1.0000, 0.4406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3691],\n",
      "        [1.0000, 0.5823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544107913970947 \tStep Time:  0.006982326507568359 s \tTotal Time:  13.265908241271973 s \n",
      "\n",
      "\n",
      "\tEpisode 2125 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5724],\n",
      "        [1.0000, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4823],\n",
      "        [1.0000, 0.4684]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450733959674835 \tStep Time:  0.00698089599609375 s \tTotal Time:  13.272889137268066 s \n",
      "\n",
      "\n",
      "\tEpisode 2126 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3966],\n",
      "        [1.0000, 0.5507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5560],\n",
      "        [1.0000, 0.4392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.655421614646912 \tStep Time:  0.006982088088989258 s \tTotal Time:  13.279871225357056 s \n",
      "\n",
      "\n",
      "\tEpisode 2127 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5103],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5910],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465825617313385 \tStep Time:  0.006980419158935547 s \tTotal Time:  13.286851644515991 s \n",
      "\n",
      "\n",
      "\tEpisode 2128 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5803],\n",
      "        [1.0000, 0.4297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505868434906006 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.292835474014282 s \n",
      "\n",
      "\n",
      "\tEpisode 2129 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.6307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3722],\n",
      "        [1.0000, 0.7133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.337174892425537 \tStep Time:  0.011968374252319336 s \tTotal Time:  13.304803848266602 s \n",
      "\n",
      "\n",
      "\tEpisode 2130 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5455],\n",
      "        [1.0000, 0.4279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4523],\n",
      "        [1.0000, 0.4674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57728260755539 \tStep Time:  0.011967897415161133 s \tTotal Time:  13.316771745681763 s \n",
      "\n",
      "\n",
      "\tEpisode 2131 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.4754]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3614],\n",
      "        [1.0000, 0.5784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418631732463837 \tStep Time:  0.009974241256713867 s \tTotal Time:  13.326745986938477 s \n",
      "\n",
      "\n",
      "\tEpisode 2132 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5795],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6056],\n",
      "        [1.0000, 0.5234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.611642360687256 \tStep Time:  0.00797891616821289 s \tTotal Time:  13.33472490310669 s \n",
      "\n",
      "\n",
      "\tEpisode 2133 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.4032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4752],\n",
      "        [1.0000, 0.3848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43035364151001 \tStep Time:  0.005983114242553711 s \tTotal Time:  13.340708017349243 s \n",
      "\n",
      "\n",
      "\tEpisode 2134 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4218],\n",
      "        [1.0000, 0.4447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3762],\n",
      "        [1.0000, 0.4614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584753513336182 \tStep Time:  0.0065538883209228516 s \tTotal Time:  13.347261905670166 s \n",
      "\n",
      "\n",
      "\tEpisode 2135 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3730],\n",
      "        [1.0000, 0.5282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3478],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.69473123550415 \tStep Time:  0.00641179084777832 s \tTotal Time:  13.353673696517944 s \n",
      "\n",
      "\n",
      "\tEpisode 2136 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5003],\n",
      "        [1.0000, 0.5914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3562],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.661373138427734 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.359657764434814 s \n",
      "\n",
      "\n",
      "\tEpisode 2137 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3586],\n",
      "        [1.0000, 0.5843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4052],\n",
      "        [1.0000, 0.5916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573275089263916 \tStep Time:  0.0059888362884521484 s \tTotal Time:  13.365646600723267 s \n",
      "\n",
      "\n",
      "\tEpisode 2138 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6012],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4305],\n",
      "        [1.0000, 0.5958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.673348426818848 \tStep Time:  0.005987644195556641 s \tTotal Time:  13.371634244918823 s \n",
      "\n",
      "\n",
      "\tEpisode 2139 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5088],\n",
      "        [1.0000, 0.4464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4972],\n",
      "        [1.0000, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477765083312988 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.377618074417114 s \n",
      "\n",
      "\n",
      "\tEpisode 2140 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5001],\n",
      "        [1.0000, 0.5658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.4802]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484726965427399 \tStep Time:  0.0070149898529052734 s \tTotal Time:  13.38463306427002 s \n",
      "\n",
      "\n",
      "\tEpisode 2141 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5142],\n",
      "        [1.0000, 0.4850]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533525943756104 \tStep Time:  0.0060193538665771484 s \tTotal Time:  13.390652418136597 s \n",
      "\n",
      "\n",
      "\tEpisode 2142 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5968],\n",
      "        [1.0000, 0.4978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5071],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475389003753662 \tStep Time:  0.006376028060913086 s \tTotal Time:  13.39702844619751 s \n",
      "\n",
      "\n",
      "\tEpisode 2143 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [1.0000, 0.6885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5374],\n",
      "        [1.0000, 0.5721]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.598678588867188 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.403584003448486 s \n",
      "\n",
      "\n",
      "\tEpisode 2144 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4834],\n",
      "        [1.0000, 0.6108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449067115783691 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.409568071365356 s \n",
      "\n",
      "\n",
      "\tEpisode 2145 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5388],\n",
      "        [1.0000, 0.5381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5663],\n",
      "        [1.0000, 0.4669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461509943008423 \tStep Time:  0.006981372833251953 s \tTotal Time:  13.416549444198608 s \n",
      "\n",
      "\n",
      "\tEpisode 2146 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5703],\n",
      "        [1.0000, 0.4433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5721],\n",
      "        [1.0000, 0.6114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563286185264587 \tStep Time:  0.007981300354003906 s \tTotal Time:  13.424530744552612 s \n",
      "\n",
      "\n",
      "\tEpisode 2147 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6578],\n",
      "        [1.0000, 0.6083]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.5757]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.600709438323975 \tStep Time:  0.007978200912475586 s \tTotal Time:  13.432508945465088 s \n",
      "\n",
      "\n",
      "\tEpisode 2148 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4087],\n",
      "        [1.0000, 0.5700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4591],\n",
      "        [1.0000, 0.4221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.417556703090668 \tStep Time:  0.00698089599609375 s \tTotal Time:  13.439489841461182 s \n",
      "\n",
      "\n",
      "\tEpisode 2149 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5168],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491344451904297 \tStep Time:  0.00598454475402832 s \tTotal Time:  13.44547438621521 s \n",
      "\n",
      "\n",
      "\tEpisode 2150 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4004],\n",
      "        [1.0000, 0.5207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4946],\n",
      "        [1.0000, 0.4846]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54207992553711 \tStep Time:  0.007978439331054688 s \tTotal Time:  13.453452825546265 s \n",
      "\n",
      "\n",
      "\tEpisode 2151 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5575],\n",
      "        [1.0000, 0.3980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4641],\n",
      "        [1.0000, 0.5430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489995956420898 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.459436655044556 s \n",
      "\n",
      "\n",
      "\tEpisode 2152 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5934],\n",
      "        [1.0000, 0.3462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4599],\n",
      "        [1.0000, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513516902923584 \tStep Time:  0.006981372833251953 s \tTotal Time:  13.466418027877808 s \n",
      "\n",
      "\n",
      "\tEpisode 2153 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3994],\n",
      "        [1.0000, 0.4749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4669],\n",
      "        [1.0000, 0.5690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.430728435516357 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.472401857376099 s \n",
      "\n",
      "\n",
      "\tEpisode 2154 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5563],\n",
      "        [1.0000, 0.3751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3394],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501197576522827 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.478385925292969 s \n",
      "\n",
      "\n",
      "\tEpisode 2155 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5334],\n",
      "        [1.0000, 0.3314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4952],\n",
      "        [1.0000, 0.3963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514225959777832 \tStep Time:  0.006981372833251953 s \tTotal Time:  13.48536729812622 s \n",
      "\n",
      "\n",
      "\tEpisode 2156 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6347],\n",
      "        [1.0000, 0.2636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3094],\n",
      "        [1.0000, 0.4525]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.680732071399689 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.49135136604309 s \n",
      "\n",
      "\n",
      "\tEpisode 2157 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4627],\n",
      "        [1.0000, 0.4522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.5570]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56054413318634 \tStep Time:  0.005983591079711914 s \tTotal Time:  13.497334957122803 s \n",
      "\n",
      "\n",
      "\tEpisode 2158 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5516],\n",
      "        [1.0000, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3642],\n",
      "        [1.0000, 0.5240]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579696476459503 \tStep Time:  0.006981372833251953 s \tTotal Time:  13.504316329956055 s \n",
      "\n",
      "\n",
      "\tEpisode 2159 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5578],\n",
      "        [1.0000, 0.4753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3839],\n",
      "        [1.0000, 0.4561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424222946166992 \tStep Time:  0.004987001419067383 s \tTotal Time:  13.509303331375122 s \n",
      "\n",
      "\n",
      "\tEpisode 2160 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5311],\n",
      "        [1.0000, 0.4384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5820],\n",
      "        [1.0000, 0.5118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.434977531433105 \tStep Time:  0.0069811344146728516 s \tTotal Time:  13.516284465789795 s \n",
      "\n",
      "\n",
      "\tEpisode 2161 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5830],\n",
      "        [1.0000, 0.4232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.5222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453049182891846 \tStep Time:  0.005984306335449219 s \tTotal Time:  13.522268772125244 s \n",
      "\n",
      "\n",
      "\tEpisode 2162 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5615],\n",
      "        [1.0000, 0.5773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3787],\n",
      "        [1.0000, 0.5765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.619948327541351 \tStep Time:  0.005983591079711914 s \tTotal Time:  13.528252363204956 s \n",
      "\n",
      "\n",
      "\tEpisode 2163 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6694],\n",
      "        [1.0000, 0.3550]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5016],\n",
      "        [1.0000, 0.5828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34019261598587 \tStep Time:  0.006981372833251953 s \tTotal Time:  13.535233736038208 s \n",
      "\n",
      "\n",
      "\tEpisode 2164 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4348],\n",
      "        [1.0000, 0.5428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4013],\n",
      "        [1.0000, 0.3829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46197509765625 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.541217803955078 s \n",
      "\n",
      "\n",
      "\tEpisode 2165 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4307],\n",
      "        [1.0000, 0.5965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5446],\n",
      "        [1.0000, 0.4606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559146881103516 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.547201871871948 s \n",
      "\n",
      "\n",
      "\tEpisode 2166 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5983],\n",
      "        [1.0000, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5579],\n",
      "        [1.0000, 0.4823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436310589313507 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.55318570137024 s \n",
      "\n",
      "\n",
      "\tEpisode 2167 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4689],\n",
      "        [1.0000, 0.5352]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6826],\n",
      "        [1.0000, 0.4316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.625252068042755 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.55916976928711 s \n",
      "\n",
      "\n",
      "\tEpisode 2168 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3899],\n",
      "        [1.0000, 0.4977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4031],\n",
      "        [1.0000, 0.3260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440920352935791 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.56515383720398 s \n",
      "\n",
      "\n",
      "\tEpisode 2169 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4231],\n",
      "        [1.0000, 0.3345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6250],\n",
      "        [1.0000, 0.3231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.353896141052246 \tStep Time:  0.00498652458190918 s \tTotal Time:  13.570140361785889 s \n",
      "\n",
      "\n",
      "\tEpisode 2170 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3737],\n",
      "        [1.0000, 0.4593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3365],\n",
      "        [1.0000, 0.3375]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.624090671539307 \tStep Time:  0.005984306335449219 s \tTotal Time:  13.576124668121338 s \n",
      "\n",
      "\n",
      "\tEpisode 2171 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4627],\n",
      "        [1.0000, 0.6328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5434],\n",
      "        [1.0000, 0.4175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.671703517436981 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.58310604095459 s \n",
      "\n",
      "\n",
      "\tEpisode 2172 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5388],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5978],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48723316192627 \tStep Time:  0.006980419158935547 s \tTotal Time:  13.590086460113525 s \n",
      "\n",
      "\n",
      "\tEpisode 2173 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3842],\n",
      "        [1.0000, 0.6524]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4667],\n",
      "        [1.0000, 0.4433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468113422393799 \tStep Time:  0.0049860477447509766 s \tTotal Time:  13.595072507858276 s \n",
      "\n",
      "\n",
      "\tEpisode 2174 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4935],\n",
      "        [1.0000, 0.5141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4941],\n",
      "        [1.0000, 0.4040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548754215240479 \tStep Time:  0.006982326507568359 s \tTotal Time:  13.602054834365845 s \n",
      "\n",
      "\n",
      "\tEpisode 2175 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4592],\n",
      "        [1.0000, 0.3839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4116],\n",
      "        [1.0000, 0.5359]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497575283050537 \tStep Time:  0.00498652458190918 s \tTotal Time:  13.607041358947754 s \n",
      "\n",
      "\n",
      "\tEpisode 2176 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5730],\n",
      "        [1.0000, 0.5926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5946],\n",
      "        [1.0000, 0.6006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532933354377747 \tStep Time:  0.006980419158935547 s \tTotal Time:  13.61402177810669 s \n",
      "\n",
      "\n",
      "\tEpisode 2177 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6023],\n",
      "        [1.0000, 0.6390]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4985],\n",
      "        [1.0000, 0.5841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504768371582031 \tStep Time:  0.00598454475402832 s \tTotal Time:  13.620006322860718 s \n",
      "\n",
      "\n",
      "\tEpisode 2178 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6122],\n",
      "        [1.0000, 0.6165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5901],\n",
      "        [1.0000, 0.5351]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502432346343994 \tStep Time:  0.00698089599609375 s \tTotal Time:  13.626987218856812 s \n",
      "\n",
      "\n",
      "\tEpisode 2179 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5446],\n",
      "        [1.0000, 0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5398],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442492961883545 \tStep Time:  0.006983280181884766 s \tTotal Time:  13.633970499038696 s \n",
      "\n",
      "\n",
      "\tEpisode 2180 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5782],\n",
      "        [1.0000, 0.6422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5936],\n",
      "        [1.0000, 0.4833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456915378570557 \tStep Time:  0.0059833526611328125 s \tTotal Time:  13.639953851699829 s \n",
      "\n",
      "\n",
      "\tEpisode 2181 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4357],\n",
      "        [1.0000, 0.6086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5966],\n",
      "        [1.0000, 0.6211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.606305122375488 \tStep Time:  0.006980419158935547 s \tTotal Time:  13.646934270858765 s \n",
      "\n",
      "\n",
      "\tEpisode 2182 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4955],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6109],\n",
      "        [1.0000, 0.4206]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419933319091797 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.652918100357056 s \n",
      "\n",
      "\n",
      "\tEpisode 2183 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4779],\n",
      "        [1.0000, 0.6280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4129],\n",
      "        [1.0000, 0.4501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579726159572601 \tStep Time:  0.005983591079711914 s \tTotal Time:  13.658901691436768 s \n",
      "\n",
      "\n",
      "\tEpisode 2184 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5291],\n",
      "        [1.0000, 0.5611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5451],\n",
      "        [1.0000, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497620105743408 \tStep Time:  0.006987333297729492 s \tTotal Time:  13.665889024734497 s \n",
      "\n",
      "\n",
      "\tEpisode 2185 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3987],\n",
      "        [1.0000, 0.4450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.4157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541422843933105 \tStep Time:  0.004985332489013672 s \tTotal Time:  13.67087435722351 s \n",
      "\n",
      "\n",
      "\tEpisode 2186 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4570],\n",
      "        [1.0000, 0.4232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4146],\n",
      "        [1.0000, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493284583091736 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.67685842514038 s \n",
      "\n",
      "\n",
      "\tEpisode 2187 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6047],\n",
      "        [1.0000, 0.3757]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5661],\n",
      "        [1.0000, 0.4785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489102363586426 \tStep Time:  0.006982564926147461 s \tTotal Time:  13.683840990066528 s \n",
      "\n",
      "\n",
      "\tEpisode 2188 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5605],\n",
      "        [1.0000, 0.5894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4119],\n",
      "        [1.0000, 0.4865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463046550750732 \tStep Time:  0.006014585494995117 s \tTotal Time:  13.689855575561523 s \n",
      "\n",
      "\n",
      "\tEpisode 2189 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4922],\n",
      "        [1.0000, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4090],\n",
      "        [1.0000, 0.4211]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509002029895782 \tStep Time:  0.005952358245849609 s \tTotal Time:  13.695807933807373 s \n",
      "\n",
      "\n",
      "\tEpisode 2190 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5690],\n",
      "        [1.0000, 0.5611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4780],\n",
      "        [1.0000, 0.3876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.38633918762207 \tStep Time:  0.00598454475402832 s \tTotal Time:  13.701792478561401 s \n",
      "\n",
      "\n",
      "\tEpisode 2191 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4259],\n",
      "        [1.0000, 0.5464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4633],\n",
      "        [1.0000, 0.6599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.364230811595917 \tStep Time:  0.005983591079711914 s \tTotal Time:  13.707776069641113 s \n",
      "\n",
      "\n",
      "\tEpisode 2192 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4816],\n",
      "        [1.0000, 0.3932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6074],\n",
      "        [1.0000, 0.4569]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612242698669434 \tStep Time:  0.00601506233215332 s \tTotal Time:  13.713791131973267 s \n",
      "\n",
      "\n",
      "\tEpisode 2193 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3290],\n",
      "        [1.0000, 0.6010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5869],\n",
      "        [1.0000, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612315654754639 \tStep Time:  0.006949901580810547 s \tTotal Time:  13.720741033554077 s \n",
      "\n",
      "\n",
      "\tEpisode 2194 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6350],\n",
      "        [1.0000, 0.6520]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4965],\n",
      "        [1.0000, 0.5712]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485360980033875 \tStep Time:  0.004987239837646484 s \tTotal Time:  13.725728273391724 s \n",
      "\n",
      "\n",
      "\tEpisode 2195 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3936],\n",
      "        [1.0000, 0.3300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5263],\n",
      "        [1.0000, 0.6936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43005895614624 \tStep Time:  0.007012367248535156 s \tTotal Time:  13.732740640640259 s \n",
      "\n",
      "\n",
      "\tEpisode 2196 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6840],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.3101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528645157814026 \tStep Time:  0.005986690521240234 s \tTotal Time:  13.738727331161499 s \n",
      "\n",
      "\n",
      "\tEpisode 2197 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.5913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453768253326416 \tStep Time:  0.005980968475341797 s \tTotal Time:  13.74470829963684 s \n",
      "\n",
      "\n",
      "\tEpisode 2198 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3777],\n",
      "        [1.0000, 0.5676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2914],\n",
      "        [1.0000, 0.3422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.397226810455322 \tStep Time:  0.00598454475402832 s \tTotal Time:  13.75069284439087 s \n",
      "\n",
      "\n",
      "\tEpisode 2199 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6437],\n",
      "        [1.0000, 0.4998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439042091369629 \tStep Time:  0.005985260009765625 s \tTotal Time:  13.756678104400635 s \n",
      "\n",
      "\n",
      "\tEpisode 2200 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4930],\n",
      "        [1.0000, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4725],\n",
      "        [1.0000, 0.5954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442608714103699 \tStep Time:  0.005950450897216797 s \tTotal Time:  13.762628555297852 s \n",
      "\n",
      "\n",
      "\tEpisode 2201 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6384],\n",
      "        [1.0000, 0.6630]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6378],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450611591339111 \tStep Time:  0.005984783172607422 s \tTotal Time:  13.768613338470459 s \n",
      "\n",
      "\n",
      "\tEpisode 2202 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3675],\n",
      "        [1.0000, 0.6272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3365],\n",
      "        [1.0000, 0.6702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.262049674987793 \tStep Time:  0.006015300750732422 s \tTotal Time:  13.774628639221191 s \n",
      "\n",
      "\n",
      "\tEpisode 2203 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4597],\n",
      "        [1.0000, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4238],\n",
      "        [1.0000, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49636697769165 \tStep Time:  0.005953073501586914 s \tTotal Time:  13.780581712722778 s \n",
      "\n",
      "\n",
      "\tEpisode 2204 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3158],\n",
      "        [1.0000, 0.6209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5218],\n",
      "        [1.0000, 0.3224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506083965301514 \tStep Time:  0.0069811344146728516 s \tTotal Time:  13.787562847137451 s \n",
      "\n",
      "\n",
      "\tEpisode 2205 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4220],\n",
      "        [1.0000, 0.5995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4844],\n",
      "        [1.0000, 0.2623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530067443847656 \tStep Time:  0.0069806575775146484 s \tTotal Time:  13.794543504714966 s \n",
      "\n",
      "\n",
      "\tEpisode 2206 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4545],\n",
      "        [1.0000, 0.2716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6108],\n",
      "        [1.0000, 0.3579]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441657066345215 \tStep Time:  0.005984306335449219 s \tTotal Time:  13.800527811050415 s \n",
      "\n",
      "\n",
      "\tEpisode 2207 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3298],\n",
      "        [1.0000, 0.8419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3701],\n",
      "        [1.0000, 0.4269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.832324266433716 \tStep Time:  0.006981611251831055 s \tTotal Time:  13.807509422302246 s \n",
      "\n",
      "\n",
      "\tEpisode 2208 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6071],\n",
      "        [1.0000, 0.6343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7273],\n",
      "        [1.0000, 0.2997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.355142593383789 \tStep Time:  0.005984306335449219 s \tTotal Time:  13.813493728637695 s \n",
      "\n",
      "\n",
      "\tEpisode 2209 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3693],\n",
      "        [1.0000, 0.6409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4128],\n",
      "        [1.0000, 0.7045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.622588634490967 \tStep Time:  0.00698089599609375 s \tTotal Time:  13.820474624633789 s \n",
      "\n",
      "\n",
      "\tEpisode 2210 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3617],\n",
      "        [1.0000, 0.3508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5916],\n",
      "        [1.0000, 0.4428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.616059303283691 \tStep Time:  0.005984783172607422 s \tTotal Time:  13.826459407806396 s \n",
      "\n",
      "\n",
      "\tEpisode 2211 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4742],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7095],\n",
      "        [1.0000, 0.5241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.633064329624176 \tStep Time:  0.0069806575775146484 s \tTotal Time:  13.833440065383911 s \n",
      "\n",
      "\n",
      "\tEpisode 2212 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7807],\n",
      "        [1.0000, 0.2548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6866],\n",
      "        [1.0000, 0.6096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.758511543273926 \tStep Time:  0.006982088088989258 s \tTotal Time:  13.8404221534729 s \n",
      "\n",
      "\n",
      "\tEpisode 2213 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6580],\n",
      "        [1.0000, 0.5390]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5025],\n",
      "        [1.0000, 0.6318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549018383026123 \tStep Time:  0.005984306335449219 s \tTotal Time:  13.84640645980835 s \n",
      "\n",
      "\n",
      "\tEpisode 2214 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6742],\n",
      "        [1.0000, 0.2448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3912],\n",
      "        [1.0000, 0.6551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520078599452972 \tStep Time:  0.006979703903198242 s \tTotal Time:  13.853386163711548 s \n",
      "\n",
      "\n",
      "\tEpisode 2215 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5761],\n",
      "        [1.0000, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4904],\n",
      "        [1.0000, 0.4578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.590938568115234 \tStep Time:  0.00598454475402832 s \tTotal Time:  13.859370708465576 s \n",
      "\n",
      "\n",
      "\tEpisode 2216 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3897],\n",
      "        [1.0000, 0.5573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4190],\n",
      "        [1.0000, 0.5869]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575324058532715 \tStep Time:  0.006985664367675781 s \tTotal Time:  13.866356372833252 s \n",
      "\n",
      "\n",
      "\tEpisode 2217 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.5537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4064],\n",
      "        [1.0000, 0.4410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.645218849182129 \tStep Time:  0.004986286163330078 s \tTotal Time:  13.871342658996582 s \n",
      "\n",
      "\n",
      "\tEpisode 2218 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4563],\n",
      "        [1.0000, 0.3675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4067],\n",
      "        [1.0000, 0.5277]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.616312026977539 \tStep Time:  0.005984067916870117 s \tTotal Time:  13.878324508666992 s \n",
      "\n",
      "\n",
      "\tEpisode 2219 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4827],\n",
      "        [1.0000, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.4537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491162776947021 \tStep Time:  0.006989002227783203 s \tTotal Time:  13.885313510894775 s \n",
      "\n",
      "\n",
      "\tEpisode 2220 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4655],\n",
      "        [1.0000, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4706],\n",
      "        [1.0000, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54975700378418 \tStep Time:  0.0059816837310791016 s \tTotal Time:  13.891295194625854 s \n",
      "\n",
      "\n",
      "\tEpisode 2221 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4807],\n",
      "        [1.0000, 0.4825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4856],\n",
      "        [1.0000, 0.4871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524807453155518 \tStep Time:  0.005984783172607422 s \tTotal Time:  13.897279977798462 s \n",
      "\n",
      "\n",
      "\tEpisode 2222 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4439],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5013],\n",
      "        [1.0000, 0.4869]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543392658233643 \tStep Time:  0.005983591079711914 s \tTotal Time:  13.903263568878174 s \n",
      "\n",
      "\n",
      "\tEpisode 2223 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4935],\n",
      "        [1.0000, 0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4974],\n",
      "        [1.0000, 0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519466400146484 \tStep Time:  0.0060160160064697266 s \tTotal Time:  13.909279584884644 s \n",
      "\n",
      "\n",
      "\tEpisode 2224 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4921],\n",
      "        [1.0000, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522342681884766 \tStep Time:  0.005952358245849609 s \tTotal Time:  13.915231943130493 s \n",
      "\n",
      "\n",
      "\tEpisode 2225 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5228],\n",
      "        [1.0000, 0.4972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4965],\n",
      "        [1.0000, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530845165252686 \tStep Time:  0.005983591079711914 s \tTotal Time:  13.921215534210205 s \n",
      "\n",
      "\n",
      "\tEpisode 2226 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4832],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4488],\n",
      "        [1.0000, 0.4416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459424495697021 \tStep Time:  0.005983591079711914 s \tTotal Time:  13.927199125289917 s \n",
      "\n",
      "\n",
      "\tEpisode 2227 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.5111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5197],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496102333068848 \tStep Time:  0.006981849670410156 s \tTotal Time:  13.934180974960327 s \n",
      "\n",
      "\n",
      "\tEpisode 2228 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53559923171997 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.940164804458618 s \n",
      "\n",
      "\n",
      "\tEpisode 2229 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4460],\n",
      "        [1.0000, 0.4364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4515],\n",
      "        [1.0000, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471585392951965 \tStep Time:  0.005021572113037109 s \tTotal Time:  13.945186376571655 s \n",
      "\n",
      "\n",
      "\tEpisode 2230 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5260],\n",
      "        [1.0000, 0.3933]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5244],\n",
      "        [1.0000, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61224490404129 \tStep Time:  0.006947517395019531 s \tTotal Time:  13.952133893966675 s \n",
      "\n",
      "\n",
      "\tEpisode 2231 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4660],\n",
      "        [1.0000, 0.4766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514663875102997 \tStep Time:  0.005982398986816406 s \tTotal Time:  13.958116292953491 s \n",
      "\n",
      "\n",
      "\tEpisode 2232 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5002],\n",
      "        [1.0000, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [1.0000, 0.4865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496192991733551 \tStep Time:  0.005984306335449219 s \tTotal Time:  13.96410059928894 s \n",
      "\n",
      "\n",
      "\tEpisode 2233 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4972],\n",
      "        [1.0000, 0.5118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493773460388184 \tStep Time:  0.005983829498291016 s \tTotal Time:  13.970084428787231 s \n",
      "\n",
      "\n",
      "\tEpisode 2234 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5140],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4782],\n",
      "        [1.0000, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493939399719238 \tStep Time:  0.005984783172607422 s \tTotal Time:  13.976069211959839 s \n",
      "\n",
      "\n",
      "\tEpisode 2235 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4757],\n",
      "        [1.0000, 0.4371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5176],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498985767364502 \tStep Time:  0.0059833526611328125 s \tTotal Time:  13.982052564620972 s \n",
      "\n",
      "\n",
      "\tEpisode 2236 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4570],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4928],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50187462568283 \tStep Time:  0.0069811344146728516 s \tTotal Time:  13.989033699035645 s \n",
      "\n",
      "\n",
      "\tEpisode 2237 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5031],\n",
      "        [1.0000, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5061],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517200469970703 \tStep Time:  0.005984306335449219 s \tTotal Time:  13.995018005371094 s \n",
      "\n",
      "\n",
      "\tEpisode 2238 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4999],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4951],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505841732025146 \tStep Time:  0.00598454475402832 s \tTotal Time:  14.001002550125122 s \n",
      "\n",
      "\n",
      "\tEpisode 2239 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4887],\n",
      "        [1.0000, 0.4749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5035],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489720344543457 \tStep Time:  0.005985260009765625 s \tTotal Time:  14.00798487663269 s \n",
      "\n",
      "\n",
      "\tEpisode 2240 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4408],\n",
      "        [1.0000, 0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.4592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520322322845459 \tStep Time:  0.006979703903198242 s \tTotal Time:  14.014964580535889 s \n",
      "\n",
      "\n",
      "\tEpisode 2241 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4588],\n",
      "        [1.0000, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5061],\n",
      "        [1.0000, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526213884353638 \tStep Time:  0.0059850215911865234 s \tTotal Time:  14.020949602127075 s \n",
      "\n",
      "\n",
      "\tEpisode 2242 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4651],\n",
      "        [1.0000, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5031],\n",
      "        [1.0000, 0.4511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512274742126465 \tStep Time:  0.00598454475402832 s \tTotal Time:  14.027930498123169 s \n",
      "\n",
      "\n",
      "\tEpisode 2243 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4983],\n",
      "        [1.0000, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5008],\n",
      "        [1.0000, 0.4677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473556876182556 \tStep Time:  0.007978677749633789 s \tTotal Time:  14.035909175872803 s \n",
      "\n",
      "\n",
      "\tEpisode 2244 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5050],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4721],\n",
      "        [1.0000, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519328594207764 \tStep Time:  0.009972572326660156 s \tTotal Time:  14.045881748199463 s \n",
      "\n",
      "\n",
      "\tEpisode 2245 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4606],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4858],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483478426933289 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.052862882614136 s \n",
      "\n",
      "\n",
      "\tEpisode 2246 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5016],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516697883605957 \tStep Time:  0.00598454475402832 s \tTotal Time:  14.058847427368164 s \n",
      "\n",
      "\n",
      "\tEpisode 2247 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4756],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5062],\n",
      "        [1.0000, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479051768779755 \tStep Time:  0.006981849670410156 s \tTotal Time:  14.065829277038574 s \n",
      "\n",
      "\n",
      "\tEpisode 2248 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4785],\n",
      "        [1.0000, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4494],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529695987701416 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.071813106536865 s \n",
      "\n",
      "\n",
      "\tEpisode 2249 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4811],\n",
      "        [1.0000, 0.4249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473850727081299 \tStep Time:  0.005984306335449219 s \tTotal Time:  14.078794479370117 s \n",
      "\n",
      "\n",
      "\tEpisode 2250 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5261],\n",
      "        [1.0000, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495082080364227 \tStep Time:  0.0069849491119384766 s \tTotal Time:  14.085779428482056 s \n",
      "\n",
      "\n",
      "\tEpisode 2251 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4779],\n",
      "        [1.0000, 0.4438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.3615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572201251983643 \tStep Time:  0.00698399543762207 s \tTotal Time:  14.092763423919678 s \n",
      "\n",
      "\n",
      "\tEpisode 2252 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4182],\n",
      "        [1.0000, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4852],\n",
      "        [1.0000, 0.4698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578570425510406 \tStep Time:  0.00698089599609375 s \tTotal Time:  14.099744319915771 s \n",
      "\n",
      "\n",
      "\tEpisode 2253 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5407],\n",
      "        [1.0000, 0.5363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5319],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506676197052002 \tStep Time:  0.005988121032714844 s \tTotal Time:  14.105732440948486 s \n",
      "\n",
      "\n",
      "\tEpisode 2254 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5380],\n",
      "        [1.0000, 0.5091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4490],\n",
      "        [1.0000, 0.4907]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55806827545166 \tStep Time:  0.005984306335449219 s \tTotal Time:  14.112713813781738 s \n",
      "\n",
      "\n",
      "\tEpisode 2255 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5321],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4416],\n",
      "        [1.0000, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543421566486359 \tStep Time:  0.0069806575775146484 s \tTotal Time:  14.119694471359253 s \n",
      "\n",
      "\n",
      "\tEpisode 2256 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5432],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.5360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489431858062744 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.125678300857544 s \n",
      "\n",
      "\n",
      "\tEpisode 2257 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4498],\n",
      "        [1.0000, 0.5197]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5245],\n",
      "        [1.0000, 0.4792]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493360161781311 \tStep Time:  0.007980823516845703 s \tTotal Time:  14.13365912437439 s \n",
      "\n",
      "\n",
      "\tEpisode 2258 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5460],\n",
      "        [1.0000, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5514],\n",
      "        [1.0000, 0.5288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517502307891846 \tStep Time:  0.006979227066040039 s \tTotal Time:  14.14063835144043 s \n",
      "\n",
      "\n",
      "\tEpisode 2259 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5290],\n",
      "        [1.0000, 0.5344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5537],\n",
      "        [1.0000, 0.4563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459642469882965 \tStep Time:  0.0059850215911865234 s \tTotal Time:  14.146623373031616 s \n",
      "\n",
      "\n",
      "\tEpisode 2260 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5223],\n",
      "        [1.0000, 0.5539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5531],\n",
      "        [1.0000, 0.5364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517472088336945 \tStep Time:  0.007977485656738281 s \tTotal Time:  14.154600858688354 s \n",
      "\n",
      "\n",
      "\tEpisode 2261 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5426],\n",
      "        [1.0000, 0.5479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5146],\n",
      "        [1.0000, 0.5192]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505103588104248 \tStep Time:  0.005984783172607422 s \tTotal Time:  14.160585641860962 s \n",
      "\n",
      "\n",
      "\tEpisode 2262 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5403],\n",
      "        [1.0000, 0.5122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5447],\n",
      "        [1.0000, 0.4214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460038661956787 \tStep Time:  0.006981611251831055 s \tTotal Time:  14.167567253112793 s \n",
      "\n",
      "\n",
      "\tEpisode 2263 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5095],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4058],\n",
      "        [1.0000, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445037245750427 \tStep Time:  0.006980419158935547 s \tTotal Time:  14.174547672271729 s \n",
      "\n",
      "\n",
      "\tEpisode 2264 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5411],\n",
      "        [1.0000, 0.5301]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5292],\n",
      "        [1.0000, 0.4133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44510269165039 \tStep Time:  0.0059854984283447266 s \tTotal Time:  14.180533170700073 s \n",
      "\n",
      "\n",
      "\tEpisode 2265 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.5303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4695],\n",
      "        [1.0000, 0.2731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.686264514923096 \tStep Time:  0.007977485656738281 s \tTotal Time:  14.188510656356812 s \n",
      "\n",
      "\n",
      "\tEpisode 2266 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5045],\n",
      "        [1.0000, 0.5334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4469],\n",
      "        [1.0000, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491181492805481 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.195491790771484 s \n",
      "\n",
      "\n",
      "\tEpisode 2267 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4953],\n",
      "        [1.0000, 0.5328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5275],\n",
      "        [1.0000, 0.5227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495697021484375 \tStep Time:  0.007979393005371094 s \tTotal Time:  14.203471183776855 s \n",
      "\n",
      "\n",
      "\tEpisode 2268 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5278],\n",
      "        [1.0000, 0.4021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4284],\n",
      "        [1.0000, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497649669647217 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.209455251693726 s \n",
      "\n",
      "\n",
      "\tEpisode 2269 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5170],\n",
      "        [1.0000, 0.5372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4230],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475616455078125 \tStep Time:  0.007981300354003906 s \tTotal Time:  14.21743655204773 s \n",
      "\n",
      "\n",
      "\tEpisode 2270 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5370],\n",
      "        [1.0000, 0.5360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5228],\n",
      "        [1.0000, 0.5284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509214282035828 \tStep Time:  0.0059812068939208984 s \tTotal Time:  14.22341775894165 s \n",
      "\n",
      "\n",
      "\tEpisode 2271 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5245],\n",
      "        [1.0000, 0.3554]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446735858917236 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.229401588439941 s \n",
      "\n",
      "\n",
      "\tEpisode 2272 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5286],\n",
      "        [1.0000, 0.3322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5468],\n",
      "        [1.0000, 0.3363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538948059082031 \tStep Time:  0.00797891616821289 s \tTotal Time:  14.237380504608154 s \n",
      "\n",
      "\n",
      "\tEpisode 2273 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3654],\n",
      "        [1.0000, 0.4318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5461],\n",
      "        [1.0000, 0.5508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549270153045654 \tStep Time:  0.0059833526611328125 s \tTotal Time:  14.243363857269287 s \n",
      "\n",
      "\n",
      "\tEpisode 2274 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2990],\n",
      "        [1.0000, 0.4267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5571],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.703289985656738 \tStep Time:  0.0064084529876708984 s \tTotal Time:  14.249772310256958 s \n",
      "\n",
      "\n",
      "\tEpisode 2275 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5106],\n",
      "        [1.0000, 0.5625]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5536],\n",
      "        [1.0000, 0.4685]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579473972320557 \tStep Time:  0.00655674934387207 s \tTotal Time:  14.25632905960083 s \n",
      "\n",
      "\n",
      "\tEpisode 2276 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4551],\n",
      "        [1.0000, 0.5340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4760],\n",
      "        [1.0000, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536405980587006 \tStep Time:  0.00598454475402832 s \tTotal Time:  14.262313604354858 s \n",
      "\n",
      "\n",
      "\tEpisode 2277 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5123],\n",
      "        [1.0000, 0.5521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5367],\n",
      "        [1.0000, 0.5502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522061347961426 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.26829743385315 s \n",
      "\n",
      "\n",
      "\tEpisode 2278 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5466],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5577],\n",
      "        [1.0000, 0.5594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496158599853516 \tStep Time:  0.0059833526611328125 s \tTotal Time:  14.274280786514282 s \n",
      "\n",
      "\n",
      "\tEpisode 2279 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5686],\n",
      "        [1.0000, 0.5511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5696],\n",
      "        [1.0000, 0.5655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508705139160156 \tStep Time:  0.005984783172607422 s \tTotal Time:  14.28026556968689 s \n",
      "\n",
      "\n",
      "\tEpisode 2280 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5751],\n",
      "        [1.0000, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4752],\n",
      "        [1.0000, 0.5845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492954730987549 \tStep Time:  0.006987571716308594 s \tTotal Time:  14.287253141403198 s \n",
      "\n",
      "\n",
      "\tEpisode 2281 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6003],\n",
      "        [1.0000, 0.5448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5766],\n",
      "        [1.0000, 0.4980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50404167175293 \tStep Time:  0.005983591079711914 s \tTotal Time:  14.29323673248291 s \n",
      "\n",
      "\n",
      "\tEpisode 2282 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5733],\n",
      "        [1.0000, 0.4430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5874],\n",
      "        [1.0000, 0.5661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462975263595581 \tStep Time:  0.005984306335449219 s \tTotal Time:  14.29922103881836 s \n",
      "\n",
      "\n",
      "\tEpisode 2283 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5805],\n",
      "        [1.0000, 0.5893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538761377334595 \tStep Time:  0.005988121032714844 s \tTotal Time:  14.305209159851074 s \n",
      "\n",
      "\n",
      "\tEpisode 2284 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5526],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4042],\n",
      "        [1.0000, 0.3334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.682226181030273 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.311192989349365 s \n",
      "\n",
      "\n",
      "\tEpisode 2285 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4904],\n",
      "        [1.0000, 0.5232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5564],\n",
      "        [1.0000, 0.4124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.423867285251617 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.317177057266235 s \n",
      "\n",
      "\n",
      "\tEpisode 2286 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5446],\n",
      "        [1.0000, 0.3651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5477],\n",
      "        [1.0000, 0.4376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477071762084961 \tStep Time:  0.00598454475402832 s \tTotal Time:  14.323161602020264 s \n",
      "\n",
      "\n",
      "\tEpisode 2287 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4407],\n",
      "        [1.0000, 0.5561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3958],\n",
      "        [1.0000, 0.5530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536847591400146 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.329145669937134 s \n",
      "\n",
      "\n",
      "\tEpisode 2288 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4738],\n",
      "        [1.0000, 0.4847]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5050],\n",
      "        [1.0000, 0.4575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504804134368896 \tStep Time:  0.006981372833251953 s \tTotal Time:  14.336127042770386 s \n",
      "\n",
      "\n",
      "\tEpisode 2289 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4237],\n",
      "        [1.0000, 0.4539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4285],\n",
      "        [1.0000, 0.5286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452605903148651 \tStep Time:  0.0059833526611328125 s \tTotal Time:  14.342110395431519 s \n",
      "\n",
      "\n",
      "\tEpisode 2290 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4332],\n",
      "        [1.0000, 0.5654]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4621],\n",
      "        [1.0000, 0.5161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529564380645752 \tStep Time:  0.005984306335449219 s \tTotal Time:  14.348094701766968 s \n",
      "\n",
      "\n",
      "\tEpisode 2291 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5014],\n",
      "        [1.0000, 0.4011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.4320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444745421409607 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.354078769683838 s \n",
      "\n",
      "\n",
      "\tEpisode 2292 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4174],\n",
      "        [1.0000, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446437239646912 \tStep Time:  0.00598454475402832 s \tTotal Time:  14.360063314437866 s \n",
      "\n",
      "\n",
      "\tEpisode 2293 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4238],\n",
      "        [1.0000, 0.3911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5599],\n",
      "        [1.0000, 0.3739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610261976718903 \tStep Time:  0.005983591079711914 s \tTotal Time:  14.367043733596802 s \n",
      "\n",
      "\n",
      "\tEpisode 2294 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5006],\n",
      "        [1.0000, 0.4319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4612],\n",
      "        [1.0000, 0.5963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418621838092804 \tStep Time:  0.005984306335449219 s \tTotal Time:  14.373028039932251 s \n",
      "\n",
      "\n",
      "\tEpisode 2295 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5912],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3812],\n",
      "        [1.0000, 0.3454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.71861219406128 \tStep Time:  0.0059854984283447266 s \tTotal Time:  14.379013538360596 s \n",
      "\n",
      "\n",
      "\tEpisode 2296 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6068],\n",
      "        [1.0000, 0.4250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5193],\n",
      "        [1.0000, 0.4132]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491184711456299 \tStep Time:  0.006979703903198242 s \tTotal Time:  14.385993242263794 s \n",
      "\n",
      "\n",
      "\tEpisode 2297 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5548],\n",
      "        [1.0000, 0.6007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6423],\n",
      "        [1.0000, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48649525642395 \tStep Time:  0.005984783172607422 s \tTotal Time:  14.391978025436401 s \n",
      "\n",
      "\n",
      "\tEpisode 2298 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5766],\n",
      "        [1.0000, 0.5659]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3926],\n",
      "        [1.0000, 0.5399]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418409824371338 \tStep Time:  0.0069806575775146484 s \tTotal Time:  14.398958683013916 s \n",
      "\n",
      "\n",
      "\tEpisode 2299 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5691],\n",
      "        [1.0000, 0.4245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6171],\n",
      "        [1.0000, 0.6203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.658267498016357 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.404942512512207 s \n",
      "\n",
      "\n",
      "\tEpisode 2300 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5422],\n",
      "        [1.0000, 0.5643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4392],\n",
      "        [1.0000, 0.5265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562082290649414 \tStep Time:  0.005984306335449219 s \tTotal Time:  14.410926818847656 s \n",
      "\n",
      "\n",
      "\tEpisode 2301 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3673],\n",
      "        [1.0000, 0.6045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5500],\n",
      "        [1.0000, 0.5284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.662423193454742 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.417907953262329 s \n",
      "\n",
      "\n",
      "\tEpisode 2302 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4484],\n",
      "        [1.0000, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5853],\n",
      "        [1.0000, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.616154193878174 \tStep Time:  0.00598454475402832 s \tTotal Time:  14.423892498016357 s \n",
      "\n",
      "\n",
      "\tEpisode 2303 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5918],\n",
      "        [1.0000, 0.5501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5099],\n",
      "        [1.0000, 0.5118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514233827590942 \tStep Time:  0.0059850215911865234 s \tTotal Time:  14.429877519607544 s \n",
      "\n",
      "\n",
      "\tEpisode 2304 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5350],\n",
      "        [1.0000, 0.5561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5527],\n",
      "        [1.0000, 0.5339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51604413986206 \tStep Time:  0.0069806575775146484 s \tTotal Time:  14.436858177185059 s \n",
      "\n",
      "\n",
      "\tEpisode 2305 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5306],\n",
      "        [1.0000, 0.5341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5336],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532955169677734 \tStep Time:  0.00698089599609375 s \tTotal Time:  14.443839073181152 s \n",
      "\n",
      "\n",
      "\tEpisode 2306 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5297],\n",
      "        [1.0000, 0.5252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5196],\n",
      "        [1.0000, 0.5290]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510786414146423 \tStep Time:  0.006981611251831055 s \tTotal Time:  14.450820684432983 s \n",
      "\n",
      "\n",
      "\tEpisode 2307 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5322],\n",
      "        [1.0000, 0.5171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5239],\n",
      "        [1.0000, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534767627716064 \tStep Time:  0.006979703903198242 s \tTotal Time:  14.457800388336182 s \n",
      "\n",
      "\n",
      "\tEpisode 2308 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4860],\n",
      "        [1.0000, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4977],\n",
      "        [1.0000, 0.5560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490800023078918 \tStep Time:  0.0060176849365234375 s \tTotal Time:  14.463818073272705 s \n",
      "\n",
      "\n",
      "\tEpisode 2309 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5080],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6175],\n",
      "        [1.0000, 0.5318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583876132965088 \tStep Time:  0.006983280181884766 s \tTotal Time:  14.47080135345459 s \n",
      "\n",
      "\n",
      "\tEpisode 2310 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4962],\n",
      "        [1.0000, 0.4992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5146],\n",
      "        [1.0000, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502667903900146 \tStep Time:  0.005980730056762695 s \tTotal Time:  14.476782083511353 s \n",
      "\n",
      "\n",
      "\tEpisode 2311 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4835],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505250453948975 \tStep Time:  0.0066144466400146484 s \tTotal Time:  14.483396530151367 s \n",
      "\n",
      "\n",
      "\tEpisode 2312 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4607],\n",
      "        [1.0000, 0.5463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.4999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537576675415039 \tStep Time:  0.006319761276245117 s \tTotal Time:  14.489716291427612 s \n",
      "\n",
      "\n",
      "\tEpisode 2313 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4723],\n",
      "        [1.0000, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4960],\n",
      "        [1.0000, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500264286994934 \tStep Time:  0.0059833526611328125 s \tTotal Time:  14.495699644088745 s \n",
      "\n",
      "\n",
      "\tEpisode 2314 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4785],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.4552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530128002166748 \tStep Time:  0.0059871673583984375 s \tTotal Time:  14.501686811447144 s \n",
      "\n",
      "\n",
      "\tEpisode 2315 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4666],\n",
      "        [1.0000, 0.4770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4686],\n",
      "        [1.0000, 0.4821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515400648117065 \tStep Time:  0.005986213684082031 s \tTotal Time:  14.507673025131226 s \n",
      "\n",
      "\n",
      "\tEpisode 2316 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4841],\n",
      "        [1.0000, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4715],\n",
      "        [1.0000, 0.5433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540692448616028 \tStep Time:  0.00654911994934082 s \tTotal Time:  14.514222145080566 s \n",
      "\n",
      "\n",
      "\tEpisode 2317 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4518],\n",
      "        [1.0000, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521341502666473 \tStep Time:  0.00542140007019043 s \tTotal Time:  14.519643545150757 s \n",
      "\n",
      "\n",
      "\tEpisode 2318 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4927],\n",
      "        [1.0000, 0.4961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4858],\n",
      "        [1.0000, 0.4307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469213008880615 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.52662467956543 s \n",
      "\n",
      "\n",
      "\tEpisode 2319 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5605],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4454],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45882248878479 \tStep Time:  0.005984306335449219 s \tTotal Time:  14.532608985900879 s \n",
      "\n",
      "\n",
      "\tEpisode 2320 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [1.0000, 0.5119]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5138840675354 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.538593053817749 s \n",
      "\n",
      "\n",
      "\tEpisode 2321 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4715],\n",
      "        [1.0000, 0.6173]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4506],\n",
      "        [1.0000, 0.5499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48961353302002 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.54457712173462 s \n",
      "\n",
      "\n",
      "\tEpisode 2322 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4164],\n",
      "        [1.0000, 0.3926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5719],\n",
      "        [1.0000, 0.4895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543558835983276 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.55056095123291 s \n",
      "\n",
      "\n",
      "\tEpisode 2323 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6076],\n",
      "        [1.0000, 0.5276]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4113],\n",
      "        [1.0000, 0.3768]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54250431060791 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.55654501914978 s \n",
      "\n",
      "\n",
      "\tEpisode 2324 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5366],\n",
      "        [1.0000, 0.5562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5235],\n",
      "        [1.0000, 0.5155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497561991214752 \tStep Time:  0.0060155391693115234 s \tTotal Time:  14.562560558319092 s \n",
      "\n",
      "\n",
      "\tEpisode 2325 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6161],\n",
      "        [1.0000, 0.4669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455008327960968 \tStep Time:  0.005952358245849609 s \tTotal Time:  14.568512916564941 s \n",
      "\n",
      "\n",
      "\tEpisode 2326 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4062],\n",
      "        [1.0000, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4634],\n",
      "        [1.0000, 0.5505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508565902709961 \tStep Time:  0.006018638610839844 s \tTotal Time:  14.574531555175781 s \n",
      "\n",
      "\n",
      "\tEpisode 2327 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3914],\n",
      "        [1.0000, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4391],\n",
      "        [1.0000, 0.7153]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.706294536590576 \tStep Time:  0.004952669143676758 s \tTotal Time:  14.579484224319458 s \n",
      "\n",
      "\n",
      "\tEpisode 2328 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4034],\n",
      "        [1.0000, 0.4634]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.4039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524087905883789 \tStep Time:  0.00698089599609375 s \tTotal Time:  14.586465120315552 s \n",
      "\n",
      "\n",
      "\tEpisode 2329 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4377],\n",
      "        [1.0000, 0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5031],\n",
      "        [1.0000, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484942436218262 \tStep Time:  0.005984783172607422 s \tTotal Time:  14.59244990348816 s \n",
      "\n",
      "\n",
      "\tEpisode 2330 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5389],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4550],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488032817840576 \tStep Time:  0.005983591079711914 s \tTotal Time:  14.598433494567871 s \n",
      "\n",
      "\n",
      "\tEpisode 2331 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505792617797852 \tStep Time:  0.0059833526611328125 s \tTotal Time:  14.604416847229004 s \n",
      "\n",
      "\n",
      "\tEpisode 2332 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4508],\n",
      "        [1.0000, 0.4137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561911702156067 \tStep Time:  0.0059850215911865234 s \tTotal Time:  14.61040186882019 s \n",
      "\n",
      "\n",
      "\tEpisode 2333 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [1.0000, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5290],\n",
      "        [1.0000, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496514797210693 \tStep Time:  0.006981372833251953 s \tTotal Time:  14.617383241653442 s \n",
      "\n",
      "\n",
      "\tEpisode 2334 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5097],\n",
      "        [1.0000, 0.5311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4483],\n",
      "        [1.0000, 0.5219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551880836486816 \tStep Time:  0.005985260009765625 s \tTotal Time:  14.624365091323853 s \n",
      "\n",
      "\n",
      "\tEpisode 2335 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5035],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5210],\n",
      "        [1.0000, 0.4934]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500535905361176 \tStep Time:  0.005983591079711914 s \tTotal Time:  14.630348682403564 s \n",
      "\n",
      "\n",
      "\tEpisode 2336 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5214],\n",
      "        [1.0000, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4814],\n",
      "        [1.0000, 0.5629]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519387245178223 \tStep Time:  0.007978439331054688 s \tTotal Time:  14.63832712173462 s \n",
      "\n",
      "\n",
      "\tEpisode 2337 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5237],\n",
      "        [1.0000, 0.4338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483342170715332 \tStep Time:  0.0059833526611328125 s \tTotal Time:  14.644310474395752 s \n",
      "\n",
      "\n",
      "\tEpisode 2338 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5104],\n",
      "        [1.0000, 0.5598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5353],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500367164611816 \tStep Time:  0.006981611251831055 s \tTotal Time:  14.651292085647583 s \n",
      "\n",
      "\n",
      "\tEpisode 2339 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5541],\n",
      "        [1.0000, 0.5188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [1.0000, 0.5473]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505007266998291 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.657276153564453 s \n",
      "\n",
      "\n",
      "\tEpisode 2340 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4500],\n",
      "        [1.0000, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5098],\n",
      "        [1.0000, 0.4433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508341789245605 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.664257287979126 s \n",
      "\n",
      "\n",
      "\tEpisode 2341 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4401],\n",
      "        [1.0000, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561074912548065 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.671238422393799 s \n",
      "\n",
      "\n",
      "\tEpisode 2342 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5111],\n",
      "        [1.0000, 0.4999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4911],\n",
      "        [1.0000, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508310317993164 \tStep Time:  0.007019996643066406 s \tTotal Time:  14.678258419036865 s \n",
      "\n",
      "\n",
      "\tEpisode 2343 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.5526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47791612148285 \tStep Time:  0.0069427490234375 s \tTotal Time:  14.685201168060303 s \n",
      "\n",
      "\n",
      "\tEpisode 2344 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4556],\n",
      "        [1.0000, 0.4270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5006],\n",
      "        [1.0000, 0.5368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48641037940979 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.692182302474976 s \n",
      "\n",
      "\n",
      "\tEpisode 2345 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5564],\n",
      "        [1.0000, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4821],\n",
      "        [1.0000, 0.4279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508716583251953 \tStep Time:  0.006981372833251953 s \tTotal Time:  14.699163675308228 s \n",
      "\n",
      "\n",
      "\tEpisode 2346 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6468],\n",
      "        [1.0000, 0.5233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4541],\n",
      "        [1.0000, 0.4638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463654637336731 \tStep Time:  0.006981372833251953 s \tTotal Time:  14.70614504814148 s \n",
      "\n",
      "\n",
      "\tEpisode 2347 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5169],\n",
      "        [1.0000, 0.4498]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5588],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517808437347412 \tStep Time:  0.005984783172607422 s \tTotal Time:  14.712129831314087 s \n",
      "\n",
      "\n",
      "\tEpisode 2348 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4439],\n",
      "        [1.0000, 0.5722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5830],\n",
      "        [1.0000, 0.5825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57895565032959 \tStep Time:  0.007977962493896484 s \tTotal Time:  14.720107793807983 s \n",
      "\n",
      "\n",
      "\tEpisode 2349 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6709],\n",
      "        [1.0000, 0.4462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5273],\n",
      "        [1.0000, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48170804977417 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.726091623306274 s \n",
      "\n",
      "\n",
      "\tEpisode 2350 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5487],\n",
      "        [1.0000, 0.4758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5887],\n",
      "        [1.0000, 0.3526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601320385932922 \tStep Time:  0.00701594352722168 s \tTotal Time:  14.734104871749878 s \n",
      "\n",
      "\n",
      "\tEpisode 2351 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4976],\n",
      "        [1.0000, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4675],\n",
      "        [1.0000, 0.4707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501872539520264 \tStep Time:  0.009973764419555664 s \tTotal Time:  14.744078636169434 s \n",
      "\n",
      "\n",
      "\tEpisode 2352 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5401],\n",
      "        [1.0000, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.5380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505203247070312 \tStep Time:  0.008941411972045898 s \tTotal Time:  14.75302004814148 s \n",
      "\n",
      "\n",
      "\tEpisode 2353 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5244],\n",
      "        [1.0000, 0.6296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5576],\n",
      "        [1.0000, 0.5463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459508419036865 \tStep Time:  0.006981372833251953 s \tTotal Time:  14.760001420974731 s \n",
      "\n",
      "\n",
      "\tEpisode 2354 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.4975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5103],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506667137145996 \tStep Time:  0.0079803466796875 s \tTotal Time:  14.767981767654419 s \n",
      "\n",
      "\n",
      "\tEpisode 2355 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5006],\n",
      "        [1.0000, 0.4990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5271],\n",
      "        [1.0000, 0.5344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479348182678223 \tStep Time:  0.005982160568237305 s \tTotal Time:  14.773963928222656 s \n",
      "\n",
      "\n",
      "\tEpisode 2356 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5429],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4982],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489445209503174 \tStep Time:  0.005984783172607422 s \tTotal Time:  14.779948711395264 s \n",
      "\n",
      "\n",
      "\tEpisode 2357 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5303],\n",
      "        [1.0000, 0.4925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4734],\n",
      "        [1.0000, 0.4828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483381748199463 \tStep Time:  0.007977962493896484 s \tTotal Time:  14.78792667388916 s \n",
      "\n",
      "\n",
      "\tEpisode 2358 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.5396]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5895],\n",
      "        [1.0000, 0.6510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574589252471924 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.793910503387451 s \n",
      "\n",
      "\n",
      "\tEpisode 2359 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4451],\n",
      "        [1.0000, 0.6059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4266],\n",
      "        [1.0000, 0.4718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457576274871826 \tStep Time:  0.005985260009765625 s \tTotal Time:  14.799895763397217 s \n",
      "\n",
      "\n",
      "\tEpisode 2360 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5760],\n",
      "        [1.0000, 0.8255]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4635],\n",
      "        [1.0000, 0.4871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.674076080322266 \tStep Time:  0.006983280181884766 s \tTotal Time:  14.806879043579102 s \n",
      "\n",
      "\n",
      "\tEpisode 2361 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4735],\n",
      "        [1.0000, 0.4433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5432],\n",
      "        [1.0000, 0.5421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52514362335205 \tStep Time:  0.005984067916870117 s \tTotal Time:  14.812863111495972 s \n",
      "\n",
      "\n",
      "\tEpisode 2362 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4236],\n",
      "        [1.0000, 0.5976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4466],\n",
      "        [1.0000, 0.4453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428439617156982 \tStep Time:  0.0069866180419921875 s \tTotal Time:  14.819849729537964 s \n",
      "\n",
      "\n",
      "\tEpisode 2363 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4029],\n",
      "        [1.0000, 0.4761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4954],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580018520355225 \tStep Time:  0.0069849491119384766 s \tTotal Time:  14.826834678649902 s \n",
      "\n",
      "\n",
      "\tEpisode 2364 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4045],\n",
      "        [1.0000, 0.4254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3985],\n",
      "        [1.0000, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567928791046143 \tStep Time:  0.0069811344146728516 s \tTotal Time:  14.833815813064575 s \n",
      "\n",
      "\n",
      "\tEpisode 2365 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5611],\n",
      "        [1.0000, 0.4059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4673],\n",
      "        [1.0000, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425881028175354 \tStep Time:  0.0069844722747802734 s \tTotal Time:  14.840800285339355 s \n",
      "\n",
      "\n",
      "\tEpisode 2366 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5066],\n",
      "        [1.0000, 0.4339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462442636489868 \tStep Time:  0.007978677749633789 s \tTotal Time:  14.84877896308899 s \n",
      "\n",
      "\n",
      "\tEpisode 2367 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6051],\n",
      "        [1.0000, 0.4283]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5201],\n",
      "        [1.0000, 0.4440]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491182804107666 \tStep Time:  0.005982398986816406 s \tTotal Time:  14.854761362075806 s \n",
      "\n",
      "\n",
      "\tEpisode 2368 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6042],\n",
      "        [1.0000, 0.4628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4196],\n",
      "        [1.0000, 0.5814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.679320573806763 \tStep Time:  0.006979465484619141 s \tTotal Time:  14.861740827560425 s \n",
      "\n",
      "\n",
      "\tEpisode 2369 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4503],\n",
      "        [1.0000, 0.4315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5155],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586949348449707 \tStep Time:  0.0060155391693115234 s \tTotal Time:  14.867756366729736 s \n",
      "\n",
      "\n",
      "\tEpisode 2370 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5421],\n",
      "        [1.0000, 0.4427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4637],\n",
      "        [1.0000, 0.5227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53061056137085 \tStep Time:  0.005985736846923828 s \tTotal Time:  14.87374210357666 s \n",
      "\n",
      "\n",
      "\tEpisode 2371 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4781],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5518],\n",
      "        [1.0000, 0.4456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52613115310669 \tStep Time:  0.005949974060058594 s \tTotal Time:  14.879692077636719 s \n",
      "\n",
      "\n",
      "\tEpisode 2372 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.4725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4875],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529720783233643 \tStep Time:  0.0079803466796875 s \tTotal Time:  14.887672424316406 s \n",
      "\n",
      "\n",
      "\tEpisode 2373 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.6377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4821],\n",
      "        [1.0000, 0.4640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475131154060364 \tStep Time:  0.005982875823974609 s \tTotal Time:  14.89365530014038 s \n",
      "\n",
      "\n",
      "\tEpisode 2374 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4582],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4682],\n",
      "        [1.0000, 0.5668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471278667449951 \tStep Time:  0.006981611251831055 s \tTotal Time:  14.900636911392212 s \n",
      "\n",
      "\n",
      "\tEpisode 2375 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4752],\n",
      "        [1.0000, 0.4919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4681],\n",
      "        [1.0000, 0.4704]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533395767211914 \tStep Time:  0.005983829498291016 s \tTotal Time:  14.906620740890503 s \n",
      "\n",
      "\n",
      "\tEpisode 2376 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5957],\n",
      "        [1.0000, 0.4657]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4535],\n",
      "        [1.0000, 0.5454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550288677215576 \tStep Time:  0.0059850215911865234 s \tTotal Time:  14.91260576248169 s \n",
      "\n",
      "\n",
      "\tEpisode 2377 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4863],\n",
      "        [1.0000, 0.4684]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5428],\n",
      "        [1.0000, 0.4985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547355890274048 \tStep Time:  0.007013559341430664 s \tTotal Time:  14.91961932182312 s \n",
      "\n",
      "\n",
      "\tEpisode 2378 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4799],\n",
      "        [1.0000, 0.4832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5211],\n",
      "        [1.0000, 0.4785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529437959194183 \tStep Time:  0.005982637405395508 s \tTotal Time:  14.925601959228516 s \n",
      "\n",
      "\n",
      "\tEpisode 2379 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4860],\n",
      "        [1.0000, 0.5501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5237],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494907259941101 \tStep Time:  0.0059528350830078125 s \tTotal Time:  14.931554794311523 s \n",
      "\n",
      "\n",
      "\tEpisode 2380 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5434],\n",
      "        [1.0000, 0.4874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4757],\n",
      "        [1.0000, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542176902294159 \tStep Time:  0.006982326507568359 s \tTotal Time:  14.938537120819092 s \n",
      "\n",
      "\n",
      "\tEpisode 2381 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4873],\n",
      "        [1.0000, 0.5052]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4758],\n",
      "        [1.0000, 0.5183]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517323613166809 \tStep Time:  0.005982875823974609 s \tTotal Time:  14.944519996643066 s \n",
      "\n",
      "\n",
      "\tEpisode 2382 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5197],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4698],\n",
      "        [1.0000, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48242461681366 \tStep Time:  0.006014347076416016 s \tTotal Time:  14.950534343719482 s \n",
      "\n",
      "\n",
      "\tEpisode 2383 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5393],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.4445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504746913909912 \tStep Time:  0.006982088088989258 s \tTotal Time:  14.957516431808472 s \n",
      "\n",
      "\n",
      "\tEpisode 2384 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4027],\n",
      "        [1.0000, 0.4558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6045],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49286162853241 \tStep Time:  0.005952358245849609 s \tTotal Time:  14.963468790054321 s \n",
      "\n",
      "\n",
      "\tEpisode 2385 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4545],\n",
      "        [1.0000, 0.6132]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4461],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39474868774414 \tStep Time:  0.006018877029418945 s \tTotal Time:  14.96948766708374 s \n",
      "\n",
      "\n",
      "\tEpisode 2386 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5117],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5439],\n",
      "        [1.0000, 0.5760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475350856781006 \tStep Time:  0.0059814453125 s \tTotal Time:  14.97546911239624 s \n",
      "\n",
      "\n",
      "\tEpisode 2387 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [1.0000, 0.4598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6028],\n",
      "        [1.0000, 0.3971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486247062683105 \tStep Time:  0.005951881408691406 s \tTotal Time:  14.981420993804932 s \n",
      "\n",
      "\n",
      "\tEpisode 2388 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4699],\n",
      "        [1.0000, 0.4433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4375],\n",
      "        [1.0000, 0.4506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5268235206604 \tStep Time:  0.006014823913574219 s \tTotal Time:  14.987435817718506 s \n",
      "\n",
      "\n",
      "\tEpisode 2389 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6186],\n",
      "        [1.0000, 0.4105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4445],\n",
      "        [1.0000, 0.4280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439056873321533 \tStep Time:  0.006982326507568359 s \tTotal Time:  14.994418144226074 s \n",
      "\n",
      "\n",
      "\tEpisode 2390 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6742],\n",
      "        [1.0000, 0.5202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5257],\n",
      "        [1.0000, 0.4633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.411958575248718 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.000401973724365 s \n",
      "\n",
      "\n",
      "\tEpisode 2391 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7258],\n",
      "        [1.0000, 0.4181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5281],\n",
      "        [1.0000, 0.4214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440413475036621 \tStep Time:  0.00695037841796875 s \tTotal Time:  15.007352352142334 s \n",
      "\n",
      "\n",
      "\tEpisode 2392 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5898],\n",
      "        [1.0000, 0.4544]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4428],\n",
      "        [1.0000, 0.4609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586225986480713 \tStep Time:  0.005983591079711914 s \tTotal Time:  15.013335943222046 s \n",
      "\n",
      "\n",
      "\tEpisode 2393 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4466],\n",
      "        [1.0000, 0.7741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4709],\n",
      "        [1.0000, 0.8274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.639029026031494 \tStep Time:  0.0069806575775146484 s \tTotal Time:  15.02031660079956 s \n",
      "\n",
      "\n",
      "\tEpisode 2394 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4293],\n",
      "        [1.0000, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4333],\n",
      "        [1.0000, 0.4514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571235656738281 \tStep Time:  0.006015300750732422 s \tTotal Time:  15.026331901550293 s \n",
      "\n",
      "\n",
      "\tEpisode 2395 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4335],\n",
      "        [1.0000, 0.4530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7210],\n",
      "        [1.0000, 0.4508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420378983020782 \tStep Time:  0.006951332092285156 s \tTotal Time:  15.033283233642578 s \n",
      "\n",
      "\n",
      "\tEpisode 2396 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4544],\n",
      "        [1.0000, 0.4765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4434],\n",
      "        [1.0000, 0.6970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39206862449646 \tStep Time:  0.0069806575775146484 s \tTotal Time:  15.040263891220093 s \n",
      "\n",
      "\n",
      "\tEpisode 2397 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.6780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4374],\n",
      "        [1.0000, 0.4783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402450382709503 \tStep Time:  0.006981611251831055 s \tTotal Time:  15.047245502471924 s \n",
      "\n",
      "\n",
      "\tEpisode 2398 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4393],\n",
      "        [1.0000, 0.4426]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4542],\n",
      "        [1.0000, 0.4336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521292507648468 \tStep Time:  0.006982088088989258 s \tTotal Time:  15.054227590560913 s \n",
      "\n",
      "\n",
      "\tEpisode 2399 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5227],\n",
      "        [1.0000, 0.4556]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4652],\n",
      "        [1.0000, 0.4055]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450439989566803 \tStep Time:  0.005982637405395508 s \tTotal Time:  15.060210227966309 s \n",
      "\n",
      "\n",
      "\tEpisode 2400 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5343],\n",
      "        [1.0000, 0.7790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4554],\n",
      "        [1.0000, 0.8037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567763328552246 \tStep Time:  0.0069811344146728516 s \tTotal Time:  15.067191362380981 s \n",
      "\n",
      "\n",
      "\tEpisode 2401 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5483],\n",
      "        [1.0000, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7140],\n",
      "        [1.0000, 0.5736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.628782272338867 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.073175191879272 s \n",
      "\n",
      "\n",
      "\tEpisode 2402 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4493],\n",
      "        [1.0000, 0.5659]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3699],\n",
      "        [1.0000, 0.4112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5647873878479 \tStep Time:  0.007979631423950195 s \tTotal Time:  15.081154823303223 s \n",
      "\n",
      "\n",
      "\tEpisode 2403 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6031],\n",
      "        [1.0000, 0.4024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7013],\n",
      "        [1.0000, 0.7787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.342115879058838 \tStep Time:  0.009972095489501953 s \tTotal Time:  15.091126918792725 s \n",
      "\n",
      "\n",
      "\tEpisode 2404 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4871],\n",
      "        [1.0000, 0.3468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3613],\n",
      "        [1.0000, 0.3443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476058959960938 \tStep Time:  0.011968374252319336 s \tTotal Time:  15.103095293045044 s \n",
      "\n",
      "\n",
      "\tEpisode 2405 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5377],\n",
      "        [1.0000, 0.3454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3663],\n",
      "        [1.0000, 0.3225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.663004875183105 \tStep Time:  0.006981372833251953 s \tTotal Time:  15.110076665878296 s \n",
      "\n",
      "\n",
      "\tEpisode 2406 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.6104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5559],\n",
      "        [1.0000, 0.3243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595231473445892 \tStep Time:  0.008977890014648438 s \tTotal Time:  15.119054555892944 s \n",
      "\n",
      "\n",
      "\tEpisode 2407 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3541],\n",
      "        [1.0000, 0.3811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5732],\n",
      "        [1.0000, 0.4621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468271017074585 \tStep Time:  0.006015777587890625 s \tTotal Time:  15.125070333480835 s \n",
      "\n",
      "\n",
      "\tEpisode 2408 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4477],\n",
      "        [1.0000, 0.7267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6885],\n",
      "        [1.0000, 0.4574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550762176513672 \tStep Time:  0.005986452102661133 s \tTotal Time:  15.131056785583496 s \n",
      "\n",
      "\n",
      "\tEpisode 2409 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3792],\n",
      "        [1.0000, 0.4190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4008],\n",
      "        [1.0000, 0.5325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466949939727783 \tStep Time:  0.007942914962768555 s \tTotal Time:  15.138999700546265 s \n",
      "\n",
      "\n",
      "\tEpisode 2410 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4228],\n",
      "        [1.0000, 0.3894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3979],\n",
      "        [1.0000, 0.3901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568778455257416 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.144983768463135 s \n",
      "\n",
      "\n",
      "\tEpisode 2411 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5465],\n",
      "        [1.0000, 0.3954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6435],\n",
      "        [1.0000, 0.4279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500872611999512 \tStep Time:  0.007978677749633789 s \tTotal Time:  15.152962446212769 s \n",
      "\n",
      "\n",
      "\tEpisode 2412 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4172],\n",
      "        [1.0000, 0.4135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4247],\n",
      "        [1.0000, 0.5279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474315166473389 \tStep Time:  0.006016254425048828 s \tTotal Time:  15.158978700637817 s \n",
      "\n",
      "\n",
      "\tEpisode 2413 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5569],\n",
      "        [1.0000, 0.5421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4843],\n",
      "        [1.0000, 0.4281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479558944702148 \tStep Time:  0.006982564926147461 s \tTotal Time:  15.165961265563965 s \n",
      "\n",
      "\n",
      "\tEpisode 2414 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7148],\n",
      "        [1.0000, 0.4631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.6468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.363808035850525 \tStep Time:  0.006979942321777344 s \tTotal Time:  15.172941207885742 s \n",
      "\n",
      "\n",
      "\tEpisode 2415 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4604],\n",
      "        [1.0000, 0.5397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7325],\n",
      "        [1.0000, 0.4218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43433403968811 \tStep Time:  0.00598907470703125 s \tTotal Time:  15.178930282592773 s \n",
      "\n",
      "\n",
      "\tEpisode 2416 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.4191]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6852],\n",
      "        [1.0000, 0.4388]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443135023117065 \tStep Time:  0.007941246032714844 s \tTotal Time:  15.186871528625488 s \n",
      "\n",
      "\n",
      "\tEpisode 2417 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4067],\n",
      "        [1.0000, 0.4381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4640],\n",
      "        [1.0000, 0.4283]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533698558807373 \tStep Time:  0.006017923355102539 s \tTotal Time:  15.19288945198059 s \n",
      "\n",
      "\n",
      "\tEpisode 2418 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5326],\n",
      "        [1.0000, 0.4741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5373],\n",
      "        [1.0000, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510199069976807 \tStep Time:  0.006982088088989258 s \tTotal Time:  15.19987154006958 s \n",
      "\n",
      "\n",
      "\tEpisode 2419 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7328],\n",
      "        [1.0000, 0.5993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7289],\n",
      "        [1.0000, 0.6823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497108459472656 \tStep Time:  0.007944107055664062 s \tTotal Time:  15.207815647125244 s \n",
      "\n",
      "\n",
      "\tEpisode 2420 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7729],\n",
      "        [1.0000, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4221],\n",
      "        [1.0000, 0.5326]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485858976840973 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.213799715042114 s \n",
      "\n",
      "\n",
      "\tEpisode 2421 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5030],\n",
      "        [1.0000, 0.5586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.3742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.635982513427734 \tStep Time:  0.0069925785064697266 s \tTotal Time:  15.220792293548584 s \n",
      "\n",
      "\n",
      "\tEpisode 2422 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6581],\n",
      "        [1.0000, 0.3972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4333],\n",
      "        [1.0000, 0.5577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.35943078994751 \tStep Time:  0.005972862243652344 s \tTotal Time:  15.226765155792236 s \n",
      "\n",
      "\n",
      "\tEpisode 2423 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4147],\n",
      "        [1.0000, 0.4472]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5936],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509466648101807 \tStep Time:  0.007978677749633789 s \tTotal Time:  15.23474383354187 s \n",
      "\n",
      "\n",
      "\tEpisode 2424 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3988],\n",
      "        [1.0000, 0.4282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7260],\n",
      "        [1.0000, 0.3832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455699443817139 \tStep Time:  0.006178617477416992 s \tTotal Time:  15.240922451019287 s \n",
      "\n",
      "\n",
      "\tEpisode 2425 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4201],\n",
      "        [1.0000, 0.4455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6777],\n",
      "        [1.0000, 0.3943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429203033447266 \tStep Time:  0.006981372833251953 s \tTotal Time:  15.247903823852539 s \n",
      "\n",
      "\n",
      "\tEpisode 2426 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4243],\n",
      "        [1.0000, 0.6575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3826],\n",
      "        [1.0000, 0.6188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511133193969727 \tStep Time:  0.010971307754516602 s \tTotal Time:  15.258875131607056 s \n",
      "\n",
      "\n",
      "\tEpisode 2427 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4581],\n",
      "        [1.0000, 0.4730]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4075],\n",
      "        [1.0000, 0.3877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534943580627441 \tStep Time:  0.007978677749633789 s \tTotal Time:  15.26685380935669 s \n",
      "\n",
      "\n",
      "\tEpisode 2428 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4709],\n",
      "        [1.0000, 0.4034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4421],\n",
      "        [1.0000, 0.3720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462686777114868 \tStep Time:  0.005982637405395508 s \tTotal Time:  15.272836446762085 s \n",
      "\n",
      "\n",
      "\tEpisode 2429 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3668],\n",
      "        [1.0000, 0.3905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4029],\n",
      "        [1.0000, 0.4596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595642805099487 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.278820276260376 s \n",
      "\n",
      "\n",
      "\tEpisode 2430 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4071],\n",
      "        [1.0000, 0.4063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4369],\n",
      "        [1.0000, 0.3877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560110569000244 \tStep Time:  0.007979154586791992 s \tTotal Time:  15.286799430847168 s \n",
      "\n",
      "\n",
      "\tEpisode 2431 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3734],\n",
      "        [1.0000, 0.4035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4530],\n",
      "        [1.0000, 0.3869]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580499172210693 \tStep Time:  0.00598454475402832 s \tTotal Time:  15.292783975601196 s \n",
      "\n",
      "\n",
      "\tEpisode 2432 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4717],\n",
      "        [1.0000, 0.3896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4158],\n",
      "        [1.0000, 0.4000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58833646774292 \tStep Time:  0.006665706634521484 s \tTotal Time:  15.299449682235718 s \n",
      "\n",
      "\n",
      "\tEpisode 2433 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4997],\n",
      "        [1.0000, 0.5559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5785],\n",
      "        [1.0000, 0.5373]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467302322387695 \tStep Time:  0.00629878044128418 s \tTotal Time:  15.305748462677002 s \n",
      "\n",
      "\n",
      "\tEpisode 2434 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7869],\n",
      "        [1.0000, 0.4320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7184],\n",
      "        [1.0000, 0.4536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.304271340370178 \tStep Time:  0.00598454475402832 s \tTotal Time:  15.31173300743103 s \n",
      "\n",
      "\n",
      "\tEpisode 2435 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4468],\n",
      "        [1.0000, 0.5024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7472],\n",
      "        [1.0000, 0.4525]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.390418529510498 \tStep Time:  0.0059833526611328125 s \tTotal Time:  15.317716360092163 s \n",
      "\n",
      "\n",
      "\tEpisode 2436 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4419],\n",
      "        [1.0000, 0.7733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5769],\n",
      "        [1.0000, 0.6170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564272403717041 \tStep Time:  0.0060520172119140625 s \tTotal Time:  15.323768377304077 s \n",
      "\n",
      "\n",
      "\tEpisode 2437 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4114],\n",
      "        [1.0000, 0.4052]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6491],\n",
      "        [1.0000, 0.6333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534299850463867 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.329752206802368 s \n",
      "\n",
      "\n",
      "\tEpisode 2438 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4671],\n",
      "        [1.0000, 0.3979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4828],\n",
      "        [1.0000, 0.7879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442201137542725 \tStep Time:  0.007947206497192383 s \tTotal Time:  15.33769941329956 s \n",
      "\n",
      "\n",
      "\tEpisode 2439 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5221],\n",
      "        [1.0000, 0.4242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5973],\n",
      "        [1.0000, 0.4535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547659933567047 \tStep Time:  0.006017208099365234 s \tTotal Time:  15.343716621398926 s \n",
      "\n",
      "\n",
      "\tEpisode 2440 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4332],\n",
      "        [1.0000, 0.6592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4426],\n",
      "        [1.0000, 0.6110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.33917236328125 \tStep Time:  0.005983114242553711 s \tTotal Time:  15.34969973564148 s \n",
      "\n",
      "\n",
      "\tEpisode 2441 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4074],\n",
      "        [1.0000, 0.5700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.6323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461833953857422 \tStep Time:  0.0059871673583984375 s \tTotal Time:  15.355686902999878 s \n",
      "\n",
      "\n",
      "\tEpisode 2442 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7172],\n",
      "        [1.0000, 0.5558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3594],\n",
      "        [1.0000, 0.7563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.303171277046204 \tStep Time:  0.005979299545288086 s \tTotal Time:  15.361666202545166 s \n",
      "\n",
      "\n",
      "\tEpisode 2443 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3721],\n",
      "        [1.0000, 0.5192]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5171],\n",
      "        [1.0000, 0.4029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.400627613067627 \tStep Time:  0.006981611251831055 s \tTotal Time:  15.368647813796997 s \n",
      "\n",
      "\n",
      "\tEpisode 2444 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6323],\n",
      "        [1.0000, 0.6619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4565],\n",
      "        [1.0000, 0.3650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592597842216492 \tStep Time:  0.0059854984283447266 s \tTotal Time:  15.374633312225342 s \n",
      "\n",
      "\n",
      "\tEpisode 2445 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3658],\n",
      "        [1.0000, 0.3274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6976],\n",
      "        [1.0000, 0.3726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.381791591644287 \tStep Time:  0.0049855709075927734 s \tTotal Time:  15.379618883132935 s \n",
      "\n",
      "\n",
      "\tEpisode 2446 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3706],\n",
      "        [1.0000, 0.3156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4297],\n",
      "        [1.0000, 0.2877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554429054260254 \tStep Time:  0.007978677749633789 s \tTotal Time:  15.387597560882568 s \n",
      "\n",
      "\n",
      "\tEpisode 2447 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3206],\n",
      "        [1.0000, 0.6016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5594],\n",
      "        [1.0000, 0.3844]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542537212371826 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.39358139038086 s \n",
      "\n",
      "\n",
      "\tEpisode 2448 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [1.0000, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2951],\n",
      "        [1.0000, 0.2384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535817325115204 \tStep Time:  0.005986690521240234 s \tTotal Time:  15.3995680809021 s \n",
      "\n",
      "\n",
      "\tEpisode 2449 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5508],\n",
      "        [1.0000, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7631],\n",
      "        [1.0000, 0.6318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518394470214844 \tStep Time:  0.006978034973144531 s \tTotal Time:  15.406546115875244 s \n",
      "\n",
      "\n",
      "\tEpisode 2450 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3941],\n",
      "        [1.0000, 0.3394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6114],\n",
      "        [1.0000, 0.4746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50693792104721 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.412529945373535 s \n",
      "\n",
      "\n",
      "\tEpisode 2451 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8418],\n",
      "        [1.0000, 0.4344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3254],\n",
      "        [1.0000, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.323799133300781 \tStep Time:  0.006967782974243164 s \tTotal Time:  15.419497728347778 s \n",
      "\n",
      "\n",
      "\tEpisode 2452 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9393],\n",
      "        [1.0000, 0.5269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3242],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.964517593383789 \tStep Time:  0.0059931278228759766 s \tTotal Time:  15.425490856170654 s \n",
      "\n",
      "\n",
      "\tEpisode 2453 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3479],\n",
      "        [1.0000, 0.3388]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4038],\n",
      "        [1.0000, 0.3841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.589077651500702 \tStep Time:  0.007952213287353516 s \tTotal Time:  15.433443069458008 s \n",
      "\n",
      "\n",
      "\tEpisode 2454 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5698],\n",
      "        [1.0000, 0.6860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5303],\n",
      "        [1.0000, 0.8400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544404029846191 \tStep Time:  0.006981372833251953 s \tTotal Time:  15.44042444229126 s \n",
      "\n",
      "\n",
      "\tEpisode 2455 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4052],\n",
      "        [1.0000, 0.3734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4229],\n",
      "        [1.0000, 0.4400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.608170509338379 \tStep Time:  0.005984306335449219 s \tTotal Time:  15.446408748626709 s \n",
      "\n",
      "\n",
      "\tEpisode 2456 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3899],\n",
      "        [1.0000, 0.4536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4003],\n",
      "        [1.0000, 0.4045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536718845367432 \tStep Time:  0.007977485656738281 s \tTotal Time:  15.454386234283447 s \n",
      "\n",
      "\n",
      "\tEpisode 2457 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4317],\n",
      "        [1.0000, 0.4226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.4167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547194421291351 \tStep Time:  0.006981611251831055 s \tTotal Time:  15.461367845535278 s \n",
      "\n",
      "\n",
      "\tEpisode 2458 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4563],\n",
      "        [1.0000, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4339],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546490788459778 \tStep Time:  0.0069828033447265625 s \tTotal Time:  15.468350648880005 s \n",
      "\n",
      "\n",
      "\tEpisode 2459 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.4401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6030],\n",
      "        [1.0000, 0.5741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41353988647461 \tStep Time:  0.00701141357421875 s \tTotal Time:  15.475362062454224 s \n",
      "\n",
      "\n",
      "\tEpisode 2460 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4431],\n",
      "        [1.0000, 0.8566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7759],\n",
      "        [1.0000, 0.8021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.822157382965088 \tStep Time:  0.005953550338745117 s \tTotal Time:  15.481315612792969 s \n",
      "\n",
      "\n",
      "\tEpisode 2461 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6419],\n",
      "        [1.0000, 0.4436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5574],\n",
      "        [1.0000, 0.5505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583594799041748 \tStep Time:  0.00698089599609375 s \tTotal Time:  15.488296508789062 s \n",
      "\n",
      "\n",
      "\tEpisode 2462 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4645],\n",
      "        [1.0000, 0.4522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4386],\n",
      "        [1.0000, 0.4782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554197788238525 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.494280338287354 s \n",
      "\n",
      "\n",
      "\tEpisode 2463 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7160],\n",
      "        [1.0000, 0.7482]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4400],\n",
      "        [1.0000, 0.5800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.726358532905579 \tStep Time:  0.006017208099365234 s \tTotal Time:  15.500297546386719 s \n",
      "\n",
      "\n",
      "\tEpisode 2464 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8706],\n",
      "        [1.0000, 0.6711]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6397],\n",
      "        [1.0000, 0.4396]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.837300300598145 \tStep Time:  0.005982637405395508 s \tTotal Time:  15.506280183792114 s \n",
      "\n",
      "\n",
      "\tEpisode 2465 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4521],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4743],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514626681804657 \tStep Time:  0.0059833526611328125 s \tTotal Time:  15.512263536453247 s \n",
      "\n",
      "\n",
      "\tEpisode 2466 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.4567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7324],\n",
      "        [1.0000, 0.5429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.741091251373291 \tStep Time:  0.006993770599365234 s \tTotal Time:  15.519257307052612 s \n",
      "\n",
      "\n",
      "\tEpisode 2467 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5130],\n",
      "        [1.0000, 0.4642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4687],\n",
      "        [1.0000, 0.7026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.397126197814941 \tStep Time:  0.005011081695556641 s \tTotal Time:  15.524268388748169 s \n",
      "\n",
      "\n",
      "\tEpisode 2468 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6204],\n",
      "        [1.0000, 0.6943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5923],\n",
      "        [1.0000, 0.5736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482235431671143 \tStep Time:  0.00596165657043457 s \tTotal Time:  15.530230045318604 s \n",
      "\n",
      "\n",
      "\tEpisode 2469 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.4585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4611],\n",
      "        [1.0000, 0.5594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493933200836182 \tStep Time:  0.008003711700439453 s \tTotal Time:  15.538233757019043 s \n",
      "\n",
      "\n",
      "\tEpisode 2470 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4502],\n",
      "        [1.0000, 0.6082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.602208018302917 \tStep Time:  0.004984617233276367 s \tTotal Time:  15.54321837425232 s \n",
      "\n",
      "\n",
      "\tEpisode 2471 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.4568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4703],\n",
      "        [1.0000, 0.6130]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446429252624512 \tStep Time:  0.0069844722747802734 s \tTotal Time:  15.5502028465271 s \n",
      "\n",
      "\n",
      "\tEpisode 2472 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4917],\n",
      "        [1.0000, 0.4598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4520],\n",
      "        [1.0000, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521743774414062 \tStep Time:  0.005980014801025391 s \tTotal Time:  15.556182861328125 s \n",
      "\n",
      "\n",
      "\tEpisode 2473 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4606],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [1.0000, 0.4515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529367685317993 \tStep Time:  0.005986452102661133 s \tTotal Time:  15.562169313430786 s \n",
      "\n",
      "\n",
      "\tEpisode 2474 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4802],\n",
      "        [1.0000, 0.4889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6043],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578230381011963 \tStep Time:  0.006947517395019531 s \tTotal Time:  15.569116830825806 s \n",
      "\n",
      "\n",
      "\tEpisode 2475 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5692],\n",
      "        [1.0000, 0.5626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5250],\n",
      "        [1.0000, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567426681518555 \tStep Time:  0.006015777587890625 s \tTotal Time:  15.575132608413696 s \n",
      "\n",
      "\n",
      "\tEpisode 2476 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4516],\n",
      "        [1.0000, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.6030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59807014465332 \tStep Time:  0.005953550338745117 s \tTotal Time:  15.581086158752441 s \n",
      "\n",
      "\n",
      "\tEpisode 2477 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5433],\n",
      "        [1.0000, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4774],\n",
      "        [1.0000, 0.4461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568091869354248 \tStep Time:  0.0070116519927978516 s \tTotal Time:  15.58809781074524 s \n",
      "\n",
      "\n",
      "\tEpisode 2478 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4639],\n",
      "        [1.0000, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4391],\n",
      "        [1.0000, 0.5767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591789722442627 \tStep Time:  0.005986928939819336 s \tTotal Time:  15.594084739685059 s \n",
      "\n",
      "\n",
      "\tEpisode 2479 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4403],\n",
      "        [1.0000, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4413],\n",
      "        [1.0000, 0.4454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561498880386353 \tStep Time:  0.0069789886474609375 s \tTotal Time:  15.60106372833252 s \n",
      "\n",
      "\n",
      "\tEpisode 2480 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4807],\n",
      "        [1.0000, 0.4393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4438],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493234634399414 \tStep Time:  0.004985809326171875 s \tTotal Time:  15.606049537658691 s \n",
      "\n",
      "\n",
      "\tEpisode 2481 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4332],\n",
      "        [1.0000, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5118],\n",
      "        [1.0000, 0.4355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50053071975708 \tStep Time:  0.0059528350830078125 s \tTotal Time:  15.61300015449524 s \n",
      "\n",
      "\n",
      "\tEpisode 2482 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4441],\n",
      "        [1.0000, 0.5438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4415],\n",
      "        [1.0000, 0.4377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477270126342773 \tStep Time:  0.0069811344146728516 s \tTotal Time:  15.619981288909912 s \n",
      "\n",
      "\n",
      "\tEpisode 2483 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4402],\n",
      "        [1.0000, 0.4352]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.4430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493112444877625 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.625965118408203 s \n",
      "\n",
      "\n",
      "\tEpisode 2484 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4393],\n",
      "        [1.0000, 0.4326]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5597],\n",
      "        [1.0000, 0.5122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481558799743652 \tStep Time:  0.005983591079711914 s \tTotal Time:  15.631948709487915 s \n",
      "\n",
      "\n",
      "\tEpisode 2485 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5848],\n",
      "        [1.0000, 0.4472]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.4327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451918721199036 \tStep Time:  0.0069811344146728516 s \tTotal Time:  15.638929843902588 s \n",
      "\n",
      "\n",
      "\tEpisode 2486 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5602],\n",
      "        [1.0000, 0.3628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7011],\n",
      "        [1.0000, 0.3582]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.814533710479736 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.644913673400879 s \n",
      "\n",
      "\n",
      "\tEpisode 2487 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4307],\n",
      "        [1.0000, 0.4573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4408],\n",
      "        [1.0000, 0.5530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557617783546448 \tStep Time:  0.006981611251831055 s \tTotal Time:  15.65189528465271 s \n",
      "\n",
      "\n",
      "\tEpisode 2488 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5746],\n",
      "        [1.0000, 0.5503]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526069164276123 \tStep Time:  0.006021738052368164 s \tTotal Time:  15.657917022705078 s \n",
      "\n",
      "\n",
      "\tEpisode 2489 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4702],\n",
      "        [1.0000, 0.4443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5510],\n",
      "        [1.0000, 0.6001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52704268693924 \tStep Time:  0.006977081298828125 s \tTotal Time:  15.664894104003906 s \n",
      "\n",
      "\n",
      "\tEpisode 2490 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4816],\n",
      "        [1.0000, 0.4831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517103672027588 \tStep Time:  0.005952358245849609 s \tTotal Time:  15.670846462249756 s \n",
      "\n",
      "\n",
      "\tEpisode 2491 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5462],\n",
      "        [1.0000, 0.4818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46233081817627 \tStep Time:  0.00601506233215332 s \tTotal Time:  15.67686152458191 s \n",
      "\n",
      "\n",
      "\tEpisode 2492 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5547],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4764],\n",
      "        [1.0000, 0.6250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48534107208252 \tStep Time:  0.006949186325073242 s \tTotal Time:  15.683810710906982 s \n",
      "\n",
      "\n",
      "\tEpisode 2493 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4777],\n",
      "        [1.0000, 0.4712]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5501],\n",
      "        [1.0000, 0.4853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47750473022461 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.689794540405273 s \n",
      "\n",
      "\n",
      "\tEpisode 2494 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4723],\n",
      "        [1.0000, 0.5350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4784],\n",
      "        [1.0000, 0.5470]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576148629188538 \tStep Time:  0.0060160160064697266 s \tTotal Time:  15.695810556411743 s \n",
      "\n",
      "\n",
      "\tEpisode 2495 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4805],\n",
      "        [1.0000, 0.4809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5215],\n",
      "        [1.0000, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486631095409393 \tStep Time:  0.005994081497192383 s \tTotal Time:  15.701804637908936 s \n",
      "\n",
      "\n",
      "\tEpisode 2496 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5518],\n",
      "        [1.0000, 0.4721]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4958],\n",
      "        [1.0000, 0.5682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493406772613525 \tStep Time:  0.006045103073120117 s \tTotal Time:  15.707849740982056 s \n",
      "\n",
      "\n",
      "\tEpisode 2497 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4670],\n",
      "        [1.0000, 0.5782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5108],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473685085773468 \tStep Time:  0.005952119827270508 s \tTotal Time:  15.713801860809326 s \n",
      "\n",
      "\n",
      "\tEpisode 2498 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4748],\n",
      "        [1.0000, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5909],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523170948028564 \tStep Time:  0.006054878234863281 s \tTotal Time:  15.71985673904419 s \n",
      "\n",
      "\n",
      "\tEpisode 2499 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4875],\n",
      "        [1.0000, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4699],\n",
      "        [1.0000, 0.5390]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542529582977295 \tStep Time:  0.006017446517944336 s \tTotal Time:  15.725874185562134 s \n",
      "\n",
      "\n",
      "\tEpisode 2500 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5243],\n",
      "        [1.0000, 0.4679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478981018066406 \tStep Time:  0.006407260894775391 s \tTotal Time:  15.73228144645691 s \n",
      "\n",
      "\n",
      "\tEpisode 2501 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5106],\n",
      "        [1.0000, 0.4656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4661],\n",
      "        [1.0000, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542180120944977 \tStep Time:  0.006557941436767578 s \tTotal Time:  15.738839387893677 s \n",
      "\n",
      "\n",
      "\tEpisode 2502 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4848],\n",
      "        [1.0000, 0.5286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4883],\n",
      "        [1.0000, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538114070892334 \tStep Time:  0.004955768585205078 s \tTotal Time:  15.743795156478882 s \n",
      "\n",
      "\n",
      "\tEpisode 2503 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4630],\n",
      "        [1.0000, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4687],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51190710067749 \tStep Time:  0.0069811344146728516 s \tTotal Time:  15.750776290893555 s \n",
      "\n",
      "\n",
      "\tEpisode 2504 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4864],\n",
      "        [1.0000, 0.4725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4971],\n",
      "        [1.0000, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50681757926941 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.756760120391846 s \n",
      "\n",
      "\n",
      "\tEpisode 2505 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.4882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4762],\n",
      "        [1.0000, 0.5392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483936309814453 \tStep Time:  0.00601506233215332 s \tTotal Time:  15.762775182723999 s \n",
      "\n",
      "\n",
      "\tEpisode 2506 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5001],\n",
      "        [1.0000, 0.4747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.5126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492605686187744 \tStep Time:  0.005953073501586914 s \tTotal Time:  15.768728256225586 s \n",
      "\n",
      "\n",
      "\tEpisode 2507 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529792666435242 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.774712324142456 s \n",
      "\n",
      "\n",
      "\tEpisode 2508 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4854],\n",
      "        [1.0000, 0.5621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4797],\n",
      "        [1.0000, 0.5977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610469818115234 \tStep Time:  0.005983591079711914 s \tTotal Time:  15.780695915222168 s \n",
      "\n",
      "\n",
      "\tEpisode 2509 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5233],\n",
      "        [1.0000, 0.5549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5164],\n",
      "        [1.0000, 0.4792]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544412612915039 \tStep Time:  0.006982564926147461 s \tTotal Time:  15.787678480148315 s \n",
      "\n",
      "\n",
      "\tEpisode 2510 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4746],\n",
      "        [1.0000, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513523578643799 \tStep Time:  0.005982637405395508 s \tTotal Time:  15.793661117553711 s \n",
      "\n",
      "\n",
      "\tEpisode 2511 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.4682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5053],\n",
      "        [1.0000, 0.5290]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524447321891785 \tStep Time:  0.005984783172607422 s \tTotal Time:  15.799645900726318 s \n",
      "\n",
      "\n",
      "\tEpisode 2512 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4878],\n",
      "        [1.0000, 0.5202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5485],\n",
      "        [1.0000, 0.5551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557514190673828 \tStep Time:  0.005983591079711914 s \tTotal Time:  15.80562949180603 s \n",
      "\n",
      "\n",
      "\tEpisode 2513 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5460],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4898],\n",
      "        [1.0000, 0.4694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5539870262146 \tStep Time:  0.005984306335449219 s \tTotal Time:  15.81161379814148 s \n",
      "\n",
      "\n",
      "\tEpisode 2514 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4616],\n",
      "        [1.0000, 0.5193]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499319553375244 \tStep Time:  0.00698089599609375 s \tTotal Time:  15.818594694137573 s \n",
      "\n",
      "\n",
      "\tEpisode 2515 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [1.0000, 0.4591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5334],\n",
      "        [1.0000, 0.4672]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467186450958252 \tStep Time:  0.0060176849365234375 s \tTotal Time:  15.824612379074097 s \n",
      "\n",
      "\n",
      "\tEpisode 2516 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4459],\n",
      "        [1.0000, 0.5490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4489],\n",
      "        [1.0000, 0.4895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448640644550323 \tStep Time:  0.004995822906494141 s \tTotal Time:  15.82960820198059 s \n",
      "\n",
      "\n",
      "\tEpisode 2517 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4668],\n",
      "        [1.0000, 0.5224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4398],\n",
      "        [1.0000, 0.4254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476406574249268 \tStep Time:  0.007935762405395508 s \tTotal Time:  15.837543964385986 s \n",
      "\n",
      "\n",
      "\tEpisode 2518 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3581],\n",
      "        [1.0000, 0.4601]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6653],\n",
      "        [1.0000, 0.6633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592486500740051 \tStep Time:  0.0059850215911865234 s \tTotal Time:  15.843528985977173 s \n",
      "\n",
      "\n",
      "\tEpisode 2519 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5435],\n",
      "        [1.0000, 0.4238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4663],\n",
      "        [1.0000, 0.6261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.377730548381805 \tStep Time:  0.006014347076416016 s \tTotal Time:  15.849543333053589 s \n",
      "\n",
      "\n",
      "\tEpisode 2520 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [1.0000, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7712],\n",
      "        [1.0000, 0.3925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36077105998993 \tStep Time:  0.006983280181884766 s \tTotal Time:  15.856526613235474 s \n",
      "\n",
      "\n",
      "\tEpisode 2521 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3437],\n",
      "        [1.0000, 0.4770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4364],\n",
      "        [1.0000, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58623743057251 \tStep Time:  0.007945775985717773 s \tTotal Time:  15.864472389221191 s \n",
      "\n",
      "\n",
      "\tEpisode 2522 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5130],\n",
      "        [1.0000, 0.3330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6057],\n",
      "        [1.0000, 0.3383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.76201868057251 \tStep Time:  0.007978439331054688 s \tTotal Time:  15.872450828552246 s \n",
      "\n",
      "\n",
      "\tEpisode 2523 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5315],\n",
      "        [1.0000, 0.6713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5702],\n",
      "        [1.0000, 0.5929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.609663486480713 \tStep Time:  0.004986286163330078 s \tTotal Time:  15.877437114715576 s \n",
      "\n",
      "\n",
      "\tEpisode 2524 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3714],\n",
      "        [1.0000, 0.4095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4095],\n",
      "        [1.0000, 0.7988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.751224040985107 \tStep Time:  0.006981611251831055 s \tTotal Time:  15.884418725967407 s \n",
      "\n",
      "\n",
      "\tEpisode 2525 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5348],\n",
      "        [1.0000, 0.4016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.3844]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537290573120117 \tStep Time:  0.00698089599609375 s \tTotal Time:  15.891399621963501 s \n",
      "\n",
      "\n",
      "\tEpisode 2526 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4117],\n",
      "        [1.0000, 0.4316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5778],\n",
      "        [1.0000, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55255377292633 \tStep Time:  0.004987001419067383 s \tTotal Time:  15.896386623382568 s \n",
      "\n",
      "\n",
      "\tEpisode 2527 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4742],\n",
      "        [1.0000, 0.4574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2765],\n",
      "        [1.0000, 0.5543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497320175170898 \tStep Time:  0.006981372833251953 s \tTotal Time:  15.90336799621582 s \n",
      "\n",
      "\n",
      "\tEpisode 2528 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6537],\n",
      "        [1.0000, 0.4299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6432],\n",
      "        [1.0000, 0.3598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57570767402649 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.909351825714111 s \n",
      "\n",
      "\n",
      "\tEpisode 2529 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4390],\n",
      "        [1.0000, 0.3485]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5369],\n",
      "        [1.0000, 0.5343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487000942230225 \tStep Time:  0.005983591079711914 s \tTotal Time:  15.915335416793823 s \n",
      "\n",
      "\n",
      "\tEpisode 2530 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.4713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5403],\n",
      "        [1.0000, 0.4380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474298000335693 \tStep Time:  0.00598454475402832 s \tTotal Time:  15.921319961547852 s \n",
      "\n",
      "\n",
      "\tEpisode 2531 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4462],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5517],\n",
      "        [1.0000, 0.4754]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532324314117432 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.927304029464722 s \n",
      "\n",
      "\n",
      "\tEpisode 2532 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5862],\n",
      "        [1.0000, 0.5459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5409],\n",
      "        [1.0000, 0.4445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456227779388428 \tStep Time:  0.006981372833251953 s \tTotal Time:  15.934285402297974 s \n",
      "\n",
      "\n",
      "\tEpisode 2533 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4476],\n",
      "        [1.0000, 0.5911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3235],\n",
      "        [1.0000, 0.4121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.392334461212158 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.940269231796265 s \n",
      "\n",
      "\n",
      "\tEpisode 2534 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4978],\n",
      "        [1.0000, 0.5955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4621],\n",
      "        [1.0000, 0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432490825653076 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.946253299713135 s \n",
      "\n",
      "\n",
      "\tEpisode 2535 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4883],\n",
      "        [1.0000, 0.5178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4685],\n",
      "        [1.0000, 0.5620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553133487701416 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.952237367630005 s \n",
      "\n",
      "\n",
      "\tEpisode 2536 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5416],\n",
      "        [1.0000, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4183],\n",
      "        [1.0000, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459757149219513 \tStep Time:  0.00498652458190918 s \tTotal Time:  15.958221197128296 s \n",
      "\n",
      "\n",
      "\tEpisode 2537 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6209],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4476],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499399840831757 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.964205265045166 s \n",
      "\n",
      "\n",
      "\tEpisode 2538 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3994],\n",
      "        [1.0000, 0.6661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2771],\n",
      "        [1.0000, 0.4676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.785558342933655 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.970189332962036 s \n",
      "\n",
      "\n",
      "\tEpisode 2539 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3632],\n",
      "        [1.0000, 0.4537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6232],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.38440990447998 \tStep Time:  0.005984067916870117 s \tTotal Time:  15.976173400878906 s \n",
      "\n",
      "\n",
      "\tEpisode 2540 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5729],\n",
      "        [1.0000, 0.5563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6424],\n",
      "        [1.0000, 0.5736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48541110754013 \tStep Time:  0.006981372833251953 s \tTotal Time:  15.983154773712158 s \n",
      "\n",
      "\n",
      "\tEpisode 2541 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4642],\n",
      "        [1.0000, 0.4275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6426],\n",
      "        [1.0000, 0.3879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46116304397583 \tStep Time:  0.005983829498291016 s \tTotal Time:  15.98913860321045 s \n",
      "\n",
      "\n",
      "\tEpisode 2542 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.4361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4202],\n",
      "        [1.0000, 0.4707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515505313873291 \tStep Time:  0.005983591079711914 s \tTotal Time:  15.995122194290161 s \n",
      "\n",
      "\n",
      "\tEpisode 2543 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4843],\n",
      "        [1.0000, 0.4437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3812],\n",
      "        [1.0000, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59607982635498 \tStep Time:  0.006981849670410156 s \tTotal Time:  16.00210404396057 s \n",
      "\n",
      "\n",
      "\tEpisode 2544 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2244],\n",
      "        [1.0000, 0.6918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7135],\n",
      "        [1.0000, 0.5618]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.285353183746338 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.00808811187744 s \n",
      "\n",
      "\n",
      "\tEpisode 2545 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4295],\n",
      "        [1.0000, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4514],\n",
      "        [1.0000, 0.6181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.635759353637695 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.015069246292114 s \n",
      "\n",
      "\n",
      "\tEpisode 2546 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.6506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5268],\n",
      "        [1.0000, 0.6385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505475878715515 \tStep Time:  0.007016658782958984 s \tTotal Time:  16.022085905075073 s \n",
      "\n",
      "\n",
      "\tEpisode 2547 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4996],\n",
      "        [1.0000, 0.6370]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4504],\n",
      "        [1.0000, 0.6466]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50075101852417 \tStep Time:  0.004986286163330078 s \tTotal Time:  16.027072191238403 s \n",
      "\n",
      "\n",
      "\tEpisode 2548 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5068],\n",
      "        [1.0000, 0.4554]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5631],\n",
      "        [1.0000, 0.4270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512821674346924 \tStep Time:  0.006978511810302734 s \tTotal Time:  16.034050703048706 s \n",
      "\n",
      "\n",
      "\tEpisode 2549 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5083],\n",
      "        [1.0000, 0.4448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5578],\n",
      "        [1.0000, 0.4209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484268367290497 \tStep Time:  0.006949186325073242 s \tTotal Time:  16.04099988937378 s \n",
      "\n",
      "\n",
      "\tEpisode 2550 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4878],\n",
      "        [1.0000, 0.4804]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528787612915039 \tStep Time:  0.005984783172607422 s \tTotal Time:  16.046984672546387 s \n",
      "\n",
      "\n",
      "\tEpisode 2551 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5327],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6295],\n",
      "        [1.0000, 0.5942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424800395965576 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.05396580696106 s \n",
      "\n",
      "\n",
      "\tEpisode 2552 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4824],\n",
      "        [1.0000, 0.3947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5906],\n",
      "        [1.0000, 0.4484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443116664886475 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.05994987487793 s \n",
      "\n",
      "\n",
      "\tEpisode 2553 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6049],\n",
      "        [1.0000, 0.4445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3658],\n",
      "        [1.0000, 0.4571]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.409206867218018 \tStep Time:  0.00698089599609375 s \tTotal Time:  16.066930770874023 s \n",
      "\n",
      "\n",
      "\tEpisode 2554 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5297],\n",
      "        [1.0000, 0.2699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4349],\n",
      "        [1.0000, 0.6201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.329339981079102 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.072914838790894 s \n",
      "\n",
      "\n",
      "\tEpisode 2555 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6163],\n",
      "        [1.0000, 0.6478]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6210],\n",
      "        [1.0000, 0.5872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554745435714722 \tStep Time:  0.007015228271484375 s \tTotal Time:  16.079930067062378 s \n",
      "\n",
      "\n",
      "\tEpisode 2556 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4307],\n",
      "        [1.0000, 0.4122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4490],\n",
      "        [1.0000, 0.6551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.625082612037659 \tStep Time:  0.006947517395019531 s \tTotal Time:  16.086877584457397 s \n",
      "\n",
      "\n",
      "\tEpisode 2557 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3586],\n",
      "        [1.0000, 0.4517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4803],\n",
      "        [1.0000, 0.4177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552935600280762 \tStep Time:  0.0070133209228515625 s \tTotal Time:  16.09389090538025 s \n",
      "\n",
      "\n",
      "\tEpisode 2558 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4195],\n",
      "        [1.0000, 0.5419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5454],\n",
      "        [1.0000, 0.3278]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.714752197265625 \tStep Time:  0.006986141204833984 s \tTotal Time:  16.100877046585083 s \n",
      "\n",
      "\n",
      "\tEpisode 2559 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4504],\n",
      "        [1.0000, 0.3940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4383],\n",
      "        [1.0000, 0.4230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538394927978516 \tStep Time:  0.005984783172607422 s \tTotal Time:  16.10686182975769 s \n",
      "\n",
      "\n",
      "\tEpisode 2560 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7290],\n",
      "        [1.0000, 0.4819]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4886],\n",
      "        [1.0000, 0.4519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.410639762878418 \tStep Time:  0.005952358245849609 s \tTotal Time:  16.113805770874023 s \n",
      "\n",
      "\n",
      "\tEpisode 2561 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5957],\n",
      "        [1.0000, 0.7707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4026],\n",
      "        [1.0000, 0.6823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63241982460022 \tStep Time:  0.007012844085693359 s \tTotal Time:  16.120818614959717 s \n",
      "\n",
      "\n",
      "\tEpisode 2562 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4721],\n",
      "        [1.0000, 0.7363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.4518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.412280559539795 \tStep Time:  0.006983757019042969 s \tTotal Time:  16.12780237197876 s \n",
      "\n",
      "\n",
      "\tEpisode 2563 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4564],\n",
      "        [1.0000, 0.4811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5543],\n",
      "        [1.0000, 0.8170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.688854694366455 \tStep Time:  0.006978750228881836 s \tTotal Time:  16.13478112220764 s \n",
      "\n",
      "\n",
      "\tEpisode 2564 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6864],\n",
      "        [1.0000, 0.3182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5131],\n",
      "        [1.0000, 0.5974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.412887513637543 \tStep Time:  0.006983041763305664 s \tTotal Time:  16.141764163970947 s \n",
      "\n",
      "\n",
      "\tEpisode 2565 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4497],\n",
      "        [1.0000, 0.4382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4489],\n",
      "        [1.0000, 0.5811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474127769470215 \tStep Time:  0.0059506893157958984 s \tTotal Time:  16.147714853286743 s \n",
      "\n",
      "\n",
      "\tEpisode 2566 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.6098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3879],\n",
      "        [1.0000, 0.5861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.404574632644653 \tStep Time:  0.006981849670410156 s \tTotal Time:  16.154696702957153 s \n",
      "\n",
      "\n",
      "\tEpisode 2567 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4568],\n",
      "        [1.0000, 0.5527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7374],\n",
      "        [1.0000, 0.4829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474543690681458 \tStep Time:  0.00698089599609375 s \tTotal Time:  16.161677598953247 s \n",
      "\n",
      "\n",
      "\tEpisode 2568 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5500],\n",
      "        [1.0000, 0.5425]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4225],\n",
      "        [1.0000, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.604361534118652 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.16766119003296 s \n",
      "\n",
      "\n",
      "\tEpisode 2569 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3963],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4448],\n",
      "        [1.0000, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515267848968506 \tStep Time:  0.005985736846923828 s \tTotal Time:  16.173646926879883 s \n",
      "\n",
      "\n",
      "\tEpisode 2570 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4158],\n",
      "        [1.0000, 0.4477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4466],\n",
      "        [1.0000, 0.4043]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575168132781982 \tStep Time:  0.005982398986816406 s \tTotal Time:  16.1796293258667 s \n",
      "\n",
      "\n",
      "\tEpisode 2571 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5307],\n",
      "        [1.0000, 0.4626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5329],\n",
      "        [1.0000, 0.4497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55078399181366 \tStep Time:  0.00598597526550293 s \tTotal Time:  16.185615301132202 s \n",
      "\n",
      "\n",
      "\tEpisode 2572 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6323],\n",
      "        [1.0000, 0.5502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4519],\n",
      "        [1.0000, 0.5525]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.616920411586761 \tStep Time:  0.004987478256225586 s \tTotal Time:  16.191598415374756 s \n",
      "\n",
      "\n",
      "\tEpisode 2573 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4548],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2915],\n",
      "        [1.0000, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43575382232666 \tStep Time:  0.005982398986816406 s \tTotal Time:  16.197580814361572 s \n",
      "\n",
      "\n",
      "\tEpisode 2574 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4598],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.3567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.647058486938477 \tStep Time:  0.005987405776977539 s \tTotal Time:  16.20356822013855 s \n",
      "\n",
      "\n",
      "\tEpisode 2575 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5566],\n",
      "        [1.0000, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5177],\n",
      "        [1.0000, 0.4652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527591049671173 \tStep Time:  0.005984306335449219 s \tTotal Time:  16.209552526474 s \n",
      "\n",
      "\n",
      "\tEpisode 2576 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4495],\n",
      "        [1.0000, 0.4696]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4491],\n",
      "        [1.0000, 0.4694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543856859207153 \tStep Time:  0.0063877105712890625 s \tTotal Time:  16.215940237045288 s \n",
      "\n",
      "\n",
      "\tEpisode 2577 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4618],\n",
      "        [1.0000, 0.4635]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4577],\n",
      "        [1.0000, 0.5583]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490828037261963 \tStep Time:  0.00609135627746582 s \tTotal Time:  16.222031593322754 s \n",
      "\n",
      "\n",
      "\tEpisode 2578 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5537],\n",
      "        [1.0000, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4443],\n",
      "        [1.0000, 0.5337]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508781671524048 \tStep Time:  0.005986690521240234 s \tTotal Time:  16.228018283843994 s \n",
      "\n",
      "\n",
      "\tEpisode 2579 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5939],\n",
      "        [1.0000, 0.5896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6198],\n",
      "        [1.0000, 0.6165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566871643066406 \tStep Time:  0.006983280181884766 s \tTotal Time:  16.23500156402588 s \n",
      "\n",
      "\n",
      "\tEpisode 2580 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4494],\n",
      "        [1.0000, 0.5227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4258],\n",
      "        [1.0000, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550457000732422 \tStep Time:  0.006979465484619141 s \tTotal Time:  16.241981029510498 s \n",
      "\n",
      "\n",
      "\tEpisode 2581 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4281],\n",
      "        [1.0000, 0.6155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4294],\n",
      "        [1.0000, 0.4535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419219851493835 \tStep Time:  0.005985260009765625 s \tTotal Time:  16.247966289520264 s \n",
      "\n",
      "\n",
      "\tEpisode 2582 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4201],\n",
      "        [1.0000, 0.4005]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4082],\n",
      "        [1.0000, 0.5787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.607876300811768 \tStep Time:  0.006979942321777344 s \tTotal Time:  16.25494623184204 s \n",
      "\n",
      "\n",
      "\tEpisode 2583 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4480],\n",
      "        [1.0000, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4316],\n",
      "        [1.0000, 0.5333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554941654205322 \tStep Time:  0.005984306335449219 s \tTotal Time:  16.26093053817749 s \n",
      "\n",
      "\n",
      "\tEpisode 2584 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6006],\n",
      "        [1.0000, 0.4176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.4333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.381984233856201 \tStep Time:  0.010010719299316406 s \tTotal Time:  16.271939516067505 s \n",
      "\n",
      "\n",
      "\tEpisode 2585 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4699],\n",
      "        [1.0000, 0.6298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4358],\n",
      "        [1.0000, 0.5413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.395306587219238 \tStep Time:  0.005945444107055664 s \tTotal Time:  16.27788496017456 s \n",
      "\n",
      "\n",
      "\tEpisode 2586 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3901],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5586],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433285236358643 \tStep Time:  0.006982088088989258 s \tTotal Time:  16.28486704826355 s \n",
      "\n",
      "\n",
      "\tEpisode 2587 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5481],\n",
      "        [1.0000, 0.5732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5411],\n",
      "        [1.0000, 0.4369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474400043487549 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.29085063934326 s \n",
      "\n",
      "\n",
      "\tEpisode 2588 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4049],\n",
      "        [1.0000, 0.3340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3790],\n",
      "        [1.0000, 0.4540]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580379009246826 \tStep Time:  0.005983114242553711 s \tTotal Time:  16.296833753585815 s \n",
      "\n",
      "\n",
      "\tEpisode 2589 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4027],\n",
      "        [1.0000, 0.4364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.4355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499210834503174 \tStep Time:  0.005987405776977539 s \tTotal Time:  16.302821159362793 s \n",
      "\n",
      "\n",
      "\tEpisode 2590 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4185],\n",
      "        [1.0000, 0.4147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5723],\n",
      "        [1.0000, 0.6861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476519525051117 \tStep Time:  0.004986286163330078 s \tTotal Time:  16.307807445526123 s \n",
      "\n",
      "\n",
      "\tEpisode 2591 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5294],\n",
      "        [1.0000, 0.4387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5541],\n",
      "        [1.0000, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542202293872833 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.313791275024414 s \n",
      "\n",
      "\n",
      "\tEpisode 2592 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5525],\n",
      "        [1.0000, 0.2971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4609],\n",
      "        [1.0000, 0.7057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.306548297405243 \tStep Time:  0.005984783172607422 s \tTotal Time:  16.31977605819702 s \n",
      "\n",
      "\n",
      "\tEpisode 2593 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6170],\n",
      "        [1.0000, 0.4631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5727],\n",
      "        [1.0000, 0.6158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588316917419434 \tStep Time:  0.0050201416015625 s \tTotal Time:  16.324796199798584 s \n",
      "\n",
      "\n",
      "\tEpisode 2594 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3207],\n",
      "        [1.0000, 0.5717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4218],\n",
      "        [1.0000, 0.4324]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554981708526611 \tStep Time:  0.00498652458190918 s \tTotal Time:  16.329782724380493 s \n",
      "\n",
      "\n",
      "\tEpisode 2595 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3450],\n",
      "        [1.0000, 0.5483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3597],\n",
      "        [1.0000, 0.4001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481223583221436 \tStep Time:  0.006948709487915039 s \tTotal Time:  16.336731433868408 s \n",
      "\n",
      "\n",
      "\tEpisode 2596 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4209],\n",
      "        [1.0000, 0.6336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4278],\n",
      "        [1.0000, 0.3816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.628115892410278 \tStep Time:  0.0060138702392578125 s \tTotal Time:  16.342745304107666 s \n",
      "\n",
      "\n",
      "\tEpisode 2597 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6510],\n",
      "        [1.0000, 0.4567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4356],\n",
      "        [1.0000, 0.3484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.691162586212158 \tStep Time:  0.0049555301666259766 s \tTotal Time:  16.347700834274292 s \n",
      "\n",
      "\n",
      "\tEpisode 2598 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5139],\n",
      "        [1.0000, 0.5965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.4568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.590526103973389 \tStep Time:  0.006018161773681641 s \tTotal Time:  16.353718996047974 s \n",
      "\n",
      "\n",
      "\tEpisode 2599 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3451],\n",
      "        [1.0000, 0.4533]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3993],\n",
      "        [1.0000, 0.6736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.364214897155762 \tStep Time:  0.005984783172607422 s \tTotal Time:  16.35970377922058 s \n",
      "\n",
      "\n",
      "\tEpisode 2600 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7644],\n",
      "        [1.0000, 0.7537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3887],\n",
      "        [1.0000, 0.5971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492504596710205 \tStep Time:  0.005981922149658203 s \tTotal Time:  16.36568570137024 s \n",
      "\n",
      "\n",
      "\tEpisode 2601 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4787],\n",
      "        [1.0000, 0.7112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5221],\n",
      "        [1.0000, 0.3672]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516611993312836 \tStep Time:  0.004954814910888672 s \tTotal Time:  16.370640516281128 s \n",
      "\n",
      "\n",
      "\tEpisode 2602 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4928],\n",
      "        [1.0000, 0.4145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4658],\n",
      "        [1.0000, 0.7376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456992447376251 \tStep Time:  0.006015300750732422 s \tTotal Time:  16.37665581703186 s \n",
      "\n",
      "\n",
      "\tEpisode 2603 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4417],\n",
      "        [1.0000, 0.4232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4005],\n",
      "        [1.0000, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523910999298096 \tStep Time:  0.0049555301666259766 s \tTotal Time:  16.381611347198486 s \n",
      "\n",
      "\n",
      "\tEpisode 2604 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4102],\n",
      "        [1.0000, 0.7059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6886],\n",
      "        [1.0000, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493918657302856 \tStep Time:  0.006014823913574219 s \tTotal Time:  16.38762617111206 s \n",
      "\n",
      "\n",
      "\tEpisode 2605 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7121],\n",
      "        [1.0000, 0.4953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7057],\n",
      "        [1.0000, 0.4078]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610004186630249 \tStep Time:  0.005985736846923828 s \tTotal Time:  16.393611907958984 s \n",
      "\n",
      "\n",
      "\tEpisode 2606 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2701],\n",
      "        [1.0000, 0.4581]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4273],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505897521972656 \tStep Time:  0.004987478256225586 s \tTotal Time:  16.39859938621521 s \n",
      "\n",
      "\n",
      "\tEpisode 2607 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4221],\n",
      "        [1.0000, 0.5554]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3687],\n",
      "        [1.0000, 0.6288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481374144554138 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.404582977294922 s \n",
      "\n",
      "\n",
      "\tEpisode 2608 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5563],\n",
      "        [1.0000, 0.4860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5164],\n",
      "        [1.0000, 0.4250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467049598693848 \tStep Time:  0.00498652458190918 s \tTotal Time:  16.40956950187683 s \n",
      "\n",
      "\n",
      "\tEpisode 2609 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6831],\n",
      "        [1.0000, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5811],\n",
      "        [1.0000, 0.6959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520278453826904 \tStep Time:  0.005985260009765625 s \tTotal Time:  16.415554761886597 s \n",
      "\n",
      "\n",
      "\tEpisode 2610 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3158],\n",
      "        [1.0000, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4586],\n",
      "        [1.0000, 0.6124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548461377620697 \tStep Time:  0.005980730056762695 s \tTotal Time:  16.42153549194336 s \n",
      "\n",
      "\n",
      "\tEpisode 2611 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5191],\n",
      "        [1.0000, 0.4288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3361],\n",
      "        [1.0000, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.663722515106201 \tStep Time:  0.005952358245849609 s \tTotal Time:  16.42748785018921 s \n",
      "\n",
      "\n",
      "\tEpisode 2612 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4429],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509028851985931 \tStep Time:  0.006981372833251953 s \tTotal Time:  16.43446922302246 s \n",
      "\n",
      "\n",
      "\tEpisode 2613 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6096],\n",
      "        [1.0000, 0.4612]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5557],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502882480621338 \tStep Time:  0.006981849670410156 s \tTotal Time:  16.44145107269287 s \n",
      "\n",
      "\n",
      "\tEpisode 2614 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6582],\n",
      "        [1.0000, 0.6423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5246],\n",
      "        [1.0000, 0.5715]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.642326354980469 \tStep Time:  0.006981372833251953 s \tTotal Time:  16.448432445526123 s \n",
      "\n",
      "\n",
      "\tEpisode 2615 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5593],\n",
      "        [1.0000, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4816],\n",
      "        [1.0000, 0.5035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561620235443115 \tStep Time:  0.005983114242553711 s \tTotal Time:  16.454415559768677 s \n",
      "\n",
      "\n",
      "\tEpisode 2616 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5979],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5120],\n",
      "        [1.0000, 0.4778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535924792289734 \tStep Time:  0.005985260009765625 s \tTotal Time:  16.460400819778442 s \n",
      "\n",
      "\n",
      "\tEpisode 2617 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5018],\n",
      "        [1.0000, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5441],\n",
      "        [1.0000, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484584331512451 \tStep Time:  0.006980419158935547 s \tTotal Time:  16.467381238937378 s \n",
      "\n",
      "\n",
      "\tEpisode 2618 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4783],\n",
      "        [1.0000, 0.4953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4951],\n",
      "        [1.0000, 0.4758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509450435638428 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.47336506843567 s \n",
      "\n",
      "\n",
      "\tEpisode 2619 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5416],\n",
      "        [1.0000, 0.5321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.5290]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48662281036377 \tStep Time:  0.006981611251831055 s \tTotal Time:  16.4803466796875 s \n",
      "\n",
      "\n",
      "\tEpisode 2620 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4865],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48814707994461 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.487327814102173 s \n",
      "\n",
      "\n",
      "\tEpisode 2621 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5054],\n",
      "        [1.0000, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.4823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508876383304596 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.493311643600464 s \n",
      "\n",
      "\n",
      "\tEpisode 2622 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5136],\n",
      "        [1.0000, 0.5558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4955],\n",
      "        [1.0000, 0.5449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555108070373535 \tStep Time:  0.0065457820892333984 s \tTotal Time:  16.499857425689697 s \n",
      "\n",
      "\n",
      "\tEpisode 2623 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4816],\n",
      "        [1.0000, 0.4978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4727],\n",
      "        [1.0000, 0.5358]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544443905353546 \tStep Time:  0.005422353744506836 s \tTotal Time:  16.505279779434204 s \n",
      "\n",
      "\n",
      "\tEpisode 2624 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.4714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5665],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551923274993896 \tStep Time:  0.005984306335449219 s \tTotal Time:  16.511264085769653 s \n",
      "\n",
      "\n",
      "\tEpisode 2625 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4921],\n",
      "        [1.0000, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4744],\n",
      "        [1.0000, 0.5320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4798264503479 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.517247676849365 s \n",
      "\n",
      "\n",
      "\tEpisode 2626 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4728],\n",
      "        [1.0000, 0.5170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4773],\n",
      "        [1.0000, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501516342163086 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.523231744766235 s \n",
      "\n",
      "\n",
      "\tEpisode 2627 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4854],\n",
      "        [1.0000, 0.4812]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4887],\n",
      "        [1.0000, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507947444915771 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.529215812683105 s \n",
      "\n",
      "\n",
      "\tEpisode 2628 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.4851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5574],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478055953979492 \tStep Time:  0.006981372833251953 s \tTotal Time:  16.536197185516357 s \n",
      "\n",
      "\n",
      "\tEpisode 2629 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3991],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4801],\n",
      "        [1.0000, 0.4725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48015022277832 \tStep Time:  0.004986286163330078 s \tTotal Time:  16.541183471679688 s \n",
      "\n",
      "\n",
      "\tEpisode 2630 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.5880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5197],\n",
      "        [1.0000, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541921615600586 \tStep Time:  0.00498652458190918 s \tTotal Time:  16.547167539596558 s \n",
      "\n",
      "\n",
      "\tEpisode 2631 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4668],\n",
      "        [1.0000, 0.4628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4587],\n",
      "        [1.0000, 0.5727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580372750759125 \tStep Time:  0.006981611251831055 s \tTotal Time:  16.55414915084839 s \n",
      "\n",
      "\n",
      "\tEpisode 2632 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5276],\n",
      "        [1.0000, 0.5245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5758],\n",
      "        [1.0000, 0.6189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546908378601074 \tStep Time:  0.005984306335449219 s \tTotal Time:  16.560133457183838 s \n",
      "\n",
      "\n",
      "\tEpisode 2633 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6574],\n",
      "        [1.0000, 0.4458]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5626],\n",
      "        [1.0000, 0.4882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.665014743804932 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.56611704826355 s \n",
      "\n",
      "\n",
      "\tEpisode 2634 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5512],\n",
      "        [1.0000, 0.4675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4499],\n",
      "        [1.0000, 0.4647]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564206600189209 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.57210111618042 s \n",
      "\n",
      "\n",
      "\tEpisode 2635 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5944],\n",
      "        [1.0000, 0.5302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4999],\n",
      "        [1.0000, 0.4574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521507263183594 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.57808518409729 s \n",
      "\n",
      "\n",
      "\tEpisode 2636 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4796],\n",
      "        [1.0000, 0.4590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4581],\n",
      "        [1.0000, 0.4651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522640228271484 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.58406901359558 s \n",
      "\n",
      "\n",
      "\tEpisode 2637 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4517],\n",
      "        [1.0000, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4763],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509815692901611 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.591050148010254 s \n",
      "\n",
      "\n",
      "\tEpisode 2638 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4710],\n",
      "        [1.0000, 0.4517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4823],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522350788116455 \tStep Time:  0.004987001419067383 s \tTotal Time:  16.59603714942932 s \n",
      "\n",
      "\n",
      "\tEpisode 2639 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5062],\n",
      "        [1.0000, 0.4590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4561],\n",
      "        [1.0000, 0.4567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484363317489624 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.60202121734619 s \n",
      "\n",
      "\n",
      "\tEpisode 2640 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.4746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4520],\n",
      "        [1.0000, 0.4379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51219254732132 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.609002351760864 s \n",
      "\n",
      "\n",
      "\tEpisode 2641 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4736],\n",
      "        [1.0000, 0.3955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4530],\n",
      "        [1.0000, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552496910095215 \tStep Time:  0.004986763000488281 s \tTotal Time:  16.613989114761353 s \n",
      "\n",
      "\n",
      "\tEpisode 2642 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4616],\n",
      "        [1.0000, 0.5343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4652],\n",
      "        [1.0000, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57330870628357 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.619973182678223 s \n",
      "\n",
      "\n",
      "\tEpisode 2643 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4713],\n",
      "        [1.0000, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4558],\n",
      "        [1.0000, 0.4576]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55013132095337 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.626954317092896 s \n",
      "\n",
      "\n",
      "\tEpisode 2644 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5250],\n",
      "        [1.0000, 0.4687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5253],\n",
      "        [1.0000, 0.4626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509064197540283 \tStep Time:  0.006981611251831055 s \tTotal Time:  16.633935928344727 s \n",
      "\n",
      "\n",
      "\tEpisode 2645 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4635],\n",
      "        [1.0000, 0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4682],\n",
      "        [1.0000, 0.4830]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50994062423706 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.639919757843018 s \n",
      "\n",
      "\n",
      "\tEpisode 2646 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4792],\n",
      "        [1.0000, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4891],\n",
      "        [1.0000, 0.5226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530593872070312 \tStep Time:  0.009973287582397461 s \tTotal Time:  16.649893045425415 s \n",
      "\n",
      "\n",
      "\tEpisode 2647 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5147],\n",
      "        [1.0000, 0.5197]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4956],\n",
      "        [1.0000, 0.4907]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52513074874878 \tStep Time:  0.010970592498779297 s \tTotal Time:  16.660863637924194 s \n",
      "\n",
      "\n",
      "\tEpisode 2648 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5137],\n",
      "        [1.0000, 0.5361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495061874389648 \tStep Time:  0.009974479675292969 s \tTotal Time:  16.670838117599487 s \n",
      "\n",
      "\n",
      "\tEpisode 2649 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4958],\n",
      "        [1.0000, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514416694641113 \tStep Time:  0.008976221084594727 s \tTotal Time:  16.679814338684082 s \n",
      "\n",
      "\n",
      "\tEpisode 2650 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4901],\n",
      "        [1.0000, 0.3973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5067],\n",
      "        [1.0000, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468595027923584 \tStep Time:  0.009972572326660156 s \tTotal Time:  16.689786911010742 s \n",
      "\n",
      "\n",
      "\tEpisode 2651 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5147],\n",
      "        [1.0000, 0.5914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5864],\n",
      "        [1.0000, 0.4972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50252091884613 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.696768045425415 s \n",
      "\n",
      "\n",
      "\tEpisode 2652 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4443],\n",
      "        [1.0000, 0.5565]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446476340293884 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.702751636505127 s \n",
      "\n",
      "\n",
      "\tEpisode 2653 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3702],\n",
      "        [1.0000, 0.5704]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [1.0000, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.404130935668945 \tStep Time:  0.00498652458190918 s \tTotal Time:  16.70873522758484 s \n",
      "\n",
      "\n",
      "\tEpisode 2654 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5090],\n",
      "        [1.0000, 0.5593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6313],\n",
      "        [1.0000, 0.5398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59480619430542 \tStep Time:  0.004987478256225586 s \tTotal Time:  16.713722705841064 s \n",
      "\n",
      "\n",
      "\tEpisode 2655 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5635],\n",
      "        [1.0000, 0.6034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2725],\n",
      "        [1.0000, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425907611846924 \tStep Time:  0.005986213684082031 s \tTotal Time:  16.719708919525146 s \n",
      "\n",
      "\n",
      "\tEpisode 2656 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4448],\n",
      "        [1.0000, 0.5902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2865],\n",
      "        [1.0000, 0.4712]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51352572441101 \tStep Time:  0.004984855651855469 s \tTotal Time:  16.725693702697754 s \n",
      "\n",
      "\n",
      "\tEpisode 2657 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4383],\n",
      "        [1.0000, 0.4703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5342],\n",
      "        [1.0000, 0.4398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444511115550995 \tStep Time:  0.004986763000488281 s \tTotal Time:  16.730680465698242 s \n",
      "\n",
      "\n",
      "\tEpisode 2658 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4042],\n",
      "        [1.0000, 0.3443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6054],\n",
      "        [1.0000, 0.4548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.369196891784668 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.737661600112915 s \n",
      "\n",
      "\n",
      "\tEpisode 2659 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5601],\n",
      "        [1.0000, 0.3062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1587],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.696178436279297 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.743645668029785 s \n",
      "\n",
      "\n",
      "\tEpisode 2660 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5374],\n",
      "        [1.0000, 0.5548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5708],\n",
      "        [1.0000, 0.4241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450088202953339 \tStep Time:  0.005985260009765625 s \tTotal Time:  16.74963092803955 s \n",
      "\n",
      "\n",
      "\tEpisode 2661 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6613],\n",
      "        [1.0000, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5037],\n",
      "        [1.0000, 0.4464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435774326324463 \tStep Time:  0.005983114242553711 s \tTotal Time:  16.755614042282104 s \n",
      "\n",
      "\n",
      "\tEpisode 2662 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6324],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6526],\n",
      "        [1.0000, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.387160778045654 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.761597871780396 s \n",
      "\n",
      "\n",
      "\tEpisode 2663 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5533],\n",
      "        [1.0000, 0.5653]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6717],\n",
      "        [1.0000, 0.4634]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515362739562988 \tStep Time:  0.006981611251831055 s \tTotal Time:  16.768579483032227 s \n",
      "\n",
      "\n",
      "\tEpisode 2664 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5733],\n",
      "        [1.0000, 0.6381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3798],\n",
      "        [1.0000, 0.5778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422642230987549 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.774563312530518 s \n",
      "\n",
      "\n",
      "\tEpisode 2665 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4051],\n",
      "        [1.0000, 0.6307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2316],\n",
      "        [1.0000, 0.6297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66746997833252 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.780547380447388 s \n",
      "\n",
      "\n",
      "\tEpisode 2666 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6265],\n",
      "        [1.0000, 0.3496]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6921],\n",
      "        [1.0000, 0.4133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.615658283233643 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.78653120994568 s \n",
      "\n",
      "\n",
      "\tEpisode 2667 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7504],\n",
      "        [1.0000, 0.6407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4659],\n",
      "        [1.0000, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534639835357666 \tStep Time:  0.005984306335449219 s \tTotal Time:  16.792515516281128 s \n",
      "\n",
      "\n",
      "\tEpisode 2668 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7706],\n",
      "        [1.0000, 0.6269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5315],\n",
      "        [1.0000, 0.3908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.606253385543823 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.798499584197998 s \n",
      "\n",
      "\n",
      "\tEpisode 2669 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3687],\n",
      "        [1.0000, 0.5314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3891],\n",
      "        [1.0000, 0.4024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.636713147163391 \tStep Time:  0.00698089599609375 s \tTotal Time:  16.805480480194092 s \n",
      "\n",
      "\n",
      "\tEpisode 2670 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3893],\n",
      "        [1.0000, 0.6600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4533],\n",
      "        [1.0000, 0.6347]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568640232086182 \tStep Time:  0.004986763000488281 s \tTotal Time:  16.81046724319458 s \n",
      "\n",
      "\n",
      "\tEpisode 2671 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6447],\n",
      "        [1.0000, 0.4269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4711],\n",
      "        [1.0000, 0.3298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42204236984253 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.81645131111145 s \n",
      "\n",
      "\n",
      "\tEpisode 2672 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4435],\n",
      "        [1.0000, 0.4306]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6263],\n",
      "        [1.0000, 0.4289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458737194538116 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.82243537902832 s \n",
      "\n",
      "\n",
      "\tEpisode 2673 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4149],\n",
      "        [1.0000, 0.6345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3651],\n",
      "        [1.0000, 0.6752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60957133769989 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.82841920852661 s \n",
      "\n",
      "\n",
      "\tEpisode 2674 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4757],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6396],\n",
      "        [1.0000, 0.3723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.67642891407013 \tStep Time:  0.00797891616821289 s \tTotal Time:  16.836398124694824 s \n",
      "\n",
      "\n",
      "\tEpisode 2675 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4416],\n",
      "        [1.0000, 0.4405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4331],\n",
      "        [1.0000, 0.3763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522315502166748 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.842381715774536 s \n",
      "\n",
      "\n",
      "\tEpisode 2676 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4472],\n",
      "        [1.0000, 0.4123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4740],\n",
      "        [1.0000, 0.6272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.661274909973145 \tStep Time:  0.0059850215911865234 s \tTotal Time:  16.848366737365723 s \n",
      "\n",
      "\n",
      "\tEpisode 2677 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5795],\n",
      "        [1.0000, 0.4465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4844],\n",
      "        [1.0000, 0.5764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.418530106544495 \tStep Time:  0.006980419158935547 s \tTotal Time:  16.855347156524658 s \n",
      "\n",
      "\n",
      "\tEpisode 2678 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5484],\n",
      "        [1.0000, 0.4457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4648],\n",
      "        [1.0000, 0.5413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44471263885498 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.86133122444153 s \n",
      "\n",
      "\n",
      "\tEpisode 2679 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.4682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4460],\n",
      "        [1.0000, 0.4097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588306427001953 \tStep Time:  0.00598454475402832 s \tTotal Time:  16.867315769195557 s \n",
      "\n",
      "\n",
      "\tEpisode 2680 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4685],\n",
      "        [1.0000, 0.5589]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5296],\n",
      "        [1.0000, 0.5739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483260154724121 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.87429690361023 s \n",
      "\n",
      "\n",
      "\tEpisode 2681 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.5146]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5018],\n",
      "        [1.0000, 0.2866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41449248790741 \tStep Time:  0.00698089599609375 s \tTotal Time:  16.881277799606323 s \n",
      "\n",
      "\n",
      "\tEpisode 2682 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6098],\n",
      "        [1.0000, 0.5891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3584],\n",
      "        [1.0000, 0.4454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.337634086608887 \tStep Time:  0.006981849670410156 s \tTotal Time:  16.888259649276733 s \n",
      "\n",
      "\n",
      "\tEpisode 2683 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3500],\n",
      "        [1.0000, 0.6615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4773],\n",
      "        [1.0000, 0.4663]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.391626358032227 \tStep Time:  0.005983591079711914 s \tTotal Time:  16.894243240356445 s \n",
      "\n",
      "\n",
      "\tEpisode 2684 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4311],\n",
      "        [1.0000, 0.5527]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3421],\n",
      "        [1.0000, 0.4436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.645563125610352 \tStep Time:  0.006982564926147461 s \tTotal Time:  16.901225805282593 s \n",
      "\n",
      "\n",
      "\tEpisode 2685 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6106],\n",
      "        [1.0000, 0.5870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6051],\n",
      "        [1.0000, 0.7080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.607507228851318 \tStep Time:  0.005982637405395508 s \tTotal Time:  16.90720844268799 s \n",
      "\n",
      "\n",
      "\tEpisode 2686 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5434],\n",
      "        [1.0000, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5422],\n",
      "        [1.0000, 0.5873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51659345626831 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.91319251060486 s \n",
      "\n",
      "\n",
      "\tEpisode 2687 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4281],\n",
      "        [1.0000, 0.4418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4202],\n",
      "        [1.0000, 0.4595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537249505519867 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.91917634010315 s \n",
      "\n",
      "\n",
      "\tEpisode 2688 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2223],\n",
      "        [1.0000, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7019],\n",
      "        [1.0000, 0.3176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.744205951690674 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.92516040802002 s \n",
      "\n",
      "\n",
      "\tEpisode 2689 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7291],\n",
      "        [1.0000, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5769],\n",
      "        [1.0000, 0.4409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.427491664886475 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.93114447593689 s \n",
      "\n",
      "\n",
      "\tEpisode 2690 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5675],\n",
      "        [1.0000, 0.3062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6062],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474755644798279 \tStep Time:  0.006981372833251953 s \tTotal Time:  16.93812584877014 s \n",
      "\n",
      "\n",
      "\tEpisode 2691 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1719],\n",
      "        [1.0000, 0.5775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5639],\n",
      "        [1.0000, 0.4650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.72003173828125 \tStep Time:  0.00498652458190918 s \tTotal Time:  16.94311237335205 s \n",
      "\n",
      "\n",
      "\tEpisode 2692 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4554],\n",
      "        [1.0000, 0.7294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3762],\n",
      "        [1.0000, 0.6396]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.585534930229187 \tStep Time:  0.006415605545043945 s \tTotal Time:  16.949527978897095 s \n",
      "\n",
      "\n",
      "\tEpisode 2693 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4472],\n",
      "        [1.0000, 0.5602]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4927],\n",
      "        [1.0000, 0.4424]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507099628448486 \tStep Time:  0.006549835205078125 s \tTotal Time:  16.956077814102173 s \n",
      "\n",
      "\n",
      "\tEpisode 2694 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4834],\n",
      "        [1.0000, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5778],\n",
      "        [1.0000, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459898471832275 \tStep Time:  0.00498652458190918 s \tTotal Time:  16.961064338684082 s \n",
      "\n",
      "\n",
      "\tEpisode 2695 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5619],\n",
      "        [1.0000, 0.5619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5478],\n",
      "        [1.0000, 0.4428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475225448608398 \tStep Time:  0.006981372833251953 s \tTotal Time:  16.968045711517334 s \n",
      "\n",
      "\n",
      "\tEpisode 2696 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4731],\n",
      "        [1.0000, 0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4569],\n",
      "        [1.0000, 0.4391]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55230188369751 \tStep Time:  0.004986763000488281 s \tTotal Time:  16.973032474517822 s \n",
      "\n",
      "\n",
      "\tEpisode 2697 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5914],\n",
      "        [1.0000, 0.4651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5637],\n",
      "        [1.0000, 0.5265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503663063049316 \tStep Time:  0.004987001419067383 s \tTotal Time:  16.97901678085327 s \n",
      "\n",
      "\n",
      "\tEpisode 2698 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5981],\n",
      "        [1.0000, 0.3180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6004],\n",
      "        [1.0000, 0.4786]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.338181018829346 \tStep Time:  0.0069811344146728516 s \tTotal Time:  16.985997915267944 s \n",
      "\n",
      "\n",
      "\tEpisode 2699 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2131],\n",
      "        [1.0000, 0.6027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5248],\n",
      "        [1.0000, 0.4177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.7044997215271 \tStep Time:  0.005984067916870117 s \tTotal Time:  16.991981983184814 s \n",
      "\n",
      "\n",
      "\tEpisode 2700 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5712],\n",
      "        [1.0000, 0.5275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4584],\n",
      "        [1.0000, 0.5627]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561171054840088 \tStep Time:  0.005983829498291016 s \tTotal Time:  16.997965812683105 s \n",
      "\n",
      "\n",
      "\tEpisode 2701 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5154],\n",
      "        [1.0000, 0.6774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5600],\n",
      "        [1.0000, 0.2863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.333967208862305 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.003949880599976 s \n",
      "\n",
      "\n",
      "\tEpisode 2702 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1573],\n",
      "        [1.0000, 0.4615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5391],\n",
      "        [1.0000, 0.4104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.410502433776855 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.009933710098267 s \n",
      "\n",
      "\n",
      "\tEpisode 2703 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4031],\n",
      "        [1.0000, 0.6441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6433],\n",
      "        [1.0000, 0.4539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512761116027832 \tStep Time:  0.005984306335449219 s \tTotal Time:  17.015918016433716 s \n",
      "\n",
      "\n",
      "\tEpisode 2704 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4523],\n",
      "        [1.0000, 0.6191]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3340],\n",
      "        [1.0000, 0.5364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.349453926086426 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.021901845932007 s \n",
      "\n",
      "\n",
      "\tEpisode 2705 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6550],\n",
      "        [1.0000, 0.5740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4258],\n",
      "        [1.0000, 0.6287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5988050699234 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.027885913848877 s \n",
      "\n",
      "\n",
      "\tEpisode 2706 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5220],\n",
      "        [1.0000, 0.5308]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4473],\n",
      "        [1.0000, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54233592748642 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.033869981765747 s \n",
      "\n",
      "\n",
      "\tEpisode 2707 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6239],\n",
      "        [1.0000, 0.4517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6012],\n",
      "        [1.0000, 0.6400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62335729598999 \tStep Time:  0.0069811344146728516 s \tTotal Time:  17.04085111618042 s \n",
      "\n",
      "\n",
      "\tEpisode 2708 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6768],\n",
      "        [1.0000, 0.3179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5158],\n",
      "        [1.0000, 0.5929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.771502494812012 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.04683518409729 s \n",
      "\n",
      "\n",
      "\tEpisode 2709 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4325],\n",
      "        [1.0000, 0.5840]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4314],\n",
      "        [1.0000, 0.4252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46353554725647 \tStep Time:  0.005985736846923828 s \tTotal Time:  17.052820920944214 s \n",
      "\n",
      "\n",
      "\tEpisode 2710 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4270],\n",
      "        [1.0000, 0.4590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6729],\n",
      "        [1.0000, 0.4972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61668062210083 \tStep Time:  0.006979703903198242 s \tTotal Time:  17.059800624847412 s \n",
      "\n",
      "\n",
      "\tEpisode 2711 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4640],\n",
      "        [1.0000, 0.6086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4475],\n",
      "        [1.0000, 0.4941]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577536582946777 \tStep Time:  0.00598454475402832 s \tTotal Time:  17.06578516960144 s \n",
      "\n",
      "\n",
      "\tEpisode 2712 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3903],\n",
      "        [1.0000, 0.4547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5289],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555444717407227 \tStep Time:  0.0069806575775146484 s \tTotal Time:  17.072765827178955 s \n",
      "\n",
      "\n",
      "\tEpisode 2713 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.4582]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3757],\n",
      "        [1.0000, 0.2825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416666507720947 \tStep Time:  0.00598454475402832 s \tTotal Time:  17.078750371932983 s \n",
      "\n",
      "\n",
      "\tEpisode 2714 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4658],\n",
      "        [1.0000, 0.3693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.5026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595729351043701 \tStep Time:  0.009974002838134766 s \tTotal Time:  17.088724374771118 s \n",
      "\n",
      "\n",
      "\tEpisode 2715 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4739],\n",
      "        [1.0000, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4750],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536845207214355 \tStep Time:  0.005982637405395508 s \tTotal Time:  17.094707012176514 s \n",
      "\n",
      "\n",
      "\tEpisode 2716 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4807],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4690],\n",
      "        [1.0000, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511084079742432 \tStep Time:  0.0059986114501953125 s \tTotal Time:  17.10070562362671 s \n",
      "\n",
      "\n",
      "\tEpisode 2717 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4693],\n",
      "        [1.0000, 0.4647]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4763],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50317531824112 \tStep Time:  0.005970954895019531 s \tTotal Time:  17.10667657852173 s \n",
      "\n",
      "\n",
      "\tEpisode 2718 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4648],\n",
      "        [1.0000, 0.4834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5336],\n",
      "        [1.0000, 0.4961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536981582641602 \tStep Time:  0.005982637405395508 s \tTotal Time:  17.112659215927124 s \n",
      "\n",
      "\n",
      "\tEpisode 2719 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4703],\n",
      "        [1.0000, 0.4172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5417],\n",
      "        [1.0000, 0.4524]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582659721374512 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.118643283843994 s \n",
      "\n",
      "\n",
      "\tEpisode 2720 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.4658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3815],\n",
      "        [1.0000, 0.5486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504951477050781 \tStep Time:  0.005984783172607422 s \tTotal Time:  17.1246280670166 s \n",
      "\n",
      "\n",
      "\tEpisode 2721 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5372],\n",
      "        [1.0000, 0.4667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.4720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489748477935791 \tStep Time:  0.005982875823974609 s \tTotal Time:  17.130610942840576 s \n",
      "\n",
      "\n",
      "\tEpisode 2722 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5569],\n",
      "        [1.0000, 0.5181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4666],\n",
      "        [1.0000, 0.5714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437106132507324 \tStep Time:  0.005987882614135742 s \tTotal Time:  17.136598825454712 s \n",
      "\n",
      "\n",
      "\tEpisode 2723 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5452],\n",
      "        [1.0000, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4682],\n",
      "        [1.0000, 0.5562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416556715965271 \tStep Time:  0.006020307540893555 s \tTotal Time:  17.142619132995605 s \n",
      "\n",
      "\n",
      "\tEpisode 2724 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [1.0000, 0.5618]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5199],\n",
      "        [1.0000, 0.4418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492372334003448 \tStep Time:  0.005982637405395508 s \tTotal Time:  17.148601770401 s \n",
      "\n",
      "\n",
      "\tEpisode 2725 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.3108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4453],\n",
      "        [1.0000, 0.4410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440211772918701 \tStep Time:  0.006052255630493164 s \tTotal Time:  17.154654026031494 s \n",
      "\n",
      "\n",
      "\tEpisode 2726 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5809],\n",
      "        [1.0000, 0.4125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4287],\n",
      "        [1.0000, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546495914459229 \tStep Time:  0.005020618438720703 s \tTotal Time:  17.159674644470215 s \n",
      "\n",
      "\n",
      "\tEpisode 2727 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3632],\n",
      "        [1.0000, 0.4943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4777],\n",
      "        [1.0000, 0.4938]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458500862121582 \tStep Time:  0.006775617599487305 s \tTotal Time:  17.166450262069702 s \n",
      "\n",
      "\n",
      "\tEpisode 2728 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4427],\n",
      "        [1.0000, 0.3368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4601],\n",
      "        [1.0000, 0.4574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578653335571289 \tStep Time:  0.0050203800201416016 s \tTotal Time:  17.171644926071167 s \n",
      "\n",
      "\n",
      "\tEpisode 2729 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6160],\n",
      "        [1.0000, 0.6595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5372],\n",
      "        [1.0000, 0.5529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436878204345703 \tStep Time:  0.005982160568237305 s \tTotal Time:  17.177627086639404 s \n",
      "\n",
      "\n",
      "\tEpisode 2730 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5520],\n",
      "        [1.0000, 0.3314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.5984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4137544631958 \tStep Time:  0.005953073501586914 s \tTotal Time:  17.18358016014099 s \n",
      "\n",
      "\n",
      "\tEpisode 2731 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4176],\n",
      "        [1.0000, 0.6537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4128],\n",
      "        [1.0000, 0.5583]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478273391723633 \tStep Time:  0.00601506233215332 s \tTotal Time:  17.189595222473145 s \n",
      "\n",
      "\n",
      "\tEpisode 2732 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6460],\n",
      "        [1.0000, 0.6236]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3622],\n",
      "        [1.0000, 0.6515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39714765548706 \tStep Time:  0.00598597526550293 s \tTotal Time:  17.195581197738647 s \n",
      "\n",
      "\n",
      "\tEpisode 2733 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6801],\n",
      "        [1.0000, 0.5199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6875],\n",
      "        [1.0000, 0.6981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.663792133331299 \tStep Time:  0.0059812068939208984 s \tTotal Time:  17.20156240463257 s \n",
      "\n",
      "\n",
      "\tEpisode 2734 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6810],\n",
      "        [1.0000, 0.3587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4580],\n",
      "        [1.0000, 0.3585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.340112686157227 \tStep Time:  0.005984306335449219 s \tTotal Time:  17.207546710968018 s \n",
      "\n",
      "\n",
      "\tEpisode 2735 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5429],\n",
      "        [1.0000, 0.3776]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5878],\n",
      "        [1.0000, 0.6876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.676608562469482 \tStep Time:  0.005984783172607422 s \tTotal Time:  17.213531494140625 s \n",
      "\n",
      "\n",
      "\tEpisode 2736 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6719],\n",
      "        [1.0000, 0.5561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6096],\n",
      "        [1.0000, 0.4458]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.401837885379791 \tStep Time:  0.0059854984283447266 s \tTotal Time:  17.21951699256897 s \n",
      "\n",
      "\n",
      "\tEpisode 2737 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5031],\n",
      "        [1.0000, 0.6444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4230],\n",
      "        [1.0000, 0.3468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432287693023682 \tStep Time:  0.0059816837310791016 s \tTotal Time:  17.22549867630005 s \n",
      "\n",
      "\n",
      "\tEpisode 2738 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4917],\n",
      "        [1.0000, 0.6291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4538],\n",
      "        [1.0000, 0.6607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533889770507812 \tStep Time:  0.0049893856048583984 s \tTotal Time:  17.230488061904907 s \n",
      "\n",
      "\n",
      "\tEpisode 2739 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3866],\n",
      "        [1.0000, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3359],\n",
      "        [1.0000, 0.6339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.343120813369751 \tStep Time:  0.007943868637084961 s \tTotal Time:  17.238431930541992 s \n",
      "\n",
      "\n",
      "\tEpisode 2740 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6554],\n",
      "        [1.0000, 0.4273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6227],\n",
      "        [1.0000, 0.2374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.660077154636383 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.244415998458862 s \n",
      "\n",
      "\n",
      "\tEpisode 2741 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6248],\n",
      "        [1.0000, 0.6441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5719],\n",
      "        [1.0000, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446162700653076 \tStep Time:  0.004986763000488281 s \tTotal Time:  17.24940276145935 s \n",
      "\n",
      "\n",
      "\tEpisode 2742 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4341],\n",
      "        [1.0000, 0.3162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6082],\n",
      "        [1.0000, 0.4739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558457851409912 \tStep Time:  0.007978677749633789 s \tTotal Time:  17.257381439208984 s \n",
      "\n",
      "\n",
      "\tEpisode 2743 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3843],\n",
      "        [1.0000, 0.4859]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4565],\n",
      "        [1.0000, 0.5624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428942918777466 \tStep Time:  0.00797891616821289 s \tTotal Time:  17.265360355377197 s \n",
      "\n",
      "\n",
      "\tEpisode 2744 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5630],\n",
      "        [1.0000, 0.6214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2780],\n",
      "        [1.0000, 0.6235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429431438446045 \tStep Time:  0.0069811344146728516 s \tTotal Time:  17.27234148979187 s \n",
      "\n",
      "\n",
      "\tEpisode 2745 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3630],\n",
      "        [1.0000, 0.5967]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5732],\n",
      "        [1.0000, 0.4393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507784366607666 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.27832555770874 s \n",
      "\n",
      "\n",
      "\tEpisode 2746 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6132],\n",
      "        [1.0000, 0.4359]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3908],\n",
      "        [1.0000, 0.3632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63336992263794 \tStep Time:  0.0079803466796875 s \tTotal Time:  17.286305904388428 s \n",
      "\n",
      "\n",
      "\tEpisode 2747 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5814],\n",
      "        [1.0000, 0.1217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3507],\n",
      "        [1.0000, 0.4220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.415535926818848 \tStep Time:  0.006979465484619141 s \tTotal Time:  17.293285369873047 s \n",
      "\n",
      "\n",
      "\tEpisode 2748 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4107],\n",
      "        [1.0000, 0.4141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3117],\n",
      "        [1.0000, 0.2795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582327604293823 \tStep Time:  0.00698542594909668 s \tTotal Time:  17.300270795822144 s \n",
      "\n",
      "\n",
      "\tEpisode 2749 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5730],\n",
      "        [1.0000, 0.4023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2193],\n",
      "        [1.0000, 0.6175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463806748390198 \tStep Time:  0.005980491638183594 s \tTotal Time:  17.306251287460327 s \n",
      "\n",
      "\n",
      "\tEpisode 2750 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6064],\n",
      "        [1.0000, 0.5631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5954],\n",
      "        [1.0000, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576923847198486 \tStep Time:  0.0059833526611328125 s \tTotal Time:  17.31223464012146 s \n",
      "\n",
      "\n",
      "\tEpisode 2751 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6100],\n",
      "        [1.0000, 0.3447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4966],\n",
      "        [1.0000, 0.4198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567169666290283 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.31821870803833 s \n",
      "\n",
      "\n",
      "\tEpisode 2752 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3128],\n",
      "        [1.0000, 0.4058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.6021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.37064504623413 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.3242027759552 s \n",
      "\n",
      "\n",
      "\tEpisode 2753 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.3806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2523],\n",
      "        [1.0000, 0.5593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612131118774414 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.33018684387207 s \n",
      "\n",
      "\n",
      "\tEpisode 2754 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5440],\n",
      "        [1.0000, 0.3406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6162],\n",
      "        [1.0000, 0.4343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.35250473022461 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.337168216705322 s \n",
      "\n",
      "\n",
      "\tEpisode 2755 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4973],\n",
      "        [1.0000, 0.5897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4739],\n",
      "        [1.0000, 0.4427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56798142194748 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.343152046203613 s \n",
      "\n",
      "\n",
      "\tEpisode 2756 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4124],\n",
      "        [1.0000, 0.6340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3605],\n",
      "        [1.0000, 0.4499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480598151683807 \tStep Time:  0.004986763000488281 s \tTotal Time:  17.3481388092041 s \n",
      "\n",
      "\n",
      "\tEpisode 2757 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4807],\n",
      "        [1.0000, 0.5202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6264],\n",
      "        [1.0000, 0.4444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.652429521083832 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.354122638702393 s \n",
      "\n",
      "\n",
      "\tEpisode 2758 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4091],\n",
      "        [1.0000, 0.4499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6430],\n",
      "        [1.0000, 0.3131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.37054181098938 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.361104011535645 s \n",
      "\n",
      "\n",
      "\tEpisode 2759 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3726],\n",
      "        [1.0000, 0.1785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4776],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.681561946868896 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.367088079452515 s \n",
      "\n",
      "\n",
      "\tEpisode 2760 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6569],\n",
      "        [1.0000, 0.6675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4822],\n",
      "        [1.0000, 0.5491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507116317749023 \tStep Time:  0.004986763000488281 s \tTotal Time:  17.372074842453003 s \n",
      "\n",
      "\n",
      "\tEpisode 2761 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5428],\n",
      "        [1.0000, 0.6707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6647],\n",
      "        [1.0000, 0.3767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46109390258789 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.378058671951294 s \n",
      "\n",
      "\n",
      "\tEpisode 2762 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3220],\n",
      "        [1.0000, 0.6750]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5304],\n",
      "        [1.0000, 0.4443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53382396697998 \tStep Time:  0.00598454475402832 s \tTotal Time:  17.384043216705322 s \n",
      "\n",
      "\n",
      "\tEpisode 2763 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3321],\n",
      "        [1.0000, 0.5635]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6622],\n",
      "        [1.0000, 0.6418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.360396385192871 \tStep Time:  0.00698089599609375 s \tTotal Time:  17.391024112701416 s \n",
      "\n",
      "\n",
      "\tEpisode 2764 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6685],\n",
      "        [1.0000, 0.5005]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.6770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564659118652344 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.397008180618286 s \n",
      "\n",
      "\n",
      "\tEpisode 2765 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3778],\n",
      "        [1.0000, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5845],\n",
      "        [1.0000, 0.4139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470011711120605 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.402992010116577 s \n",
      "\n",
      "\n",
      "\tEpisode 2766 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4727],\n",
      "        [1.0000, 0.6000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6595],\n",
      "        [1.0000, 0.3956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484889030456543 \tStep Time:  0.004986763000488281 s \tTotal Time:  17.407978773117065 s \n",
      "\n",
      "\n",
      "\tEpisode 2767 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.4135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.6180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420026779174805 \tStep Time:  0.005984306335449219 s \tTotal Time:  17.413963079452515 s \n",
      "\n",
      "\n",
      "\tEpisode 2768 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3680],\n",
      "        [1.0000, 0.5701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3070],\n",
      "        [1.0000, 0.3948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.409672617912292 \tStep Time:  0.005983591079711914 s \tTotal Time:  17.419946670532227 s \n",
      "\n",
      "\n",
      "\tEpisode 2769 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5124],\n",
      "        [1.0000, 0.5109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5941],\n",
      "        [1.0000, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566180646419525 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.425930738449097 s \n",
      "\n",
      "\n",
      "\tEpisode 2770 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5601],\n",
      "        [1.0000, 0.2231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6124],\n",
      "        [1.0000, 0.4018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.292745292186737 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.431914806365967 s \n",
      "\n",
      "\n",
      "\tEpisode 2771 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6578],\n",
      "        [1.0000, 0.5446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5539],\n",
      "        [1.0000, 0.6920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56868314743042 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.43889617919922 s \n",
      "\n",
      "\n",
      "\tEpisode 2772 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6827],\n",
      "        [1.0000, 0.2206]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4424],\n",
      "        [1.0000, 0.3346]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.759834289550781 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.44488000869751 s \n",
      "\n",
      "\n",
      "\tEpisode 2773 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5449],\n",
      "        [1.0000, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6765],\n",
      "        [1.0000, 0.6732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.392573356628418 \tStep Time:  0.006982326507568359 s \tTotal Time:  17.451862335205078 s \n",
      "\n",
      "\n",
      "\tEpisode 2774 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1072],\n",
      "        [1.0000, 0.4769]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6370],\n",
      "        [1.0000, 0.4296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529634177684784 \tStep Time:  0.007978439331054688 s \tTotal Time:  17.459840774536133 s \n",
      "\n",
      "\n",
      "\tEpisode 2775 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.5585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2827],\n",
      "        [1.0000, 0.6564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.726734638214111 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.466822147369385 s \n",
      "\n",
      "\n",
      "\tEpisode 2776 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5345],\n",
      "        [1.0000, 0.4443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6084],\n",
      "        [1.0000, 0.6519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532063007354736 \tStep Time:  0.00698089599609375 s \tTotal Time:  17.47380304336548 s \n",
      "\n",
      "\n",
      "\tEpisode 2777 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3853],\n",
      "        [1.0000, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6222],\n",
      "        [1.0000, 0.3139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66398298740387 \tStep Time:  0.005983591079711914 s \tTotal Time:  17.47978663444519 s \n",
      "\n",
      "\n",
      "\tEpisode 2778 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5269],\n",
      "        [1.0000, 0.5764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6056],\n",
      "        [1.0000, 0.4991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493262767791748 \tStep Time:  0.00797891616821289 s \tTotal Time:  17.487765550613403 s \n",
      "\n",
      "\n",
      "\tEpisode 2779 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5916],\n",
      "        [1.0000, 0.5386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4104],\n",
      "        [1.0000, 0.3862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496092557907104 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.493749380111694 s \n",
      "\n",
      "\n",
      "\tEpisode 2780 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5395],\n",
      "        [1.0000, 0.4169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3352],\n",
      "        [1.0000, 0.3964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.665066719055176 \tStep Time:  0.006981849670410156 s \tTotal Time:  17.500731229782104 s \n",
      "\n",
      "\n",
      "\tEpisode 2781 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3958],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5902],\n",
      "        [1.0000, 0.4158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501029968261719 \tStep Time:  0.006982088088989258 s \tTotal Time:  17.507713317871094 s \n",
      "\n",
      "\n",
      "\tEpisode 2782 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5512],\n",
      "        [1.0000, 0.4100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6072],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545586109161377 \tStep Time:  0.007978200912475586 s \tTotal Time:  17.51569151878357 s \n",
      "\n",
      "\n",
      "\tEpisode 2783 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5656],\n",
      "        [1.0000, 0.4904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6025],\n",
      "        [1.0000, 0.6076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.590261697769165 \tStep Time:  0.007977962493896484 s \tTotal Time:  17.523669481277466 s \n",
      "\n",
      "\n",
      "\tEpisode 2784 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4998],\n",
      "        [1.0000, 0.6093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4478],\n",
      "        [1.0000, 0.6018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556827425956726 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.529653549194336 s \n",
      "\n",
      "\n",
      "\tEpisode 2785 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5726],\n",
      "        [1.0000, 0.5993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5609],\n",
      "        [1.0000, 0.5937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51166296005249 \tStep Time:  0.00797891616821289 s \tTotal Time:  17.53763246536255 s \n",
      "\n",
      "\n",
      "\tEpisode 2786 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5418],\n",
      "        [1.0000, 0.4748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5063],\n",
      "        [1.0000, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497381687164307 \tStep Time:  0.00698089599609375 s \tTotal Time:  17.544613361358643 s \n",
      "\n",
      "\n",
      "\tEpisode 2787 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5771],\n",
      "        [1.0000, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5612],\n",
      "        [1.0000, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453542709350586 \tStep Time:  0.00598454475402832 s \tTotal Time:  17.55059790611267 s \n",
      "\n",
      "\n",
      "\tEpisode 2788 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5648],\n",
      "        [1.0000, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4753],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507916450500488 \tStep Time:  0.006981849670410156 s \tTotal Time:  17.55757975578308 s \n",
      "\n",
      "\n",
      "\tEpisode 2789 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5685],\n",
      "        [1.0000, 0.4753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4779],\n",
      "        [1.0000, 0.3503]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505732536315918 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.564561128616333 s \n",
      "\n",
      "\n",
      "\tEpisode 2790 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4936],\n",
      "        [1.0000, 0.5512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4425],\n",
      "        [1.0000, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460634291172028 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.571542501449585 s \n",
      "\n",
      "\n",
      "\tEpisode 2791 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5030],\n",
      "        [1.0000, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5055],\n",
      "        [1.0000, 0.5318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524448871612549 \tStep Time:  0.00698089599609375 s \tTotal Time:  17.57852339744568 s \n",
      "\n",
      "\n",
      "\tEpisode 2792 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5273],\n",
      "        [1.0000, 0.5118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5099],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514706015586853 \tStep Time:  0.009973764419555664 s \tTotal Time:  17.588497161865234 s \n",
      "\n",
      "\n",
      "\tEpisode 2793 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5046],\n",
      "        [1.0000, 0.5178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517256617546082 \tStep Time:  0.00698089599609375 s \tTotal Time:  17.595478057861328 s \n",
      "\n",
      "\n",
      "\tEpisode 2794 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.6020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5008],\n",
      "        [1.0000, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464738547801971 \tStep Time:  0.005982875823974609 s \tTotal Time:  17.601460933685303 s \n",
      "\n",
      "\n",
      "\tEpisode 2795 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5024],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7151],\n",
      "        [1.0000, 0.4953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453275203704834 \tStep Time:  0.006018161773681641 s \tTotal Time:  17.607479095458984 s \n",
      "\n",
      "\n",
      "\tEpisode 2796 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6195],\n",
      "        [1.0000, 0.5565]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4824],\n",
      "        [1.0000, 0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597031593322754 \tStep Time:  0.0049936771392822266 s \tTotal Time:  17.612472772598267 s \n",
      "\n",
      "\n",
      "\tEpisode 2797 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5646],\n",
      "        [1.0000, 0.4829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4057],\n",
      "        [1.0000, 0.6859]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.627232551574707 \tStep Time:  0.005977153778076172 s \tTotal Time:  17.618449926376343 s \n",
      "\n",
      "\n",
      "\tEpisode 2798 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4639],\n",
      "        [1.0000, 0.3613]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4778],\n",
      "        [1.0000, 0.4455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490878164768219 \tStep Time:  0.0050547122955322266 s \tTotal Time:  17.623504638671875 s \n",
      "\n",
      "\n",
      "\tEpisode 2799 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4761],\n",
      "        [1.0000, 0.4739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.4509]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528996467590332 \tStep Time:  0.005993366241455078 s \tTotal Time:  17.62949800491333 s \n",
      "\n",
      "\n",
      "\tEpisode 2800 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5197],\n",
      "        [1.0000, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494534492492676 \tStep Time:  0.005982398986816406 s \tTotal Time:  17.635480403900146 s \n",
      "\n",
      "\n",
      "\tEpisode 2801 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5428],\n",
      "        [1.0000, 0.3436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3377],\n",
      "        [1.0000, 0.4797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.71281123161316 \tStep Time:  0.0065152645111083984 s \tTotal Time:  17.641995668411255 s \n",
      "\n",
      "\n",
      "\tEpisode 2802 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4921],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4798],\n",
      "        [1.0000, 0.5078]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574899673461914 \tStep Time:  0.006948232650756836 s \tTotal Time:  17.64894390106201 s \n",
      "\n",
      "\n",
      "\tEpisode 2803 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5449],\n",
      "        [1.0000, 0.4807]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53163194656372 \tStep Time:  0.005983591079711914 s \tTotal Time:  17.654927492141724 s \n",
      "\n",
      "\n",
      "\tEpisode 2804 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4930],\n",
      "        [1.0000, 0.5664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492818832397461 \tStep Time:  0.0069811344146728516 s \tTotal Time:  17.661908626556396 s \n",
      "\n",
      "\n",
      "\tEpisode 2805 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.3753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4523],\n",
      "        [1.0000, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533205509185791 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.667892694473267 s \n",
      "\n",
      "\n",
      "\tEpisode 2806 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5285],\n",
      "        [1.0000, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3769],\n",
      "        [1.0000, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578779697418213 \tStep Time:  0.004986763000488281 s \tTotal Time:  17.673876762390137 s \n",
      "\n",
      "\n",
      "\tEpisode 2807 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5809],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4929],\n",
      "        [1.0000, 0.5603]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576789379119873 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.68085813522339 s \n",
      "\n",
      "\n",
      "\tEpisode 2808 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4836],\n",
      "        [1.0000, 0.4895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.608916759490967 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.68783950805664 s \n",
      "\n",
      "\n",
      "\tEpisode 2809 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [1.0000, 0.6504]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6935],\n",
      "        [1.0000, 0.5835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66053295135498 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.69382357597351 s \n",
      "\n",
      "\n",
      "\tEpisode 2810 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4643],\n",
      "        [1.0000, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3073],\n",
      "        [1.0000, 0.4047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597297668457031 \tStep Time:  0.005988121032714844 s \tTotal Time:  17.70080876350403 s \n",
      "\n",
      "\n",
      "\tEpisode 2811 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.6885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484541416168213 \tStep Time:  0.005979776382446289 s \tTotal Time:  17.706788539886475 s \n",
      "\n",
      "\n",
      "\tEpisode 2812 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.6627]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.4120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66953855752945 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.712772607803345 s \n",
      "\n",
      "\n",
      "\tEpisode 2813 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.5669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5178],\n",
      "        [1.0000, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500198364257812 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.719753980636597 s \n",
      "\n",
      "\n",
      "\tEpisode 2814 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5934],\n",
      "        [1.0000, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5344],\n",
      "        [1.0000, 0.4906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579088985919952 \tStep Time:  0.00498652458190918 s \tTotal Time:  17.724740505218506 s \n",
      "\n",
      "\n",
      "\tEpisode 2815 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5255],\n",
      "        [1.0000, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4811],\n",
      "        [1.0000, 0.5372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582945823669434 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.730724573135376 s \n",
      "\n",
      "\n",
      "\tEpisode 2816 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [1.0000, 0.5193]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547521591186523 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.737705945968628 s \n",
      "\n",
      "\n",
      "\tEpisode 2817 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4864],\n",
      "        [1.0000, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4828],\n",
      "        [1.0000, 0.4808]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55132007598877 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.743690013885498 s \n",
      "\n",
      "\n",
      "\tEpisode 2818 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [1.0000, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5529],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496675372123718 \tStep Time:  0.00598454475402832 s \tTotal Time:  17.749674558639526 s \n",
      "\n",
      "\n",
      "\tEpisode 2819 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5361],\n",
      "        [1.0000, 0.4973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4922],\n",
      "        [1.0000, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510859489440918 \tStep Time:  0.0059833526611328125 s \tTotal Time:  17.75565791130066 s \n",
      "\n",
      "\n",
      "\tEpisode 2820 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.4828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4815],\n",
      "        [1.0000, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50200879573822 \tStep Time:  0.004986763000488281 s \tTotal Time:  17.760644674301147 s \n",
      "\n",
      "\n",
      "\tEpisode 2821 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4847],\n",
      "        [1.0000, 0.6464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4760],\n",
      "        [1.0000, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.604134559631348 \tStep Time:  0.004987001419067383 s \tTotal Time:  17.766628742218018 s \n",
      "\n",
      "\n",
      "\tEpisode 2822 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5064],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [1.0000, 0.5295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516243696212769 \tStep Time:  0.0069811344146728516 s \tTotal Time:  17.77360987663269 s \n",
      "\n",
      "\n",
      "\tEpisode 2823 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4687],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53468942642212 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.77959394454956 s \n",
      "\n",
      "\n",
      "\tEpisode 2824 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.4562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.4973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53853988647461 \tStep Time:  0.00598454475402832 s \tTotal Time:  17.78557848930359 s \n",
      "\n",
      "\n",
      "\tEpisode 2825 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.4706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516007423400879 \tStep Time:  0.005983591079711914 s \tTotal Time:  17.7915620803833 s \n",
      "\n",
      "\n",
      "\tEpisode 2826 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4961],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4902],\n",
      "        [1.0000, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505102515220642 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.797545909881592 s \n",
      "\n",
      "\n",
      "\tEpisode 2827 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4730],\n",
      "        [1.0000, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4942],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54178524017334 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.803529977798462 s \n",
      "\n",
      "\n",
      "\tEpisode 2828 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5917],\n",
      "        [1.0000, 0.5652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.589715480804443 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.809514045715332 s \n",
      "\n",
      "\n",
      "\tEpisode 2829 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5865],\n",
      "        [1.0000, 0.6036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4888],\n",
      "        [1.0000, 0.4871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523244500160217 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.815497875213623 s \n",
      "\n",
      "\n",
      "\tEpisode 2830 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5529],\n",
      "        [1.0000, 0.4698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50468921661377 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.821481943130493 s \n",
      "\n",
      "\n",
      "\tEpisode 2831 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4761],\n",
      "        [1.0000, 0.5305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.5245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502751350402832 \tStep Time:  0.004986763000488281 s \tTotal Time:  17.82646870613098 s \n",
      "\n",
      "\n",
      "\tEpisode 2832 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4783],\n",
      "        [1.0000, 0.5857]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5252],\n",
      "        [1.0000, 0.4749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441700339317322 \tStep Time:  0.00498652458190918 s \tTotal Time:  17.832452535629272 s \n",
      "\n",
      "\n",
      "\tEpisode 2833 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5744],\n",
      "        [1.0000, 0.4696]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572460174560547 \tStep Time:  0.0069811344146728516 s \tTotal Time:  17.839433670043945 s \n",
      "\n",
      "\n",
      "\tEpisode 2834 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6040],\n",
      "        [1.0000, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4586],\n",
      "        [1.0000, 0.4682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579653799533844 \tStep Time:  0.005984306335449219 s \tTotal Time:  17.845417976379395 s \n",
      "\n",
      "\n",
      "\tEpisode 2835 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4684],\n",
      "        [1.0000, 0.5201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5249],\n",
      "        [1.0000, 0.4782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504382133483887 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.851401805877686 s \n",
      "\n",
      "\n",
      "\tEpisode 2836 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5382],\n",
      "        [1.0000, 0.4500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5500],\n",
      "        [1.0000, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489009857177734 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.857385873794556 s \n",
      "\n",
      "\n",
      "\tEpisode 2837 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4523],\n",
      "        [1.0000, 0.4679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5753],\n",
      "        [1.0000, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60556697845459 \tStep Time:  0.006981372833251953 s \tTotal Time:  17.864367246627808 s \n",
      "\n",
      "\n",
      "\tEpisode 2838 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5315],\n",
      "        [1.0000, 0.4961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.4593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49880313873291 \tStep Time:  0.005983829498291016 s \tTotal Time:  17.8703510761261 s \n",
      "\n",
      "\n",
      "\tEpisode 2839 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.4662]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.4412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541423320770264 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.87733244895935 s \n",
      "\n",
      "\n",
      "\tEpisode 2840 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2350],\n",
      "        [1.0000, 0.4651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4667],\n",
      "        [1.0000, 0.4575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429724216461182 \tStep Time:  0.005984783172607422 s \tTotal Time:  17.883317232131958 s \n",
      "\n",
      "\n",
      "\tEpisode 2841 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4708],\n",
      "        [1.0000, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.4717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52605676651001 \tStep Time:  0.009973287582397461 s \tTotal Time:  17.893290519714355 s \n",
      "\n",
      "\n",
      "\tEpisode 2842 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4711],\n",
      "        [1.0000, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4855],\n",
      "        [1.0000, 0.4719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500194370746613 \tStep Time:  0.007978439331054688 s \tTotal Time:  17.90126895904541 s \n",
      "\n",
      "\n",
      "\tEpisode 2843 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4784],\n",
      "        [1.0000, 0.4905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.4860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497189998626709 \tStep Time:  0.0059833526611328125 s \tTotal Time:  17.907252311706543 s \n",
      "\n",
      "\n",
      "\tEpisode 2844 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4835],\n",
      "        [1.0000, 0.4924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4962],\n",
      "        [1.0000, 0.4787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501845836639404 \tStep Time:  0.00498652458190918 s \tTotal Time:  17.912238836288452 s \n",
      "\n",
      "\n",
      "\tEpisode 2845 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4958],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4907],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509859561920166 \tStep Time:  0.006998300552368164 s \tTotal Time:  17.91923713684082 s \n",
      "\n",
      "\n",
      "\tEpisode 2846 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4937],\n",
      "        [1.0000, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3087],\n",
      "        [1.0000, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425545692443848 \tStep Time:  0.004984855651855469 s \tTotal Time:  17.924221992492676 s \n",
      "\n",
      "\n",
      "\tEpisode 2847 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4927],\n",
      "        [1.0000, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4968],\n",
      "        [1.0000, 0.5026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513089179992676 \tStep Time:  0.005984067916870117 s \tTotal Time:  17.930206060409546 s \n",
      "\n",
      "\n",
      "\tEpisode 2848 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5240],\n",
      "        [1.0000, 0.5476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489328384399414 \tStep Time:  0.0059909820556640625 s \tTotal Time:  17.93619704246521 s \n",
      "\n",
      "\n",
      "\tEpisode 2849 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4993],\n",
      "        [1.0000, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509747505187988 \tStep Time:  0.005983114242553711 s \tTotal Time:  17.942180156707764 s \n",
      "\n",
      "\n",
      "\tEpisode 2850 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4901],\n",
      "        [1.0000, 0.4952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4993],\n",
      "        [1.0000, 0.5807]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54855740070343 \tStep Time:  0.006016731262207031 s \tTotal Time:  17.94819688796997 s \n",
      "\n",
      "\n",
      "\tEpisode 2851 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5056],\n",
      "        [1.0000, 0.4775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5233],\n",
      "        [1.0000, 0.5378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527166187763214 \tStep Time:  0.00698399543762207 s \tTotal Time:  17.955180883407593 s \n",
      "\n",
      "\n",
      "\tEpisode 2852 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5193],\n",
      "        [1.0000, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4977],\n",
      "        [1.0000, 0.4952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513275265693665 \tStep Time:  0.0049855709075927734 s \tTotal Time:  17.960166454315186 s \n",
      "\n",
      "\n",
      "\tEpisode 2853 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.5095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4976],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501816749572754 \tStep Time:  0.0059511661529541016 s \tTotal Time:  17.96611762046814 s \n",
      "\n",
      "\n",
      "\tEpisode 2854 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5030],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4723],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535399913787842 \tStep Time:  0.007014036178588867 s \tTotal Time:  17.97313165664673 s \n",
      "\n",
      "\n",
      "\tEpisode 2855 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4903],\n",
      "        [1.0000, 0.3698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5106],\n",
      "        [1.0000, 0.4941]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583653926849365 \tStep Time:  0.004984617233276367 s \tTotal Time:  17.978116273880005 s \n",
      "\n",
      "\n",
      "\tEpisode 2856 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4922],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5007],\n",
      "        [1.0000, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561264216899872 \tStep Time:  0.006982564926147461 s \tTotal Time:  17.985098838806152 s \n",
      "\n",
      "\n",
      "\tEpisode 2857 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.4569]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.4511]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494071960449219 \tStep Time:  0.005982875823974609 s \tTotal Time:  17.991081714630127 s \n",
      "\n",
      "\n",
      "\tEpisode 2858 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.4271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4849],\n",
      "        [1.0000, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463469982147217 \tStep Time:  0.005986928939819336 s \tTotal Time:  17.997068643569946 s \n",
      "\n",
      "\n",
      "\tEpisode 2859 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5199],\n",
      "        [1.0000, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5755],\n",
      "        [1.0000, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50645923614502 \tStep Time:  0.00598454475402832 s \tTotal Time:  18.003053188323975 s \n",
      "\n",
      "\n",
      "\tEpisode 2860 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4373],\n",
      "        [1.0000, 0.4213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4889],\n",
      "        [1.0000, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531117916107178 \tStep Time:  0.005980253219604492 s \tTotal Time:  18.00903344154358 s \n",
      "\n",
      "\n",
      "\tEpisode 2861 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4792],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5267],\n",
      "        [1.0000, 0.4168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542568683624268 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.01501727104187 s \n",
      "\n",
      "\n",
      "\tEpisode 2862 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5293],\n",
      "        [1.0000, 0.4460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4041],\n",
      "        [1.0000, 0.5502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542163848876953 \tStep Time:  0.005986452102661133 s \tTotal Time:  18.02100372314453 s \n",
      "\n",
      "\n",
      "\tEpisode 2863 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.5232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3074],\n",
      "        [1.0000, 0.6520]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.347170352935791 \tStep Time:  0.0059850215911865234 s \tTotal Time:  18.026988744735718 s \n",
      "\n",
      "\n",
      "\tEpisode 2864 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5817],\n",
      "        [1.0000, 0.6140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4558],\n",
      "        [1.0000, 0.5887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575138747692108 \tStep Time:  0.005949974060058594 s \tTotal Time:  18.032938718795776 s \n",
      "\n",
      "\n",
      "\tEpisode 2865 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.6223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4771],\n",
      "        [1.0000, 0.5363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440578997135162 \tStep Time:  0.006014585494995117 s \tTotal Time:  18.03895330429077 s \n",
      "\n",
      "\n",
      "\tEpisode 2866 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6126],\n",
      "        [1.0000, 0.3429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6529],\n",
      "        [1.0000, 0.2642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.248685836791992 \tStep Time:  0.005986690521240234 s \tTotal Time:  18.04493999481201 s \n",
      "\n",
      "\n",
      "\tEpisode 2867 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3345],\n",
      "        [1.0000, 0.3456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4064],\n",
      "        [1.0000, 0.5706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.395769119262695 \tStep Time:  0.006947040557861328 s \tTotal Time:  18.051887035369873 s \n",
      "\n",
      "\n",
      "\tEpisode 2868 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7017],\n",
      "        [1.0000, 0.7477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6204],\n",
      "        [1.0000, 0.3324]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.331478118896484 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.057871103286743 s \n",
      "\n",
      "\n",
      "\tEpisode 2869 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6916],\n",
      "        [1.0000, 0.2615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6887],\n",
      "        [1.0000, 0.4387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489397525787354 \tStep Time:  0.004986763000488281 s \tTotal Time:  18.06285786628723 s \n",
      "\n",
      "\n",
      "\tEpisode 2870 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3590],\n",
      "        [1.0000, 0.7045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5131],\n",
      "        [1.0000, 0.3849]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.639923095703125 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.070836544036865 s \n",
      "\n",
      "\n",
      "\tEpisode 2871 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6513],\n",
      "        [1.0000, 0.3992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6366],\n",
      "        [1.0000, 0.6957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425949096679688 \tStep Time:  0.004986763000488281 s \tTotal Time:  18.075823307037354 s \n",
      "\n",
      "\n",
      "\tEpisode 2872 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5997],\n",
      "        [1.0000, 0.7270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3258],\n",
      "        [1.0000, 0.5461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.739966869354248 \tStep Time:  0.006981611251831055 s \tTotal Time:  18.082804918289185 s \n",
      "\n",
      "\n",
      "\tEpisode 2873 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4326],\n",
      "        [1.0000, 0.3175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4747],\n",
      "        [1.0000, 0.3536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54736328125 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.089786052703857 s \n",
      "\n",
      "\n",
      "\tEpisode 2874 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3340],\n",
      "        [1.0000, 0.5833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5085],\n",
      "        [1.0000, 0.3392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583752155303955 \tStep Time:  0.00797891616821289 s \tTotal Time:  18.09776496887207 s \n",
      "\n",
      "\n",
      "\tEpisode 2875 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2922],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3984],\n",
      "        [1.0000, 0.6459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443403720855713 \tStep Time:  0.010970354080200195 s \tTotal Time:  18.10873532295227 s \n",
      "\n",
      "\n",
      "\tEpisode 2876 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3741],\n",
      "        [1.0000, 0.6147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3668],\n",
      "        [1.0000, 0.3901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448068618774414 \tStep Time:  0.009974479675292969 s \tTotal Time:  18.118709802627563 s \n",
      "\n",
      "\n",
      "\tEpisode 2877 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4446],\n",
      "        [1.0000, 0.3142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3969],\n",
      "        [1.0000, 0.7397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.350728511810303 \tStep Time:  0.007979154586791992 s \tTotal Time:  18.126688957214355 s \n",
      "\n",
      "\n",
      "\tEpisode 2878 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5271],\n",
      "        [1.0000, 0.6225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4059],\n",
      "        [1.0000, 0.2944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467828154563904 \tStep Time:  0.008009195327758789 s \tTotal Time:  18.134698152542114 s \n",
      "\n",
      "\n",
      "\tEpisode 2879 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6586],\n",
      "        [1.0000, 0.5271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5107],\n",
      "        [1.0000, 0.4010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433985710144043 \tStep Time:  0.006022453308105469 s \tTotal Time:  18.14072060585022 s \n",
      "\n",
      "\n",
      "\tEpisode 2880 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3640],\n",
      "        [1.0000, 0.6604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4433],\n",
      "        [1.0000, 0.2945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.726295948028564 \tStep Time:  0.00598597526550293 s \tTotal Time:  18.146706581115723 s \n",
      "\n",
      "\n",
      "\tEpisode 2881 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7327],\n",
      "        [1.0000, 0.7336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7176],\n",
      "        [1.0000, 0.5439]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.704887390136719 \tStep Time:  0.005982160568237305 s \tTotal Time:  18.15268874168396 s \n",
      "\n",
      "\n",
      "\tEpisode 2882 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7851],\n",
      "        [1.0000, 0.1714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3783],\n",
      "        [1.0000, 0.2061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.915221571922302 \tStep Time:  0.006027698516845703 s \tTotal Time:  18.158716440200806 s \n",
      "\n",
      "\n",
      "\tEpisode 2883 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4570],\n",
      "        [1.0000, 0.7628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4753],\n",
      "        [1.0000, 0.5799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510191917419434 \tStep Time:  0.00597834587097168 s \tTotal Time:  18.164694786071777 s \n",
      "\n",
      "\n",
      "\tEpisode 2884 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7799],\n",
      "        [1.0000, 0.3589]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4657],\n",
      "        [1.0000, 0.3791]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451606571674347 \tStep Time:  0.00598597526550293 s \tTotal Time:  18.17068076133728 s \n",
      "\n",
      "\n",
      "\tEpisode 2885 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6005],\n",
      "        [1.0000, 0.4446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5935],\n",
      "        [1.0000, 0.4559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566847801208496 \tStep Time:  0.005982875823974609 s \tTotal Time:  18.176663637161255 s \n",
      "\n",
      "\n",
      "\tEpisode 2886 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7456],\n",
      "        [1.0000, 0.3127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3734],\n",
      "        [1.0000, 0.2767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.785623550415039 \tStep Time:  0.004987001419067383 s \tTotal Time:  18.181650638580322 s \n",
      "\n",
      "\n",
      "\tEpisode 2887 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5556],\n",
      "        [1.0000, 0.3453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6187],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44318437576294 \tStep Time:  0.0069806575775146484 s \tTotal Time:  18.188631296157837 s \n",
      "\n",
      "\n",
      "\tEpisode 2888 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5274],\n",
      "        [1.0000, 0.3133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6025],\n",
      "        [1.0000, 0.5519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479701042175293 \tStep Time:  0.005986213684082031 s \tTotal Time:  18.19461750984192 s \n",
      "\n",
      "\n",
      "\tEpisode 2889 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6871],\n",
      "        [1.0000, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3822],\n",
      "        [1.0000, 0.7311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.324645042419434 \tStep Time:  0.0059816837310791016 s \tTotal Time:  18.200599193572998 s \n",
      "\n",
      "\n",
      "\tEpisode 2890 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6314],\n",
      "        [1.0000, 0.5986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5073],\n",
      "        [1.0000, 0.7338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.667054176330566 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.20658302307129 s \n",
      "\n",
      "\n",
      "\tEpisode 2891 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3633],\n",
      "        [1.0000, 0.6683]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.3822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46022367477417 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.21256709098816 s \n",
      "\n",
      "\n",
      "\tEpisode 2892 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4654],\n",
      "        [1.0000, 0.3299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3865],\n",
      "        [1.0000, 0.6478]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494852721691132 \tStep Time:  0.005990505218505859 s \tTotal Time:  18.218557596206665 s \n",
      "\n",
      "\n",
      "\tEpisode 2893 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.5106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5139],\n",
      "        [1.0000, 0.4334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48794972896576 \tStep Time:  0.0059778690338134766 s \tTotal Time:  18.22453546524048 s \n",
      "\n",
      "\n",
      "\tEpisode 2894 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4698],\n",
      "        [1.0000, 0.4286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6220],\n",
      "        [1.0000, 0.6962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570364952087402 \tStep Time:  0.0059833526611328125 s \tTotal Time:  18.23051881790161 s \n",
      "\n",
      "\n",
      "\tEpisode 2895 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.6821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6667],\n",
      "        [1.0000, 0.5339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562573432922363 \tStep Time:  0.0059528350830078125 s \tTotal Time:  18.23647165298462 s \n",
      "\n",
      "\n",
      "\tEpisode 2896 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6090],\n",
      "        [1.0000, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3774],\n",
      "        [1.0000, 0.6208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48784351348877 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.24245548248291 s \n",
      "\n",
      "\n",
      "\tEpisode 2897 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4338],\n",
      "        [1.0000, 0.3841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4713],\n",
      "        [1.0000, 0.3668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.614908218383789 \tStep Time:  0.0060160160064697266 s \tTotal Time:  18.24847149848938 s \n",
      "\n",
      "\n",
      "\tEpisode 2898 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6594],\n",
      "        [1.0000, 0.5222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6134],\n",
      "        [1.0000, 0.6636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.642551183700562 \tStep Time:  0.006948947906494141 s \tTotal Time:  18.255420446395874 s \n",
      "\n",
      "\n",
      "\tEpisode 2899 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4692],\n",
      "        [1.0000, 0.6434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5074],\n",
      "        [1.0000, 0.6238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569143414497375 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.261404037475586 s \n",
      "\n",
      "\n",
      "\tEpisode 2900 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5941],\n",
      "        [1.0000, 0.5553]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4960],\n",
      "        [1.0000, 0.4814]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561794757843018 \tStep Time:  0.005984783172607422 s \tTotal Time:  18.267388820648193 s \n",
      "\n",
      "\n",
      "\tEpisode 2901 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5946],\n",
      "        [1.0000, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.5062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469828605651855 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.273372411727905 s \n",
      "\n",
      "\n",
      "\tEpisode 2902 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5785],\n",
      "        [1.0000, 0.5819]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.4765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539770543575287 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.280353546142578 s \n",
      "\n",
      "\n",
      "\tEpisode 2903 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5522],\n",
      "        [1.0000, 0.4756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4466],\n",
      "        [1.0000, 0.5448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50112533569336 \tStep Time:  0.006982088088989258 s \tTotal Time:  18.287335634231567 s \n",
      "\n",
      "\n",
      "\tEpisode 2904 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4758],\n",
      "        [1.0000, 0.4837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4321],\n",
      "        [1.0000, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482250213623047 \tStep Time:  0.0069806575775146484 s \tTotal Time:  18.294316291809082 s \n",
      "\n",
      "\n",
      "\tEpisode 2905 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4266],\n",
      "        [1.0000, 0.5378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4691],\n",
      "        [1.0000, 0.4683]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487362384796143 \tStep Time:  0.008976221084594727 s \tTotal Time:  18.303292512893677 s \n",
      "\n",
      "\n",
      "\tEpisode 2906 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4768],\n",
      "        [1.0000, 0.4557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4325],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518157482147217 \tStep Time:  0.006981849670410156 s \tTotal Time:  18.310274362564087 s \n",
      "\n",
      "\n",
      "\tEpisode 2907 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5247],\n",
      "        [1.0000, 0.4641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4790],\n",
      "        [1.0000, 0.4338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519454002380371 \tStep Time:  0.0059833526611328125 s \tTotal Time:  18.31625771522522 s \n",
      "\n",
      "\n",
      "\tEpisode 2908 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4640],\n",
      "        [1.0000, 0.4800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54966688156128 \tStep Time:  0.007013797760009766 s \tTotal Time:  18.32327151298523 s \n",
      "\n",
      "\n",
      "\tEpisode 2909 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5285],\n",
      "        [1.0000, 0.3991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4860],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.604256629943848 \tStep Time:  0.005983114242553711 s \tTotal Time:  18.329254627227783 s \n",
      "\n",
      "\n",
      "\tEpisode 2910 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4960],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4224],\n",
      "        [1.0000, 0.4255]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51786756515503 \tStep Time:  0.005984783172607422 s \tTotal Time:  18.33523941040039 s \n",
      "\n",
      "\n",
      "\tEpisode 2911 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4960],\n",
      "        [1.0000, 0.3896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4572],\n",
      "        [1.0000, 0.4754]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599927425384521 \tStep Time:  0.006055593490600586 s \tTotal Time:  18.34129500389099 s \n",
      "\n",
      "\n",
      "\tEpisode 2912 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4804],\n",
      "        [1.0000, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516231536865234 \tStep Time:  0.004986763000488281 s \tTotal Time:  18.34728240966797 s \n",
      "\n",
      "\n",
      "\tEpisode 2913 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5190],\n",
      "        [1.0000, 0.4957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5095],\n",
      "        [1.0000, 0.4169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4485764503479 \tStep Time:  0.00643157958984375 s \tTotal Time:  18.353713989257812 s \n",
      "\n",
      "\n",
      "\tEpisode 2914 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5290],\n",
      "        [1.0000, 0.5286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518498301506042 \tStep Time:  0.006009101867675781 s \tTotal Time:  18.35972309112549 s \n",
      "\n",
      "\n",
      "\tEpisode 2915 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5271],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.4967]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476341724395752 \tStep Time:  0.005986213684082031 s \tTotal Time:  18.36570930480957 s \n",
      "\n",
      "\n",
      "\tEpisode 2916 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5462],\n",
      "        [1.0000, 0.5294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4544],\n",
      "        [1.0000, 0.4573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51190733909607 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.371692895889282 s \n",
      "\n",
      "\n",
      "\tEpisode 2917 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.4351]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3988],\n",
      "        [1.0000, 0.4339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527318954467773 \tStep Time:  0.005982875823974609 s \tTotal Time:  18.377675771713257 s \n",
      "\n",
      "\n",
      "\tEpisode 2918 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5074],\n",
      "        [1.0000, 0.5535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4850],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53239530324936 \tStep Time:  0.006571292877197266 s \tTotal Time:  18.384247064590454 s \n",
      "\n",
      "\n",
      "\tEpisode 2919 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5616],\n",
      "        [1.0000, 0.4451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5568],\n",
      "        [1.0000, 0.5765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445478916168213 \tStep Time:  0.006395101547241211 s \tTotal Time:  18.390642166137695 s \n",
      "\n",
      "\n",
      "\tEpisode 2920 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4917],\n",
      "        [1.0000, 0.5528]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5561],\n",
      "        [1.0000, 0.4766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435925006866455 \tStep Time:  0.005982160568237305 s \tTotal Time:  18.396624326705933 s \n",
      "\n",
      "\n",
      "\tEpisode 2921 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5637],\n",
      "        [1.0000, 0.4494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5408],\n",
      "        [1.0000, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518272876739502 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.402608394622803 s \n",
      "\n",
      "\n",
      "\tEpisode 2922 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6159],\n",
      "        [1.0000, 0.5588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3508],\n",
      "        [1.0000, 0.3850]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575693607330322 \tStep Time:  0.005986452102661133 s \tTotal Time:  18.408594846725464 s \n",
      "\n",
      "\n",
      "\tEpisode 2923 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6204],\n",
      "        [1.0000, 0.5428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3989],\n",
      "        [1.0000, 0.5548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.399500846862793 \tStep Time:  0.005982160568237305 s \tTotal Time:  18.4145770072937 s \n",
      "\n",
      "\n",
      "\tEpisode 2924 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3326],\n",
      "        [1.0000, 0.5907]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4395],\n",
      "        [1.0000, 0.3513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60110330581665 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.420560836791992 s \n",
      "\n",
      "\n",
      "\tEpisode 2925 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5060],\n",
      "        [1.0000, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3819],\n",
      "        [1.0000, 0.6299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416212916374207 \tStep Time:  0.005952119827270508 s \tTotal Time:  18.426512956619263 s \n",
      "\n",
      "\n",
      "\tEpisode 2926 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5369],\n",
      "        [1.0000, 0.4140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4000],\n",
      "        [1.0000, 0.4941]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498019218444824 \tStep Time:  0.005017995834350586 s \tTotal Time:  18.431530952453613 s \n",
      "\n",
      "\n",
      "\tEpisode 2927 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.4109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5680],\n",
      "        [1.0000, 0.6321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4901762008667 \tStep Time:  0.00695037841796875 s \tTotal Time:  18.438481330871582 s \n",
      "\n",
      "\n",
      "\tEpisode 2928 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6136],\n",
      "        [1.0000, 0.6225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5918],\n",
      "        [1.0000, 0.3755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426265716552734 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.444465398788452 s \n",
      "\n",
      "\n",
      "\tEpisode 2929 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4848],\n",
      "        [1.0000, 0.4543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3001],\n",
      "        [1.0000, 0.5713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561735153198242 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.450449228286743 s \n",
      "\n",
      "\n",
      "\tEpisode 2930 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4824],\n",
      "        [1.0000, 0.5177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4351],\n",
      "        [1.0000, 0.6348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550277709960938 \tStep Time:  0.007977962493896484 s \tTotal Time:  18.45842719078064 s \n",
      "\n",
      "\n",
      "\tEpisode 2931 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5948],\n",
      "        [1.0000, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3454],\n",
      "        [1.0000, 0.6057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442463397979736 \tStep Time:  0.004986763000488281 s \tTotal Time:  18.463413953781128 s \n",
      "\n",
      "\n",
      "\tEpisode 2932 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6424],\n",
      "        [1.0000, 0.3695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5506],\n",
      "        [1.0000, 0.3109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.804722011089325 \tStep Time:  0.006981611251831055 s \tTotal Time:  18.47039556503296 s \n",
      "\n",
      "\n",
      "\tEpisode 2933 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5639],\n",
      "        [1.0000, 0.6763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6602],\n",
      "        [1.0000, 0.6456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584470748901367 \tStep Time:  0.00698089599609375 s \tTotal Time:  18.477376461029053 s \n",
      "\n",
      "\n",
      "\tEpisode 2934 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4624],\n",
      "        [1.0000, 0.6255]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3835],\n",
      "        [1.0000, 0.5597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.598004341125488 \tStep Time:  0.00598454475402832 s \tTotal Time:  18.48336100578308 s \n",
      "\n",
      "\n",
      "\tEpisode 2935 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5378],\n",
      "        [1.0000, 0.6057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5546],\n",
      "        [1.0000, 0.5412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546388149261475 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.490342140197754 s \n",
      "\n",
      "\n",
      "\tEpisode 2936 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5501],\n",
      "        [1.0000, 0.6161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4554],\n",
      "        [1.0000, 0.4801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402671337127686 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.497323274612427 s \n",
      "\n",
      "\n",
      "\tEpisode 2937 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5857],\n",
      "        [1.0000, 0.5446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6029],\n",
      "        [1.0000, 0.5720]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545405387878418 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.5043044090271 s \n",
      "\n",
      "\n",
      "\tEpisode 2938 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5424],\n",
      "        [1.0000, 0.5950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5254],\n",
      "        [1.0000, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548863410949707 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.51028847694397 s \n",
      "\n",
      "\n",
      "\tEpisode 2939 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4536],\n",
      "        [1.0000, 0.5855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5261],\n",
      "        [1.0000, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.434816837310791 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.51627230644226 s \n",
      "\n",
      "\n",
      "\tEpisode 2940 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.4529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4407],\n",
      "        [1.0000, 0.5770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.417749881744385 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.52225637435913 s \n",
      "\n",
      "\n",
      "\tEpisode 2941 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4764],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5820],\n",
      "        [1.0000, 0.4026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445943832397461 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.528240442276 s \n",
      "\n",
      "\n",
      "\tEpisode 2942 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4277],\n",
      "        [1.0000, 0.5685]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5444],\n",
      "        [1.0000, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565860748291016 \tStep Time:  0.00598597526550293 s \tTotal Time:  18.534226417541504 s \n",
      "\n",
      "\n",
      "\tEpisode 2943 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4427],\n",
      "        [1.0000, 0.4475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4620],\n",
      "        [1.0000, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479394614696503 \tStep Time:  0.006979227066040039 s \tTotal Time:  18.541205644607544 s \n",
      "\n",
      "\n",
      "\tEpisode 2944 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4663],\n",
      "        [1.0000, 0.3652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6200],\n",
      "        [1.0000, 0.4340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416018962860107 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.547189712524414 s \n",
      "\n",
      "\n",
      "\tEpisode 2945 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.4919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5806],\n",
      "        [1.0000, 0.3769]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4031343460083 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.553173780441284 s \n",
      "\n",
      "\n",
      "\tEpisode 2946 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4391],\n",
      "        [1.0000, 0.4507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4362],\n",
      "        [1.0000, 0.4710]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501274764537811 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.559157848358154 s \n",
      "\n",
      "\n",
      "\tEpisode 2947 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5739],\n",
      "        [1.0000, 0.3325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6244],\n",
      "        [1.0000, 0.3707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.292147636413574 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.565141677856445 s \n",
      "\n",
      "\n",
      "\tEpisode 2948 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3141],\n",
      "        [1.0000, 0.6194]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5426],\n",
      "        [1.0000, 0.3358]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.291515111923218 \tStep Time:  0.005984306335449219 s \tTotal Time:  18.571125984191895 s \n",
      "\n",
      "\n",
      "\tEpisode 2949 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2902],\n",
      "        [1.0000, 0.6559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7181],\n",
      "        [1.0000, 0.3945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.659183502197266 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.577109813690186 s \n",
      "\n",
      "\n",
      "\tEpisode 2950 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3604],\n",
      "        [1.0000, 0.3716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6975],\n",
      "        [1.0000, 0.4376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.751978874206543 \tStep Time:  0.005984306335449219 s \tTotal Time:  18.583094120025635 s \n",
      "\n",
      "\n",
      "\tEpisode 2951 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2931],\n",
      "        [1.0000, 0.2094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4139],\n",
      "        [1.0000, 0.6219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.736100196838379 \tStep Time:  0.0069806575775146484 s \tTotal Time:  18.59007477760315 s \n",
      "\n",
      "\n",
      "\tEpisode 2952 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7545],\n",
      "        [1.0000, 0.5836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4495],\n",
      "        [1.0000, 0.2568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.924416542053223 \tStep Time:  0.004987001419067383 s \tTotal Time:  18.595061779022217 s \n",
      "\n",
      "\n",
      "\tEpisode 2953 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4513],\n",
      "        [1.0000, 0.4692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5355],\n",
      "        [1.0000, 0.3006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529964923858643 \tStep Time:  0.005984306335449219 s \tTotal Time:  18.601046085357666 s \n",
      "\n",
      "\n",
      "\tEpisode 2954 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3768],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5938],\n",
      "        [1.0000, 0.6021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492959976196289 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.607029676437378 s \n",
      "\n",
      "\n",
      "\tEpisode 2955 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6482],\n",
      "        [1.0000, 0.6931]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3729],\n",
      "        [1.0000, 0.5673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.673493385314941 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.613013744354248 s \n",
      "\n",
      "\n",
      "\tEpisode 2956 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6374],\n",
      "        [1.0000, 0.4855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4286],\n",
      "        [1.0000, 0.6404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.720487594604492 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.618997812271118 s \n",
      "\n",
      "\n",
      "\tEpisode 2957 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.5896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5140],\n",
      "        [1.0000, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55433988571167 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.62498164176941 s \n",
      "\n",
      "\n",
      "\tEpisode 2958 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.5462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5774],\n",
      "        [1.0000, 0.5453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530918419361115 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.63096570968628 s \n",
      "\n",
      "\n",
      "\tEpisode 2959 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5342],\n",
      "        [1.0000, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5635],\n",
      "        [1.0000, 0.5417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539499402046204 \tStep Time:  0.00598454475402832 s \tTotal Time:  18.636950254440308 s \n",
      "\n",
      "\n",
      "\tEpisode 2960 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5380],\n",
      "        [1.0000, 0.5361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5391],\n",
      "        [1.0000, 0.5452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516337394714355 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.64293384552002 s \n",
      "\n",
      "\n",
      "\tEpisode 2961 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5322],\n",
      "        [1.0000, 0.5324]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5754],\n",
      "        [1.0000, 0.7248]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.659897804260254 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.64891767501831 s \n",
      "\n",
      "\n",
      "\tEpisode 2962 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5553],\n",
      "        [1.0000, 0.6036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5140],\n",
      "        [1.0000, 0.5376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481775283813477 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.655899047851562 s \n",
      "\n",
      "\n",
      "\tEpisode 2963 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5358],\n",
      "        [1.0000, 0.5310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5224],\n",
      "        [1.0000, 0.5276]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522834300994873 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.661883115768433 s \n",
      "\n",
      "\n",
      "\tEpisode 2964 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.4387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5022],\n",
      "        [1.0000, 0.5734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444087386131287 \tStep Time:  0.00598454475402832 s \tTotal Time:  18.66786766052246 s \n",
      "\n",
      "\n",
      "\tEpisode 2965 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [1.0000, 0.4628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6931],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.391295433044434 \tStep Time:  0.00698089599609375 s \tTotal Time:  18.674848556518555 s \n",
      "\n",
      "\n",
      "\tEpisode 2966 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4961],\n",
      "        [1.0000, 0.6254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.6331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519561290740967 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.680832624435425 s \n",
      "\n",
      "\n",
      "\tEpisode 2967 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4169],\n",
      "        [1.0000, 0.4381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5467],\n",
      "        [1.0000, 0.5540]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.386916160583496 \tStep Time:  0.007978677749633789 s \tTotal Time:  18.68881130218506 s \n",
      "\n",
      "\n",
      "\tEpisode 2968 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7037],\n",
      "        [1.0000, 0.7008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5968],\n",
      "        [1.0000, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.585714936256409 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.69479489326477 s \n",
      "\n",
      "\n",
      "\tEpisode 2969 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4127],\n",
      "        [1.0000, 0.5308]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4455],\n",
      "        [1.0000, 0.4242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546156406402588 \tStep Time:  0.005984783172607422 s \tTotal Time:  18.700779676437378 s \n",
      "\n",
      "\n",
      "\tEpisode 2970 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.3104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5776],\n",
      "        [1.0000, 0.4046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542863368988037 \tStep Time:  0.00797891616821289 s \tTotal Time:  18.709755659103394 s \n",
      "\n",
      "\n",
      "\tEpisode 2971 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4065],\n",
      "        [1.0000, 0.6536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5870],\n",
      "        [1.0000, 0.4780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.345555424690247 \tStep Time:  0.007978439331054688 s \tTotal Time:  18.71773409843445 s \n",
      "\n",
      "\n",
      "\tEpisode 2972 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5456],\n",
      "        [1.0000, 0.6260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4474],\n",
      "        [1.0000, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581372380256653 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.7247154712677 s \n",
      "\n",
      "\n",
      "\tEpisode 2973 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3536],\n",
      "        [1.0000, 0.7691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4217],\n",
      "        [1.0000, 0.2966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43190860748291 \tStep Time:  0.005983114242553711 s \tTotal Time:  18.730698585510254 s \n",
      "\n",
      "\n",
      "\tEpisode 2974 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5822],\n",
      "        [1.0000, 0.5686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5973],\n",
      "        [1.0000, 0.3779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.642801403999329 \tStep Time:  0.005985260009765625 s \tTotal Time:  18.73668384552002 s \n",
      "\n",
      "\n",
      "\tEpisode 2975 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8905],\n",
      "        [1.0000, 0.2146]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4610],\n",
      "        [1.0000, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.257317781448364 \tStep Time:  0.005983114242553711 s \tTotal Time:  18.742666959762573 s \n",
      "\n",
      "\n",
      "\tEpisode 2976 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4236],\n",
      "        [1.0000, 0.4235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5349],\n",
      "        [1.0000, 0.7319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.754368782043457 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.748650789260864 s \n",
      "\n",
      "\n",
      "\tEpisode 2977 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.6131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4934],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47243595123291 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.754634380340576 s \n",
      "\n",
      "\n",
      "\tEpisode 2978 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4742],\n",
      "        [1.0000, 0.5985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5811],\n",
      "        [1.0000, 0.3536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484236717224121 \tStep Time:  0.006016254425048828 s \tTotal Time:  18.760650634765625 s \n",
      "\n",
      "\n",
      "\tEpisode 2979 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5077],\n",
      "        [1.0000, 0.3196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3447],\n",
      "        [1.0000, 0.5774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572694301605225 \tStep Time:  0.005953073501586914 s \tTotal Time:  18.766603708267212 s \n",
      "\n",
      "\n",
      "\tEpisode 2980 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4114],\n",
      "        [1.0000, 0.3094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3484],\n",
      "        [1.0000, 0.4677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545356392860413 \tStep Time:  0.006016969680786133 s \tTotal Time:  18.772620677947998 s \n",
      "\n",
      "\n",
      "\tEpisode 2981 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5054],\n",
      "        [1.0000, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3731],\n",
      "        [1.0000, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471280574798584 \tStep Time:  0.005984306335449219 s \tTotal Time:  18.778604984283447 s \n",
      "\n",
      "\n",
      "\tEpisode 2982 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4727],\n",
      "        [1.0000, 0.4277]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5235],\n",
      "        [1.0000, 0.4705]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474864959716797 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.78558611869812 s \n",
      "\n",
      "\n",
      "\tEpisode 2983 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4006],\n",
      "        [1.0000, 0.5728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4329],\n",
      "        [1.0000, 0.3782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.614311218261719 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.79156994819641 s \n",
      "\n",
      "\n",
      "\tEpisode 2984 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5354],\n",
      "        [1.0000, 0.4654]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5132],\n",
      "        [1.0000, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528789520263672 \tStep Time:  0.005982160568237305 s \tTotal Time:  18.79755210876465 s \n",
      "\n",
      "\n",
      "\tEpisode 2985 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5685],\n",
      "        [1.0000, 0.5062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.4166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533691346645355 \tStep Time:  0.00598597526550293 s \tTotal Time:  18.804535627365112 s \n",
      "\n",
      "\n",
      "\tEpisode 2986 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3455],\n",
      "        [1.0000, 0.4967]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.4570]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448903441429138 \tStep Time:  0.0049839019775390625 s \tTotal Time:  18.80951952934265 s \n",
      "\n",
      "\n",
      "\tEpisode 2987 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5660],\n",
      "        [1.0000, 0.3603]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4321],\n",
      "        [1.0000, 0.4061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49304485321045 \tStep Time:  0.006949663162231445 s \tTotal Time:  18.816469192504883 s \n",
      "\n",
      "\n",
      "\tEpisode 2988 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5065],\n",
      "        [1.0000, 0.4441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5489],\n",
      "        [1.0000, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580466747283936 \tStep Time:  0.00601649284362793 s \tTotal Time:  18.82248568534851 s \n",
      "\n",
      "\n",
      "\tEpisode 2989 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3726],\n",
      "        [1.0000, 0.3893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6608],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.362907886505127 \tStep Time:  0.005986928939819336 s \tTotal Time:  18.82847261428833 s \n",
      "\n",
      "\n",
      "\tEpisode 2990 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5620],\n",
      "        [1.0000, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4910],\n",
      "        [1.0000, 0.6303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56631463766098 \tStep Time:  0.005949735641479492 s \tTotal Time:  18.83442234992981 s \n",
      "\n",
      "\n",
      "\tEpisode 2991 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4775],\n",
      "        [1.0000, 0.4819]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4303],\n",
      "        [1.0000, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46798849105835 \tStep Time:  0.007012128829956055 s \tTotal Time:  18.841434478759766 s \n",
      "\n",
      "\n",
      "\tEpisode 2992 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6426],\n",
      "        [1.0000, 0.5821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [1.0000, 0.4185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.682466983795166 \tStep Time:  0.0059528350830078125 s \tTotal Time:  18.847387313842773 s \n",
      "\n",
      "\n",
      "\tEpisode 2993 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3118],\n",
      "        [1.0000, 0.5354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4692],\n",
      "        [1.0000, 0.4987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473241329193115 \tStep Time:  0.0060155391693115234 s \tTotal Time:  18.853402853012085 s \n",
      "\n",
      "\n",
      "\tEpisode 2994 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5797],\n",
      "        [1.0000, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5530],\n",
      "        [1.0000, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526765823364258 \tStep Time:  0.005983829498291016 s \tTotal Time:  18.859386682510376 s \n",
      "\n",
      "\n",
      "\tEpisode 2995 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5278],\n",
      "        [1.0000, 0.4059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4323],\n",
      "        [1.0000, 0.5284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.410059928894043 \tStep Time:  0.006949901580810547 s \tTotal Time:  18.866336584091187 s \n",
      "\n",
      "\n",
      "\tEpisode 2996 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5062],\n",
      "        [1.0000, 0.5413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5407],\n",
      "        [1.0000, 0.5767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548954010009766 \tStep Time:  0.005983591079711914 s \tTotal Time:  18.8723201751709 s \n",
      "\n",
      "\n",
      "\tEpisode 2997 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3850],\n",
      "        [1.0000, 0.5599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5276],\n",
      "        [1.0000, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464263916015625 \tStep Time:  0.00598454475402832 s \tTotal Time:  18.878304719924927 s \n",
      "\n",
      "\n",
      "\tEpisode 2998 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5234],\n",
      "        [1.0000, 0.5232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3799],\n",
      "        [1.0000, 0.4303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544134974479675 \tStep Time:  0.007004976272583008 s \tTotal Time:  18.88530969619751 s \n",
      "\n",
      "\n",
      "\tEpisode 2999 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4481],\n",
      "        [1.0000, 0.5244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4704],\n",
      "        [1.0000, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515660762786865 \tStep Time:  0.0069577693939208984 s \tTotal Time:  18.89226746559143 s \n",
      "\n",
      "\n",
      "\tEpisode 3000 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5212],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3783],\n",
      "        [1.0000, 0.5068]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45172643661499 \tStep Time:  0.0069806575775146484 s \tTotal Time:  18.899248123168945 s \n",
      "\n",
      "\n",
      "\tEpisode 3001 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5094],\n",
      "        [1.0000, 0.4667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4539],\n",
      "        [1.0000, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468674004077911 \tStep Time:  0.005984306335449219 s \tTotal Time:  18.905232429504395 s \n",
      "\n",
      "\n",
      "\tEpisode 3002 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.5313]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4642],\n",
      "        [1.0000, 0.4484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504736423492432 \tStep Time:  0.005984067916870117 s \tTotal Time:  18.912213802337646 s \n",
      "\n",
      "\n",
      "\tEpisode 3003 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3501],\n",
      "        [1.0000, 0.5577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4493],\n",
      "        [1.0000, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.408568859100342 \tStep Time:  0.006981611251831055 s \tTotal Time:  18.919195413589478 s \n",
      "\n",
      "\n",
      "\tEpisode 3004 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5694],\n",
      "        [1.0000, 0.4185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542764663696289 \tStep Time:  0.00698089599609375 s \tTotal Time:  18.92617630958557 s \n",
      "\n",
      "\n",
      "\tEpisode 3005 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5316],\n",
      "        [1.0000, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5947],\n",
      "        [1.0000, 0.4392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597252607345581 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.933157682418823 s \n",
      "\n",
      "\n",
      "\tEpisode 3006 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3573],\n",
      "        [1.0000, 0.2681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3680],\n",
      "        [1.0000, 0.5752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.411989212036133 \tStep Time:  0.007978677749633789 s \tTotal Time:  18.941136360168457 s \n",
      "\n",
      "\n",
      "\tEpisode 3007 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3600],\n",
      "        [1.0000, 0.5183]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5024],\n",
      "        [1.0000, 0.5832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.640697002410889 \tStep Time:  0.0069811344146728516 s \tTotal Time:  18.94811749458313 s \n",
      "\n",
      "\n",
      "\tEpisode 3008 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5945],\n",
      "        [1.0000, 0.5911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4634],\n",
      "        [1.0000, 0.6032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468720436096191 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.955098867416382 s \n",
      "\n",
      "\n",
      "\tEpisode 3009 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6530],\n",
      "        [1.0000, 0.6030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6167],\n",
      "        [1.0000, 0.5533]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493393421173096 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.962080240249634 s \n",
      "\n",
      "\n",
      "\tEpisode 3010 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5719],\n",
      "        [1.0000, 0.2636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5655],\n",
      "        [1.0000, 0.5489]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.415637969970703 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.969061613082886 s \n",
      "\n",
      "\n",
      "\tEpisode 3011 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3634],\n",
      "        [1.0000, 0.5853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4919],\n",
      "        [1.0000, 0.6063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480039119720459 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.976042985916138 s \n",
      "\n",
      "\n",
      "\tEpisode 3012 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5535],\n",
      "        [1.0000, 0.4633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5300],\n",
      "        [1.0000, 0.5843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478256702423096 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.98302435874939 s \n",
      "\n",
      "\n",
      "\tEpisode 3013 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4136],\n",
      "        [1.0000, 0.4075]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.6016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576859951019287 \tStep Time:  0.007978439331054688 s \tTotal Time:  18.991002798080444 s \n",
      "\n",
      "\n",
      "\tEpisode 3014 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5417],\n",
      "        [1.0000, 0.5219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5532],\n",
      "        [1.0000, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511077880859375 \tStep Time:  0.006981372833251953 s \tTotal Time:  18.997984170913696 s \n",
      "\n",
      "\n",
      "\tEpisode 3015 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1591],\n",
      "        [1.0000, 0.5815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6063],\n",
      "        [1.0000, 0.5892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.376501083374023 \tStep Time:  0.006981849670410156 s \tTotal Time:  19.004966020584106 s \n",
      "\n",
      "\n",
      "\tEpisode 3016 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5792],\n",
      "        [1.0000, 0.3685]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5453],\n",
      "        [1.0000, 0.3813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554731845855713 \tStep Time:  0.005983591079711914 s \tTotal Time:  19.01094961166382 s \n",
      "\n",
      "\n",
      "\tEpisode 3017 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5209],\n",
      "        [1.0000, 0.4277]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4253],\n",
      "        [1.0000, 0.3700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447941303253174 \tStep Time:  0.005984306335449219 s \tTotal Time:  19.016933917999268 s \n",
      "\n",
      "\n",
      "\tEpisode 3018 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5809],\n",
      "        [1.0000, 0.5883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5608],\n",
      "        [1.0000, 0.2698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.694779396057129 \tStep Time:  0.00698089599609375 s \tTotal Time:  19.02391481399536 s \n",
      "\n",
      "\n",
      "\tEpisode 3019 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5814],\n",
      "        [1.0000, 0.5740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5391],\n",
      "        [1.0000, 0.4755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558939516544342 \tStep Time:  0.005983829498291016 s \tTotal Time:  19.029898643493652 s \n",
      "\n",
      "\n",
      "\tEpisode 3020 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4160],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.3136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60513025522232 \tStep Time:  0.006051063537597656 s \tTotal Time:  19.03594970703125 s \n",
      "\n",
      "\n",
      "\tEpisode 3021 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5200],\n",
      "        [1.0000, 0.5303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5427],\n",
      "        [1.0000, 0.2124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.7078857421875 \tStep Time:  0.006914377212524414 s \tTotal Time:  19.042864084243774 s \n",
      "\n",
      "\n",
      "\tEpisode 3022 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3420],\n",
      "        [1.0000, 0.3991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50133991241455 \tStep Time:  0.004986763000488281 s \tTotal Time:  19.047850847244263 s \n",
      "\n",
      "\n",
      "\tEpisode 3023 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5353],\n",
      "        [1.0000, 0.5416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4973],\n",
      "        [1.0000, 0.5361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496533870697021 \tStep Time:  0.005984067916870117 s \tTotal Time:  19.054832220077515 s \n",
      "\n",
      "\n",
      "\tEpisode 3024 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5024],\n",
      "        [1.0000, 0.5387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519548892974854 \tStep Time:  0.005983829498291016 s \tTotal Time:  19.060816049575806 s \n",
      "\n",
      "\n",
      "\tEpisode 3025 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5471]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5460],\n",
      "        [1.0000, 0.4847]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57813835144043 \tStep Time:  0.005984306335449219 s \tTotal Time:  19.066800355911255 s \n",
      "\n",
      "\n",
      "\tEpisode 3026 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5557],\n",
      "        [1.0000, 0.4004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5584],\n",
      "        [1.0000, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47447681427002 \tStep Time:  0.0069811344146728516 s \tTotal Time:  19.073781490325928 s \n",
      "\n",
      "\n",
      "\tEpisode 3027 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4535],\n",
      "        [1.0000, 0.5360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5372],\n",
      "        [1.0000, 0.5460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474775433540344 \tStep Time:  0.00498652458190918 s \tTotal Time:  19.078768014907837 s \n",
      "\n",
      "\n",
      "\tEpisode 3028 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5268],\n",
      "        [1.0000, 0.5638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5337],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531514286994934 \tStep Time:  0.007979393005371094 s \tTotal Time:  19.086747407913208 s \n",
      "\n",
      "\n",
      "\tEpisode 3029 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5209],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5239],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542675018310547 \tStep Time:  0.006981611251831055 s \tTotal Time:  19.09372901916504 s \n",
      "\n",
      "\n",
      "\tEpisode 3030 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [1.0000, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4911],\n",
      "        [1.0000, 0.5298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51811695098877 \tStep Time:  0.009975433349609375 s \tTotal Time:  19.10370445251465 s \n",
      "\n",
      "\n",
      "\tEpisode 3031 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4745],\n",
      "        [1.0000, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5250],\n",
      "        [1.0000, 0.4178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4837007522583 \tStep Time:  0.00997018814086914 s \tTotal Time:  19.113674640655518 s \n",
      "\n",
      "\n",
      "\tEpisode 3032 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4098],\n",
      "        [1.0000, 0.5412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4985],\n",
      "        [1.0000, 0.3181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.600078105926514 \tStep Time:  0.00897669792175293 s \tTotal Time:  19.12265133857727 s \n",
      "\n",
      "\n",
      "\tEpisode 3033 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5265],\n",
      "        [1.0000, 0.5265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4796],\n",
      "        [1.0000, 0.5177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537786483764648 \tStep Time:  0.0069828033447265625 s \tTotal Time:  19.129634141921997 s \n",
      "\n",
      "\n",
      "\tEpisode 3034 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5164],\n",
      "        [1.0000, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.4698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507908344268799 \tStep Time:  0.0069806575775146484 s \tTotal Time:  19.13661479949951 s \n",
      "\n",
      "\n",
      "\tEpisode 3035 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5136],\n",
      "        [1.0000, 0.5192]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [1.0000, 0.5163]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5343337059021 \tStep Time:  0.006980180740356445 s \tTotal Time:  19.143594980239868 s \n",
      "\n",
      "\n",
      "\tEpisode 3036 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5107],\n",
      "        [1.0000, 0.5249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4368],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476558208465576 \tStep Time:  0.005983829498291016 s \tTotal Time:  19.14957880973816 s \n",
      "\n",
      "\n",
      "\tEpisode 3037 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.5261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5355],\n",
      "        [1.0000, 0.5348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528142929077148 \tStep Time:  0.006998538970947266 s \tTotal Time:  19.156577348709106 s \n",
      "\n",
      "\n",
      "\tEpisode 3038 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5135],\n",
      "        [1.0000, 0.5269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5322],\n",
      "        [1.0000, 0.5324]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495347499847412 \tStep Time:  0.006418466567993164 s \tTotal Time:  19.1629958152771 s \n",
      "\n",
      "\n",
      "\tEpisode 3039 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4040],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464775562286377 \tStep Time:  0.0069828033447265625 s \tTotal Time:  19.169978618621826 s \n",
      "\n",
      "\n",
      "\tEpisode 3040 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.5456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5588],\n",
      "        [1.0000, 0.5518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535450100898743 \tStep Time:  0.007146596908569336 s \tTotal Time:  19.177125215530396 s \n",
      "\n",
      "\n",
      "\tEpisode 3041 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5278],\n",
      "        [1.0000, 0.5482]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4710],\n",
      "        [1.0000, 0.5483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559404850006104 \tStep Time:  0.006982564926147461 s \tTotal Time:  19.184107780456543 s \n",
      "\n",
      "\n",
      "\tEpisode 3042 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.4110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5244],\n",
      "        [1.0000, 0.4597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549131393432617 \tStep Time:  0.007976770401000977 s \tTotal Time:  19.192084550857544 s \n",
      "\n",
      "\n",
      "\tEpisode 3043 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.3774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4940],\n",
      "        [1.0000, 0.3787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522703647613525 \tStep Time:  0.006982088088989258 s \tTotal Time:  19.199066638946533 s \n",
      "\n",
      "\n",
      "\tEpisode 3044 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5147],\n",
      "        [1.0000, 0.4039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4613],\n",
      "        [1.0000, 0.3748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.618093490600586 \tStep Time:  0.007978439331054688 s \tTotal Time:  19.207045078277588 s \n",
      "\n",
      "\n",
      "\tEpisode 3045 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [1.0000, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4295],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550344467163086 \tStep Time:  0.005983829498291016 s \tTotal Time:  19.21302890777588 s \n",
      "\n",
      "\n",
      "\tEpisode 3046 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4256],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5065],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509929656982422 \tStep Time:  0.006981372833251953 s \tTotal Time:  19.22001028060913 s \n",
      "\n",
      "\n",
      "\tEpisode 3047 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4945],\n",
      "        [1.0000, 0.5226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508662223815918 \tStep Time:  0.006983041763305664 s \tTotal Time:  19.226993322372437 s \n",
      "\n",
      "\n",
      "\tEpisode 3048 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.4635]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4816],\n",
      "        [1.0000, 0.5178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504788756370544 \tStep Time:  0.005981922149658203 s \tTotal Time:  19.232975244522095 s \n",
      "\n",
      "\n",
      "\tEpisode 3049 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489377975463867 \tStep Time:  0.006981849670410156 s \tTotal Time:  19.239957094192505 s \n",
      "\n",
      "\n",
      "\tEpisode 3050 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4901],\n",
      "        [1.0000, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5210],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526394009590149 \tStep Time:  0.007012844085693359 s \tTotal Time:  19.2469699382782 s \n",
      "\n",
      "\n",
      "\tEpisode 3051 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.5335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5225],\n",
      "        [1.0000, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519660949707031 \tStep Time:  0.006986141204833984 s \tTotal Time:  19.253956079483032 s \n",
      "\n",
      "\n",
      "\tEpisode 3052 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6056],\n",
      "        [1.0000, 0.5310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4623],\n",
      "        [1.0000, 0.5340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510647296905518 \tStep Time:  0.005979299545288086 s \tTotal Time:  19.25993537902832 s \n",
      "\n",
      "\n",
      "\tEpisode 3053 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5298],\n",
      "        [1.0000, 0.4278]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3507],\n",
      "        [1.0000, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56701374053955 \tStep Time:  0.007946968078613281 s \tTotal Time:  19.267882347106934 s \n",
      "\n",
      "\n",
      "\tEpisode 3054 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.4643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5302],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482975006103516 \tStep Time:  0.00797724723815918 s \tTotal Time:  19.275859594345093 s \n",
      "\n",
      "\n",
      "\tEpisode 3055 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4866],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5127],\n",
      "        [1.0000, 0.4062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549983203411102 \tStep Time:  0.005984067916870117 s \tTotal Time:  19.281843662261963 s \n",
      "\n",
      "\n",
      "\tEpisode 3056 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5283],\n",
      "        [1.0000, 0.5399]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4950],\n",
      "        [1.0000, 0.4689]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561729907989502 \tStep Time:  0.007978677749633789 s \tTotal Time:  19.290820121765137 s \n",
      "\n",
      "\n",
      "\tEpisode 3057 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5132],\n",
      "        [1.0000, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5309],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52377861738205 \tStep Time:  0.008975982666015625 s \tTotal Time:  19.299796104431152 s \n",
      "\n",
      "\n",
      "\tEpisode 3058 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5058],\n",
      "        [1.0000, 0.5302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52118694782257 \tStep Time:  0.00797891616821289 s \tTotal Time:  19.307775020599365 s \n",
      "\n",
      "\n",
      "\tEpisode 3059 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5198],\n",
      "        [1.0000, 0.5225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5331],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495997428894043 \tStep Time:  0.007981538772583008 s \tTotal Time:  19.31575655937195 s \n",
      "\n",
      "\n",
      "\tEpisode 3060 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5006],\n",
      "        [1.0000, 0.4735]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5255],\n",
      "        [1.0000, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516236782073975 \tStep Time:  0.008011817932128906 s \tTotal Time:  19.323768377304077 s \n",
      "\n",
      "\n",
      "\tEpisode 3061 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5240],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5208],\n",
      "        [1.0000, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515371799468994 \tStep Time:  0.006944894790649414 s \tTotal Time:  19.330713272094727 s \n",
      "\n",
      "\n",
      "\tEpisode 3062 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4512],\n",
      "        [1.0000, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4467],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499695777893066 \tStep Time:  0.00900888442993164 s \tTotal Time:  19.339722156524658 s \n",
      "\n",
      "\n",
      "\tEpisode 3063 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4884],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530280590057373 \tStep Time:  0.0069789886474609375 s \tTotal Time:  19.34670114517212 s \n",
      "\n",
      "\n",
      "\tEpisode 3064 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4754],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5044],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496156454086304 \tStep Time:  0.00698399543762207 s \tTotal Time:  19.35368514060974 s \n",
      "\n",
      "\n",
      "\tEpisode 3065 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4949],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5100],\n",
      "        [1.0000, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511139869689941 \tStep Time:  0.007020711898803711 s \tTotal Time:  19.360705852508545 s \n",
      "\n",
      "\n",
      "\tEpisode 3066 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5127],\n",
      "        [1.0000, 0.5036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4684],\n",
      "        [1.0000, 0.4338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4830482006073 \tStep Time:  0.006981611251831055 s \tTotal Time:  19.367687463760376 s \n",
      "\n",
      "\n",
      "\tEpisode 3067 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5225],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5237],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504675686359406 \tStep Time:  0.007056474685668945 s \tTotal Time:  19.374743938446045 s \n",
      "\n",
      "\n",
      "\tEpisode 3068 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4873],\n",
      "        [1.0000, 0.4984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4092],\n",
      "        [1.0000, 0.4535]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478686153888702 \tStep Time:  0.005983829498291016 s \tTotal Time:  19.380727767944336 s \n",
      "\n",
      "\n",
      "\tEpisode 3069 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4828],\n",
      "        [1.0000, 0.5346]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.4833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466166496276855 \tStep Time:  0.007978200912475586 s \tTotal Time:  19.38870596885681 s \n",
      "\n",
      "\n",
      "\tEpisode 3070 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5354],\n",
      "        [1.0000, 0.5298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49040812253952 \tStep Time:  0.006981611251831055 s \tTotal Time:  19.395687580108643 s \n",
      "\n",
      "\n",
      "\tEpisode 3071 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5505],\n",
      "        [1.0000, 0.4567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5255],\n",
      "        [1.0000, 0.5410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54354476928711 \tStep Time:  0.006982326507568359 s \tTotal Time:  19.40266990661621 s \n",
      "\n",
      "\n",
      "\tEpisode 3072 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3741],\n",
      "        [1.0000, 0.5500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5244],\n",
      "        [1.0000, 0.5776]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.603408336639404 \tStep Time:  0.006980180740356445 s \tTotal Time:  19.409650087356567 s \n",
      "\n",
      "\n",
      "\tEpisode 3073 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4935],\n",
      "        [1.0000, 0.5471]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.4823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447761535644531 \tStep Time:  0.00698399543762207 s \tTotal Time:  19.41663408279419 s \n",
      "\n",
      "\n",
      "\tEpisode 3074 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5129],\n",
      "        [1.0000, 0.5597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4616],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575992107391357 \tStep Time:  0.0069789886474609375 s \tTotal Time:  19.42361307144165 s \n",
      "\n",
      "\n",
      "\tEpisode 3075 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3848],\n",
      "        [1.0000, 0.5453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5271],\n",
      "        [1.0000, 0.5450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581576824188232 \tStep Time:  0.00698089599609375 s \tTotal Time:  19.430593967437744 s \n",
      "\n",
      "\n",
      "\tEpisode 3076 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4484],\n",
      "        [1.0000, 0.5209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543621063232422 \tStep Time:  0.0069849491119384766 s \tTotal Time:  19.437578916549683 s \n",
      "\n",
      "\n",
      "\tEpisode 3077 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5406],\n",
      "        [1.0000, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5419],\n",
      "        [1.0000, 0.5244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506566047668457 \tStep Time:  0.0069773197174072266 s \tTotal Time:  19.44455623626709 s \n",
      "\n",
      "\n",
      "\tEpisode 3078 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5307],\n",
      "        [1.0000, 0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5374],\n",
      "        [1.0000, 0.5340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48659372329712 \tStep Time:  0.006982088088989258 s \tTotal Time:  19.45153832435608 s \n",
      "\n",
      "\n",
      "\tEpisode 3079 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5353],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5379],\n",
      "        [1.0000, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452160835266113 \tStep Time:  0.00794529914855957 s \tTotal Time:  19.45948362350464 s \n",
      "\n",
      "\n",
      "\tEpisode 3080 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4508],\n",
      "        [1.0000, 0.5386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5213],\n",
      "        [1.0000, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543985724449158 \tStep Time:  0.007014274597167969 s \tTotal Time:  19.466497898101807 s \n",
      "\n",
      "\n",
      "\tEpisode 3081 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4316],\n",
      "        [1.0000, 0.5547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5679],\n",
      "        [1.0000, 0.4573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52868366241455 \tStep Time:  0.006949663162231445 s \tTotal Time:  19.473447561264038 s \n",
      "\n",
      "\n",
      "\tEpisode 3082 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5419],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.4577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480090975761414 \tStep Time:  0.007978200912475586 s \tTotal Time:  19.481425762176514 s \n",
      "\n",
      "\n",
      "\tEpisode 3083 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5713],\n",
      "        [1.0000, 0.5440]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5505],\n",
      "        [1.0000, 0.5209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5385160446167 \tStep Time:  0.006982088088989258 s \tTotal Time:  19.488407850265503 s \n",
      "\n",
      "\n",
      "\tEpisode 3084 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5173],\n",
      "        [1.0000, 0.5330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5380],\n",
      "        [1.0000, 0.5649]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537960052490234 \tStep Time:  0.010970592498779297 s \tTotal Time:  19.500375270843506 s \n",
      "\n",
      "\n",
      "\tEpisode 3085 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4446],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5028],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495213031768799 \tStep Time:  0.01196742057800293 s \tTotal Time:  19.51234269142151 s \n",
      "\n",
      "\n",
      "\tEpisode 3086 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5331],\n",
      "        [1.0000, 0.5549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5137],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500513076782227 \tStep Time:  0.009973287582397461 s \tTotal Time:  19.523313760757446 s \n",
      "\n",
      "\n",
      "\tEpisode 3087 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5516],\n",
      "        [1.0000, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5423],\n",
      "        [1.0000, 0.5296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482041954994202 \tStep Time:  0.008977651596069336 s \tTotal Time:  19.532291412353516 s \n",
      "\n",
      "\n",
      "\tEpisode 3088 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4900],\n",
      "        [1.0000, 0.5532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4294],\n",
      "        [1.0000, 0.5269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551655292510986 \tStep Time:  0.012005805969238281 s \tTotal Time:  19.544297218322754 s \n",
      "\n",
      "\n",
      "\tEpisode 3089 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5143],\n",
      "        [1.0000, 0.4821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5010],\n",
      "        [1.0000, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518112540245056 \tStep Time:  0.007984161376953125 s \tTotal Time:  19.552281379699707 s \n",
      "\n",
      "\n",
      "\tEpisode 3090 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4878],\n",
      "        [1.0000, 0.4270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532060146331787 \tStep Time:  0.006967067718505859 s \tTotal Time:  19.559248447418213 s \n",
      "\n",
      "\n",
      "\tEpisode 3091 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4642],\n",
      "        [1.0000, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4943],\n",
      "        [1.0000, 0.4702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492996215820312 \tStep Time:  0.00598907470703125 s \tTotal Time:  19.565237522125244 s \n",
      "\n",
      "\n",
      "\tEpisode 3092 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4906],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4914],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500455379486084 \tStep Time:  0.007012128829956055 s \tTotal Time:  19.5722496509552 s \n",
      "\n",
      "\n",
      "\tEpisode 3093 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5030],\n",
      "        [1.0000, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500438690185547 \tStep Time:  0.006019115447998047 s \tTotal Time:  19.5782687664032 s \n",
      "\n",
      "\n",
      "\tEpisode 3094 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4682],\n",
      "        [1.0000, 0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508256435394287 \tStep Time:  0.0072021484375 s \tTotal Time:  19.5854709148407 s \n",
      "\n",
      "\n",
      "\tEpisode 3095 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4988],\n",
      "        [1.0000, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5007],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499183177947998 \tStep Time:  0.005835771560668945 s \tTotal Time:  19.591306686401367 s \n",
      "\n",
      "\n",
      "\tEpisode 3096 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4650],\n",
      "        [1.0000, 0.4834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4735],\n",
      "        [1.0000, 0.4589]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486120522022247 \tStep Time:  0.005984067916870117 s \tTotal Time:  19.598286151885986 s \n",
      "\n",
      "\n",
      "\tEpisode 3097 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5002],\n",
      "        [1.0000, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4876],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498595237731934 \tStep Time:  0.0069501399993896484 s \tTotal Time:  19.605236291885376 s \n",
      "\n",
      "\n",
      "\tEpisode 3098 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4862],\n",
      "        [1.0000, 0.4572]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5029],\n",
      "        [1.0000, 0.4571]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51088011264801 \tStep Time:  0.006015300750732422 s \tTotal Time:  19.61125159263611 s \n",
      "\n",
      "\n",
      "\tEpisode 3099 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4785],\n",
      "        [1.0000, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [1.0000, 0.4472]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527812480926514 \tStep Time:  0.0070018768310546875 s \tTotal Time:  19.618253469467163 s \n",
      "\n",
      "\n",
      "\tEpisode 3100 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4979],\n",
      "        [1.0000, 0.4894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4405],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477965831756592 \tStep Time:  0.00596308708190918 s \tTotal Time:  19.624216556549072 s \n",
      "\n",
      "\n",
      "\tEpisode 3101 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4703],\n",
      "        [1.0000, 0.4761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514411926269531 \tStep Time:  0.005984306335449219 s \tTotal Time:  19.63020086288452 s \n",
      "\n",
      "\n",
      "\tEpisode 3102 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5054],\n",
      "        [1.0000, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4379],\n",
      "        [1.0000, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473202228546143 \tStep Time:  0.006984710693359375 s \tTotal Time:  19.63718557357788 s \n",
      "\n",
      "\n",
      "\tEpisode 3103 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.5303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473311424255371 \tStep Time:  0.0059812068939208984 s \tTotal Time:  19.6431667804718 s \n",
      "\n",
      "\n",
      "\tEpisode 3104 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.4489]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5447],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464081287384033 \tStep Time:  0.005983114242553711 s \tTotal Time:  19.649149894714355 s \n",
      "\n",
      "\n",
      "\tEpisode 3105 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5103],\n",
      "        [1.0000, 0.4756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5872],\n",
      "        [1.0000, 0.5512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506670117378235 \tStep Time:  0.006981849670410156 s \tTotal Time:  19.656131744384766 s \n",
      "\n",
      "\n",
      "\tEpisode 3106 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5049],\n",
      "        [1.0000, 0.5777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5585],\n",
      "        [1.0000, 0.5974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.454854488372803 \tStep Time:  0.005983829498291016 s \tTotal Time:  19.662115573883057 s \n",
      "\n",
      "\n",
      "\tEpisode 3107 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5753],\n",
      "        [1.0000, 0.4408]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5535],\n",
      "        [1.0000, 0.4808]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476319313049316 \tStep Time:  0.0069844722747802734 s \tTotal Time:  19.669100046157837 s \n",
      "\n",
      "\n",
      "\tEpisode 3108 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5527],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5185],\n",
      "        [1.0000, 0.4456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468450546264648 \tStep Time:  0.0059816837310791016 s \tTotal Time:  19.675081729888916 s \n",
      "\n",
      "\n",
      "\tEpisode 3109 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6712],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3982],\n",
      "        [1.0000, 0.5831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541194796562195 \tStep Time:  0.007946014404296875 s \tTotal Time:  19.683027744293213 s \n",
      "\n",
      "\n",
      "\tEpisode 3110 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4804],\n",
      "        [1.0000, 0.4263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5488],\n",
      "        [1.0000, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591924667358398 \tStep Time:  0.006982088088989258 s \tTotal Time:  19.691007375717163 s \n",
      "\n",
      "\n",
      "\tEpisode 3111 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3895],\n",
      "        [1.0000, 0.5635]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6225],\n",
      "        [1.0000, 0.3298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539946556091309 \tStep Time:  0.006980419158935547 s \tTotal Time:  19.6979877948761 s \n",
      "\n",
      "\n",
      "\tEpisode 3112 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5876],\n",
      "        [1.0000, 0.4902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3903],\n",
      "        [1.0000, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504614472389221 \tStep Time:  0.0069811344146728516 s \tTotal Time:  19.70496892929077 s \n",
      "\n",
      "\n",
      "\tEpisode 3113 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4770],\n",
      "        [1.0000, 0.5905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.6141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511596322059631 \tStep Time:  0.006981372833251953 s \tTotal Time:  19.711950302124023 s \n",
      "\n",
      "\n",
      "\tEpisode 3114 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5325],\n",
      "        [1.0000, 0.5419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1881],\n",
      "        [1.0000, 0.3872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466498851776123 \tStep Time:  0.008976459503173828 s \tTotal Time:  19.72192406654358 s \n",
      "\n",
      "\n",
      "\tEpisode 3115 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4539],\n",
      "        [1.0000, 0.6162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5966],\n",
      "        [1.0000, 0.4624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.67661726474762 \tStep Time:  0.007977962493896484 s \tTotal Time:  19.729902029037476 s \n",
      "\n",
      "\n",
      "\tEpisode 3116 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3790],\n",
      "        [1.0000, 0.3902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6695],\n",
      "        [1.0000, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62291818857193 \tStep Time:  0.006983041763305664 s \tTotal Time:  19.73688507080078 s \n",
      "\n",
      "\n",
      "\tEpisode 3117 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4621],\n",
      "        [1.0000, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5111],\n",
      "        [1.0000, 0.3723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44055461883545 \tStep Time:  0.006980419158935547 s \tTotal Time:  19.743865489959717 s \n",
      "\n",
      "\n",
      "\tEpisode 3118 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4817],\n",
      "        [1.0000, 0.4469]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5574],\n",
      "        [1.0000, 0.4151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440764665603638 \tStep Time:  0.006982564926147461 s \tTotal Time:  19.750848054885864 s \n",
      "\n",
      "\n",
      "\tEpisode 3119 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5472],\n",
      "        [1.0000, 0.4642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5553],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533864974975586 \tStep Time:  0.006979703903198242 s \tTotal Time:  19.758824586868286 s \n",
      "\n",
      "\n",
      "\tEpisode 3120 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4887],\n",
      "        [1.0000, 0.5434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5778],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539454936981201 \tStep Time:  0.006981849670410156 s \tTotal Time:  19.765806436538696 s \n",
      "\n",
      "\n",
      "\tEpisode 3121 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5579],\n",
      "        [1.0000, 0.4759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5602],\n",
      "        [1.0000, 0.5958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469157338142395 \tStep Time:  0.007978677749633789 s \tTotal Time:  19.77378511428833 s \n",
      "\n",
      "\n",
      "\tEpisode 3122 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5963],\n",
      "        [1.0000, 0.2958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.5690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.350539684295654 \tStep Time:  0.007978677749633789 s \tTotal Time:  19.781763792037964 s \n",
      "\n",
      "\n",
      "\tEpisode 3123 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.5010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4333],\n",
      "        [1.0000, 0.3565]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.640149116516113 \tStep Time:  0.007979393005371094 s \tTotal Time:  19.789743185043335 s \n",
      "\n",
      "\n",
      "\tEpisode 3124 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3310],\n",
      "        [1.0000, 0.4719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4038],\n",
      "        [1.0000, 0.5728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.382611274719238 \tStep Time:  0.006983041763305664 s \tTotal Time:  19.79672622680664 s \n",
      "\n",
      "\n",
      "\tEpisode 3125 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5061],\n",
      "        [1.0000, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5352],\n",
      "        [1.0000, 0.4707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475553691387177 \tStep Time:  0.007976055145263672 s \tTotal Time:  19.804702281951904 s \n",
      "\n",
      "\n",
      "\tEpisode 3126 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3372],\n",
      "        [1.0000, 0.6382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4274],\n",
      "        [1.0000, 0.4142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39895212650299 \tStep Time:  0.006982326507568359 s \tTotal Time:  19.811684608459473 s \n",
      "\n",
      "\n",
      "\tEpisode 3127 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5679],\n",
      "        [1.0000, 0.4518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3052],\n",
      "        [1.0000, 0.4633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66294002532959 \tStep Time:  0.007977962493896484 s \tTotal Time:  19.81966257095337 s \n",
      "\n",
      "\n",
      "\tEpisode 3128 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5768],\n",
      "        [1.0000, 0.5428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4554],\n",
      "        [1.0000, 0.5404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495704650878906 \tStep Time:  0.00698089599609375 s \tTotal Time:  19.826643466949463 s \n",
      "\n",
      "\n",
      "\tEpisode 3129 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3776],\n",
      "        [1.0000, 0.5026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5233],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452804565429688 \tStep Time:  0.007981300354003906 s \tTotal Time:  19.834624767303467 s \n",
      "\n",
      "\n",
      "\tEpisode 3130 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6137],\n",
      "        [1.0000, 0.3589]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6085],\n",
      "        [1.0000, 0.5487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63640546798706 \tStep Time:  0.006978511810302734 s \tTotal Time:  19.84160327911377 s \n",
      "\n",
      "\n",
      "\tEpisode 3131 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4625],\n",
      "        [1.0000, 0.6338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7651],\n",
      "        [1.0000, 0.1523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  14.009640455245972 \tStep Time:  0.005984067916870117 s \tTotal Time:  19.84758734703064 s \n",
      "\n",
      "\n",
      "\tEpisode 3132 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4130],\n",
      "        [1.0000, 0.5474]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4481],\n",
      "        [1.0000, 0.4851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569194793701172 \tStep Time:  0.006981611251831055 s \tTotal Time:  19.85456895828247 s \n",
      "\n",
      "\n",
      "\tEpisode 3133 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5969],\n",
      "        [1.0000, 0.5550]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.5495]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519575119018555 \tStep Time:  0.0069811344146728516 s \tTotal Time:  19.861550092697144 s \n",
      "\n",
      "\n",
      "\tEpisode 3134 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5820],\n",
      "        [1.0000, 0.5055]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4807],\n",
      "        [1.0000, 0.4656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595448017120361 \tStep Time:  0.006981611251831055 s \tTotal Time:  19.868531703948975 s \n",
      "\n",
      "\n",
      "\tEpisode 3135 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.4250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5506],\n",
      "        [1.0000, 0.5283]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433552265167236 \tStep Time:  0.0069811344146728516 s \tTotal Time:  19.875512838363647 s \n",
      "\n",
      "\n",
      "\tEpisode 3136 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3464],\n",
      "        [1.0000, 0.5387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5086],\n",
      "        [1.0000, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593717575073242 \tStep Time:  0.007979631423950195 s \tTotal Time:  19.88448977470398 s \n",
      "\n",
      "\n",
      "\tEpisode 3137 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4886],\n",
      "        [1.0000, 0.2661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.4995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.423169136047363 \tStep Time:  0.007978439331054688 s \tTotal Time:  19.892468214035034 s \n",
      "\n",
      "\n",
      "\tEpisode 3138 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5422],\n",
      "        [1.0000, 0.4744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5185],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542165756225586 \tStep Time:  0.007978677749633789 s \tTotal Time:  19.900446891784668 s \n",
      "\n",
      "\n",
      "\tEpisode 3139 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5386],\n",
      "        [1.0000, 0.5393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.4890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48391342163086 \tStep Time:  0.008975982666015625 s \tTotal Time:  19.909422874450684 s \n",
      "\n",
      "\n",
      "\tEpisode 3140 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4579],\n",
      "        [1.0000, 0.4821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5391],\n",
      "        [1.0000, 0.5416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50130319595337 \tStep Time:  0.007978200912475586 s \tTotal Time:  19.91740107536316 s \n",
      "\n",
      "\n",
      "\tEpisode 3141 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4366],\n",
      "        [1.0000, 0.5199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5598],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566884756088257 \tStep Time:  0.008011817932128906 s \tTotal Time:  19.925412893295288 s \n",
      "\n",
      "\n",
      "\tEpisode 3142 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5389],\n",
      "        [1.0000, 0.4568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5309],\n",
      "        [1.0000, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483938217163086 \tStep Time:  0.007946968078613281 s \tTotal Time:  19.9333598613739 s \n",
      "\n",
      "\n",
      "\tEpisode 3143 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.4938]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5142],\n",
      "        [1.0000, 0.5250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535177528858185 \tStep Time:  0.011968851089477539 s \tTotal Time:  19.946324825286865 s \n",
      "\n",
      "\n",
      "\tEpisode 3144 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5056],\n",
      "        [1.0000, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5187],\n",
      "        [1.0000, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501489639282227 \tStep Time:  0.007979393005371094 s \tTotal Time:  19.954304218292236 s \n",
      "\n",
      "\n",
      "\tEpisode 3145 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5048],\n",
      "        [1.0000, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52127456665039 \tStep Time:  0.00598597526550293 s \tTotal Time:  19.961287021636963 s \n",
      "\n",
      "\n",
      "\tEpisode 3146 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.4676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5219],\n",
      "        [1.0000, 0.4323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493077754974365 \tStep Time:  0.006978750228881836 s \tTotal Time:  19.968265771865845 s \n",
      "\n",
      "\n",
      "\tEpisode 3147 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5070],\n",
      "        [1.0000, 0.4907]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513675689697266 \tStep Time:  0.0070002079010009766 s \tTotal Time:  19.975265979766846 s \n",
      "\n",
      "\n",
      "\tEpisode 3148 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5176],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516439139842987 \tStep Time:  0.0070073604583740234 s \tTotal Time:  19.98227334022522 s \n",
      "\n",
      "\n",
      "\tEpisode 3149 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5100],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5190],\n",
      "        [1.0000, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499876022338867 \tStep Time:  0.0069506168365478516 s \tTotal Time:  19.989223957061768 s \n",
      "\n",
      "\n",
      "\tEpisode 3150 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [1.0000, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5228],\n",
      "        [1.0000, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509514808654785 \tStep Time:  0.0069849491119384766 s \tTotal Time:  19.996208906173706 s \n",
      "\n",
      "\n",
      "\tEpisode 3151 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5244],\n",
      "        [1.0000, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5080],\n",
      "        [1.0000, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510284900665283 \tStep Time:  0.007013797760009766 s \tTotal Time:  20.003222703933716 s \n",
      "\n",
      "\n",
      "\tEpisode 3152 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5075],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482240676879883 \tStep Time:  0.006980419158935547 s \tTotal Time:  20.01020312309265 s \n",
      "\n",
      "\n",
      "\tEpisode 3153 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5078],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5467],\n",
      "        [1.0000, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521933436393738 \tStep Time:  0.0059854984283447266 s \tTotal Time:  20.016188621520996 s \n",
      "\n",
      "\n",
      "\tEpisode 3154 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4974],\n",
      "        [1.0000, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5381],\n",
      "        [1.0000, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519703388214111 \tStep Time:  0.007017374038696289 s \tTotal Time:  20.023205995559692 s \n",
      "\n",
      "\n",
      "\tEpisode 3155 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5342],\n",
      "        [1.0000, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483529269695282 \tStep Time:  0.005983829498291016 s \tTotal Time:  20.029189825057983 s \n",
      "\n",
      "\n",
      "\tEpisode 3156 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.4748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5056],\n",
      "        [1.0000, 0.4788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52023983001709 \tStep Time:  0.006983518600463867 s \tTotal Time:  20.036173343658447 s \n",
      "\n",
      "\n",
      "\tEpisode 3157 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5159],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4417],\n",
      "        [1.0000, 0.4981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548545360565186 \tStep Time:  0.006980419158935547 s \tTotal Time:  20.043153762817383 s \n",
      "\n",
      "\n",
      "\tEpisode 3158 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5108],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4738],\n",
      "        [1.0000, 0.4630]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530715942382812 \tStep Time:  0.005951642990112305 s \tTotal Time:  20.049105405807495 s \n",
      "\n",
      "\n",
      "\tEpisode 3159 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4855],\n",
      "        [1.0000, 0.4840]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4821],\n",
      "        [1.0000, 0.4924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497014045715332 \tStep Time:  0.007015228271484375 s \tTotal Time:  20.05612063407898 s \n",
      "\n",
      "\n",
      "\tEpisode 3160 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4924],\n",
      "        [1.0000, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4924],\n",
      "        [1.0000, 0.4575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483716487884521 \tStep Time:  0.005981922149658203 s \tTotal Time:  20.062102556228638 s \n",
      "\n",
      "\n",
      "\tEpisode 3161 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4944],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5002],\n",
      "        [1.0000, 0.4930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494020462036133 \tStep Time:  0.006981372833251953 s \tTotal Time:  20.06908392906189 s \n",
      "\n",
      "\n",
      "\tEpisode 3162 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4958],\n",
      "        [1.0000, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4922],\n",
      "        [1.0000, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504003524780273 \tStep Time:  0.00598454475402832 s \tTotal Time:  20.075068473815918 s \n",
      "\n",
      "\n",
      "\tEpisode 3163 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5041],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502722263336182 \tStep Time:  0.005982875823974609 s \tTotal Time:  20.081051349639893 s \n",
      "\n",
      "\n",
      "\tEpisode 3164 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4781],\n",
      "        [1.0000, 0.3568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5003],\n",
      "        [1.0000, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5685373544693 \tStep Time:  0.007946491241455078 s \tTotal Time:  20.088997840881348 s \n",
      "\n",
      "\n",
      "\tEpisode 3165 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.5109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4493],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550971984863281 \tStep Time:  0.006015777587890625 s \tTotal Time:  20.09501361846924 s \n",
      "\n",
      "\n",
      "\tEpisode 3166 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4601],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5127],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546897888183594 \tStep Time:  0.005952119827270508 s \tTotal Time:  20.10096573829651 s \n",
      "\n",
      "\n",
      "\tEpisode 3167 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4995],\n",
      "        [1.0000, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.5108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527650356292725 \tStep Time:  0.008009910583496094 s \tTotal Time:  20.108975648880005 s \n",
      "\n",
      "\n",
      "\tEpisode 3168 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5210],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5362],\n",
      "        [1.0000, 0.5249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5139799118042 \tStep Time:  0.005985260009765625 s \tTotal Time:  20.11496090888977 s \n",
      "\n",
      "\n",
      "\tEpisode 3169 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508527517318726 \tStep Time:  0.005983114242553711 s \tTotal Time:  20.120944023132324 s \n",
      "\n",
      "\n",
      "\tEpisode 3170 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5212],\n",
      "        [1.0000, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5070],\n",
      "        [1.0000, 0.5380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505992412567139 \tStep Time:  0.005987405776977539 s \tTotal Time:  20.1269314289093 s \n",
      "\n",
      "\n",
      "\tEpisode 3171 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4667],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.5388]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548043251037598 \tStep Time:  0.006978511810302734 s \tTotal Time:  20.133909940719604 s \n",
      "\n",
      "\n",
      "\tEpisode 3172 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5289],\n",
      "        [1.0000, 0.4973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5136],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5092191696167 \tStep Time:  0.006949424743652344 s \tTotal Time:  20.140859365463257 s \n",
      "\n",
      "\n",
      "\tEpisode 3173 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5320],\n",
      "        [1.0000, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5256],\n",
      "        [1.0000, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502173662185669 \tStep Time:  0.006016731262207031 s \tTotal Time:  20.146876096725464 s \n",
      "\n",
      "\n",
      "\tEpisode 3174 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5399],\n",
      "        [1.0000, 0.5269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5174],\n",
      "        [1.0000, 0.5210]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495307922363281 \tStep Time:  0.006418704986572266 s \tTotal Time:  20.153294801712036 s \n",
      "\n",
      "\n",
      "\tEpisode 3175 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5092],\n",
      "        [1.0000, 0.5361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5232],\n",
      "        [1.0000, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502877235412598 \tStep Time:  0.006548404693603516 s \tTotal Time:  20.15984320640564 s \n",
      "\n",
      "\n",
      "\tEpisode 3176 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5355],\n",
      "        [1.0000, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5313],\n",
      "        [1.0000, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510453701019287 \tStep Time:  0.0059735774993896484 s \tTotal Time:  20.16581678390503 s \n",
      "\n",
      "\n",
      "\tEpisode 3177 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5275],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5042],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491508960723877 \tStep Time:  0.007972955703735352 s \tTotal Time:  20.173789739608765 s \n",
      "\n",
      "\n",
      "\tEpisode 3178 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5613],\n",
      "        [1.0000, 0.4997]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5454],\n",
      "        [1.0000, 0.5462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521262645721436 \tStep Time:  0.005965232849121094 s \tTotal Time:  20.179754972457886 s \n",
      "\n",
      "\n",
      "\tEpisode 3179 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5592],\n",
      "        [1.0000, 0.5171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5369],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487824440002441 \tStep Time:  0.006982326507568359 s \tTotal Time:  20.186737298965454 s \n",
      "\n",
      "\n",
      "\tEpisode 3180 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5614],\n",
      "        [1.0000, 0.6375]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5027],\n",
      "        [1.0000, 0.5788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515510082244873 \tStep Time:  0.006989002227783203 s \tTotal Time:  20.193726301193237 s \n",
      "\n",
      "\n",
      "\tEpisode 3181 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.5400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5186],\n",
      "        [1.0000, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51975154876709 \tStep Time:  0.0059850215911865234 s \tTotal Time:  20.199711322784424 s \n",
      "\n",
      "\n",
      "\tEpisode 3182 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5369],\n",
      "        [1.0000, 0.4413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5212],\n",
      "        [1.0000, 0.4506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588045120239258 \tStep Time:  0.007978677749633789 s \tTotal Time:  20.207690000534058 s \n",
      "\n",
      "\n",
      "\tEpisode 3183 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4821],\n",
      "        [1.0000, 0.4363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4684],\n",
      "        [1.0000, 0.4646]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511054515838623 \tStep Time:  0.005982875823974609 s \tTotal Time:  20.213672876358032 s \n",
      "\n",
      "\n",
      "\tEpisode 3184 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [1.0000, 0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4651],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5065598487854 \tStep Time:  0.0059850215911865234 s \tTotal Time:  20.21965789794922 s \n",
      "\n",
      "\n",
      "\tEpisode 3185 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5607],\n",
      "        [1.0000, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4699],\n",
      "        [1.0000, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464860260486603 \tStep Time:  0.007984399795532227 s \tTotal Time:  20.22764229774475 s \n",
      "\n",
      "\n",
      "\tEpisode 3186 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4648],\n",
      "        [1.0000, 0.4874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5036],\n",
      "        [1.0000, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520906925201416 \tStep Time:  0.005984306335449219 s \tTotal Time:  20.2336266040802 s \n",
      "\n",
      "\n",
      "\tEpisode 3187 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4842],\n",
      "        [1.0000, 0.4857]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530462265014648 \tStep Time:  0.007978200912475586 s \tTotal Time:  20.241604804992676 s \n",
      "\n",
      "\n",
      "\tEpisode 3188 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4803],\n",
      "        [1.0000, 0.4716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4871],\n",
      "        [1.0000, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527958869934082 \tStep Time:  0.006981849670410156 s \tTotal Time:  20.248586654663086 s \n",
      "\n",
      "\n",
      "\tEpisode 3189 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4899],\n",
      "        [1.0000, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4841],\n",
      "        [1.0000, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509373188018799 \tStep Time:  0.00698089599609375 s \tTotal Time:  20.25556755065918 s \n",
      "\n",
      "\n",
      "\tEpisode 3190 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.4880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5187],\n",
      "        [1.0000, 0.5359]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511753141880035 \tStep Time:  0.006981372833251953 s \tTotal Time:  20.26254892349243 s \n",
      "\n",
      "\n",
      "\tEpisode 3191 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.4860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4888],\n",
      "        [1.0000, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50716495513916 \tStep Time:  0.008976221084594727 s \tTotal Time:  20.271525144577026 s \n",
      "\n",
      "\n",
      "\tEpisode 3192 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.4907]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.4902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501852989196777 \tStep Time:  0.0069811344146728516 s \tTotal Time:  20.2785062789917 s \n",
      "\n",
      "\n",
      "\tEpisode 3193 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4947],\n",
      "        [1.0000, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4985],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509462833404541 \tStep Time:  0.007979631423950195 s \tTotal Time:  20.28648591041565 s \n",
      "\n",
      "\n",
      "\tEpisode 3194 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5025],\n",
      "        [1.0000, 0.5479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5022],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481395721435547 \tStep Time:  0.007964611053466797 s \tTotal Time:  20.294450521469116 s \n",
      "\n",
      "\n",
      "\tEpisode 3195 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5064],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4971],\n",
      "        [1.0000, 0.4980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504084825515747 \tStep Time:  0.006984710693359375 s \tTotal Time:  20.301435232162476 s \n",
      "\n",
      "\n",
      "\tEpisode 3196 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.4967]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5056],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496851027011871 \tStep Time:  0.00797414779663086 s \tTotal Time:  20.309409379959106 s \n",
      "\n",
      "\n",
      "\tEpisode 3197 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5304],\n",
      "        [1.0000, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514564037322998 \tStep Time:  0.006981611251831055 s \tTotal Time:  20.316390991210938 s \n",
      "\n",
      "\n",
      "\tEpisode 3198 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5048],\n",
      "        [1.0000, 0.5146]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5264],\n",
      "        [1.0000, 0.4976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503166675567627 \tStep Time:  0.008975744247436523 s \tTotal Time:  20.325366735458374 s \n",
      "\n",
      "\n",
      "\tEpisode 3199 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6014],\n",
      "        [1.0000, 0.5354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4958],\n",
      "        [1.0000, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47631025314331 \tStep Time:  0.007978677749633789 s \tTotal Time:  20.333345413208008 s \n",
      "\n",
      "\n",
      "\tEpisode 3200 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4814],\n",
      "        [1.0000, 0.5354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [1.0000, 0.5537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530944347381592 \tStep Time:  0.008975744247436523 s \tTotal Time:  20.342321157455444 s \n",
      "\n",
      "\n",
      "\tEpisode 3201 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4995],\n",
      "        [1.0000, 0.6168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5297],\n",
      "        [1.0000, 0.5400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456539154052734 \tStep Time:  0.011003494262695312 s \tTotal Time:  20.35332465171814 s \n",
      "\n",
      "\n",
      "\tEpisode 3202 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5133],\n",
      "        [1.0000, 0.4632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4766],\n",
      "        [1.0000, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480258405208588 \tStep Time:  0.007977962493896484 s \tTotal Time:  20.361302614212036 s \n",
      "\n",
      "\n",
      "\tEpisode 3203 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4617],\n",
      "        [1.0000, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4613],\n",
      "        [1.0000, 0.4537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483310222625732 \tStep Time:  0.005986928939819336 s \tTotal Time:  20.367289543151855 s \n",
      "\n",
      "\n",
      "\tEpisode 3204 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4324],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5003],\n",
      "        [1.0000, 0.4430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527947902679443 \tStep Time:  0.007977962493896484 s \tTotal Time:  20.375267505645752 s \n",
      "\n",
      "\n",
      "\tEpisode 3205 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [1.0000, 0.5263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4798],\n",
      "        [1.0000, 0.5639]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566573143005371 \tStep Time:  0.00698399543762207 s \tTotal Time:  20.382251501083374 s \n",
      "\n",
      "\n",
      "\tEpisode 3206 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3410],\n",
      "        [1.0000, 0.5488]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4655],\n",
      "        [1.0000, 0.4124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442381381988525 \tStep Time:  0.007946968078613281 s \tTotal Time:  20.390198469161987 s \n",
      "\n",
      "\n",
      "\tEpisode 3207 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4689],\n",
      "        [1.0000, 0.5456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4290],\n",
      "        [1.0000, 0.4494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478773474693298 \tStep Time:  0.006986141204833984 s \tTotal Time:  20.39718461036682 s \n",
      "\n",
      "\n",
      "\tEpisode 3208 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4730],\n",
      "        [1.0000, 0.4379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5496],\n",
      "        [1.0000, 0.5321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.421036720275879 \tStep Time:  0.00698089599609375 s \tTotal Time:  20.404165506362915 s \n",
      "\n",
      "\n",
      "\tEpisode 3209 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5091],\n",
      "        [1.0000, 0.4218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4288],\n",
      "        [1.0000, 0.4677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572203636169434 \tStep Time:  0.005988359451293945 s \tTotal Time:  20.41015386581421 s \n",
      "\n",
      "\n",
      "\tEpisode 3210 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4702],\n",
      "        [1.0000, 0.5670]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5768],\n",
      "        [1.0000, 0.5688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46590006351471 \tStep Time:  0.006980419158935547 s \tTotal Time:  20.417134284973145 s \n",
      "\n",
      "\n",
      "\tEpisode 3211 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7386],\n",
      "        [1.0000, 0.4664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4584],\n",
      "        [1.0000, 0.7796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.274935364723206 \tStep Time:  0.0069811344146728516 s \tTotal Time:  20.424115419387817 s \n",
      "\n",
      "\n",
      "\tEpisode 3212 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4387],\n",
      "        [1.0000, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4897],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48003625869751 \tStep Time:  0.00698089599609375 s \tTotal Time:  20.43109631538391 s \n",
      "\n",
      "\n",
      "\tEpisode 3213 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5086],\n",
      "        [1.0000, 0.4564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4532],\n",
      "        [1.0000, 0.4229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521245002746582 \tStep Time:  0.006981849670410156 s \tTotal Time:  20.43807816505432 s \n",
      "\n",
      "\n",
      "\tEpisode 3214 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5675],\n",
      "        [1.0000, 0.5718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5869],\n",
      "        [1.0000, 0.4350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593614280223846 \tStep Time:  0.005983829498291016 s \tTotal Time:  20.444061994552612 s \n",
      "\n",
      "\n",
      "\tEpisode 3215 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5269],\n",
      "        [1.0000, 0.4405]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4464],\n",
      "        [1.0000, 0.8642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.403841972351074 \tStep Time:  0.008981466293334961 s \tTotal Time:  20.453043460845947 s \n",
      "\n",
      "\n",
      "\tEpisode 3216 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4241],\n",
      "        [1.0000, 0.5580]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6069],\n",
      "        [1.0000, 0.6160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.404284000396729 \tStep Time:  0.008971691131591797 s \tTotal Time:  20.46201515197754 s \n",
      "\n",
      "\n",
      "\tEpisode 3217 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4988],\n",
      "        [1.0000, 0.4169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4690],\n",
      "        [1.0000, 0.4758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496459484100342 \tStep Time:  0.00998067855834961 s \tTotal Time:  20.47199583053589 s \n",
      "\n",
      "\n",
      "\tEpisode 3218 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6696],\n",
      "        [1.0000, 0.4993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5812],\n",
      "        [1.0000, 0.6905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574733257293701 \tStep Time:  0.0069732666015625 s \tTotal Time:  20.47896909713745 s \n",
      "\n",
      "\n",
      "\tEpisode 3219 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5030],\n",
      "        [1.0000, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4569],\n",
      "        [1.0000, 0.4121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493435382843018 \tStep Time:  0.0059854984283447266 s \tTotal Time:  20.484954595565796 s \n",
      "\n",
      "\n",
      "\tEpisode 3220 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5750],\n",
      "        [1.0000, 0.6349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4127],\n",
      "        [1.0000, 0.4410]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.352104663848877 \tStep Time:  0.00897359848022461 s \tTotal Time:  20.49392819404602 s \n",
      "\n",
      "\n",
      "\tEpisode 3221 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3747],\n",
      "        [1.0000, 0.4304]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3543],\n",
      "        [1.0000, 0.5193]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584351539611816 \tStep Time:  0.004987239837646484 s \tTotal Time:  20.498915433883667 s \n",
      "\n",
      "\n",
      "\tEpisode 3222 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3848],\n",
      "        [1.0000, 0.4453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3730],\n",
      "        [1.0000, 0.4344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588796138763428 \tStep Time:  0.007979631423950195 s \tTotal Time:  20.506895065307617 s \n",
      "\n",
      "\n",
      "\tEpisode 3223 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6155],\n",
      "        [1.0000, 0.4578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3709],\n",
      "        [1.0000, 0.3928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468523502349854 \tStep Time:  0.006979942321777344 s \tTotal Time:  20.513875007629395 s \n",
      "\n",
      "\n",
      "\tEpisode 3224 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5519],\n",
      "        [1.0000, 0.4273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6228],\n",
      "        [1.0000, 0.5542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.623116970062256 \tStep Time:  0.007980108261108398 s \tTotal Time:  20.521855115890503 s \n",
      "\n",
      "\n",
      "\tEpisode 3225 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3989],\n",
      "        [1.0000, 0.4163]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4587],\n",
      "        [1.0000, 0.4241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499596774578094 \tStep Time:  0.0069811344146728516 s \tTotal Time:  20.528836250305176 s \n",
      "\n",
      "\n",
      "\tEpisode 3226 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4071],\n",
      "        [1.0000, 0.4360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5501],\n",
      "        [1.0000, 0.6194]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576134383678436 \tStep Time:  0.0069844722747802734 s \tTotal Time:  20.535820722579956 s \n",
      "\n",
      "\n",
      "\tEpisode 3227 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.7760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4988],\n",
      "        [1.0000, 0.4672]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.693569660186768 \tStep Time:  0.006978273391723633 s \tTotal Time:  20.54379653930664 s \n",
      "\n",
      "\n",
      "\tEpisode 3228 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5579],\n",
      "        [1.0000, 0.5033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4713],\n",
      "        [1.0000, 0.5024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556807518005371 \tStep Time:  0.0069811344146728516 s \tTotal Time:  20.550777673721313 s \n",
      "\n",
      "\n",
      "\tEpisode 3229 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4797],\n",
      "        [1.0000, 0.4770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4910],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519194066524506 \tStep Time:  0.006981372833251953 s \tTotal Time:  20.557759046554565 s \n",
      "\n",
      "\n",
      "\tEpisode 3230 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6683],\n",
      "        [1.0000, 0.4846]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6831],\n",
      "        [1.0000, 0.5533]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582867622375488 \tStep Time:  0.006981849670410156 s \tTotal Time:  20.564740896224976 s \n",
      "\n",
      "\n",
      "\tEpisode 3231 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5681],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4885],\n",
      "        [1.0000, 0.5166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549530982971191 \tStep Time:  0.00698089599609375 s \tTotal Time:  20.57172179222107 s \n",
      "\n",
      "\n",
      "\tEpisode 3232 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6215],\n",
      "        [1.0000, 0.5580]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4762],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.629010200500488 \tStep Time:  0.005983591079711914 s \tTotal Time:  20.57770538330078 s \n",
      "\n",
      "\n",
      "\tEpisode 3233 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.4713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4715],\n",
      "        [1.0000, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517539978027344 \tStep Time:  0.006982326507568359 s \tTotal Time:  20.58468770980835 s \n",
      "\n",
      "\n",
      "\tEpisode 3234 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4891],\n",
      "        [1.0000, 0.7067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4662],\n",
      "        [1.0000, 0.4660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424001097679138 \tStep Time:  0.006980419158935547 s \tTotal Time:  20.591668128967285 s \n",
      "\n",
      "\n",
      "\tEpisode 3235 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6560],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.4662]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.613431930541992 \tStep Time:  0.006981372833251953 s \tTotal Time:  20.598649501800537 s \n",
      "\n",
      "\n",
      "\tEpisode 3236 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.4783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4621],\n",
      "        [1.0000, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510631561279297 \tStep Time:  0.006981372833251953 s \tTotal Time:  20.60563087463379 s \n",
      "\n",
      "\n",
      "\tEpisode 3237 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4591],\n",
      "        [1.0000, 0.4623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4752],\n",
      "        [1.0000, 0.4608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521089673042297 \tStep Time:  0.0069811344146728516 s \tTotal Time:  20.612612009048462 s \n",
      "\n",
      "\n",
      "\tEpisode 3238 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4614],\n",
      "        [1.0000, 0.5598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4569],\n",
      "        [1.0000, 0.4632]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556946754455566 \tStep Time:  0.006982564926147461 s \tTotal Time:  20.61959457397461 s \n",
      "\n",
      "\n",
      "\tEpisode 3239 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.4593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4399],\n",
      "        [1.0000, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502711296081543 \tStep Time:  0.007978200912475586 s \tTotal Time:  20.627572774887085 s \n",
      "\n",
      "\n",
      "\tEpisode 3240 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [1.0000, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4498],\n",
      "        [1.0000, 0.4384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497151494026184 \tStep Time:  0.007980585098266602 s \tTotal Time:  20.63555335998535 s \n",
      "\n",
      "\n",
      "\tEpisode 3241 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4684],\n",
      "        [1.0000, 0.4498]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520111560821533 \tStep Time:  0.00797581672668457 s \tTotal Time:  20.643529176712036 s \n",
      "\n",
      "\n",
      "\tEpisode 3242 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5168],\n",
      "        [1.0000, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4273],\n",
      "        [1.0000, 0.4811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566761493682861 \tStep Time:  0.007978677749633789 s \tTotal Time:  20.65150785446167 s \n",
      "\n",
      "\n",
      "\tEpisode 3243 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4708],\n",
      "        [1.0000, 0.4544]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.4339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48403549194336 \tStep Time:  0.008975505828857422 s \tTotal Time:  20.660483360290527 s \n",
      "\n",
      "\n",
      "\tEpisode 3244 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4492],\n",
      "        [1.0000, 0.4713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4709],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5037202835083 \tStep Time:  0.006980419158935547 s \tTotal Time:  20.667463779449463 s \n",
      "\n",
      "\n",
      "\tEpisode 3245 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4767],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4478],\n",
      "        [1.0000, 0.4677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484200477600098 \tStep Time:  0.007021188735961914 s \tTotal Time:  20.674484968185425 s \n",
      "\n",
      "\n",
      "\tEpisode 3246 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4818],\n",
      "        [1.0000, 0.4837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4663],\n",
      "        [1.0000, 0.4980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503340721130371 \tStep Time:  0.010970830917358398 s \tTotal Time:  20.685455799102783 s \n",
      "\n",
      "\n",
      "\tEpisode 3247 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4769],\n",
      "        [1.0000, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4682],\n",
      "        [1.0000, 0.4964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51209545135498 \tStep Time:  0.011967658996582031 s \tTotal Time:  20.697423458099365 s \n",
      "\n",
      "\n",
      "\tEpisode 3248 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4518],\n",
      "        [1.0000, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [1.0000, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54816484451294 \tStep Time:  0.009972572326660156 s \tTotal Time:  20.707396030426025 s \n",
      "\n",
      "\n",
      "\tEpisode 3249 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4631],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5035],\n",
      "        [1.0000, 0.4925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522213220596313 \tStep Time:  0.008980274200439453 s \tTotal Time:  20.716376304626465 s \n",
      "\n",
      "\n",
      "\tEpisode 3250 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4976],\n",
      "        [1.0000, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5092],\n",
      "        [1.0000, 0.4762]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500635623931885 \tStep Time:  0.011116266250610352 s \tTotal Time:  20.727492570877075 s \n",
      "\n",
      "\n",
      "\tEpisode 3251 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.4987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502864837646484 \tStep Time:  0.008976459503173828 s \tTotal Time:  20.73646903038025 s \n",
      "\n",
      "\n",
      "\tEpisode 3252 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5222],\n",
      "        [1.0000, 0.5035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491039752960205 \tStep Time:  0.010970592498779297 s \tTotal Time:  20.74743962287903 s \n",
      "\n",
      "\n",
      "\tEpisode 3253 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.5341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517914295196533 \tStep Time:  0.010972261428833008 s \tTotal Time:  20.75841188430786 s \n",
      "\n",
      "\n",
      "\tEpisode 3254 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5057],\n",
      "        [1.0000, 0.5272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5053],\n",
      "        [1.0000, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515083312988281 \tStep Time:  0.007977724075317383 s \tTotal Time:  20.76638960838318 s \n",
      "\n",
      "\n",
      "\tEpisode 3255 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5154],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [1.0000, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493003904819489 \tStep Time:  0.00897669792175293 s \tTotal Time:  20.77536630630493 s \n",
      "\n",
      "\n",
      "\tEpisode 3256 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5016],\n",
      "        [1.0000, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5166],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488260865211487 \tStep Time:  0.00797724723815918 s \tTotal Time:  20.78334355354309 s \n",
      "\n",
      "\n",
      "\tEpisode 3257 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5089],\n",
      "        [1.0000, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.4729]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526655673980713 \tStep Time:  0.0082550048828125 s \tTotal Time:  20.791598558425903 s \n",
      "\n",
      "\n",
      "\tEpisode 3258 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5824],\n",
      "        [1.0000, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4570],\n",
      "        [1.0000, 0.4833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57385778427124 \tStep Time:  0.006986379623413086 s \tTotal Time:  20.798584938049316 s \n",
      "\n",
      "\n",
      "\tEpisode 3259 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4903],\n",
      "        [1.0000, 0.4640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4480],\n",
      "        [1.0000, 0.4712]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50274384021759 \tStep Time:  0.007976293563842773 s \tTotal Time:  20.80656123161316 s \n",
      "\n",
      "\n",
      "\tEpisode 3260 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4535],\n",
      "        [1.0000, 0.4465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5584],\n",
      "        [1.0000, 0.4135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546704292297363 \tStep Time:  0.005984306335449219 s \tTotal Time:  20.81254553794861 s \n",
      "\n",
      "\n",
      "\tEpisode 3261 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5130],\n",
      "        [1.0000, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507951140403748 \tStep Time:  0.011968135833740234 s \tTotal Time:  20.82451367378235 s \n",
      "\n",
      "\n",
      "\tEpisode 3262 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.4069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561655163764954 \tStep Time:  0.011968135833740234 s \tTotal Time:  20.83648180961609 s \n",
      "\n",
      "\n",
      "\tEpisode 3263 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5039],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4945],\n",
      "        [1.0000, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490907371044159 \tStep Time:  0.010005950927734375 s \tTotal Time:  20.846487760543823 s \n",
      "\n",
      "\n",
      "\tEpisode 3264 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5047],\n",
      "        [1.0000, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5058],\n",
      "        [1.0000, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505807280540466 \tStep Time:  0.007946014404296875 s \tTotal Time:  20.85443377494812 s \n",
      "\n",
      "\n",
      "\tEpisode 3265 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5073],\n",
      "        [1.0000, 0.5082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.5034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50457239151001 \tStep Time:  0.0069806575775146484 s \tTotal Time:  20.861414432525635 s \n",
      "\n",
      "\n",
      "\tEpisode 3266 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5007],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514960765838623 \tStep Time:  0.005984067916870117 s \tTotal Time:  20.867398500442505 s \n",
      "\n",
      "\n",
      "\tEpisode 3267 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5180],\n",
      "        [1.0000, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4970],\n",
      "        [1.0000, 0.4874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498057842254639 \tStep Time:  0.005984306335449219 s \tTotal Time:  20.873382806777954 s \n",
      "\n",
      "\n",
      "\tEpisode 3268 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5249],\n",
      "        [1.0000, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.4526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467899322509766 \tStep Time:  0.005983591079711914 s \tTotal Time:  20.879366397857666 s \n",
      "\n",
      "\n",
      "\tEpisode 3269 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4698],\n",
      "        [1.0000, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493777751922607 \tStep Time:  0.0059850215911865234 s \tTotal Time:  20.885351419448853 s \n",
      "\n",
      "\n",
      "\tEpisode 3270 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.5312]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4139],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463113486766815 \tStep Time:  0.0069811344146728516 s \tTotal Time:  20.892332553863525 s \n",
      "\n",
      "\n",
      "\tEpisode 3271 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5259],\n",
      "        [1.0000, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3727],\n",
      "        [1.0000, 0.4776]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584190368652344 \tStep Time:  0.007977962493896484 s \tTotal Time:  20.900310516357422 s \n",
      "\n",
      "\n",
      "\tEpisode 3272 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.4391]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5194],\n",
      "        [1.0000, 0.5173]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545741379261017 \tStep Time:  0.005984067916870117 s \tTotal Time:  20.906294584274292 s \n",
      "\n",
      "\n",
      "\tEpisode 3273 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5217],\n",
      "        [1.0000, 0.5400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5177],\n",
      "        [1.0000, 0.4124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44601023197174 \tStep Time:  0.006981611251831055 s \tTotal Time:  20.913276195526123 s \n",
      "\n",
      "\n",
      "\tEpisode 3274 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4624],\n",
      "        [1.0000, 0.4041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5266],\n",
      "        [1.0000, 0.4588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448757648468018 \tStep Time:  0.0069942474365234375 s \tTotal Time:  20.920270442962646 s \n",
      "\n",
      "\n",
      "\tEpisode 3275 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4762],\n",
      "        [1.0000, 0.5448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [1.0000, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.453464031219482 \tStep Time:  0.006968975067138672 s \tTotal Time:  20.927239418029785 s \n",
      "\n",
      "\n",
      "\tEpisode 3276 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5131],\n",
      "        [1.0000, 0.4067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6038],\n",
      "        [1.0000, 0.5283]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.406053066253662 \tStep Time:  0.006980419158935547 s \tTotal Time:  20.93421983718872 s \n",
      "\n",
      "\n",
      "\tEpisode 3277 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3973],\n",
      "        [1.0000, 0.3523]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49944019317627 \tStep Time:  0.006981849670410156 s \tTotal Time:  20.94120168685913 s \n",
      "\n",
      "\n",
      "\tEpisode 3278 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4320],\n",
      "        [1.0000, 0.3293]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6209],\n",
      "        [1.0000, 0.6251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48727560043335 \tStep Time:  0.00698089599609375 s \tTotal Time:  20.948182582855225 s \n",
      "\n",
      "\n",
      "\tEpisode 3279 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6546],\n",
      "        [1.0000, 0.4501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5018],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48523998260498 \tStep Time:  0.006981372833251953 s \tTotal Time:  20.955163955688477 s \n",
      "\n",
      "\n",
      "\tEpisode 3280 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6765],\n",
      "        [1.0000, 0.4490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3153],\n",
      "        [1.0000, 0.4228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601887226104736 \tStep Time:  0.005983591079711914 s \tTotal Time:  20.96114754676819 s \n",
      "\n",
      "\n",
      "\tEpisode 3281 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3662],\n",
      "        [1.0000, 0.5574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5815],\n",
      "        [1.0000, 0.4638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.366226017475128 \tStep Time:  0.005984067916870117 s \tTotal Time:  20.96713161468506 s \n",
      "\n",
      "\n",
      "\tEpisode 3282 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.4719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [1.0000, 0.5622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545764446258545 \tStep Time:  0.006981611251831055 s \tTotal Time:  20.97411322593689 s \n",
      "\n",
      "\n",
      "\tEpisode 3283 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6644],\n",
      "        [1.0000, 0.5655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.2604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.626198768615723 \tStep Time:  0.006981611251831055 s \tTotal Time:  20.98109483718872 s \n",
      "\n",
      "\n",
      "\tEpisode 3284 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4569],\n",
      "        [1.0000, 0.4297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4372],\n",
      "        [1.0000, 0.3420]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463359832763672 \tStep Time:  0.007979154586791992 s \tTotal Time:  20.989073991775513 s \n",
      "\n",
      "\n",
      "\tEpisode 3285 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4737],\n",
      "        [1.0000, 0.3626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4911],\n",
      "        [1.0000, 0.3718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526606678962708 \tStep Time:  0.00698089599609375 s \tTotal Time:  20.996054887771606 s \n",
      "\n",
      "\n",
      "\tEpisode 3286 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3616],\n",
      "        [1.0000, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4973],\n",
      "        [1.0000, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464113414287567 \tStep Time:  0.007978439331054688 s \tTotal Time:  21.00403332710266 s \n",
      "\n",
      "\n",
      "\tEpisode 3287 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5991],\n",
      "        [1.0000, 0.6669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4538],\n",
      "        [1.0000, 0.4279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.342672348022461 \tStep Time:  0.006981372833251953 s \tTotal Time:  21.011014699935913 s \n",
      "\n",
      "\n",
      "\tEpisode 3288 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5189],\n",
      "        [1.0000, 0.5555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6353],\n",
      "        [1.0000, 0.6722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510759472846985 \tStep Time:  0.006981611251831055 s \tTotal Time:  21.017996311187744 s \n",
      "\n",
      "\n",
      "\tEpisode 3289 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4380],\n",
      "        [1.0000, 0.4883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4695],\n",
      "        [1.0000, 0.4943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484593212604523 \tStep Time:  0.007978200912475586 s \tTotal Time:  21.02597451210022 s \n",
      "\n",
      "\n",
      "\tEpisode 3290 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6977],\n",
      "        [1.0000, 0.5866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6129],\n",
      "        [1.0000, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457903861999512 \tStep Time:  0.006981372833251953 s \tTotal Time:  21.03295588493347 s \n",
      "\n",
      "\n",
      "\tEpisode 3291 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6126],\n",
      "        [1.0000, 0.5538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6271],\n",
      "        [1.0000, 0.3116]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.363859176635742 \tStep Time:  0.007978677749633789 s \tTotal Time:  21.040934562683105 s \n",
      "\n",
      "\n",
      "\tEpisode 3292 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6041],\n",
      "        [1.0000, 0.7152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6331],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433939099311829 \tStep Time:  0.005983829498291016 s \tTotal Time:  21.046918392181396 s \n",
      "\n",
      "\n",
      "\tEpisode 3293 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5580],\n",
      "        [1.0000, 0.2790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4692],\n",
      "        [1.0000, 0.5743]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.626943171024323 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.05389976501465 s \n",
      "\n",
      "\n",
      "\tEpisode 3294 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4976],\n",
      "        [1.0000, 0.6749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5303],\n",
      "        [1.0000, 0.7075]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516985416412354 \tStep Time:  0.005983829498291016 s \tTotal Time:  21.05988359451294 s \n",
      "\n",
      "\n",
      "\tEpisode 3295 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6291],\n",
      "        [1.0000, 0.4623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4564],\n",
      "        [1.0000, 0.3577]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497639179229736 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.06586766242981 s \n",
      "\n",
      "\n",
      "\tEpisode 3296 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3716],\n",
      "        [1.0000, 0.6977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5290],\n",
      "        [1.0000, 0.4882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.35934829711914 \tStep Time:  0.005983829498291016 s \tTotal Time:  21.07284903526306 s \n",
      "\n",
      "\n",
      "\tEpisode 3297 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2402],\n",
      "        [1.0000, 0.4753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3577],\n",
      "        [1.0000, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.374177098274231 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.07883310317993 s \n",
      "\n",
      "\n",
      "\tEpisode 3298 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4594],\n",
      "        [1.0000, 0.4615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3314],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446745753288269 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.0848171710968 s \n",
      "\n",
      "\n",
      "\tEpisode 3299 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4883],\n",
      "        [1.0000, 0.6783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4886],\n",
      "        [1.0000, 0.4417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6495401263237 \tStep Time:  0.0069811344146728516 s \tTotal Time:  21.091798305511475 s \n",
      "\n",
      "\n",
      "\tEpisode 3300 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7546],\n",
      "        [1.0000, 0.2491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5434],\n",
      "        [1.0000, 0.5652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.340589046478271 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.097782373428345 s \n",
      "\n",
      "\n",
      "\tEpisode 3301 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6941],\n",
      "        [1.0000, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2928],\n",
      "        [1.0000, 0.5526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.726411819458008 \tStep Time:  0.007613658905029297 s \tTotal Time:  21.105396032333374 s \n",
      "\n",
      "\n",
      "\tEpisode 3302 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1647],\n",
      "        [1.0000, 0.6294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6465],\n",
      "        [1.0000, 0.6382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.375981330871582 \tStep Time:  0.005987882614135742 s \tTotal Time:  21.11138391494751 s \n",
      "\n",
      "\n",
      "\tEpisode 3303 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5345],\n",
      "        [1.0000, 0.7429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4751],\n",
      "        [1.0000, 0.2314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36079066991806 \tStep Time:  0.005948305130004883 s \tTotal Time:  21.117332220077515 s \n",
      "\n",
      "\n",
      "\tEpisode 3304 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6194],\n",
      "        [1.0000, 0.3530]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4357],\n",
      "        [1.0000, 0.6173]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587838649749756 \tStep Time:  0.006981611251831055 s \tTotal Time:  21.124313831329346 s \n",
      "\n",
      "\n",
      "\tEpisode 3305 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3759],\n",
      "        [1.0000, 0.3986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4175],\n",
      "        [1.0000, 0.2468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.644845008850098 \tStep Time:  0.007013797760009766 s \tTotal Time:  21.131327629089355 s \n",
      "\n",
      "\n",
      "\tEpisode 3306 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4958],\n",
      "        [1.0000, 0.4188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5265],\n",
      "        [1.0000, 0.4526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535521268844604 \tStep Time:  0.006948947906494141 s \tTotal Time:  21.13827657699585 s \n",
      "\n",
      "\n",
      "\tEpisode 3307 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6587],\n",
      "        [1.0000, 0.6086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3690],\n",
      "        [1.0000, 0.4243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557101249694824 \tStep Time:  0.00699615478515625 s \tTotal Time:  21.145272731781006 s \n",
      "\n",
      "\n",
      "\tEpisode 3308 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5213],\n",
      "        [1.0000, 0.1797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3816],\n",
      "        [1.0000, 0.7343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60673475265503 \tStep Time:  0.010987281799316406 s \tTotal Time:  21.156260013580322 s \n",
      "\n",
      "\n",
      "\tEpisode 3309 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3577],\n",
      "        [1.0000, 0.6634]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5022],\n",
      "        [1.0000, 0.6815]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.324360013008118 \tStep Time:  0.006949663162231445 s \tTotal Time:  21.163209676742554 s \n",
      "\n",
      "\n",
      "\tEpisode 3310 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3115],\n",
      "        [1.0000, 0.4868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4145],\n",
      "        [1.0000, 0.6295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43626880645752 \tStep Time:  0.006982088088989258 s \tTotal Time:  21.170191764831543 s \n",
      "\n",
      "\n",
      "\tEpisode 3311 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6460],\n",
      "        [1.0000, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4749],\n",
      "        [1.0000, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450958251953125 \tStep Time:  0.006980419158935547 s \tTotal Time:  21.17717218399048 s \n",
      "\n",
      "\n",
      "\tEpisode 3312 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3561],\n",
      "        [1.0000, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5690],\n",
      "        [1.0000, 0.3004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.349127411842346 \tStep Time:  0.006981372833251953 s \tTotal Time:  21.18415355682373 s \n",
      "\n",
      "\n",
      "\tEpisode 3313 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5761],\n",
      "        [1.0000, 0.5991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2605],\n",
      "        [1.0000, 0.6018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.412434101104736 \tStep Time:  0.006985664367675781 s \tTotal Time:  21.191139221191406 s \n",
      "\n",
      "\n",
      "\tEpisode 3314 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4709],\n",
      "        [1.0000, 0.2909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7338],\n",
      "        [1.0000, 0.3135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.286426484584808 \tStep Time:  0.006983280181884766 s \tTotal Time:  21.19812250137329 s \n",
      "\n",
      "\n",
      "\tEpisode 3315 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6075],\n",
      "        [1.0000, 0.4507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4650],\n",
      "        [1.0000, 0.5436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.410276472568512 \tStep Time:  0.0059850215911865234 s \tTotal Time:  21.204107522964478 s \n",
      "\n",
      "\n",
      "\tEpisode 3316 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6931],\n",
      "        [1.0000, 0.5880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2253],\n",
      "        [1.0000, 0.2777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.21603012084961 \tStep Time:  0.007014751434326172 s \tTotal Time:  21.211122274398804 s \n",
      "\n",
      "\n",
      "\tEpisode 3317 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3205],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6172],\n",
      "        [1.0000, 0.5840]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.754321575164795 \tStep Time:  0.00598454475402832 s \tTotal Time:  21.217106819152832 s \n",
      "\n",
      "\n",
      "\tEpisode 3318 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1737],\n",
      "        [1.0000, 0.5542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6020],\n",
      "        [1.0000, 0.8189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.933142185211182 \tStep Time:  0.005986213684082031 s \tTotal Time:  21.223093032836914 s \n",
      "\n",
      "\n",
      "\tEpisode 3319 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.4088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4076],\n",
      "        [1.0000, 0.6339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495082020759583 \tStep Time:  0.006978750228881836 s \tTotal Time:  21.230071783065796 s \n",
      "\n",
      "\n",
      "\tEpisode 3320 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5554],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3972],\n",
      "        [1.0000, 0.5110]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.634592533111572 \tStep Time:  0.006175518035888672 s \tTotal Time:  21.236247301101685 s \n",
      "\n",
      "\n",
      "\tEpisode 3321 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5795],\n",
      "        [1.0000, 0.7061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1599],\n",
      "        [1.0000, 0.5509]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36178207397461 \tStep Time:  0.0067899227142333984 s \tTotal Time:  21.243037223815918 s \n",
      "\n",
      "\n",
      "\tEpisode 3322 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.8159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4383],\n",
      "        [1.0000, 0.7311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.683266162872314 \tStep Time:  0.005984306335449219 s \tTotal Time:  21.249021530151367 s \n",
      "\n",
      "\n",
      "\tEpisode 3323 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6112],\n",
      "        [1.0000, 0.2456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5580],\n",
      "        [1.0000, 0.2617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.621474266052246 \tStep Time:  0.006981849670410156 s \tTotal Time:  21.256003379821777 s \n",
      "\n",
      "\n",
      "\tEpisode 3324 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6907],\n",
      "        [1.0000, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.6501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.386500835418701 \tStep Time:  0.005952596664428711 s \tTotal Time:  21.261955976486206 s \n",
      "\n",
      "\n",
      "\tEpisode 3325 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7299],\n",
      "        [1.0000, 0.2921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [1.0000, 0.4517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.328047275543213 \tStep Time:  0.007977724075317383 s \tTotal Time:  21.269933700561523 s \n",
      "\n",
      "\n",
      "\tEpisode 3326 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2590],\n",
      "        [1.0000, 0.6629]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4037],\n",
      "        [1.0000, 0.2397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465223789215088 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.275917768478394 s \n",
      "\n",
      "\n",
      "\tEpisode 3327 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4104],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3903],\n",
      "        [1.0000, 0.3328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484462141990662 \tStep Time:  0.00698089599609375 s \tTotal Time:  21.282898664474487 s \n",
      "\n",
      "\n",
      "\tEpisode 3328 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5285],\n",
      "        [1.0000, 0.7217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2162],\n",
      "        [1.0000, 0.6918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.75794267654419 \tStep Time:  0.006981849670410156 s \tTotal Time:  21.289880514144897 s \n",
      "\n",
      "\n",
      "\tEpisode 3329 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3447],\n",
      "        [1.0000, 0.3881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5485],\n",
      "        [1.0000, 0.4190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502789974212646 \tStep Time:  0.0069811344146728516 s \tTotal Time:  21.29686164855957 s \n",
      "\n",
      "\n",
      "\tEpisode 3330 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4669],\n",
      "        [1.0000, 0.5207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5166],\n",
      "        [1.0000, 0.6598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439424574375153 \tStep Time:  0.00698089599609375 s \tTotal Time:  21.303842544555664 s \n",
      "\n",
      "\n",
      "\tEpisode 3331 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4321],\n",
      "        [1.0000, 0.2993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3997],\n",
      "        [1.0000, 0.5658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570008099079132 \tStep Time:  0.00698089599609375 s \tTotal Time:  21.310823440551758 s \n",
      "\n",
      "\n",
      "\tEpisode 3332 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8259],\n",
      "        [1.0000, 0.3378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4103],\n",
      "        [1.0000, 0.6486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481073021888733 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.316807508468628 s \n",
      "\n",
      "\n",
      "\tEpisode 3333 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6164],\n",
      "        [1.0000, 0.7545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5793],\n",
      "        [1.0000, 0.5257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475566387176514 \tStep Time:  0.006981611251831055 s \tTotal Time:  21.32378911972046 s \n",
      "\n",
      "\n",
      "\tEpisode 3334 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6569],\n",
      "        [1.0000, 0.3740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3621],\n",
      "        [1.0000, 0.4943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.763181567192078 \tStep Time:  0.007013082504272461 s \tTotal Time:  21.33080220222473 s \n",
      "\n",
      "\n",
      "\tEpisode 3335 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5452],\n",
      "        [1.0000, 0.4787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6222],\n",
      "        [1.0000, 0.4533]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588175296783447 \tStep Time:  0.005953073501586914 s \tTotal Time:  21.33675527572632 s \n",
      "\n",
      "\n",
      "\tEpisode 3336 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4617],\n",
      "        [1.0000, 0.7600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3994],\n",
      "        [1.0000, 0.7688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57608413696289 \tStep Time:  0.006981611251831055 s \tTotal Time:  21.34373688697815 s \n",
      "\n",
      "\n",
      "\tEpisode 3337 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5363],\n",
      "        [1.0000, 0.4257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5645],\n",
      "        [1.0000, 0.6924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.403222560882568 \tStep Time:  0.005983114242553711 s \tTotal Time:  21.349720001220703 s \n",
      "\n",
      "\n",
      "\tEpisode 3338 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5789],\n",
      "        [1.0000, 0.4893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4123],\n",
      "        [1.0000, 0.3910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57067060470581 \tStep Time:  0.006981611251831055 s \tTotal Time:  21.356701612472534 s \n",
      "\n",
      "\n",
      "\tEpisode 3339 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3919],\n",
      "        [1.0000, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4031],\n",
      "        [1.0000, 0.7463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.782788753509521 \tStep Time:  0.0069811344146728516 s \tTotal Time:  21.363682746887207 s \n",
      "\n",
      "\n",
      "\tEpisode 3340 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4985],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541821479797363 \tStep Time:  0.006018161773681641 s \tTotal Time:  21.36970090866089 s \n",
      "\n",
      "\n",
      "\tEpisode 3341 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5168],\n",
      "        [1.0000, 0.7022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.5127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6368927359581 \tStep Time:  0.006947517395019531 s \tTotal Time:  21.376648426055908 s \n",
      "\n",
      "\n",
      "\tEpisode 3342 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5384],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521673679351807 \tStep Time:  0.005983829498291016 s \tTotal Time:  21.3826322555542 s \n",
      "\n",
      "\n",
      "\tEpisode 3343 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5320],\n",
      "        [1.0000, 0.6065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5276],\n",
      "        [1.0000, 0.5227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560087382793427 \tStep Time:  0.005983829498291016 s \tTotal Time:  21.38861608505249 s \n",
      "\n",
      "\n",
      "\tEpisode 3344 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5164],\n",
      "        [1.0000, 0.5187]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5505],\n",
      "        [1.0000, 0.5311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502455234527588 \tStep Time:  0.006987571716308594 s \tTotal Time:  21.3956036567688 s \n",
      "\n",
      "\n",
      "\tEpisode 3345 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5372],\n",
      "        [1.0000, 0.5619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5109],\n",
      "        [1.0000, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551204204559326 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.40158772468567 s \n",
      "\n",
      "\n",
      "\tEpisode 3346 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.5349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5048],\n",
      "        [1.0000, 0.5506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521971702575684 \tStep Time:  0.006051301956176758 s \tTotal Time:  21.407639026641846 s \n",
      "\n",
      "\n",
      "\tEpisode 3347 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6609],\n",
      "        [1.0000, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472001135349274 \tStep Time:  0.005983591079711914 s \tTotal Time:  21.413622617721558 s \n",
      "\n",
      "\n",
      "\tEpisode 3348 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5304],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49649953842163 \tStep Time:  0.0062520503997802734 s \tTotal Time:  21.419874668121338 s \n",
      "\n",
      "\n",
      "\tEpisode 3349 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4942],\n",
      "        [1.0000, 0.5014]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5027],\n",
      "        [1.0000, 0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49150562286377 \tStep Time:  0.0056841373443603516 s \tTotal Time:  21.4255588054657 s \n",
      "\n",
      "\n",
      "\tEpisode 3350 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4721],\n",
      "        [1.0000, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4929],\n",
      "        [1.0000, 0.5616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475377082824707 \tStep Time:  0.005983114242553711 s \tTotal Time:  21.431541919708252 s \n",
      "\n",
      "\n",
      "\tEpisode 3351 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5083],\n",
      "        [1.0000, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4675],\n",
      "        [1.0000, 0.4697]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484293937683105 \tStep Time:  0.006982564926147461 s \tTotal Time:  21.4385244846344 s \n",
      "\n",
      "\n",
      "\tEpisode 3352 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.5301]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6136],\n",
      "        [1.0000, 0.4787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60970538854599 \tStep Time:  0.005983591079711914 s \tTotal Time:  21.44450807571411 s \n",
      "\n",
      "\n",
      "\tEpisode 3353 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4470],\n",
      "        [1.0000, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4533],\n",
      "        [1.0000, 0.4554]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535276889801025 \tStep Time:  0.006015300750732422 s \tTotal Time:  21.450523376464844 s \n",
      "\n",
      "\n",
      "\tEpisode 3354 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5846],\n",
      "        [1.0000, 0.4674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3535],\n",
      "        [1.0000, 0.4581]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533933162689209 \tStep Time:  0.006949186325073242 s \tTotal Time:  21.457472562789917 s \n",
      "\n",
      "\n",
      "\tEpisode 3355 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5245],\n",
      "        [1.0000, 0.4601]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4635],\n",
      "        [1.0000, 0.4629]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48708975315094 \tStep Time:  0.005021810531616211 s \tTotal Time:  21.462494373321533 s \n",
      "\n",
      "\n",
      "\tEpisode 3356 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4785],\n",
      "        [1.0000, 0.5386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4701],\n",
      "        [1.0000, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466893315315247 \tStep Time:  0.006947040557861328 s \tTotal Time:  21.469441413879395 s \n",
      "\n",
      "\n",
      "\tEpisode 3357 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4571],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.4640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528173804283142 \tStep Time:  0.00579524040222168 s \tTotal Time:  21.475456953048706 s \n",
      "\n",
      "\n",
      "\tEpisode 3358 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5304],\n",
      "        [1.0000, 0.4784]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5256],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467301666736603 \tStep Time:  0.005986452102661133 s \tTotal Time:  21.481443405151367 s \n",
      "\n",
      "\n",
      "\tEpisode 3359 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4782],\n",
      "        [1.0000, 0.5312]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.5108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509351193904877 \tStep Time:  0.005950212478637695 s \tTotal Time:  21.487393617630005 s \n",
      "\n",
      "\n",
      "\tEpisode 3360 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5280],\n",
      "        [1.0000, 0.4765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5260],\n",
      "        [1.0000, 0.4690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463873386383057 \tStep Time:  0.0069806575775146484 s \tTotal Time:  21.49437427520752 s \n",
      "\n",
      "\n",
      "\tEpisode 3361 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4740],\n",
      "        [1.0000, 0.6506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5402],\n",
      "        [1.0000, 0.5551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542628288269043 \tStep Time:  0.005984306335449219 s \tTotal Time:  21.50035858154297 s \n",
      "\n",
      "\n",
      "\tEpisode 3362 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.5480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4863],\n",
      "        [1.0000, 0.4782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496521472930908 \tStep Time:  0.007013559341430664 s \tTotal Time:  21.5073721408844 s \n",
      "\n",
      "\n",
      "\tEpisode 3363 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3804],\n",
      "        [1.0000, 0.5552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5154],\n",
      "        [1.0000, 0.5551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587405681610107 \tStep Time:  0.0056378841400146484 s \tTotal Time:  21.513010025024414 s \n",
      "\n",
      "\n",
      "\tEpisode 3364 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4946],\n",
      "        [1.0000, 0.5763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5804],\n",
      "        [1.0000, 0.5930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468515396118164 \tStep Time:  0.0059850215911865234 s \tTotal Time:  21.5189950466156 s \n",
      "\n",
      "\n",
      "\tEpisode 3365 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4396],\n",
      "        [1.0000, 0.3929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4153],\n",
      "        [1.0000, 0.4415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50389289855957 \tStep Time:  0.006980180740356445 s \tTotal Time:  21.525975227355957 s \n",
      "\n",
      "\n",
      "\tEpisode 3366 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4215],\n",
      "        [1.0000, 0.4470]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4494],\n",
      "        [1.0000, 0.4281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491938352584839 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.531959295272827 s \n",
      "\n",
      "\n",
      "\tEpisode 3367 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4525],\n",
      "        [1.0000, 0.4363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4904],\n",
      "        [1.0000, 0.4608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480058670043945 \tStep Time:  0.0069828033447265625 s \tTotal Time:  21.538942098617554 s \n",
      "\n",
      "\n",
      "\tEpisode 3368 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5783],\n",
      "        [1.0000, 0.4600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1810],\n",
      "        [1.0000, 0.4230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50338363647461 \tStep Time:  0.006979942321777344 s \tTotal Time:  21.54592204093933 s \n",
      "\n",
      "\n",
      "\tEpisode 3369 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4854],\n",
      "        [1.0000, 0.7029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4686],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.413913249969482 \tStep Time:  0.006982564926147461 s \tTotal Time:  21.55290460586548 s \n",
      "\n",
      "\n",
      "\tEpisode 3370 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4527],\n",
      "        [1.0000, 0.5699]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5028],\n",
      "        [1.0000, 0.6387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39083480834961 \tStep Time:  0.011966705322265625 s \tTotal Time:  21.564871311187744 s \n",
      "\n",
      "\n",
      "\tEpisode 3371 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5644],\n",
      "        [1.0000, 0.4544]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5248],\n",
      "        [1.0000, 0.6645]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539569973945618 \tStep Time:  0.00598454475402832 s \tTotal Time:  21.570855855941772 s \n",
      "\n",
      "\n",
      "\tEpisode 3372 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5135],\n",
      "        [1.0000, 0.5701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4340],\n",
      "        [1.0000, 0.4889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510771751403809 \tStep Time:  0.0069806575775146484 s \tTotal Time:  21.577836513519287 s \n",
      "\n",
      "\n",
      "\tEpisode 3373 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.5254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4595],\n",
      "        [1.0000, 0.4255]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497905254364014 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.583820581436157 s \n",
      "\n",
      "\n",
      "\tEpisode 3374 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4589],\n",
      "        [1.0000, 0.4617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4993],\n",
      "        [1.0000, 0.3214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582250595092773 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.589804649353027 s \n",
      "\n",
      "\n",
      "\tEpisode 3375 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4764],\n",
      "        [1.0000, 0.7882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4751],\n",
      "        [1.0000, 0.6744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51370906829834 \tStep Time:  0.005983591079711914 s \tTotal Time:  21.59578824043274 s \n",
      "\n",
      "\n",
      "\tEpisode 3376 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5363],\n",
      "        [1.0000, 0.6142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4787],\n",
      "        [1.0000, 0.5674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601720809936523 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.60177230834961 s \n",
      "\n",
      "\n",
      "\tEpisode 3377 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4273],\n",
      "        [1.0000, 0.4344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6564],\n",
      "        [1.0000, 0.4957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44868141412735 \tStep Time:  0.00598454475402832 s \tTotal Time:  21.607756853103638 s \n",
      "\n",
      "\n",
      "\tEpisode 3378 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5792],\n",
      "        [1.0000, 0.4747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5971],\n",
      "        [1.0000, 0.6126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479288101196289 \tStep Time:  0.00701594352722168 s \tTotal Time:  21.61477279663086 s \n",
      "\n",
      "\n",
      "\tEpisode 3379 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4840],\n",
      "        [1.0000, 0.4587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4527],\n",
      "        [1.0000, 0.5414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480358242988586 \tStep Time:  0.00598454475402832 s \tTotal Time:  21.620757341384888 s \n",
      "\n",
      "\n",
      "\tEpisode 3380 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4361],\n",
      "        [1.0000, 0.4321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5854],\n",
      "        [1.0000, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563712060451508 \tStep Time:  0.005980491638183594 s \tTotal Time:  21.62673783302307 s \n",
      "\n",
      "\n",
      "\tEpisode 3381 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5435],\n",
      "        [1.0000, 0.5294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4864],\n",
      "        [1.0000, 0.4333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58884859085083 \tStep Time:  0.00598454475402832 s \tTotal Time:  21.6327223777771 s \n",
      "\n",
      "\n",
      "\tEpisode 3382 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6190],\n",
      "        [1.0000, 0.4386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4457],\n",
      "        [1.0000, 0.4581]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447280883789062 \tStep Time:  0.005981922149658203 s \tTotal Time:  21.639703512191772 s \n",
      "\n",
      "\n",
      "\tEpisode 3383 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4719],\n",
      "        [1.0000, 0.5722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5463],\n",
      "        [1.0000, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428699254989624 \tStep Time:  0.0059833526611328125 s \tTotal Time:  21.645686864852905 s \n",
      "\n",
      "\n",
      "\tEpisode 3384 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4408],\n",
      "        [1.0000, 0.4707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6251],\n",
      "        [1.0000, 0.4740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45973253250122 \tStep Time:  0.0069506168365478516 s \tTotal Time:  21.652637481689453 s \n",
      "\n",
      "\n",
      "\tEpisode 3385 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4649],\n",
      "        [1.0000, 0.4408]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6795],\n",
      "        [1.0000, 0.4756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41691541671753 \tStep Time:  0.0060160160064697266 s \tTotal Time:  21.658653497695923 s \n",
      "\n",
      "\n",
      "\tEpisode 3386 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.5536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6346],\n",
      "        [1.0000, 0.4301]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534456729888916 \tStep Time:  0.005983114242553711 s \tTotal Time:  21.664636611938477 s \n",
      "\n",
      "\n",
      "\tEpisode 3387 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4398],\n",
      "        [1.0000, 0.4622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4530],\n",
      "        [1.0000, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486257553100586 \tStep Time:  0.0069811344146728516 s \tTotal Time:  21.67161774635315 s \n",
      "\n",
      "\n",
      "\tEpisode 3388 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5104],\n",
      "        [1.0000, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4301],\n",
      "        [1.0000, 0.4357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506499290466309 \tStep Time:  0.005984783172607422 s \tTotal Time:  21.677602529525757 s \n",
      "\n",
      "\n",
      "\tEpisode 3389 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4471],\n",
      "        [1.0000, 0.4317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4406],\n",
      "        [1.0000, 0.4371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506091594696045 \tStep Time:  0.005984783172607422 s \tTotal Time:  21.683587312698364 s \n",
      "\n",
      "\n",
      "\tEpisode 3390 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4703],\n",
      "        [1.0000, 0.4518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5488],\n",
      "        [1.0000, 0.5441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426777362823486 \tStep Time:  0.006980180740356445 s \tTotal Time:  21.69056749343872 s \n",
      "\n",
      "\n",
      "\tEpisode 3391 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6353],\n",
      "        [1.0000, 0.5939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4518],\n",
      "        [1.0000, 0.6209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46638834476471 \tStep Time:  0.005983591079711914 s \tTotal Time:  21.696551084518433 s \n",
      "\n",
      "\n",
      "\tEpisode 3392 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5291],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7069],\n",
      "        [1.0000, 0.4536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446829319000244 \tStep Time:  0.006981849670410156 s \tTotal Time:  21.703532934188843 s \n",
      "\n",
      "\n",
      "\tEpisode 3393 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4394],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6041],\n",
      "        [1.0000, 0.4651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550942420959473 \tStep Time:  0.006948947906494141 s \tTotal Time:  21.710481882095337 s \n",
      "\n",
      "\n",
      "\tEpisode 3394 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4097],\n",
      "        [1.0000, 0.5993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4024],\n",
      "        [1.0000, 0.4651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39555013179779 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.716465950012207 s \n",
      "\n",
      "\n",
      "\tEpisode 3395 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6326],\n",
      "        [1.0000, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3968],\n",
      "        [1.0000, 0.4639]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.414852142333984 \tStep Time:  0.005983591079711914 s \tTotal Time:  21.72244954109192 s \n",
      "\n",
      "\n",
      "\tEpisode 3396 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5092],\n",
      "        [1.0000, 0.6957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4621],\n",
      "        [1.0000, 0.7047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.338692545890808 \tStep Time:  0.005984306335449219 s \tTotal Time:  21.728433847427368 s \n",
      "\n",
      "\n",
      "\tEpisode 3397 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3876],\n",
      "        [1.0000, 0.3963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8581],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.27779483795166 \tStep Time:  0.005984306335449219 s \tTotal Time:  21.734418153762817 s \n",
      "\n",
      "\n",
      "\tEpisode 3398 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6875],\n",
      "        [1.0000, 0.5678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3567],\n",
      "        [1.0000, 0.7051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445435047149658 \tStep Time:  0.013962984085083008 s \tTotal Time:  21.7483811378479 s \n",
      "\n",
      "\n",
      "\tEpisode 3399 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4685],\n",
      "        [1.0000, 0.6742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6239],\n",
      "        [1.0000, 0.5985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425494015216827 \tStep Time:  0.0109710693359375 s \tTotal Time:  21.759352207183838 s \n",
      "\n",
      "\n",
      "\tEpisode 3400 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3652],\n",
      "        [1.0000, 0.6152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7149],\n",
      "        [1.0000, 0.6959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.78298568725586 \tStep Time:  0.008977413177490234 s \tTotal Time:  21.768329620361328 s \n",
      "\n",
      "\n",
      "\tEpisode 3401 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4228],\n",
      "        [1.0000, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4435],\n",
      "        [1.0000, 0.4683]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559139966964722 \tStep Time:  0.008008956909179688 s \tTotal Time:  21.776338577270508 s \n",
      "\n",
      "\n",
      "\tEpisode 3402 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3756],\n",
      "        [1.0000, 0.5176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4710],\n",
      "        [1.0000, 0.4118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500896453857422 \tStep Time:  0.0059854984283447266 s \tTotal Time:  21.782324075698853 s \n",
      "\n",
      "\n",
      "\tEpisode 3403 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4883],\n",
      "        [1.0000, 0.3912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3937],\n",
      "        [1.0000, 0.5888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.688383638858795 \tStep Time:  0.006948232650756836 s \tTotal Time:  21.78927230834961 s \n",
      "\n",
      "\n",
      "\tEpisode 3404 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3830],\n",
      "        [1.0000, 0.4949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4180],\n",
      "        [1.0000, 0.2924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.674383342266083 \tStep Time:  0.005982875823974609 s \tTotal Time:  21.795255184173584 s \n",
      "\n",
      "\n",
      "\tEpisode 3405 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5104],\n",
      "        [1.0000, 0.3452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2249],\n",
      "        [1.0000, 0.2956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.753105163574219 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.801239252090454 s \n",
      "\n",
      "\n",
      "\tEpisode 3406 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2828],\n",
      "        [1.0000, 0.7749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5046],\n",
      "        [1.0000, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.34188586473465 \tStep Time:  0.005984783172607422 s \tTotal Time:  21.80722403526306 s \n",
      "\n",
      "\n",
      "\tEpisode 3407 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3816],\n",
      "        [1.0000, 0.3269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4277],\n",
      "        [1.0000, 0.4634]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561084270477295 \tStep Time:  0.005983591079711914 s \tTotal Time:  21.813207626342773 s \n",
      "\n",
      "\n",
      "\tEpisode 3408 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6273],\n",
      "        [1.0000, 0.3330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5386],\n",
      "        [1.0000, 0.5783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489388942718506 \tStep Time:  0.0059833526611328125 s \tTotal Time:  21.819190979003906 s \n",
      "\n",
      "\n",
      "\tEpisode 3409 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6719],\n",
      "        [1.0000, 0.4296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3488],\n",
      "        [1.0000, 0.5281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.358396768569946 \tStep Time:  0.01100611686706543 s \tTotal Time:  21.83019709587097 s \n",
      "\n",
      "\n",
      "\tEpisode 3410 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5730],\n",
      "        [1.0000, 0.5872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5082],\n",
      "        [1.0000, 0.5337]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520593166351318 \tStep Time:  0.005950212478637695 s \tTotal Time:  21.83614730834961 s \n",
      "\n",
      "\n",
      "\tEpisode 3411 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.9233],\n",
      "        [1.0000, 0.4267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4306],\n",
      "        [1.0000, 0.3954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.893427550792694 \tStep Time:  0.005964517593383789 s \tTotal Time:  21.843127727508545 s \n",
      "\n",
      "\n",
      "\tEpisode 3412 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5433],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569263935089111 \tStep Time:  0.006018400192260742 s \tTotal Time:  21.849146127700806 s \n",
      "\n",
      "\n",
      "\tEpisode 3413 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2165],\n",
      "        [1.0000, 0.4404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3755],\n",
      "        [1.0000, 0.5399]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473631858825684 \tStep Time:  0.004985332489013672 s \tTotal Time:  21.85413146018982 s \n",
      "\n",
      "\n",
      "\tEpisode 3414 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5943],\n",
      "        [1.0000, 0.3480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3893],\n",
      "        [1.0000, 0.5024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.738545894622803 \tStep Time:  0.004985332489013672 s \tTotal Time:  21.859116792678833 s \n",
      "\n",
      "\n",
      "\tEpisode 3415 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5778],\n",
      "        [1.0000, 0.4284]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5488],\n",
      "        [1.0000, 0.5492]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484920501708984 \tStep Time:  0.005985260009765625 s \tTotal Time:  21.8651020526886 s \n",
      "\n",
      "\n",
      "\tEpisode 3416 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6691],\n",
      "        [1.0000, 0.3222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4298],\n",
      "        [1.0000, 0.4685]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36912727355957 \tStep Time:  0.004991292953491211 s \tTotal Time:  21.87009334564209 s \n",
      "\n",
      "\n",
      "\tEpisode 3417 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4239],\n",
      "        [1.0000, 0.4239]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5490],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52474719285965 \tStep Time:  0.005980491638183594 s \tTotal Time:  21.876073837280273 s \n",
      "\n",
      "\n",
      "\tEpisode 3418 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7132],\n",
      "        [1.0000, 0.6906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5565],\n",
      "        [1.0000, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586368918418884 \tStep Time:  0.004991769790649414 s \tTotal Time:  21.881065607070923 s \n",
      "\n",
      "\n",
      "\tEpisode 3419 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5716],\n",
      "        [1.0000, 0.6984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.5021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.390045166015625 \tStep Time:  0.005944967269897461 s \tTotal Time:  21.88701057434082 s \n",
      "\n",
      "\n",
      "\tEpisode 3420 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4425],\n",
      "        [1.0000, 0.7166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6164],\n",
      "        [1.0000, 0.7868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489631175994873 \tStep Time:  0.006014585494995117 s \tTotal Time:  21.893025159835815 s \n",
      "\n",
      "\n",
      "\tEpisode 3421 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4839],\n",
      "        [1.0000, 0.4477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4780],\n",
      "        [1.0000, 0.4222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534159898757935 \tStep Time:  0.004986763000488281 s \tTotal Time:  21.898011922836304 s \n",
      "\n",
      "\n",
      "\tEpisode 3422 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4269],\n",
      "        [1.0000, 0.4451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4150],\n",
      "        [1.0000, 0.5690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.594732284545898 \tStep Time:  0.00598454475402832 s \tTotal Time:  21.903996467590332 s \n",
      "\n",
      "\n",
      "\tEpisode 3423 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4228],\n",
      "        [1.0000, 0.5397]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4604],\n",
      "        [1.0000, 0.4609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466642439365387 \tStep Time:  0.0049860477447509766 s \tTotal Time:  21.908982515335083 s \n",
      "\n",
      "\n",
      "\tEpisode 3424 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3757],\n",
      "        [1.0000, 0.4257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6107],\n",
      "        [1.0000, 0.3597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459798812866211 \tStep Time:  0.006949901580810547 s \tTotal Time:  21.915932416915894 s \n",
      "\n",
      "\n",
      "\tEpisode 3425 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4762],\n",
      "        [1.0000, 0.4257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4279],\n",
      "        [1.0000, 0.6841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.387901306152344 \tStep Time:  0.005984067916870117 s \tTotal Time:  21.921916484832764 s \n",
      "\n",
      "\n",
      "\tEpisode 3426 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5388],\n",
      "        [1.0000, 0.6968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4199],\n",
      "        [1.0000, 0.4427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45424509048462 \tStep Time:  0.00598454475402832 s \tTotal Time:  21.927901029586792 s \n",
      "\n",
      "\n",
      "\tEpisode 3427 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4700],\n",
      "        [1.0000, 0.4159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4126],\n",
      "        [1.0000, 0.4601]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519580841064453 \tStep Time:  0.0069806575775146484 s \tTotal Time:  21.934881687164307 s \n",
      "\n",
      "\n",
      "\tEpisode 3428 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6149],\n",
      "        [1.0000, 0.4921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6167],\n",
      "        [1.0000, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.613110542297363 \tStep Time:  0.0069811344146728516 s \tTotal Time:  21.94186282157898 s \n",
      "\n",
      "\n",
      "\tEpisode 3429 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5439],\n",
      "        [1.0000, 0.4776]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4561],\n",
      "        [1.0000, 0.4199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564781785011292 \tStep Time:  0.006982088088989258 s \tTotal Time:  21.94884490966797 s \n",
      "\n",
      "\n",
      "\tEpisode 3430 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5050],\n",
      "        [1.0000, 0.6035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4122],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506712913513184 \tStep Time:  0.006981372833251953 s \tTotal Time:  21.95582628250122 s \n",
      "\n",
      "\n",
      "\tEpisode 3431 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4728],\n",
      "        [1.0000, 0.4169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4200],\n",
      "        [1.0000, 0.4196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545351505279541 \tStep Time:  0.01100301742553711 s \tTotal Time:  21.966829299926758 s \n",
      "\n",
      "\n",
      "\tEpisode 3432 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4249],\n",
      "        [1.0000, 0.4510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4685],\n",
      "        [1.0000, 0.6841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66759967803955 \tStep Time:  0.006983280181884766 s \tTotal Time:  21.973812580108643 s \n",
      "\n",
      "\n",
      "\tEpisode 3433 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5140],\n",
      "        [1.0000, 0.4608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5630],\n",
      "        [1.0000, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.430585861206055 \tStep Time:  0.006978034973144531 s \tTotal Time:  21.980790615081787 s \n",
      "\n",
      "\n",
      "\tEpisode 3434 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5833],\n",
      "        [1.0000, 0.6414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4504],\n",
      "        [1.0000, 0.6016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481506824493408 \tStep Time:  0.005986452102661133 s \tTotal Time:  21.98677706718445 s \n",
      "\n",
      "\n",
      "\tEpisode 3435 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4897],\n",
      "        [1.0000, 0.5900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543729901313782 \tStep Time:  0.006979703903198242 s \tTotal Time:  21.993756771087646 s \n",
      "\n",
      "\n",
      "\tEpisode 3436 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5648],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5976],\n",
      "        [1.0000, 0.4769]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522164821624756 \tStep Time:  0.005987882614135742 s \tTotal Time:  21.999744653701782 s \n",
      "\n",
      "\n",
      "\tEpisode 3437 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4775],\n",
      "        [1.0000, 0.6540]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4670],\n",
      "        [1.0000, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601249694824219 \tStep Time:  0.005982637405395508 s \tTotal Time:  22.005727291107178 s \n",
      "\n",
      "\n",
      "\tEpisode 3438 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5383],\n",
      "        [1.0000, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4623],\n",
      "        [1.0000, 0.4619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525224208831787 \tStep Time:  0.005984306335449219 s \tTotal Time:  22.011711597442627 s \n",
      "\n",
      "\n",
      "\tEpisode 3439 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4536],\n",
      "        [1.0000, 0.2986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4585],\n",
      "        [1.0000, 0.4693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462823390960693 \tStep Time:  0.005982875823974609 s \tTotal Time:  22.0176944732666 s \n",
      "\n",
      "\n",
      "\tEpisode 3440 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4675],\n",
      "        [1.0000, 0.4607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4910],\n",
      "        [1.0000, 0.4454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482962608337402 \tStep Time:  0.006981372833251953 s \tTotal Time:  22.024675846099854 s \n",
      "\n",
      "\n",
      "\tEpisode 3441 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4998],\n",
      "        [1.0000, 0.4744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6490],\n",
      "        [1.0000, 0.5230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428283214569092 \tStep Time:  0.005982637405395508 s \tTotal Time:  22.03065848350525 s \n",
      "\n",
      "\n",
      "\tEpisode 3442 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.4519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5717],\n",
      "        [1.0000, 0.4830]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45396900177002 \tStep Time:  0.005964756011962891 s \tTotal Time:  22.036623239517212 s \n",
      "\n",
      "\n",
      "\tEpisode 3443 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4705],\n",
      "        [1.0000, 0.4422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4574],\n",
      "        [1.0000, 0.6364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420071125030518 \tStep Time:  0.007002830505371094 s \tTotal Time:  22.043626070022583 s \n",
      "\n",
      "\n",
      "\tEpisode 3444 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4323],\n",
      "        [1.0000, 0.4307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4349],\n",
      "        [1.0000, 0.5728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591757774353027 \tStep Time:  0.00694727897644043 s \tTotal Time:  22.050573348999023 s \n",
      "\n",
      "\n",
      "\tEpisode 3445 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5460],\n",
      "        [1.0000, 0.4576]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4658],\n",
      "        [1.0000, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539426803588867 \tStep Time:  0.007012605667114258 s \tTotal Time:  22.057585954666138 s \n",
      "\n",
      "\n",
      "\tEpisode 3446 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [1.0000, 0.5748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5507],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513657093048096 \tStep Time:  0.005985736846923828 s \tTotal Time:  22.06357169151306 s \n",
      "\n",
      "\n",
      "\tEpisode 3447 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4346],\n",
      "        [1.0000, 0.6054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4813],\n",
      "        [1.0000, 0.5014]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438418447971344 \tStep Time:  0.006979703903198242 s \tTotal Time:  22.07055139541626 s \n",
      "\n",
      "\n",
      "\tEpisode 3448 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4978],\n",
      "        [1.0000, 0.6531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5729],\n",
      "        [1.0000, 0.6363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424484431743622 \tStep Time:  0.005984306335449219 s \tTotal Time:  22.07653570175171 s \n",
      "\n",
      "\n",
      "\tEpisode 3449 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4447],\n",
      "        [1.0000, 0.4359]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4013],\n",
      "        [1.0000, 0.2919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49056851863861 \tStep Time:  0.006983757019042969 s \tTotal Time:  22.083519458770752 s \n",
      "\n",
      "\n",
      "\tEpisode 3450 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7511],\n",
      "        [1.0000, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7121],\n",
      "        [1.0000, 0.4243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.827373504638672 \tStep Time:  0.005949497222900391 s \tTotal Time:  22.089468955993652 s \n",
      "\n",
      "\n",
      "\tEpisode 3451 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4906],\n",
      "        [1.0000, 0.4062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6743],\n",
      "        [1.0000, 0.7007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502217292785645 \tStep Time:  0.005952596664428711 s \tTotal Time:  22.096449851989746 s \n",
      "\n",
      "\n",
      "\tEpisode 3452 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3960],\n",
      "        [1.0000, 0.4635]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4748],\n",
      "        [1.0000, 0.4411]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578414618968964 \tStep Time:  0.005983591079711914 s \tTotal Time:  22.102433443069458 s \n",
      "\n",
      "\n",
      "\tEpisode 3453 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.4939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4303],\n",
      "        [1.0000, 0.5661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.448505401611328 \tStep Time:  0.0070154666900634766 s \tTotal Time:  22.10944890975952 s \n",
      "\n",
      "\n",
      "\tEpisode 3454 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5566],\n",
      "        [1.0000, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4251],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612497329711914 \tStep Time:  0.006491661071777344 s \tTotal Time:  22.1159405708313 s \n",
      "\n",
      "\n",
      "\tEpisode 3455 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4934],\n",
      "        [1.0000, 0.5086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4477],\n",
      "        [1.0000, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463923454284668 \tStep Time:  0.0059854984283447266 s \tTotal Time:  22.121926069259644 s \n",
      "\n",
      "\n",
      "\tEpisode 3456 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4686],\n",
      "        [1.0000, 0.4663]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4839],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483396530151367 \tStep Time:  0.005949735641479492 s \tTotal Time:  22.127875804901123 s \n",
      "\n",
      "\n",
      "\tEpisode 3457 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4526],\n",
      "        [1.0000, 0.4895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5298],\n",
      "        [1.0000, 0.4676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521943747997284 \tStep Time:  0.006015300750732422 s \tTotal Time:  22.133891105651855 s \n",
      "\n",
      "\n",
      "\tEpisode 3458 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5400],\n",
      "        [1.0000, 0.4979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5295],\n",
      "        [1.0000, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509315013885498 \tStep Time:  0.0069806575775146484 s \tTotal Time:  22.14087176322937 s \n",
      "\n",
      "\n",
      "\tEpisode 3459 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5211],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5260],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484585285186768 \tStep Time:  0.0059850215911865234 s \tTotal Time:  22.146856784820557 s \n",
      "\n",
      "\n",
      "\tEpisode 3460 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5400],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5590],\n",
      "        [1.0000, 0.5386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517115414142609 \tStep Time:  0.006950855255126953 s \tTotal Time:  22.153807640075684 s \n",
      "\n",
      "\n",
      "\tEpisode 3461 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4577],\n",
      "        [1.0000, 0.4549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3531],\n",
      "        [1.0000, 0.4894]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484621524810791 \tStep Time:  0.005982398986816406 s \tTotal Time:  22.1597900390625 s \n",
      "\n",
      "\n",
      "\tEpisode 3462 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3691],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5196],\n",
      "        [1.0000, 0.4227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520438194274902 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.16577410697937 s \n",
      "\n",
      "\n",
      "\tEpisode 3463 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4711],\n",
      "        [1.0000, 0.4961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4847],\n",
      "        [1.0000, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521138191223145 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.17175817489624 s \n",
      "\n",
      "\n",
      "\tEpisode 3464 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6441],\n",
      "        [1.0000, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544332504272461 \tStep Time:  0.00598597526550293 s \tTotal Time:  22.177744150161743 s \n",
      "\n",
      "\n",
      "\tEpisode 3465 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4708],\n",
      "        [1.0000, 0.4870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4679],\n",
      "        [1.0000, 0.4719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516212821006775 \tStep Time:  0.006013393402099609 s \tTotal Time:  22.183757543563843 s \n",
      "\n",
      "\n",
      "\tEpisode 3466 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4897],\n",
      "        [1.0000, 0.4799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3933],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465476036071777 \tStep Time:  0.0069501399993896484 s \tTotal Time:  22.190707683563232 s \n",
      "\n",
      "\n",
      "\tEpisode 3467 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5272],\n",
      "        [1.0000, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5389723777771 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.196691513061523 s \n",
      "\n",
      "\n",
      "\tEpisode 3468 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4655],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5115],\n",
      "        [1.0000, 0.5404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502250373363495 \tStep Time:  0.005984783172607422 s \tTotal Time:  22.20267629623413 s \n",
      "\n",
      "\n",
      "\tEpisode 3469 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4919],\n",
      "        [1.0000, 0.5454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.4314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563541889190674 \tStep Time:  0.005983114242553711 s \tTotal Time:  22.208659410476685 s \n",
      "\n",
      "\n",
      "\tEpisode 3470 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5070],\n",
      "        [1.0000, 0.4747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519128978252411 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.214643478393555 s \n",
      "\n",
      "\n",
      "\tEpisode 3471 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4852],\n",
      "        [1.0000, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4788],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486657619476318 \tStep Time:  0.006981611251831055 s \tTotal Time:  22.221625089645386 s \n",
      "\n",
      "\n",
      "\tEpisode 3472 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4775],\n",
      "        [1.0000, 0.4977]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511302947998047 \tStep Time:  0.005984306335449219 s \tTotal Time:  22.227609395980835 s \n",
      "\n",
      "\n",
      "\tEpisode 3473 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [1.0000, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5042],\n",
      "        [1.0000, 0.5343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514991760253906 \tStep Time:  0.005983591079711914 s \tTotal Time:  22.233592987060547 s \n",
      "\n",
      "\n",
      "\tEpisode 3474 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1901],\n",
      "        [1.0000, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4878],\n",
      "        [1.0000, 0.4570]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.67375373840332 \tStep Time:  0.006982088088989258 s \tTotal Time:  22.240575075149536 s \n",
      "\n",
      "\n",
      "\tEpisode 3475 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4929],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5073],\n",
      "        [1.0000, 0.4970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51252269744873 \tStep Time:  0.005983114242553711 s \tTotal Time:  22.24655818939209 s \n",
      "\n",
      "\n",
      "\tEpisode 3476 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5187],\n",
      "        [1.0000, 0.4991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493141412734985 \tStep Time:  0.006981849670410156 s \tTotal Time:  22.2535400390625 s \n",
      "\n",
      "\n",
      "\tEpisode 3477 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4978],\n",
      "        [1.0000, 0.5264]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495747148990631 \tStep Time:  0.0069806575775146484 s \tTotal Time:  22.260520696640015 s \n",
      "\n",
      "\n",
      "\tEpisode 3478 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5378],\n",
      "        [1.0000, 0.3250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4869],\n",
      "        [1.0000, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.413732528686523 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.266504764556885 s \n",
      "\n",
      "\n",
      "\tEpisode 3479 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5450],\n",
      "        [1.0000, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4901],\n",
      "        [1.0000, 0.5409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503012657165527 \tStep Time:  0.006981849670410156 s \tTotal Time:  22.273486614227295 s \n",
      "\n",
      "\n",
      "\tEpisode 3480 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5552],\n",
      "        [1.0000, 0.5573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5164],\n",
      "        [1.0000, 0.4973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496293544769287 \tStep Time:  0.0059833526611328125 s \tTotal Time:  22.279469966888428 s \n",
      "\n",
      "\n",
      "\tEpisode 3481 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4638],\n",
      "        [1.0000, 0.4667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.5713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526859283447266 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.285454034805298 s \n",
      "\n",
      "\n",
      "\tEpisode 3482 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4534],\n",
      "        [1.0000, 0.5786]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5560],\n",
      "        [1.0000, 0.5034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52561330795288 \tStep Time:  0.006981372833251953 s \tTotal Time:  22.29243540763855 s \n",
      "\n",
      "\n",
      "\tEpisode 3483 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4711],\n",
      "        [1.0000, 0.5799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4823],\n",
      "        [1.0000, 0.5010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462539196014404 \tStep Time:  0.006015300750732422 s \tTotal Time:  22.298450708389282 s \n",
      "\n",
      "\n",
      "\tEpisode 3484 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4305],\n",
      "        [1.0000, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5711],\n",
      "        [1.0000, 0.4572]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456307411193848 \tStep Time:  0.005986213684082031 s \tTotal Time:  22.304436922073364 s \n",
      "\n",
      "\n",
      "\tEpisode 3485 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5608],\n",
      "        [1.0000, 0.4456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.4443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495909690856934 \tStep Time:  0.0059833526611328125 s \tTotal Time:  22.310420274734497 s \n",
      "\n",
      "\n",
      "\tEpisode 3486 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2679],\n",
      "        [1.0000, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5088],\n",
      "        [1.0000, 0.6351]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.366952419281006 \tStep Time:  0.005862712860107422 s \tTotal Time:  22.316282987594604 s \n",
      "\n",
      "\n",
      "\tEpisode 3487 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4798],\n",
      "        [1.0000, 0.5387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4482],\n",
      "        [1.0000, 0.4355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488533973693848 \tStep Time:  0.006983757019042969 s \tTotal Time:  22.323266744613647 s \n",
      "\n",
      "\n",
      "\tEpisode 3488 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4314],\n",
      "        [1.0000, 0.5412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3968],\n",
      "        [1.0000, 0.4166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471314907073975 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.32925057411194 s \n",
      "\n",
      "\n",
      "\tEpisode 3489 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4646],\n",
      "        [1.0000, 0.5874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5426],\n",
      "        [1.0000, 0.5305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444645524024963 \tStep Time:  0.006981611251831055 s \tTotal Time:  22.33623218536377 s \n",
      "\n",
      "\n",
      "\tEpisode 3490 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2770],\n",
      "        [1.0000, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4209],\n",
      "        [1.0000, 0.4339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.652497291564941 \tStep Time:  0.0069811344146728516 s \tTotal Time:  22.343213319778442 s \n",
      "\n",
      "\n",
      "\tEpisode 3491 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4250],\n",
      "        [1.0000, 0.3983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6200],\n",
      "        [1.0000, 0.6376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.768805027008057 \tStep Time:  0.006981611251831055 s \tTotal Time:  22.350194931030273 s \n",
      "\n",
      "\n",
      "\tEpisode 3492 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5374],\n",
      "        [1.0000, 0.5684]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6041],\n",
      "        [1.0000, 0.6907]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521289825439453 \tStep Time:  0.005984306335449219 s \tTotal Time:  22.356179237365723 s \n",
      "\n",
      "\n",
      "\tEpisode 3493 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5194],\n",
      "        [1.0000, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3557],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547898828983307 \tStep Time:  0.009009122848510742 s \tTotal Time:  22.366185903549194 s \n",
      "\n",
      "\n",
      "\tEpisode 3494 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5860],\n",
      "        [1.0000, 0.5788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4698],\n",
      "        [1.0000, 0.6035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470834732055664 \tStep Time:  0.006947994232177734 s \tTotal Time:  22.373133897781372 s \n",
      "\n",
      "\n",
      "\tEpisode 3495 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6431],\n",
      "        [1.0000, 0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4857],\n",
      "        [1.0000, 0.4487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63166618347168 \tStep Time:  0.005982875823974609 s \tTotal Time:  22.379116773605347 s \n",
      "\n",
      "\n",
      "\tEpisode 3496 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5115],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4877],\n",
      "        [1.0000, 0.5948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567496299743652 \tStep Time:  0.005984306335449219 s \tTotal Time:  22.385101079940796 s \n",
      "\n",
      "\n",
      "\tEpisode 3497 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.4880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.5996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.585958421230316 \tStep Time:  0.006982326507568359 s \tTotal Time:  22.392083406448364 s \n",
      "\n",
      "\n",
      "\tEpisode 3498 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.6080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5039],\n",
      "        [1.0000, 0.5001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480668306350708 \tStep Time:  0.00598454475402832 s \tTotal Time:  22.398067951202393 s \n",
      "\n",
      "\n",
      "\tEpisode 3499 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5537],\n",
      "        [1.0000, 0.5597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6574],\n",
      "        [1.0000, 0.6609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542697608470917 \tStep Time:  0.006980180740356445 s \tTotal Time:  22.40504813194275 s \n",
      "\n",
      "\n",
      "\tEpisode 3500 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5018],\n",
      "        [1.0000, 0.4948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5785],\n",
      "        [1.0000, 0.5777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514047384262085 \tStep Time:  0.005986213684082031 s \tTotal Time:  22.41103434562683 s \n",
      "\n",
      "\n",
      "\tEpisode 3501 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5440],\n",
      "        [1.0000, 0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4917],\n",
      "        [1.0000, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505939960479736 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.4170184135437 s \n",
      "\n",
      "\n",
      "\tEpisode 3502 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5298],\n",
      "        [1.0000, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4914],\n",
      "        [1.0000, 0.5130]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536901891231537 \tStep Time:  0.006028413772583008 s \tTotal Time:  22.423046827316284 s \n",
      "\n",
      "\n",
      "\tEpisode 3503 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4813],\n",
      "        [1.0000, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49790620803833 \tStep Time:  0.006023883819580078 s \tTotal Time:  22.430090188980103 s \n",
      "\n",
      "\n",
      "\tEpisode 3504 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5353],\n",
      "        [1.0000, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4812],\n",
      "        [1.0000, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515573024749756 \tStep Time:  0.005951642990112305 s \tTotal Time:  22.436041831970215 s \n",
      "\n",
      "\n",
      "\tEpisode 3505 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5589],\n",
      "        [1.0000, 0.4929]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5446],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506928443908691 \tStep Time:  0.007010936737060547 s \tTotal Time:  22.443052768707275 s \n",
      "\n",
      "\n",
      "\tEpisode 3506 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5210],\n",
      "        [1.0000, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5225],\n",
      "        [1.0000, 0.5263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499260902404785 \tStep Time:  0.0059871673583984375 s \tTotal Time:  22.449039936065674 s \n",
      "\n",
      "\n",
      "\tEpisode 3507 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5230],\n",
      "        [1.0000, 0.4392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4256],\n",
      "        [1.0000, 0.4315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562300205230713 \tStep Time:  0.006979942321777344 s \tTotal Time:  22.45601987838745 s \n",
      "\n",
      "\n",
      "\tEpisode 3508 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4746],\n",
      "        [1.0000, 0.4569]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5130],\n",
      "        [1.0000, 0.4933]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540302276611328 \tStep Time:  0.006980180740356445 s \tTotal Time:  22.463000059127808 s \n",
      "\n",
      "\n",
      "\tEpisode 3509 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4494],\n",
      "        [1.0000, 0.4398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4279],\n",
      "        [1.0000, 0.4422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52040958404541 \tStep Time:  0.005983591079711914 s \tTotal Time:  22.46898365020752 s \n",
      "\n",
      "\n",
      "\tEpisode 3510 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5257],\n",
      "        [1.0000, 0.4598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5590],\n",
      "        [1.0000, 0.5412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46427059173584 \tStep Time:  0.006983518600463867 s \tTotal Time:  22.475967168807983 s \n",
      "\n",
      "\n",
      "\tEpisode 3511 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4660],\n",
      "        [1.0000, 0.4297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4306],\n",
      "        [1.0000, 0.4305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491554856300354 \tStep Time:  0.0059816837310791016 s \tTotal Time:  22.481948852539062 s \n",
      "\n",
      "\n",
      "\tEpisode 3512 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4188],\n",
      "        [1.0000, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5097],\n",
      "        [1.0000, 0.5183]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553835034370422 \tStep Time:  0.005955696105957031 s \tTotal Time:  22.48790454864502 s \n",
      "\n",
      "\n",
      "\tEpisode 3513 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4396],\n",
      "        [1.0000, 0.4566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.4931]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496315479278564 \tStep Time:  0.006978273391723633 s \tTotal Time:  22.494882822036743 s \n",
      "\n",
      "\n",
      "\tEpisode 3514 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5503],\n",
      "        [1.0000, 0.4652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5100],\n",
      "        [1.0000, 0.4614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492332577705383 \tStep Time:  0.00598454475402832 s \tTotal Time:  22.50086736679077 s \n",
      "\n",
      "\n",
      "\tEpisode 3515 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4620],\n",
      "        [1.0000, 0.5853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5224],\n",
      "        [1.0000, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478293895721436 \tStep Time:  0.007021188735961914 s \tTotal Time:  22.507888555526733 s \n",
      "\n",
      "\n",
      "\tEpisode 3516 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4609],\n",
      "        [1.0000, 0.4640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4382],\n",
      "        [1.0000, 0.5797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438718318939209 \tStep Time:  0.005975008010864258 s \tTotal Time:  22.513863563537598 s \n",
      "\n",
      "\n",
      "\tEpisode 3517 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4578],\n",
      "        [1.0000, 0.4578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4525],\n",
      "        [1.0000, 0.4597]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516658782958984 \tStep Time:  0.007946968078613281 s \tTotal Time:  22.52181053161621 s \n",
      "\n",
      "\n",
      "\tEpisode 3518 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4542],\n",
      "        [1.0000, 0.4629]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5941],\n",
      "        [1.0000, 0.4650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579591393470764 \tStep Time:  0.005983591079711914 s \tTotal Time:  22.527794122695923 s \n",
      "\n",
      "\n",
      "\tEpisode 3519 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5427],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5427],\n",
      "        [1.0000, 0.4764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494865417480469 \tStep Time:  0.004986763000488281 s \tTotal Time:  22.53278088569641 s \n",
      "\n",
      "\n",
      "\tEpisode 3520 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4807],\n",
      "        [1.0000, 0.5357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527642250061035 \tStep Time:  0.007979869842529297 s \tTotal Time:  22.54076075553894 s \n",
      "\n",
      "\n",
      "\tEpisode 3521 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4731],\n",
      "        [1.0000, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4987],\n",
      "        [1.0000, 0.5318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51979684829712 \tStep Time:  0.006980180740356445 s \tTotal Time:  22.547740936279297 s \n",
      "\n",
      "\n",
      "\tEpisode 3522 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.4765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5339],\n",
      "        [1.0000, 0.5281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545225143432617 \tStep Time:  0.005987644195556641 s \tTotal Time:  22.553728580474854 s \n",
      "\n",
      "\n",
      "\tEpisode 3523 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5452],\n",
      "        [1.0000, 0.5240]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493151187896729 \tStep Time:  0.006978034973144531 s \tTotal Time:  22.560706615447998 s \n",
      "\n",
      "\n",
      "\tEpisode 3524 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5549],\n",
      "        [1.0000, 0.6167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4902],\n",
      "        [1.0000, 0.5227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529026448726654 \tStep Time:  0.005983591079711914 s \tTotal Time:  22.56669020652771 s \n",
      "\n",
      "\n",
      "\tEpisode 3525 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5569],\n",
      "        [1.0000, 0.4893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4865],\n",
      "        [1.0000, 0.5341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495737314224243 \tStep Time:  0.006981611251831055 s \tTotal Time:  22.57367181777954 s \n",
      "\n",
      "\n",
      "\tEpisode 3526 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4872],\n",
      "        [1.0000, 0.5210]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4866],\n",
      "        [1.0000, 0.5359]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494860470294952 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.579655647277832 s \n",
      "\n",
      "\n",
      "\tEpisode 3527 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4857],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4923],\n",
      "        [1.0000, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50802993774414 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.585639476776123 s \n",
      "\n",
      "\n",
      "\tEpisode 3528 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4898],\n",
      "        [1.0000, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5213],\n",
      "        [1.0000, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505287170410156 \tStep Time:  0.00797891616821289 s \tTotal Time:  22.593618392944336 s \n",
      "\n",
      "\n",
      "\tEpisode 3529 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.4865]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4801],\n",
      "        [1.0000, 0.4855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507972717285156 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.599602222442627 s \n",
      "\n",
      "\n",
      "\tEpisode 3530 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4827],\n",
      "        [1.0000, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5025],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52400255203247 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.605586051940918 s \n",
      "\n",
      "\n",
      "\tEpisode 3531 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [1.0000, 0.4857]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4797],\n",
      "        [1.0000, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540002048015594 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.611570119857788 s \n",
      "\n",
      "\n",
      "\tEpisode 3532 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4730],\n",
      "        [1.0000, 0.4695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5364],\n",
      "        [1.0000, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543307304382324 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.617554187774658 s \n",
      "\n",
      "\n",
      "\tEpisode 3533 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4772],\n",
      "        [1.0000, 0.4849]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4976],\n",
      "        [1.0000, 0.5010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502885341644287 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.62353825569153 s \n",
      "\n",
      "\n",
      "\tEpisode 3534 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.4987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4827],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510234355926514 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.62952208518982 s \n",
      "\n",
      "\n",
      "\tEpisode 3535 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4856],\n",
      "        [1.0000, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498615205287933 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.63550615310669 s \n",
      "\n",
      "\n",
      "\tEpisode 3536 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4829],\n",
      "        [1.0000, 0.4873]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50118738412857 \tStep Time:  0.006981372833251953 s \tTotal Time:  22.64248752593994 s \n",
      "\n",
      "\n",
      "\tEpisode 3537 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4849],\n",
      "        [1.0000, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4695],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49697732925415 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.64847159385681 s \n",
      "\n",
      "\n",
      "\tEpisode 3538 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4834],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4762],\n",
      "        [1.0000, 0.4566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510120391845703 \tStep Time:  0.00598454475402832 s \tTotal Time:  22.65445613861084 s \n",
      "\n",
      "\n",
      "\tEpisode 3539 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4862],\n",
      "        [1.0000, 0.4692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4827],\n",
      "        [1.0000, 0.4616]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499943852424622 \tStep Time:  0.00698089599609375 s \tTotal Time:  22.661437034606934 s \n",
      "\n",
      "\n",
      "\tEpisode 3540 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [1.0000, 0.4791]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501708984375 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.667420864105225 s \n",
      "\n",
      "\n",
      "\tEpisode 3541 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.4822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4714],\n",
      "        [1.0000, 0.4897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512558937072754 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.673404693603516 s \n",
      "\n",
      "\n",
      "\tEpisode 3542 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4986],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4690],\n",
      "        [1.0000, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544875621795654 \tStep Time:  0.005984306335449219 s \tTotal Time:  22.679388999938965 s \n",
      "\n",
      "\n",
      "\tEpisode 3543 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4735],\n",
      "        [1.0000, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5139],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500523567199707 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.685372829437256 s \n",
      "\n",
      "\n",
      "\tEpisode 3544 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4943],\n",
      "        [1.0000, 0.4943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4919],\n",
      "        [1.0000, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489611625671387 \tStep Time:  0.0069811344146728516 s \tTotal Time:  22.69235396385193 s \n",
      "\n",
      "\n",
      "\tEpisode 3545 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6225],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5344],\n",
      "        [1.0000, 0.5605]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51914358139038 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.6983380317688 s \n",
      "\n",
      "\n",
      "\tEpisode 3546 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5035],\n",
      "        [1.0000, 0.5076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5137],\n",
      "        [1.0000, 0.5432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498397052288055 \tStep Time:  0.0059850215911865234 s \tTotal Time:  22.704323053359985 s \n",
      "\n",
      "\n",
      "\tEpisode 3547 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.5955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554189205169678 \tStep Time:  0.0059833526611328125 s \tTotal Time:  22.710306406021118 s \n",
      "\n",
      "\n",
      "\tEpisode 3548 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5504],\n",
      "        [1.0000, 0.5492]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5357],\n",
      "        [1.0000, 0.5181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518624901771545 \tStep Time:  0.006981372833251953 s \tTotal Time:  22.71728777885437 s \n",
      "\n",
      "\n",
      "\tEpisode 3549 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5364],\n",
      "        [1.0000, 0.4120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5279],\n",
      "        [1.0000, 0.5001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550371170043945 \tStep Time:  0.007978439331054688 s \tTotal Time:  22.725266218185425 s \n",
      "\n",
      "\n",
      "\tEpisode 3550 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.5043]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493052959442139 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.731250286102295 s \n",
      "\n",
      "\n",
      "\tEpisode 3551 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5314],\n",
      "        [1.0000, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5004],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521152019500732 \tStep Time:  0.005986690521240234 s \tTotal Time:  22.737236976623535 s \n",
      "\n",
      "\n",
      "\tEpisode 3552 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.5370]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5267],\n",
      "        [1.0000, 0.5678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527807712554932 \tStep Time:  0.010968923568725586 s \tTotal Time:  22.74820590019226 s \n",
      "\n",
      "\n",
      "\tEpisode 3553 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5045],\n",
      "        [1.0000, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.5417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513817310333252 \tStep Time:  0.009972572326660156 s \tTotal Time:  22.75817847251892 s \n",
      "\n",
      "\n",
      "\tEpisode 3554 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5022],\n",
      "        [1.0000, 0.5178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5120],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501605987548828 \tStep Time:  0.00897669792175293 s \tTotal Time:  22.767155170440674 s \n",
      "\n",
      "\n",
      "\tEpisode 3555 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5190],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.4910]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527641296386719 \tStep Time:  0.009974479675292969 s \tTotal Time:  22.777129650115967 s \n",
      "\n",
      "\n",
      "\tEpisode 3556 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4853],\n",
      "        [1.0000, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4940],\n",
      "        [1.0000, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505778312683105 \tStep Time:  0.00997161865234375 s \tTotal Time:  22.78710126876831 s \n",
      "\n",
      "\n",
      "\tEpisode 3557 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4970],\n",
      "        [1.0000, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525113105773926 \tStep Time:  0.007978200912475586 s \tTotal Time:  22.795079469680786 s \n",
      "\n",
      "\n",
      "\tEpisode 3558 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5008],\n",
      "        [1.0000, 0.5758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4873],\n",
      "        [1.0000, 0.4974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551111221313477 \tStep Time:  0.005983591079711914 s \tTotal Time:  22.801063060760498 s \n",
      "\n",
      "\n",
      "\tEpisode 3559 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4972],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4865],\n",
      "        [1.0000, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479814231395721 \tStep Time:  0.006981372833251953 s \tTotal Time:  22.80804443359375 s \n",
      "\n",
      "\n",
      "\tEpisode 3560 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4698],\n",
      "        [1.0000, 0.4826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5269],\n",
      "        [1.0000, 0.5558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48506498336792 \tStep Time:  0.005984783172607422 s \tTotal Time:  22.814029216766357 s \n",
      "\n",
      "\n",
      "\tEpisode 3561 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5596],\n",
      "        [1.0000, 0.4781]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4785],\n",
      "        [1.0000, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465081691741943 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.820013284683228 s \n",
      "\n",
      "\n",
      "\tEpisode 3562 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4697],\n",
      "        [1.0000, 0.5647]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4819],\n",
      "        [1.0000, 0.4759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555202424526215 \tStep Time:  0.007083892822265625 s \tTotal Time:  22.827097177505493 s \n",
      "\n",
      "\n",
      "\tEpisode 3563 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4484],\n",
      "        [1.0000, 0.5068]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4692],\n",
      "        [1.0000, 0.4692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532623052597046 \tStep Time:  0.005984783172607422 s \tTotal Time:  22.8330819606781 s \n",
      "\n",
      "\n",
      "\tEpisode 3564 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [1.0000, 0.4774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52580738067627 \tStep Time:  0.006948709487915039 s \tTotal Time:  22.840030670166016 s \n",
      "\n",
      "\n",
      "\tEpisode 3565 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3126],\n",
      "        [1.0000, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4631],\n",
      "        [1.0000, 0.4685]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.618115425109863 \tStep Time:  0.006087541580200195 s \tTotal Time:  22.846118211746216 s \n",
      "\n",
      "\n",
      "\tEpisode 3566 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4829],\n",
      "        [1.0000, 0.4781]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [1.0000, 0.5052]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510002195835114 \tStep Time:  0.0059854984283447266 s \tTotal Time:  22.85210371017456 s \n",
      "\n",
      "\n",
      "\tEpisode 3567 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4563],\n",
      "        [1.0000, 0.4742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4887],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537320137023926 \tStep Time:  0.0069789886474609375 s \tTotal Time:  22.85908269882202 s \n",
      "\n",
      "\n",
      "\tEpisode 3568 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4849],\n",
      "        [1.0000, 0.4999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4853],\n",
      "        [1.0000, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512224197387695 \tStep Time:  0.005953073501586914 s \tTotal Time:  22.86503577232361 s \n",
      "\n",
      "\n",
      "\tEpisode 3569 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4750],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5057],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484611511230469 \tStep Time:  0.005983114242553711 s \tTotal Time:  22.871018886566162 s \n",
      "\n",
      "\n",
      "\tEpisode 3570 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.4950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [1.0000, 0.5709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566723346710205 \tStep Time:  0.006022453308105469 s \tTotal Time:  22.877041339874268 s \n",
      "\n",
      "\n",
      "\tEpisode 3571 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4998],\n",
      "        [1.0000, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4888],\n",
      "        [1.0000, 0.4774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504368782043457 \tStep Time:  0.005986690521240234 s \tTotal Time:  22.884018898010254 s \n",
      "\n",
      "\n",
      "\tEpisode 3572 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.5268]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.4896]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49606990814209 \tStep Time:  0.006979703903198242 s \tTotal Time:  22.890998601913452 s \n",
      "\n",
      "\n",
      "\tEpisode 3573 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4668],\n",
      "        [1.0000, 0.5258]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5095],\n",
      "        [1.0000, 0.4853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470940709114075 \tStep Time:  0.005985736846923828 s \tTotal Time:  22.896984338760376 s \n",
      "\n",
      "\n",
      "\tEpisode 3574 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4825],\n",
      "        [1.0000, 0.5534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477248132228851 \tStep Time:  0.0059833526611328125 s \tTotal Time:  22.90296769142151 s \n",
      "\n",
      "\n",
      "\tEpisode 3575 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4911],\n",
      "        [1.0000, 0.5291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4372],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460555076599121 \tStep Time:  0.005982637405395508 s \tTotal Time:  22.908950328826904 s \n",
      "\n",
      "\n",
      "\tEpisode 3576 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4299],\n",
      "        [1.0000, 0.4324]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5491],\n",
      "        [1.0000, 0.5796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.394928455352783 \tStep Time:  0.004986763000488281 s \tTotal Time:  22.914933443069458 s \n",
      "\n",
      "\n",
      "\tEpisode 3577 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4690],\n",
      "        [1.0000, 0.6415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4440],\n",
      "        [1.0000, 0.5312]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457818984985352 \tStep Time:  0.006985187530517578 s \tTotal Time:  22.921918630599976 s \n",
      "\n",
      "\n",
      "\tEpisode 3578 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4777],\n",
      "        [1.0000, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3673],\n",
      "        [1.0000, 0.4383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580973625183105 \tStep Time:  0.006945133209228516 s \tTotal Time:  22.928863763809204 s \n",
      "\n",
      "\n",
      "\tEpisode 3579 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4968],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4689],\n",
      "        [1.0000, 0.5595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560504913330078 \tStep Time:  0.004986763000488281 s \tTotal Time:  22.933850526809692 s \n",
      "\n",
      "\n",
      "\tEpisode 3580 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4681],\n",
      "        [1.0000, 0.7266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5530],\n",
      "        [1.0000, 0.4382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.639647483825684 \tStep Time:  0.006982564926147461 s \tTotal Time:  22.94083309173584 s \n",
      "\n",
      "\n",
      "\tEpisode 3581 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5602],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4457],\n",
      "        [1.0000, 0.5377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501796126365662 \tStep Time:  0.005982875823974609 s \tTotal Time:  22.946815967559814 s \n",
      "\n",
      "\n",
      "\tEpisode 3582 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6156],\n",
      "        [1.0000, 0.4465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.4916]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462913513183594 \tStep Time:  0.005984067916870117 s \tTotal Time:  22.952800035476685 s \n",
      "\n",
      "\n",
      "\tEpisode 3583 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3886],\n",
      "        [1.0000, 0.7213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6825],\n",
      "        [1.0000, 0.4872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.628706932067871 \tStep Time:  0.006981372833251953 s \tTotal Time:  22.959781408309937 s \n",
      "\n",
      "\n",
      "\tEpisode 3584 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.4488]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5474],\n",
      "        [1.0000, 0.4190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50730848312378 \tStep Time:  0.006981849670410156 s \tTotal Time:  22.966763257980347 s \n",
      "\n",
      "\n",
      "\tEpisode 3585 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6661],\n",
      "        [1.0000, 0.6662]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.359292984008789 \tStep Time:  0.006981372833251953 s \tTotal Time:  22.9737446308136 s \n",
      "\n",
      "\n",
      "\tEpisode 3586 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6365],\n",
      "        [1.0000, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5687],\n",
      "        [1.0000, 0.5651]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.616174399852753 \tStep Time:  0.005984783172607422 s \tTotal Time:  22.979729413986206 s \n",
      "\n",
      "\n",
      "\tEpisode 3587 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6571],\n",
      "        [1.0000, 0.4828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3588],\n",
      "        [1.0000, 0.5733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.33814001083374 \tStep Time:  0.006980180740356445 s \tTotal Time:  22.986709594726562 s \n",
      "\n",
      "\n",
      "\tEpisode 3588 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3838],\n",
      "        [1.0000, 0.3994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4330],\n",
      "        [1.0000, 0.4320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533195972442627 \tStep Time:  0.0069811344146728516 s \tTotal Time:  22.993690729141235 s \n",
      "\n",
      "\n",
      "\tEpisode 3589 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [1.0000, 0.4837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4585],\n",
      "        [1.0000, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539057731628418 \tStep Time:  0.005983829498291016 s \tTotal Time:  22.999674558639526 s \n",
      "\n",
      "\n",
      "\tEpisode 3590 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5850],\n",
      "        [1.0000, 0.4473]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3442],\n",
      "        [1.0000, 0.3520]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.70178508758545 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.005658864974976 s \n",
      "\n",
      "\n",
      "\tEpisode 3591 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5241],\n",
      "        [1.0000, 0.3803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4738],\n",
      "        [1.0000, 0.5234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56814956665039 \tStep Time:  0.00698089599609375 s \tTotal Time:  23.01263976097107 s \n",
      "\n",
      "\n",
      "\tEpisode 3592 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4469],\n",
      "        [1.0000, 0.4536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5437],\n",
      "        [1.0000, 0.4515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471922397613525 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.01862406730652 s \n",
      "\n",
      "\n",
      "\tEpisode 3593 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4375],\n",
      "        [1.0000, 0.5717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6227],\n",
      "        [1.0000, 0.5643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.623445093631744 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.02460789680481 s \n",
      "\n",
      "\n",
      "\tEpisode 3594 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.4305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4460],\n",
      "        [1.0000, 0.5522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553025722503662 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.03059196472168 s \n",
      "\n",
      "\n",
      "\tEpisode 3595 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.4799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4771],\n",
      "        [1.0000, 0.5948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568178653717041 \tStep Time:  0.00598454475402832 s \tTotal Time:  23.036576509475708 s \n",
      "\n",
      "\n",
      "\tEpisode 3596 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [1.0000, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517671585083008 \tStep Time:  0.00698089599609375 s \tTotal Time:  23.0435574054718 s \n",
      "\n",
      "\n",
      "\tEpisode 3597 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4834],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506564140319824 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.050538539886475 s \n",
      "\n",
      "\n",
      "\tEpisode 3598 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4970],\n",
      "        [1.0000, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511277675628662 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.056522607803345 s \n",
      "\n",
      "\n",
      "\tEpisode 3599 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4832],\n",
      "        [1.0000, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4872],\n",
      "        [1.0000, 0.5263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53032010793686 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.062506675720215 s \n",
      "\n",
      "\n",
      "\tEpisode 3600 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4875],\n",
      "        [1.0000, 0.4887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4916],\n",
      "        [1.0000, 0.4878]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51002836227417 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.068490743637085 s \n",
      "\n",
      "\n",
      "\tEpisode 3601 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5092],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506903290748596 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.075472116470337 s \n",
      "\n",
      "\n",
      "\tEpisode 3602 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4752],\n",
      "        [1.0000, 0.4429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.4640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504789352416992 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.081455945968628 s \n",
      "\n",
      "\n",
      "\tEpisode 3603 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5470],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [1.0000, 0.4745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552037715911865 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.087440013885498 s \n",
      "\n",
      "\n",
      "\tEpisode 3604 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5326],\n",
      "        [1.0000, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4701],\n",
      "        [1.0000, 0.5782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51062297821045 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.09442114830017 s \n",
      "\n",
      "\n",
      "\tEpisode 3605 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5189],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4986],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522926926612854 \tStep Time:  0.00598454475402832 s \tTotal Time:  23.1004056930542 s \n",
      "\n",
      "\n",
      "\tEpisode 3606 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.5495]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4422],\n",
      "        [1.0000, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444807529449463 \tStep Time:  0.00698089599609375 s \tTotal Time:  23.107386589050293 s \n",
      "\n",
      "\n",
      "\tEpisode 3607 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4822],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4322],\n",
      "        [1.0000, 0.4638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54004955291748 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.113370656967163 s \n",
      "\n",
      "\n",
      "\tEpisode 3608 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4913],\n",
      "        [1.0000, 0.3189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4398],\n",
      "        [1.0000, 0.4636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42536997795105 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.119354963302612 s \n",
      "\n",
      "\n",
      "\tEpisode 3609 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4516],\n",
      "        [1.0000, 0.4835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6341],\n",
      "        [1.0000, 0.4558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41928768157959 \tStep Time:  0.006979703903198242 s \tTotal Time:  23.127333164215088 s \n",
      "\n",
      "\n",
      "\tEpisode 3610 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3266],\n",
      "        [1.0000, 0.4738]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.4202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45557975769043 \tStep Time:  0.007978677749633789 s \tTotal Time:  23.13531184196472 s \n",
      "\n",
      "\n",
      "\tEpisode 3611 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5898],\n",
      "        [1.0000, 0.5340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5075],\n",
      "        [1.0000, 0.5513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516027271747589 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.142293453216553 s \n",
      "\n",
      "\n",
      "\tEpisode 3612 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5379],\n",
      "        [1.0000, 0.4690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3932],\n",
      "        [1.0000, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420176386833191 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.149274587631226 s \n",
      "\n",
      "\n",
      "\tEpisode 3613 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.3765]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7684],\n",
      "        [1.0000, 0.4098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.398216724395752 \tStep Time:  0.00598454475402832 s \tTotal Time:  23.155259132385254 s \n",
      "\n",
      "\n",
      "\tEpisode 3614 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4355],\n",
      "        [1.0000, 0.6365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8279],\n",
      "        [1.0000, 0.4036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.262867450714111 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.162240505218506 s \n",
      "\n",
      "\n",
      "\tEpisode 3615 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5435],\n",
      "        [1.0000, 0.4114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6042],\n",
      "        [1.0000, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.64198637008667 \tStep Time:  0.0069806575775146484 s \tTotal Time:  23.16922116279602 s \n",
      "\n",
      "\n",
      "\tEpisode 3616 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3834],\n",
      "        [1.0000, 0.4840]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3704],\n",
      "        [1.0000, 0.4832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530317783355713 \tStep Time:  0.008976936340332031 s \tTotal Time:  23.178198099136353 s \n",
      "\n",
      "\n",
      "\tEpisode 3617 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5212],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2806],\n",
      "        [1.0000, 0.4050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463401794433594 \tStep Time:  0.007978677749633789 s \tTotal Time:  23.186176776885986 s \n",
      "\n",
      "\n",
      "\tEpisode 3618 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2952],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6169],\n",
      "        [1.0000, 0.4268]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422588348388672 \tStep Time:  0.00698089599609375 s \tTotal Time:  23.19315767288208 s \n",
      "\n",
      "\n",
      "\tEpisode 3619 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3953],\n",
      "        [1.0000, 0.3670]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3942],\n",
      "        [1.0000, 0.4904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59606409072876 \tStep Time:  0.005983114242553711 s \tTotal Time:  23.199140787124634 s \n",
      "\n",
      "\n",
      "\tEpisode 3620 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2839],\n",
      "        [1.0000, 0.5631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4616],\n",
      "        [1.0000, 0.5510]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444007754325867 \tStep Time:  0.006982326507568359 s \tTotal Time:  23.206123113632202 s \n",
      "\n",
      "\n",
      "\tEpisode 3621 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8937],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5042],\n",
      "        [1.0000, 0.5541]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.425143599510193 \tStep Time:  0.006980419158935547 s \tTotal Time:  23.213103532791138 s \n",
      "\n",
      "\n",
      "\tEpisode 3622 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5309],\n",
      "        [1.0000, 0.5335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6235],\n",
      "        [1.0000, 0.6486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519646167755127 \tStep Time:  0.00598454475402832 s \tTotal Time:  23.219088077545166 s \n",
      "\n",
      "\n",
      "\tEpisode 3623 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2573],\n",
      "        [1.0000, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6869],\n",
      "        [1.0000, 0.6257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.680382311344147 \tStep Time:  0.007978439331054688 s \tTotal Time:  23.22706651687622 s \n",
      "\n",
      "\n",
      "\tEpisode 3624 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5492],\n",
      "        [1.0000, 0.6174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4754],\n",
      "        [1.0000, 0.7590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.763862907886505 \tStep Time:  0.007013559341430664 s \tTotal Time:  23.23408007621765 s \n",
      "\n",
      "\n",
      "\tEpisode 3625 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4308],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5880],\n",
      "        [1.0000, 0.5587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.625679433345795 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.240063905715942 s \n",
      "\n",
      "\n",
      "\tEpisode 3626 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5280],\n",
      "        [1.0000, 0.5928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.4049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531590938568115 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.247045516967773 s \n",
      "\n",
      "\n",
      "\tEpisode 3627 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4790],\n",
      "        [1.0000, 0.3893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4559],\n",
      "        [1.0000, 0.3595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444547176361084 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.253029346466064 s \n",
      "\n",
      "\n",
      "\tEpisode 3628 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4462],\n",
      "        [1.0000, 0.4927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4575],\n",
      "        [1.0000, 0.3856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552515268325806 \tStep Time:  0.006983757019042969 s \tTotal Time:  23.260013103485107 s \n",
      "\n",
      "\n",
      "\tEpisode 3629 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4246],\n",
      "        [1.0000, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.4650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565523624420166 \tStep Time:  0.007945060729980469 s \tTotal Time:  23.267958164215088 s \n",
      "\n",
      "\n",
      "\tEpisode 3630 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4368],\n",
      "        [1.0000, 0.5516]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5246],\n",
      "        [1.0000, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.589178085327148 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.273942470550537 s \n",
      "\n",
      "\n",
      "\tEpisode 3631 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5542],\n",
      "        [1.0000, 0.5453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6018],\n",
      "        [1.0000, 0.5868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524743497371674 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.28092360496521 s \n",
      "\n",
      "\n",
      "\tEpisode 3632 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4775],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5377],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547394275665283 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.28790521621704 s \n",
      "\n",
      "\n",
      "\tEpisode 3633 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4952],\n",
      "        [1.0000, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4697],\n",
      "        [1.0000, 0.6013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510203838348389 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.293889045715332 s \n",
      "\n",
      "\n",
      "\tEpisode 3634 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4936],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5015],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536317825317383 \tStep Time:  0.006982088088989258 s \tTotal Time:  23.30087113380432 s \n",
      "\n",
      "\n",
      "\tEpisode 3635 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5190],\n",
      "        [1.0000, 0.5312]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4274],\n",
      "        [1.0000, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480328559875488 \tStep Time:  0.006016254425048828 s \tTotal Time:  23.30688738822937 s \n",
      "\n",
      "\n",
      "\tEpisode 3636 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4953],\n",
      "        [1.0000, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521723747253418 \tStep Time:  0.004990577697753906 s \tTotal Time:  23.312872648239136 s \n",
      "\n",
      "\n",
      "\tEpisode 3637 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4745],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5031],\n",
      "        [1.0000, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529969036579132 \tStep Time:  0.00498509407043457 s \tTotal Time:  23.31785774230957 s \n",
      "\n",
      "\n",
      "\tEpisode 3638 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4969],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.4700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497501134872437 \tStep Time:  0.005984783172607422 s \tTotal Time:  23.323842525482178 s \n",
      "\n",
      "\n",
      "\tEpisode 3639 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4936],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5008],\n",
      "        [1.0000, 0.4964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516644954681396 \tStep Time:  0.005980730056762695 s \tTotal Time:  23.32982325553894 s \n",
      "\n",
      "\n",
      "\tEpisode 3640 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4962],\n",
      "        [1.0000, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4906],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511808931827545 \tStep Time:  0.006949901580810547 s \tTotal Time:  23.33677315711975 s \n",
      "\n",
      "\n",
      "\tEpisode 3641 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3767],\n",
      "        [1.0000, 0.4950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.7236]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.706417918205261 \tStep Time:  0.005983591079711914 s \tTotal Time:  23.342756748199463 s \n",
      "\n",
      "\n",
      "\tEpisode 3642 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4626],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5100],\n",
      "        [1.0000, 0.5079]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536128044128418 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.348741054534912 s \n",
      "\n",
      "\n",
      "\tEpisode 3643 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5108],\n",
      "        [1.0000, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4190],\n",
      "        [1.0000, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482953310012817 \tStep Time:  0.006981849670410156 s \tTotal Time:  23.355722904205322 s \n",
      "\n",
      "\n",
      "\tEpisode 3644 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4404],\n",
      "        [1.0000, 0.5208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5284],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46672260761261 \tStep Time:  0.005983591079711914 s \tTotal Time:  23.361706495285034 s \n",
      "\n",
      "\n",
      "\tEpisode 3645 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5492],\n",
      "        [1.0000, 0.3915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.4228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526164054870605 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.367690563201904 s \n",
      "\n",
      "\n",
      "\tEpisode 3646 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4161],\n",
      "        [1.0000, 0.5675]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3296],\n",
      "        [1.0000, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.37775731086731 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.374671697616577 s \n",
      "\n",
      "\n",
      "\tEpisode 3647 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4043],\n",
      "        [1.0000, 0.5001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3095],\n",
      "        [1.0000, 0.2986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.598552942276001 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.38165307044983 s \n",
      "\n",
      "\n",
      "\tEpisode 3648 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5715],\n",
      "        [1.0000, 0.5512]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2759],\n",
      "        [1.0000, 0.3751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.330559730529785 \tStep Time:  0.006476163864135742 s \tTotal Time:  23.388129234313965 s \n",
      "\n",
      "\n",
      "\tEpisode 3649 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5817],\n",
      "        [1.0000, 0.4151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5409],\n",
      "        [1.0000, 0.5647]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429516792297363 \tStep Time:  0.006491184234619141 s \tTotal Time:  23.394620418548584 s \n",
      "\n",
      "\n",
      "\tEpisode 3650 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5651],\n",
      "        [1.0000, 0.5315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6820],\n",
      "        [1.0000, 0.6691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523073732852936 \tStep Time:  0.005981922149658203 s \tTotal Time:  23.400602340698242 s \n",
      "\n",
      "\n",
      "\tEpisode 3651 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6090],\n",
      "        [1.0000, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6773],\n",
      "        [1.0000, 0.2830]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.295336961746216 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.407583951950073 s \n",
      "\n",
      "\n",
      "\tEpisode 3652 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5522],\n",
      "        [1.0000, 0.4626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2692],\n",
      "        [1.0000, 0.3371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577970266342163 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.413567781448364 s \n",
      "\n",
      "\n",
      "\tEpisode 3653 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.7250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7338],\n",
      "        [1.0000, 0.3849]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507524967193604 \tStep Time:  0.005983591079711914 s \tTotal Time:  23.419551372528076 s \n",
      "\n",
      "\n",
      "\tEpisode 3654 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6246],\n",
      "        [1.0000, 0.6208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4522],\n",
      "        [1.0000, 0.3969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520741939544678 \tStep Time:  0.006981849670410156 s \tTotal Time:  23.426533222198486 s \n",
      "\n",
      "\n",
      "\tEpisode 3655 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3901],\n",
      "        [1.0000, 0.2761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3755],\n",
      "        [1.0000, 0.2667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50151014328003 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.432517051696777 s \n",
      "\n",
      "\n",
      "\tEpisode 3656 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3548],\n",
      "        [1.0000, 0.7225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5745],\n",
      "        [1.0000, 0.7738]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.336647510528564 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.438501119613647 s \n",
      "\n",
      "\n",
      "\tEpisode 3657 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4391],\n",
      "        [1.0000, 0.7003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3141],\n",
      "        [1.0000, 0.5774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.821456730365753 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.44548225402832 s \n",
      "\n",
      "\n",
      "\tEpisode 3658 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5957],\n",
      "        [1.0000, 0.2515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4566],\n",
      "        [1.0000, 0.6030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.660953998565674 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.45146632194519 s \n",
      "\n",
      "\n",
      "\tEpisode 3659 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7768],\n",
      "        [1.0000, 0.6676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6706],\n",
      "        [1.0000, 0.2404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.360262870788574 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.45745038986206 s \n",
      "\n",
      "\n",
      "\tEpisode 3660 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4997],\n",
      "        [1.0000, 0.7587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3410],\n",
      "        [1.0000, 0.7703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526094913482666 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.464431524276733 s \n",
      "\n",
      "\n",
      "\tEpisode 3661 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7663],\n",
      "        [1.0000, 0.5547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5454],\n",
      "        [1.0000, 0.2001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.343783378601074 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.470415592193604 s \n",
      "\n",
      "\n",
      "\tEpisode 3662 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2165],\n",
      "        [1.0000, 0.6843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7599],\n",
      "        [1.0000, 0.3106]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.630900382995605 \tStep Time:  0.006983280181884766 s \tTotal Time:  23.47739887237549 s \n",
      "\n",
      "\n",
      "\tEpisode 3663 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2564],\n",
      "        [1.0000, 0.5116]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4147],\n",
      "        [1.0000, 0.2112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.706506729125977 \tStep Time:  0.005982160568237305 s \tTotal Time:  23.483381032943726 s \n",
      "\n",
      "\n",
      "\tEpisode 3664 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7205],\n",
      "        [1.0000, 0.3214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1971],\n",
      "        [1.0000, 0.6592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.635097622871399 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.490362405776978 s \n",
      "\n",
      "\n",
      "\tEpisode 3665 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6296],\n",
      "        [1.0000, 0.7148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5749],\n",
      "        [1.0000, 0.4272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441495418548584 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.496346473693848 s \n",
      "\n",
      "\n",
      "\tEpisode 3666 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3847],\n",
      "        [1.0000, 0.5451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5689],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.635343551635742 \tStep Time:  0.005983591079711914 s \tTotal Time:  23.50233006477356 s \n",
      "\n",
      "\n",
      "\tEpisode 3667 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3035],\n",
      "        [1.0000, 0.4955]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3208],\n",
      "        [1.0000, 0.3587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524848401546478 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.50831413269043 s \n",
      "\n",
      "\n",
      "\tEpisode 3668 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5631],\n",
      "        [1.0000, 0.5282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6901],\n",
      "        [1.0000, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.675965785980225 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.5142982006073 s \n",
      "\n",
      "\n",
      "\tEpisode 3669 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4307],\n",
      "        [1.0000, 0.5626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6206],\n",
      "        [1.0000, 0.2493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530606746673584 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.52028203010559 s \n",
      "\n",
      "\n",
      "\tEpisode 3670 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6804],\n",
      "        [1.0000, 0.5244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4677],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47137999534607 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.527263402938843 s \n",
      "\n",
      "\n",
      "\tEpisode 3671 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2992],\n",
      "        [1.0000, 0.4966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4332],\n",
      "        [1.0000, 0.4550]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441962659358978 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.534244537353516 s \n",
      "\n",
      "\n",
      "\tEpisode 3672 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5782],\n",
      "        [1.0000, 0.6658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7002],\n",
      "        [1.0000, 0.2586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422387659549713 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.541226148605347 s \n",
      "\n",
      "\n",
      "\tEpisode 3673 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5435],\n",
      "        [1.0000, 0.6265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6796],\n",
      "        [1.0000, 0.6562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.644119262695312 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.547210216522217 s \n",
      "\n",
      "\n",
      "\tEpisode 3674 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6387],\n",
      "        [1.0000, 0.6883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6150],\n",
      "        [1.0000, 0.4985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445420742034912 \tStep Time:  0.006982088088989258 s \tTotal Time:  23.554192304611206 s \n",
      "\n",
      "\n",
      "\tEpisode 3675 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5480],\n",
      "        [1.0000, 0.5634]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5327],\n",
      "        [1.0000, 0.5253]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51641321182251 \tStep Time:  0.0069806575775146484 s \tTotal Time:  23.56117296218872 s \n",
      "\n",
      "\n",
      "\tEpisode 3676 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4054],\n",
      "        [1.0000, 0.6226]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6393],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579600989818573 \tStep Time:  0.0059850215911865234 s \tTotal Time:  23.567157983779907 s \n",
      "\n",
      "\n",
      "\tEpisode 3677 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5403],\n",
      "        [1.0000, 0.4403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6249],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529097080230713 \tStep Time:  0.007977724075317383 s \tTotal Time:  23.575135707855225 s \n",
      "\n",
      "\n",
      "\tEpisode 3678 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2727],\n",
      "        [1.0000, 0.4341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5595],\n",
      "        [1.0000, 0.2777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.635658740997314 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.581119537353516 s \n",
      "\n",
      "\n",
      "\tEpisode 3679 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5495],\n",
      "        [1.0000, 0.3936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5657],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459228992462158 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.588100910186768 s \n",
      "\n",
      "\n",
      "\tEpisode 3680 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5411],\n",
      "        [1.0000, 0.4596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3643],\n",
      "        [1.0000, 0.5330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58394193649292 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.594085216522217 s \n",
      "\n",
      "\n",
      "\tEpisode 3681 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5431],\n",
      "        [1.0000, 0.4588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2872],\n",
      "        [1.0000, 0.4508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.669514656066895 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.60106635093689 s \n",
      "\n",
      "\n",
      "\tEpisode 3682 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4341],\n",
      "        [1.0000, 0.4049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3498],\n",
      "        [1.0000, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566479682922363 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.60804772377014 s \n",
      "\n",
      "\n",
      "\tEpisode 3683 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4176],\n",
      "        [1.0000, 0.4486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5058],\n",
      "        [1.0000, 0.4329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.556490421295166 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.61403203010559 s \n",
      "\n",
      "\n",
      "\tEpisode 3684 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5236],\n",
      "        [1.0000, 0.3703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5246],\n",
      "        [1.0000, 0.4219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402330875396729 \tStep Time:  0.005983829498291016 s \tTotal Time:  23.620015859603882 s \n",
      "\n",
      "\n",
      "\tEpisode 3685 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3860],\n",
      "        [1.0000, 0.4624]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47869062423706 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.626996994018555 s \n",
      "\n",
      "\n",
      "\tEpisode 3686 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3674],\n",
      "        [1.0000, 0.5315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4780],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499578952789307 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.633978128433228 s \n",
      "\n",
      "\n",
      "\tEpisode 3687 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3766],\n",
      "        [1.0000, 0.3638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4198],\n",
      "        [1.0000, 0.5380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464883387088776 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.639962196350098 s \n",
      "\n",
      "\n",
      "\tEpisode 3688 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.5458]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5495],\n",
      "        [1.0000, 0.5465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48445749282837 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.645946264266968 s \n",
      "\n",
      "\n",
      "\tEpisode 3689 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4037],\n",
      "        [1.0000, 0.4604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5627],\n",
      "        [1.0000, 0.5660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49741017818451 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.65292739868164 s \n",
      "\n",
      "\n",
      "\tEpisode 3690 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.5551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4671],\n",
      "        [1.0000, 0.5279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530323028564453 \tStep Time:  0.006981372833251953 s \tTotal Time:  23.659908771514893 s \n",
      "\n",
      "\n",
      "\tEpisode 3691 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5443],\n",
      "        [1.0000, 0.5619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3777],\n",
      "        [1.0000, 0.3623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.719319343566895 \tStep Time:  0.0059850215911865234 s \tTotal Time:  23.66589379310608 s \n",
      "\n",
      "\n",
      "\tEpisode 3692 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5756],\n",
      "        [1.0000, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5248],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458340167999268 \tStep Time:  0.009973287582397461 s \tTotal Time:  23.675867080688477 s \n",
      "\n",
      "\n",
      "\tEpisode 3693 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5610],\n",
      "        [1.0000, 0.3682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3943],\n",
      "        [1.0000, 0.4243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.605597019195557 \tStep Time:  0.006980419158935547 s \tTotal Time:  23.682847499847412 s \n",
      "\n",
      "\n",
      "\tEpisode 3694 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5748],\n",
      "        [1.0000, 0.4089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.4516]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558687210083008 \tStep Time:  0.004988193511962891 s \tTotal Time:  23.687835693359375 s \n",
      "\n",
      "\n",
      "\tEpisode 3695 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5108],\n",
      "        [1.0000, 0.5285]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482221603393555 \tStep Time:  0.007977008819580078 s \tTotal Time:  23.695812702178955 s \n",
      "\n",
      "\n",
      "\tEpisode 3696 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5729],\n",
      "        [1.0000, 0.4797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4863],\n",
      "        [1.0000, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45171070098877 \tStep Time:  0.004986286163330078 s \tTotal Time:  23.700798988342285 s \n",
      "\n",
      "\n",
      "\tEpisode 3697 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5406],\n",
      "        [1.0000, 0.5407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5305],\n",
      "        [1.0000, 0.5644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532367706298828 \tStep Time:  0.005984783172607422 s \tTotal Time:  23.706783771514893 s \n",
      "\n",
      "\n",
      "\tEpisode 3698 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5223],\n",
      "        [1.0000, 0.5449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4207],\n",
      "        [1.0000, 0.5606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428874969482422 \tStep Time:  0.005991220474243164 s \tTotal Time:  23.712774991989136 s \n",
      "\n",
      "\n",
      "\tEpisode 3699 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4959],\n",
      "        [1.0000, 0.5555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5242],\n",
      "        [1.0000, 0.5568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463111877441406 \tStep Time:  0.006017923355102539 s \tTotal Time:  23.71879291534424 s \n",
      "\n",
      "\n",
      "\tEpisode 3700 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4872],\n",
      "        [1.0000, 0.5559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5280],\n",
      "        [1.0000, 0.5493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495599746704102 \tStep Time:  0.0049855709075927734 s \tTotal Time:  23.72377848625183 s \n",
      "\n",
      "\n",
      "\tEpisode 3701 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5431],\n",
      "        [1.0000, 0.5392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503320693969727 \tStep Time:  0.007437944412231445 s \tTotal Time:  23.731216430664062 s \n",
      "\n",
      "\n",
      "\tEpisode 3702 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4633],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3754],\n",
      "        [1.0000, 0.5455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577799797058105 \tStep Time:  0.004996776580810547 s \tTotal Time:  23.736213207244873 s \n",
      "\n",
      "\n",
      "\tEpisode 3703 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4599],\n",
      "        [1.0000, 0.5484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5189],\n",
      "        [1.0000, 0.5303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530362129211426 \tStep Time:  0.006949663162231445 s \tTotal Time:  23.743162870407104 s \n",
      "\n",
      "\n",
      "\tEpisode 3704 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5389],\n",
      "        [1.0000, 0.5301]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5313],\n",
      "        [1.0000, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512779712677002 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.749146938323975 s \n",
      "\n",
      "\n",
      "\tEpisode 3705 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4449],\n",
      "        [1.0000, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [1.0000, 0.5334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541921138763428 \tStep Time:  0.006298065185546875 s \tTotal Time:  23.75544500350952 s \n",
      "\n",
      "\n",
      "\tEpisode 3706 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5086],\n",
      "        [1.0000, 0.5372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5361],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507253646850586 \tStep Time:  0.006667613983154297 s \tTotal Time:  23.762112617492676 s \n",
      "\n",
      "\n",
      "\tEpisode 3707 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5524],\n",
      "        [1.0000, 0.5409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5429],\n",
      "        [1.0000, 0.5458]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512222290039062 \tStep Time:  0.0069806575775146484 s \tTotal Time:  23.76909327507019 s \n",
      "\n",
      "\n",
      "\tEpisode 3708 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5145],\n",
      "        [1.0000, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.4378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471812725067139 \tStep Time:  0.006982088088989258 s \tTotal Time:  23.77607536315918 s \n",
      "\n",
      "\n",
      "\tEpisode 3709 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4036],\n",
      "        [1.0000, 0.5349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5184],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568275451660156 \tStep Time:  0.00698089599609375 s \tTotal Time:  23.783056259155273 s \n",
      "\n",
      "\n",
      "\tEpisode 3710 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3649],\n",
      "        [1.0000, 0.4978]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5118],\n",
      "        [1.0000, 0.4866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58213758468628 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.789040327072144 s \n",
      "\n",
      "\n",
      "\tEpisode 3711 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4141],\n",
      "        [1.0000, 0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5152],\n",
      "        [1.0000, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472581386566162 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.796021461486816 s \n",
      "\n",
      "\n",
      "\tEpisode 3712 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [1.0000, 0.3933]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5048],\n",
      "        [1.0000, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459417462348938 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.802005529403687 s \n",
      "\n",
      "\n",
      "\tEpisode 3713 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.5502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5602],\n",
      "        [1.0000, 0.5620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548873901367188 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.80898666381836 s \n",
      "\n",
      "\n",
      "\tEpisode 3714 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5289],\n",
      "        [1.0000, 0.5571]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.5575]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530416488647461 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.81497097015381 s \n",
      "\n",
      "\n",
      "\tEpisode 3715 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5627],\n",
      "        [1.0000, 0.5538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5515],\n",
      "        [1.0000, 0.5645]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511651992797852 \tStep Time:  0.006982088088989258 s \tTotal Time:  23.821953058242798 s \n",
      "\n",
      "\n",
      "\tEpisode 3716 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4579],\n",
      "        [1.0000, 0.5573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5476],\n",
      "        [1.0000, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563657343387604 \tStep Time:  0.005982875823974609 s \tTotal Time:  23.827935934066772 s \n",
      "\n",
      "\n",
      "\tEpisode 3717 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3299],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3171],\n",
      "        [1.0000, 0.5358]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.32675313949585 \tStep Time:  0.005984306335449219 s \tTotal Time:  23.83392024040222 s \n",
      "\n",
      "\n",
      "\tEpisode 3718 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4415],\n",
      "        [1.0000, 0.4381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5482],\n",
      "        [1.0000, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48831558227539 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.840901374816895 s \n",
      "\n",
      "\n",
      "\tEpisode 3719 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5520],\n",
      "        [1.0000, 0.4230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3456],\n",
      "        [1.0000, 0.5321]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575506687164307 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.846885442733765 s \n",
      "\n",
      "\n",
      "\tEpisode 3720 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5296],\n",
      "        [1.0000, 0.5285]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.4340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51348340511322 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.853867053985596 s \n",
      "\n",
      "\n",
      "\tEpisode 3721 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5280],\n",
      "        [1.0000, 0.4062]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5224],\n",
      "        [1.0000, 0.4563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480025291442871 \tStep Time:  0.005983591079711914 s \tTotal Time:  23.859850645065308 s \n",
      "\n",
      "\n",
      "\tEpisode 3722 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4517],\n",
      "        [1.0000, 0.5404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4465],\n",
      "        [1.0000, 0.4155]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57574987411499 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.865834712982178 s \n",
      "\n",
      "\n",
      "\tEpisode 3723 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5336],\n",
      "        [1.0000, 0.4532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5430],\n",
      "        [1.0000, 0.5115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540621280670166 \tStep Time:  0.006983041763305664 s \tTotal Time:  23.872817754745483 s \n",
      "\n",
      "\n",
      "\tEpisode 3724 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4688],\n",
      "        [1.0000, 0.4902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5211],\n",
      "        [1.0000, 0.2826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433764457702637 \tStep Time:  0.006979703903198242 s \tTotal Time:  23.87979745864868 s \n",
      "\n",
      "\n",
      "\tEpisode 3725 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [1.0000, 0.5440]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2698],\n",
      "        [1.0000, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4043550491333 \tStep Time:  0.005984067916870117 s \tTotal Time:  23.88578152656555 s \n",
      "\n",
      "\n",
      "\tEpisode 3726 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5488],\n",
      "        [1.0000, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.3710]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.403604507446289 \tStep Time:  0.006981849670410156 s \tTotal Time:  23.892763376235962 s \n",
      "\n",
      "\n",
      "\tEpisode 3727 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3521],\n",
      "        [1.0000, 0.4073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4872],\n",
      "        [1.0000, 0.5489]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66670846939087 \tStep Time:  0.005983591079711914 s \tTotal Time:  23.898746967315674 s \n",
      "\n",
      "\n",
      "\tEpisode 3728 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5343],\n",
      "        [1.0000, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5571],\n",
      "        [1.0000, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518032550811768 \tStep Time:  0.006982088088989258 s \tTotal Time:  23.905729055404663 s \n",
      "\n",
      "\n",
      "\tEpisode 3729 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5016],\n",
      "        [1.0000, 0.3676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3797],\n",
      "        [1.0000, 0.3291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62669324874878 \tStep Time:  0.0069806575775146484 s \tTotal Time:  23.912709712982178 s \n",
      "\n",
      "\n",
      "\tEpisode 3730 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5427],\n",
      "        [1.0000, 0.5677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5004],\n",
      "        [1.0000, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500251173973083 \tStep Time:  0.005983591079711914 s \tTotal Time:  23.91869330406189 s \n",
      "\n",
      "\n",
      "\tEpisode 3731 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3489],\n",
      "        [1.0000, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.609580636024475 \tStep Time:  0.006982326507568359 s \tTotal Time:  23.925675630569458 s \n",
      "\n",
      "\n",
      "\tEpisode 3732 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4531],\n",
      "        [1.0000, 0.4325]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3986],\n",
      "        [1.0000, 0.5545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458540141582489 \tStep Time:  0.005983114242553711 s \tTotal Time:  23.93165874481201 s \n",
      "\n",
      "\n",
      "\tEpisode 3733 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5784],\n",
      "        [1.0000, 0.5779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5676],\n",
      "        [1.0000, 0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55075216293335 \tStep Time:  0.006981849670410156 s \tTotal Time:  23.938640594482422 s \n",
      "\n",
      "\n",
      "\tEpisode 3734 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3982],\n",
      "        [1.0000, 0.5719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5648],\n",
      "        [1.0000, 0.5717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610757827758789 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.945622205734253 s \n",
      "\n",
      "\n",
      "\tEpisode 3735 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5339],\n",
      "        [1.0000, 0.5638]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5559],\n",
      "        [1.0000, 0.5574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513086795806885 \tStep Time:  0.006980419158935547 s \tTotal Time:  23.95260262489319 s \n",
      "\n",
      "\n",
      "\tEpisode 3736 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5485],\n",
      "        [1.0000, 0.5489]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5502],\n",
      "        [1.0000, 0.5484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513190567493439 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.95958423614502 s \n",
      "\n",
      "\n",
      "\tEpisode 3737 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5531],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5399],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506424903869629 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.966565370559692 s \n",
      "\n",
      "\n",
      "\tEpisode 3738 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5254],\n",
      "        [1.0000, 0.5571]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5308],\n",
      "        [1.0000, 0.4435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567837238311768 \tStep Time:  0.006981611251831055 s \tTotal Time:  23.973546981811523 s \n",
      "\n",
      "\n",
      "\tEpisode 3739 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5469],\n",
      "        [1.0000, 0.5450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3972],\n",
      "        [1.0000, 0.5315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583976745605469 \tStep Time:  0.0069811344146728516 s \tTotal Time:  23.980528116226196 s \n",
      "\n",
      "\n",
      "\tEpisode 3740 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.5363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5252],\n",
      "        [1.0000, 0.5432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542592883110046 \tStep Time:  0.009973526000976562 s \tTotal Time:  23.990501642227173 s \n",
      "\n",
      "\n",
      "\tEpisode 3741 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5253],\n",
      "        [1.0000, 0.5053]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4648],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51570987701416 \tStep Time:  0.007979869842529297 s \tTotal Time:  23.998481512069702 s \n",
      "\n",
      "\n",
      "\tEpisode 3742 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5320],\n",
      "        [1.0000, 0.4747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5330],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478084444999695 \tStep Time:  0.00698089599609375 s \tTotal Time:  24.005462408065796 s \n",
      "\n",
      "\n",
      "\tEpisode 3743 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5400],\n",
      "        [1.0000, 0.5294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4805],\n",
      "        [1.0000, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480743408203125 \tStep Time:  0.0069811344146728516 s \tTotal Time:  24.01244354248047 s \n",
      "\n",
      "\n",
      "\tEpisode 3744 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5294],\n",
      "        [1.0000, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5206],\n",
      "        [1.0000, 0.4449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546377658843994 \tStep Time:  0.010970115661621094 s \tTotal Time:  24.02341365814209 s \n",
      "\n",
      "\n",
      "\tEpisode 3745 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4560],\n",
      "        [1.0000, 0.4807]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502304434776306 \tStep Time:  0.011969327926635742 s \tTotal Time:  24.035382986068726 s \n",
      "\n",
      "\n",
      "\tEpisode 3746 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5238],\n",
      "        [1.0000, 0.5417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5256],\n",
      "        [1.0000, 0.4984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486676692962646 \tStep Time:  0.009975671768188477 s \tTotal Time:  24.045358657836914 s \n",
      "\n",
      "\n",
      "\tEpisode 3747 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4608],\n",
      "        [1.0000, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5395],\n",
      "        [1.0000, 0.4342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444669246673584 \tStep Time:  0.007979869842529297 s \tTotal Time:  24.053338527679443 s \n",
      "\n",
      "\n",
      "\tEpisode 3748 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5196],\n",
      "        [1.0000, 0.4298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.5357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464245676994324 \tStep Time:  0.00797724723815918 s \tTotal Time:  24.061315774917603 s \n",
      "\n",
      "\n",
      "\tEpisode 3749 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5397],\n",
      "        [1.0000, 0.5201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5195],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508980393409729 \tStep Time:  0.005984783172607422 s \tTotal Time:  24.06730055809021 s \n",
      "\n",
      "\n",
      "\tEpisode 3750 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5379],\n",
      "        [1.0000, 0.3700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4576],\n",
      "        [1.0000, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541846752166748 \tStep Time:  0.007979393005371094 s \tTotal Time:  24.07527995109558 s \n",
      "\n",
      "\n",
      "\tEpisode 3751 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5404],\n",
      "        [1.0000, 0.5451]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543414115905762 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.081263542175293 s \n",
      "\n",
      "\n",
      "\tEpisode 3752 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4188],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4230],\n",
      "        [1.0000, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48958969116211 \tStep Time:  0.006981611251831055 s \tTotal Time:  24.088245153427124 s \n",
      "\n",
      "\n",
      "\tEpisode 3753 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4222],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5321],\n",
      "        [1.0000, 0.5292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563496112823486 \tStep Time:  0.0059833526611328125 s \tTotal Time:  24.094228506088257 s \n",
      "\n",
      "\n",
      "\tEpisode 3754 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4351],\n",
      "        [1.0000, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4400],\n",
      "        [1.0000, 0.4493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478074193000793 \tStep Time:  0.007022857666015625 s \tTotal Time:  24.101251363754272 s \n",
      "\n",
      "\n",
      "\tEpisode 3755 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.5386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5257],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500554621219635 \tStep Time:  0.006972074508666992 s \tTotal Time:  24.10822343826294 s \n",
      "\n",
      "\n",
      "\tEpisode 3756 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5466],\n",
      "        [1.0000, 0.5233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5428],\n",
      "        [1.0000, 0.5365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51999181509018 \tStep Time:  0.00598454475402832 s \tTotal Time:  24.114207983016968 s \n",
      "\n",
      "\n",
      "\tEpisode 3757 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.5446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5356],\n",
      "        [1.0000, 0.5369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498979568481445 \tStep Time:  0.005982875823974609 s \tTotal Time:  24.120190858840942 s \n",
      "\n",
      "\n",
      "\tEpisode 3758 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.4733]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5054],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467942714691162 \tStep Time:  0.007983207702636719 s \tTotal Time:  24.12817406654358 s \n",
      "\n",
      "\n",
      "\tEpisode 3759 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4705],\n",
      "        [1.0000, 0.5378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5050],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525089263916016 \tStep Time:  0.0059814453125 s \tTotal Time:  24.13415551185608 s \n",
      "\n",
      "\n",
      "\tEpisode 3760 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.5459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4773],\n",
      "        [1.0000, 0.4516]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5338454246521 \tStep Time:  0.006979942321777344 s \tTotal Time:  24.141135454177856 s \n",
      "\n",
      "\n",
      "\tEpisode 3761 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4277],\n",
      "        [1.0000, 0.4059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4444],\n",
      "        [1.0000, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447485446929932 \tStep Time:  0.0069484710693359375 s \tTotal Time:  24.148083925247192 s \n",
      "\n",
      "\n",
      "\tEpisode 3762 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5346],\n",
      "        [1.0000, 0.4080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3979],\n",
      "        [1.0000, 0.5318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52205604314804 \tStep Time:  0.004986763000488281 s \tTotal Time:  24.15307068824768 s \n",
      "\n",
      "\n",
      "\tEpisode 3763 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4417],\n",
      "        [1.0000, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4161],\n",
      "        [1.0000, 0.4280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484616935253143 \tStep Time:  0.006981849670410156 s \tTotal Time:  24.16005253791809 s \n",
      "\n",
      "\n",
      "\tEpisode 3764 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.5299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4341],\n",
      "        [1.0000, 0.5350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56017017364502 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.166036128997803 s \n",
      "\n",
      "\n",
      "\tEpisode 3765 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4511],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5161],\n",
      "        [1.0000, 0.5271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462892830371857 \tStep Time:  0.006981611251831055 s \tTotal Time:  24.173017740249634 s \n",
      "\n",
      "\n",
      "\tEpisode 3766 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5334],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5267],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49809741973877 \tStep Time:  0.00698089599609375 s \tTotal Time:  24.179998636245728 s \n",
      "\n",
      "\n",
      "\tEpisode 3767 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5305],\n",
      "        [1.0000, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5111],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52298355102539 \tStep Time:  0.006981849670410156 s \tTotal Time:  24.186980485916138 s \n",
      "\n",
      "\n",
      "\tEpisode 3768 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5319],\n",
      "        [1.0000, 0.5346]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5044],\n",
      "        [1.0000, 0.5227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493584156036377 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.19296407699585 s \n",
      "\n",
      "\n",
      "\tEpisode 3769 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5214],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5319],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486618041992188 \tStep Time:  0.0069811344146728516 s \tTotal Time:  24.199945211410522 s \n",
      "\n",
      "\n",
      "\tEpisode 3770 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5359],\n",
      "        [1.0000, 0.5362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5401],\n",
      "        [1.0000, 0.5656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519045352935791 \tStep Time:  0.0059850215911865234 s \tTotal Time:  24.20593023300171 s \n",
      "\n",
      "\n",
      "\tEpisode 3771 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6282],\n",
      "        [1.0000, 0.5504]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5372],\n",
      "        [1.0000, 0.5391]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472147464752197 \tStep Time:  0.006980419158935547 s \tTotal Time:  24.212910652160645 s \n",
      "\n",
      "\n",
      "\tEpisode 3772 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5389],\n",
      "        [1.0000, 0.5357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4606],\n",
      "        [1.0000, 0.4330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59757661819458 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.218894481658936 s \n",
      "\n",
      "\n",
      "\tEpisode 3773 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4949],\n",
      "        [1.0000, 0.5187]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515856385231018 \tStep Time:  0.005984306335449219 s \tTotal Time:  24.224878787994385 s \n",
      "\n",
      "\n",
      "\tEpisode 3774 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5207],\n",
      "        [1.0000, 0.5217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [1.0000, 0.5006]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49634838104248 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.231859922409058 s \n",
      "\n",
      "\n",
      "\tEpisode 3775 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3276],\n",
      "        [1.0000, 0.4673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5065],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.632583141326904 \tStep Time:  0.00598454475402832 s \tTotal Time:  24.237844467163086 s \n",
      "\n",
      "\n",
      "\tEpisode 3776 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5199],\n",
      "        [1.0000, 0.1899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.4979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.378360986709595 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.243828296661377 s \n",
      "\n",
      "\n",
      "\tEpisode 3777 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5168],\n",
      "        [1.0000, 0.5063]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503818988800049 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.25080943107605 s \n",
      "\n",
      "\n",
      "\tEpisode 3778 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4998],\n",
      "        [1.0000, 0.5083]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508267283439636 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.25679326057434 s \n",
      "\n",
      "\n",
      "\tEpisode 3779 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4825],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5080],\n",
      "        [1.0000, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504366517066956 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.26277732849121 s \n",
      "\n",
      "\n",
      "\tEpisode 3780 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5225],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4570],\n",
      "        [1.0000, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487251281738281 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.26876139640808 s \n",
      "\n",
      "\n",
      "\tEpisode 3781 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5241],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5280],\n",
      "        [1.0000, 0.4450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529804706573486 \tStep Time:  0.006981372833251953 s \tTotal Time:  24.275742769241333 s \n",
      "\n",
      "\n",
      "\tEpisode 3782 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4324],\n",
      "        [1.0000, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.5266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46672248840332 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.281726598739624 s \n",
      "\n",
      "\n",
      "\tEpisode 3783 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5002],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4144],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553133010864258 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.287710666656494 s \n",
      "\n",
      "\n",
      "\tEpisode 3784 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5405],\n",
      "        [1.0000, 0.4197]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4741],\n",
      "        [1.0000, 0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532167434692383 \tStep Time:  0.0069811344146728516 s \tTotal Time:  24.294691801071167 s \n",
      "\n",
      "\n",
      "\tEpisode 3785 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4749],\n",
      "        [1.0000, 0.4869]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5294],\n",
      "        [1.0000, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55823564529419 \tStep Time:  0.005984306335449219 s \tTotal Time:  24.300676107406616 s \n",
      "\n",
      "\n",
      "\tEpisode 3786 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4695],\n",
      "        [1.0000, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5306],\n",
      "        [1.0000, 0.5393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435226440429688 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.306660175323486 s \n",
      "\n",
      "\n",
      "\tEpisode 3787 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4795],\n",
      "        [1.0000, 0.4292]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5480],\n",
      "        [1.0000, 0.5553]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477600932121277 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.312644004821777 s \n",
      "\n",
      "\n",
      "\tEpisode 3788 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5319],\n",
      "        [1.0000, 0.4260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.5439]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480681002140045 \tStep Time:  0.005984306335449219 s \tTotal Time:  24.318628311157227 s \n",
      "\n",
      "\n",
      "\tEpisode 3789 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5667],\n",
      "        [1.0000, 0.4871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5450],\n",
      "        [1.0000, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53229570388794 \tStep Time:  0.00698089599609375 s \tTotal Time:  24.32560920715332 s \n",
      "\n",
      "\n",
      "\tEpisode 3790 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [1.0000, 0.5475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5524],\n",
      "        [1.0000, 0.5452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530145168304443 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.33159327507019 s \n",
      "\n",
      "\n",
      "\tEpisode 3791 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [1.0000, 0.4296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5248],\n",
      "        [1.0000, 0.4312]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420088112354279 \tStep Time:  0.005984306335449219 s \tTotal Time:  24.33757758140564 s \n",
      "\n",
      "\n",
      "\tEpisode 3792 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4945],\n",
      "        [1.0000, 0.4378]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5136],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466145992279053 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.34356117248535 s \n",
      "\n",
      "\n",
      "\tEpisode 3793 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4904],\n",
      "        [1.0000, 0.4217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5252],\n",
      "        [1.0000, 0.4741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549745559692383 \tStep Time:  0.006981372833251953 s \tTotal Time:  24.350542545318604 s \n",
      "\n",
      "\n",
      "\tEpisode 3794 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [1.0000, 0.5136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5154],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51520586013794 \tStep Time:  0.006981372833251953 s \tTotal Time:  24.357523918151855 s \n",
      "\n",
      "\n",
      "\tEpisode 3795 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4978],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5174],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494531154632568 \tStep Time:  0.005984306335449219 s \tTotal Time:  24.363508224487305 s \n",
      "\n",
      "\n",
      "\tEpisode 3796 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4426],\n",
      "        [1.0000, 0.4539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5091],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518916606903076 \tStep Time:  0.00698089599609375 s \tTotal Time:  24.3704891204834 s \n",
      "\n",
      "\n",
      "\tEpisode 3797 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4291],\n",
      "        [1.0000, 0.4671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4358],\n",
      "        [1.0000, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488860130310059 \tStep Time:  0.006981611251831055 s \tTotal Time:  24.37747073173523 s \n",
      "\n",
      "\n",
      "\tEpisode 3798 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4913],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5056],\n",
      "        [1.0000, 0.5158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507262229919434 \tStep Time:  0.006982088088989258 s \tTotal Time:  24.38445281982422 s \n",
      "\n",
      "\n",
      "\tEpisode 3799 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.3802]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [1.0000, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.423763036727905 \tStep Time:  0.006980419158935547 s \tTotal Time:  24.391433238983154 s \n",
      "\n",
      "\n",
      "\tEpisode 3800 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4720],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5071],\n",
      "        [1.0000, 0.4735]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505605697631836 \tStep Time:  0.010970830917358398 s \tTotal Time:  24.402404069900513 s \n",
      "\n",
      "\n",
      "\tEpisode 3801 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5169],\n",
      "        [1.0000, 0.5186]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52158784866333 \tStep Time:  0.006982088088989258 s \tTotal Time:  24.409386157989502 s \n",
      "\n",
      "\n",
      "\tEpisode 3802 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5185],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5541],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47779381275177 \tStep Time:  0.005982637405395508 s \tTotal Time:  24.415368795394897 s \n",
      "\n",
      "\n",
      "\tEpisode 3803 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [1.0000, 0.4273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.4712]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463830947875977 \tStep Time:  0.00498652458190918 s \tTotal Time:  24.420355319976807 s \n",
      "\n",
      "\n",
      "\tEpisode 3804 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3862],\n",
      "        [1.0000, 0.4320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6580],\n",
      "        [1.0000, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402913987636566 \tStep Time:  0.007979631423950195 s \tTotal Time:  24.428334951400757 s \n",
      "\n",
      "\n",
      "\tEpisode 3805 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5218],\n",
      "        [1.0000, 0.4147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4124],\n",
      "        [1.0000, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512029647827148 \tStep Time:  0.005983114242553711 s \tTotal Time:  24.43431806564331 s \n",
      "\n",
      "\n",
      "\tEpisode 3806 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5620],\n",
      "        [1.0000, 0.5409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4396],\n",
      "        [1.0000, 0.5633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557548999786377 \tStep Time:  0.0049877166748046875 s \tTotal Time:  24.439305782318115 s \n",
      "\n",
      "\n",
      "\tEpisode 3807 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4756],\n",
      "        [1.0000, 0.5686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591695308685303 \tStep Time:  0.0059833526611328125 s \tTotal Time:  24.445289134979248 s \n",
      "\n",
      "\n",
      "\tEpisode 3808 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6843],\n",
      "        [1.0000, 0.4956]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4628],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60669755935669 \tStep Time:  0.005017995834350586 s \tTotal Time:  24.4503071308136 s \n",
      "\n",
      "\n",
      "\tEpisode 3809 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5742],\n",
      "        [1.0000, 0.5310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4814],\n",
      "        [1.0000, 0.5313]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470427572727203 \tStep Time:  0.006950855255126953 s \tTotal Time:  24.457257986068726 s \n",
      "\n",
      "\n",
      "\tEpisode 3810 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.5631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5526],\n",
      "        [1.0000, 0.4109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.413522243499756 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.463241577148438 s \n",
      "\n",
      "\n",
      "\tEpisode 3811 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4578],\n",
      "        [1.0000, 0.5687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5285],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52021598815918 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.46922516822815 s \n",
      "\n",
      "\n",
      "\tEpisode 3812 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2584],\n",
      "        [1.0000, 0.3906]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5550],\n",
      "        [1.0000, 0.5587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480718553066254 \tStep Time:  0.00598454475402832 s \tTotal Time:  24.475209712982178 s \n",
      "\n",
      "\n",
      "\tEpisode 3813 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5962],\n",
      "        [1.0000, 0.5395]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3959],\n",
      "        [1.0000, 0.6176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443362712860107 \tStep Time:  0.005986213684082031 s \tTotal Time:  24.48119592666626 s \n",
      "\n",
      "\n",
      "\tEpisode 3814 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5207],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5436],\n",
      "        [1.0000, 0.5639]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522673726081848 \tStep Time:  0.0059812068939208984 s \tTotal Time:  24.48717713356018 s \n",
      "\n",
      "\n",
      "\tEpisode 3815 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6049],\n",
      "        [1.0000, 0.5625]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4019],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439154148101807 \tStep Time:  0.006981372833251953 s \tTotal Time:  24.494158506393433 s \n",
      "\n",
      "\n",
      "\tEpisode 3816 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5616],\n",
      "        [1.0000, 0.3832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5356],\n",
      "        [1.0000, 0.4818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.401263535022736 \tStep Time:  0.004987239837646484 s \tTotal Time:  24.49914574623108 s \n",
      "\n",
      "\n",
      "\tEpisode 3817 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3634],\n",
      "        [1.0000, 0.6013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5752],\n",
      "        [1.0000, 0.5732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428792953491211 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.50512957572937 s \n",
      "\n",
      "\n",
      "\tEpisode 3818 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4593],\n",
      "        [1.0000, 0.4549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5281],\n",
      "        [1.0000, 0.5797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439839363098145 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.511113166809082 s \n",
      "\n",
      "\n",
      "\tEpisode 3819 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7087],\n",
      "        [1.0000, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2933],\n",
      "        [1.0000, 0.7702]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.254241466522217 \tStep Time:  0.004988193511962891 s \tTotal Time:  24.516101360321045 s \n",
      "\n",
      "\n",
      "\tEpisode 3820 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5641],\n",
      "        [1.0000, 0.5742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5623],\n",
      "        [1.0000, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5454843044281 \tStep Time:  0.006980419158935547 s \tTotal Time:  24.52308177947998 s \n",
      "\n",
      "\n",
      "\tEpisode 3821 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6167],\n",
      "        [1.0000, 0.4866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7858],\n",
      "        [1.0000, 0.2522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.794473707675934 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.52906560897827 s \n",
      "\n",
      "\n",
      "\tEpisode 3822 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3739],\n",
      "        [1.0000, 0.7555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.5519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6190767288208 \tStep Time:  0.00699615478515625 s \tTotal Time:  24.536061763763428 s \n",
      "\n",
      "\n",
      "\tEpisode 3823 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5494],\n",
      "        [1.0000, 0.5466]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5207],\n",
      "        [1.0000, 0.4132]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527679920196533 \tStep Time:  0.005968809127807617 s \tTotal Time:  24.542030572891235 s \n",
      "\n",
      "\n",
      "\tEpisode 3824 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3603],\n",
      "        [1.0000, 0.4991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4474],\n",
      "        [1.0000, 0.5818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555517196655273 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.548014640808105 s \n",
      "\n",
      "\n",
      "\tEpisode 3825 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4740],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6501],\n",
      "        [1.0000, 0.5892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529407501220703 \tStep Time:  0.006981372833251953 s \tTotal Time:  24.554996013641357 s \n",
      "\n",
      "\n",
      "\tEpisode 3826 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2616],\n",
      "        [1.0000, 0.3202]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3997],\n",
      "        [1.0000, 0.6010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51294469833374 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.56097984313965 s \n",
      "\n",
      "\n",
      "\tEpisode 3827 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5919],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3893],\n",
      "        [1.0000, 0.5436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57673454284668 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.56696367263794 s \n",
      "\n",
      "\n",
      "\tEpisode 3828 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.5818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3879],\n",
      "        [1.0000, 0.5909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.684001922607422 \tStep Time:  0.010988712310791016 s \tTotal Time:  24.578949451446533 s \n",
      "\n",
      "\n",
      "\tEpisode 3829 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5698],\n",
      "        [1.0000, 0.5795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5375],\n",
      "        [1.0000, 0.3566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490573585033417 \tStep Time:  0.008959770202636719 s \tTotal Time:  24.58790922164917 s \n",
      "\n",
      "\n",
      "\tEpisode 3830 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3924],\n",
      "        [1.0000, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6402],\n",
      "        [1.0000, 0.5534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47596263885498 \tStep Time:  0.009974002838134766 s \tTotal Time:  24.597883224487305 s \n",
      "\n",
      "\n",
      "\tEpisode 3831 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5631],\n",
      "        [1.0000, 0.4420]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5724],\n",
      "        [1.0000, 0.5552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463046550750732 \tStep Time:  0.012001752853393555 s \tTotal Time:  24.6098849773407 s \n",
      "\n",
      "\n",
      "\tEpisode 3832 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2040],\n",
      "        [1.0000, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5711],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.40657240152359 \tStep Time:  0.006977081298828125 s \tTotal Time:  24.616862058639526 s \n",
      "\n",
      "\n",
      "\tEpisode 3833 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5538],\n",
      "        [1.0000, 0.4773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5543],\n",
      "        [1.0000, 0.3940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443406105041504 \tStep Time:  0.005986690521240234 s \tTotal Time:  24.622848749160767 s \n",
      "\n",
      "\n",
      "\tEpisode 3834 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [1.0000, 0.5740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3562],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495954990386963 \tStep Time:  0.007991313934326172 s \tTotal Time:  24.630840063095093 s \n",
      "\n",
      "\n",
      "\tEpisode 3835 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3925],\n",
      "        [1.0000, 0.2590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2101],\n",
      "        [1.0000, 0.6428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.762109637260437 \tStep Time:  0.007054805755615234 s \tTotal Time:  24.637894868850708 s \n",
      "\n",
      "\n",
      "\tEpisode 3836 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3303],\n",
      "        [1.0000, 0.5824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4813],\n",
      "        [1.0000, 0.5854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533839225769043 \tStep Time:  0.007005929946899414 s \tTotal Time:  24.644900798797607 s \n",
      "\n",
      "\n",
      "\tEpisode 3837 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3297],\n",
      "        [1.0000, 0.5349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4627],\n",
      "        [1.0000, 0.4515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592549324035645 \tStep Time:  0.005986690521240234 s \tTotal Time:  24.650887489318848 s \n",
      "\n",
      "\n",
      "\tEpisode 3838 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5697],\n",
      "        [1.0000, 0.5471]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.5609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56643295288086 \tStep Time:  0.0059850215911865234 s \tTotal Time:  24.656872510910034 s \n",
      "\n",
      "\n",
      "\tEpisode 3839 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2507],\n",
      "        [1.0000, 0.4413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2898],\n",
      "        [1.0000, 0.4331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.747836112976074 \tStep Time:  0.005980491638183594 s \tTotal Time:  24.662853002548218 s \n",
      "\n",
      "\n",
      "\tEpisode 3840 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5646],\n",
      "        [1.0000, 0.7790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5786],\n",
      "        [1.0000, 0.5960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483149528503418 \tStep Time:  0.006984233856201172 s \tTotal Time:  24.66983723640442 s \n",
      "\n",
      "\n",
      "\tEpisode 3841 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1161],\n",
      "        [1.0000, 0.0623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4902],\n",
      "        [1.0000, 0.1171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.995441615581512 \tStep Time:  0.0069806575775146484 s \tTotal Time:  24.676817893981934 s \n",
      "\n",
      "\n",
      "\tEpisode 3842 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5223],\n",
      "        [1.0000, 0.3534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4656],\n",
      "        [1.0000, 0.5388]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487643718719482 \tStep Time:  0.007019758224487305 s \tTotal Time:  24.68383765220642 s \n",
      "\n",
      "\n",
      "\tEpisode 3843 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6472],\n",
      "        [1.0000, 0.6462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4094],\n",
      "        [1.0000, 0.5792]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.663101851940155 \tStep Time:  0.005949735641479492 s \tTotal Time:  24.6897873878479 s \n",
      "\n",
      "\n",
      "\tEpisode 3844 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4962],\n",
      "        [1.0000, 0.4608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3904],\n",
      "        [1.0000, 0.4483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54512733221054 \tStep Time:  0.006979942321777344 s \tTotal Time:  24.696767330169678 s \n",
      "\n",
      "\n",
      "\tEpisode 3845 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4503],\n",
      "        [1.0000, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4180],\n",
      "        [1.0000, 0.4763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499838769435883 \tStep Time:  0.007012844085693359 s \tTotal Time:  24.70378017425537 s \n",
      "\n",
      "\n",
      "\tEpisode 3846 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5409],\n",
      "        [1.0000, 0.4252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3850],\n",
      "        [1.0000, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561072826385498 \tStep Time:  0.005952358245849609 s \tTotal Time:  24.70973253250122 s \n",
      "\n",
      "\n",
      "\tEpisode 3847 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5315],\n",
      "        [1.0000, 0.3028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.4687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474482536315918 \tStep Time:  0.005983829498291016 s \tTotal Time:  24.71571636199951 s \n",
      "\n",
      "\n",
      "\tEpisode 3848 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5550],\n",
      "        [1.0000, 0.5249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5346],\n",
      "        [1.0000, 0.5288]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55036449432373 \tStep Time:  0.006981849670410156 s \tTotal Time:  24.722698211669922 s \n",
      "\n",
      "\n",
      "\tEpisode 3849 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4756],\n",
      "        [1.0000, 0.5346]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.5123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502398014068604 \tStep Time:  0.007012367248535156 s \tTotal Time:  24.729710578918457 s \n",
      "\n",
      "\n",
      "\tEpisode 3850 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5438],\n",
      "        [1.0000, 0.5336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4607],\n",
      "        [1.0000, 0.5464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470858693122864 \tStep Time:  0.006984233856201172 s \tTotal Time:  24.736694812774658 s \n",
      "\n",
      "\n",
      "\tEpisode 3851 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4034],\n",
      "        [1.0000, 0.4376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5449],\n",
      "        [1.0000, 0.5507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558175086975098 \tStep Time:  0.005949735641479492 s \tTotal Time:  24.742644548416138 s \n",
      "\n",
      "\n",
      "\tEpisode 3852 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1829],\n",
      "        [1.0000, 0.5517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5586],\n",
      "        [1.0000, 0.3123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.666147947311401 \tStep Time:  0.006982088088989258 s \tTotal Time:  24.749626636505127 s \n",
      "\n",
      "\n",
      "\tEpisode 3853 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [1.0000, 0.4263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5237],\n",
      "        [1.0000, 0.5615]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.629286110401154 \tStep Time:  0.007511615753173828 s \tTotal Time:  24.7571382522583 s \n",
      "\n",
      "\n",
      "\tEpisode 3854 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5661],\n",
      "        [1.0000, 0.5502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5623],\n",
      "        [1.0000, 0.5719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558807849884033 \tStep Time:  0.004986286163330078 s \tTotal Time:  24.76212453842163 s \n",
      "\n",
      "\n",
      "\tEpisode 3855 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5624],\n",
      "        [1.0000, 0.5592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4334],\n",
      "        [1.0000, 0.5236]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469036340713501 \tStep Time:  0.006982326507568359 s \tTotal Time:  24.7691068649292 s \n",
      "\n",
      "\n",
      "\tEpisode 3856 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6261],\n",
      "        [1.0000, 0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5607],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497082233428955 \tStep Time:  0.00698089599609375 s \tTotal Time:  24.776087760925293 s \n",
      "\n",
      "\n",
      "\tEpisode 3857 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5693],\n",
      "        [1.0000, 0.6374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3791],\n",
      "        [1.0000, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.411566734313965 \tStep Time:  0.005984067916870117 s \tTotal Time:  24.782071828842163 s \n",
      "\n",
      "\n",
      "\tEpisode 3858 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1210],\n",
      "        [1.0000, 0.5650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5613],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.813408851623535 \tStep Time:  0.006981849670410156 s \tTotal Time:  24.789053678512573 s \n",
      "\n",
      "\n",
      "\tEpisode 3859 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5047],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4445],\n",
      "        [1.0000, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560530364513397 \tStep Time:  0.00598454475402832 s \tTotal Time:  24.796034812927246 s \n",
      "\n",
      "\n",
      "\tEpisode 3860 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5207],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5427],\n",
      "        [1.0000, 0.5398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493619441986084 \tStep Time:  0.009973764419555664 s \tTotal Time:  24.8060085773468 s \n",
      "\n",
      "\n",
      "\tEpisode 3861 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6450],\n",
      "        [1.0000, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4412],\n",
      "        [1.0000, 0.5369]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.653613805770874 \tStep Time:  0.007014274597167969 s \tTotal Time:  24.81302285194397 s \n",
      "\n",
      "\n",
      "\tEpisode 3862 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5234],\n",
      "        [1.0000, 0.4715]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4065],\n",
      "        [1.0000, 0.4311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513405203819275 \tStep Time:  0.005982160568237305 s \tTotal Time:  24.819005012512207 s \n",
      "\n",
      "\n",
      "\tEpisode 3863 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5242],\n",
      "        [1.0000, 0.5532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6806],\n",
      "        [1.0000, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586095809936523 \tStep Time:  0.006949901580810547 s \tTotal Time:  24.825954914093018 s \n",
      "\n",
      "\n",
      "\tEpisode 3864 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4293],\n",
      "        [1.0000, 0.5171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7766],\n",
      "        [1.0000, 0.5111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.37342882156372 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.83193850517273 s \n",
      "\n",
      "\n",
      "\tEpisode 3865 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.4509]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4275],\n",
      "        [1.0000, 0.4840]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578877210617065 \tStep Time:  0.006032228469848633 s \tTotal Time:  24.837970733642578 s \n",
      "\n",
      "\n",
      "\tEpisode 3866 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4740],\n",
      "        [1.0000, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4426],\n",
      "        [1.0000, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508190929889679 \tStep Time:  0.007001638412475586 s \tTotal Time:  24.844972372055054 s \n",
      "\n",
      "\n",
      "\tEpisode 3867 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4958],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4544],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491310119628906 \tStep Time:  0.0060198307037353516 s \tTotal Time:  24.85099220275879 s \n",
      "\n",
      "\n",
      "\tEpisode 3868 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4936],\n",
      "        [1.0000, 0.4946]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4727],\n",
      "        [1.0000, 0.4533]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54679250717163 \tStep Time:  0.0059506893157958984 s \tTotal Time:  24.856942892074585 s \n",
      "\n",
      "\n",
      "\tEpisode 3869 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4695],\n",
      "        [1.0000, 0.4788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4730],\n",
      "        [1.0000, 0.4667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520190715789795 \tStep Time:  0.005983591079711914 s \tTotal Time:  24.862926483154297 s \n",
      "\n",
      "\n",
      "\tEpisode 3870 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4071],\n",
      "        [1.0000, 0.4727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.4678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541091918945312 \tStep Time:  0.0060138702392578125 s \tTotal Time:  24.869937658309937 s \n",
      "\n",
      "\n",
      "\tEpisode 3871 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4659],\n",
      "        [1.0000, 0.4687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4712],\n",
      "        [1.0000, 0.4701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506178319454193 \tStep Time:  0.006952524185180664 s \tTotal Time:  24.876890182495117 s \n",
      "\n",
      "\n",
      "\tEpisode 3872 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4960],\n",
      "        [1.0000, 0.4820]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4828],\n",
      "        [1.0000, 0.4790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510684490203857 \tStep Time:  0.005983114242553711 s \tTotal Time:  24.88287329673767 s \n",
      "\n",
      "\n",
      "\tEpisode 3873 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4689],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4642],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500977993011475 \tStep Time:  0.006020307540893555 s \tTotal Time:  24.888893604278564 s \n",
      "\n",
      "\n",
      "\tEpisode 3874 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.4744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480029582977295 \tStep Time:  0.0069789886474609375 s \tTotal Time:  24.895872592926025 s \n",
      "\n",
      "\n",
      "\tEpisode 3875 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4890],\n",
      "        [1.0000, 0.4558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.4494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465417504310608 \tStep Time:  0.0059511661529541016 s \tTotal Time:  24.90182375907898 s \n",
      "\n",
      "\n",
      "\tEpisode 3876 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5112],\n",
      "        [1.0000, 0.4808]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4468],\n",
      "        [1.0000, 0.4669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47755742073059 \tStep Time:  0.0070116519927978516 s \tTotal Time:  24.908835411071777 s \n",
      "\n",
      "\n",
      "\tEpisode 3877 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5548],\n",
      "        [1.0000, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [1.0000, 0.5350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460189819335938 \tStep Time:  0.005986690521240234 s \tTotal Time:  24.914822101593018 s \n",
      "\n",
      "\n",
      "\tEpisode 3878 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5388],\n",
      "        [1.0000, 0.5357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5501],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482326030731201 \tStep Time:  0.005987882614135742 s \tTotal Time:  24.920809984207153 s \n",
      "\n",
      "\n",
      "\tEpisode 3879 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6214],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5118],\n",
      "        [1.0000, 0.4657]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575984001159668 \tStep Time:  0.006974697113037109 s \tTotal Time:  24.92778468132019 s \n",
      "\n",
      "\n",
      "\tEpisode 3880 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4320],\n",
      "        [1.0000, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4147],\n",
      "        [1.0000, 0.4544]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499558925628662 \tStep Time:  0.0059871673583984375 s \tTotal Time:  24.93377184867859 s \n",
      "\n",
      "\n",
      "\tEpisode 3881 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4086],\n",
      "        [1.0000, 0.4082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.399973392486572 \tStep Time:  0.006978034973144531 s \tTotal Time:  24.940749883651733 s \n",
      "\n",
      "\n",
      "\tEpisode 3882 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6187],\n",
      "        [1.0000, 0.6546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4070],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560738682746887 \tStep Time:  0.005953073501586914 s \tTotal Time:  24.94670295715332 s \n",
      "\n",
      "\n",
      "\tEpisode 3883 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6566],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6123],\n",
      "        [1.0000, 0.5400]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.391654372215271 \tStep Time:  0.006981372833251953 s \tTotal Time:  24.953684329986572 s \n",
      "\n",
      "\n",
      "\tEpisode 3884 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4722],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5246],\n",
      "        [1.0000, 0.4949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531660556793213 \tStep Time:  0.006982326507568359 s \tTotal Time:  24.96066665649414 s \n",
      "\n",
      "\n",
      "\tEpisode 3885 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4765],\n",
      "        [1.0000, 0.4468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5068],\n",
      "        [1.0000, 0.6294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.61832046508789 \tStep Time:  0.005982160568237305 s \tTotal Time:  24.966648817062378 s \n",
      "\n",
      "\n",
      "\tEpisode 3886 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5867],\n",
      "        [1.0000, 0.5563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5364],\n",
      "        [1.0000, 0.5335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5618314743042 \tStep Time:  0.009975194931030273 s \tTotal Time:  24.976624011993408 s \n",
      "\n",
      "\n",
      "\tEpisode 3887 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6494],\n",
      "        [1.0000, 0.6399]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5306],\n",
      "        [1.0000, 0.4971]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557841181755066 \tStep Time:  0.010968923568725586 s \tTotal Time:  24.987592935562134 s \n",
      "\n",
      "\n",
      "\tEpisode 3888 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5672],\n",
      "        [1.0000, 0.6232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5607],\n",
      "        [1.0000, 0.5484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.540919303894043 \tStep Time:  0.011002540588378906 s \tTotal Time:  24.998595476150513 s \n",
      "\n",
      "\n",
      "\tEpisode 3889 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.3965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3532],\n",
      "        [1.0000, 0.4854]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420628726482391 \tStep Time:  0.008944988250732422 s \tTotal Time:  25.007540464401245 s \n",
      "\n",
      "\n",
      "\tEpisode 3890 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5536],\n",
      "        [1.0000, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [1.0000, 0.6902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.398834109306335 \tStep Time:  0.008009910583496094 s \tTotal Time:  25.01555037498474 s \n",
      "\n",
      "\n",
      "\tEpisode 3891 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2943],\n",
      "        [1.0000, 0.4039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2896],\n",
      "        [1.0000, 0.6728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442115783691406 \tStep Time:  0.00794672966003418 s \tTotal Time:  25.023497104644775 s \n",
      "\n",
      "\n",
      "\tEpisode 3892 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5454],\n",
      "        [1.0000, 0.3673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2742],\n",
      "        [1.0000, 0.2747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.75932788848877 \tStep Time:  0.005984306335449219 s \tTotal Time:  25.029481410980225 s \n",
      "\n",
      "\n",
      "\tEpisode 3893 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3500],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.2853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59030818939209 \tStep Time:  0.0049860477447509766 s \tTotal Time:  25.034467458724976 s \n",
      "\n",
      "\n",
      "\tEpisode 3894 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5608],\n",
      "        [1.0000, 0.3601]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5367],\n",
      "        [1.0000, 0.5998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46182632446289 \tStep Time:  0.006982564926147461 s \tTotal Time:  25.041450023651123 s \n",
      "\n",
      "\n",
      "\tEpisode 3895 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3038],\n",
      "        [1.0000, 0.3041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6093],\n",
      "        [1.0000, 0.3157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.429672718048096 \tStep Time:  0.005983114242553711 s \tTotal Time:  25.047433137893677 s \n",
      "\n",
      "\n",
      "\tEpisode 3896 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5836],\n",
      "        [1.0000, 0.3222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3363],\n",
      "        [1.0000, 0.6072]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.562657833099365 \tStep Time:  0.004986286163330078 s \tTotal Time:  25.052419424057007 s \n",
      "\n",
      "\n",
      "\tEpisode 3897 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3455],\n",
      "        [1.0000, 0.3463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3544],\n",
      "        [1.0000, 0.4328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586860120296478 \tStep Time:  0.006982088088989258 s \tTotal Time:  25.059401512145996 s \n",
      "\n",
      "\n",
      "\tEpisode 3898 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4079],\n",
      "        [1.0000, 0.4623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5893],\n",
      "        [1.0000, 0.4730]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.607354164123535 \tStep Time:  0.004988670349121094 s \tTotal Time:  25.064390182495117 s \n",
      "\n",
      "\n",
      "\tEpisode 3899 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6441],\n",
      "        [1.0000, 0.4177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6929],\n",
      "        [1.0000, 0.6112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.423808574676514 \tStep Time:  0.0059833526611328125 s \tTotal Time:  25.07037353515625 s \n",
      "\n",
      "\n",
      "\tEpisode 3900 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4073],\n",
      "        [1.0000, 0.4248]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5377],\n",
      "        [1.0000, 0.4834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537139356136322 \tStep Time:  0.006982326507568359 s \tTotal Time:  25.07735586166382 s \n",
      "\n",
      "\n",
      "\tEpisode 3901 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5294],\n",
      "        [1.0000, 0.5758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5077],\n",
      "        [1.0000, 0.5982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518243312835693 \tStep Time:  0.0059833526611328125 s \tTotal Time:  25.08333921432495 s \n",
      "\n",
      "\n",
      "\tEpisode 3902 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4542],\n",
      "        [1.0000, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4639],\n",
      "        [1.0000, 0.6696]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457103729248047 \tStep Time:  0.004986763000488281 s \tTotal Time:  25.08832597732544 s \n",
      "\n",
      "\n",
      "\tEpisode 3903 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6391],\n",
      "        [1.0000, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5027],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581075668334961 \tStep Time:  0.006981611251831055 s \tTotal Time:  25.09530758857727 s \n",
      "\n",
      "\n",
      "\tEpisode 3904 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6149],\n",
      "        [1.0000, 0.5945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5109],\n",
      "        [1.0000, 0.5177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527942657470703 \tStep Time:  0.0059833526611328125 s \tTotal Time:  25.101290941238403 s \n",
      "\n",
      "\n",
      "\tEpisode 3905 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5529],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6339],\n",
      "        [1.0000, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551770687103271 \tStep Time:  0.0059850215911865234 s \tTotal Time:  25.10727596282959 s \n",
      "\n",
      "\n",
      "\tEpisode 3906 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7274],\n",
      "        [1.0000, 0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5556],\n",
      "        [1.0000, 0.5496]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.661856651306152 \tStep Time:  0.005983114242553711 s \tTotal Time:  25.113259077072144 s \n",
      "\n",
      "\n",
      "\tEpisode 3907 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6404],\n",
      "        [1.0000, 0.5842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5987],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47266435623169 \tStep Time:  0.005983829498291016 s \tTotal Time:  25.119242906570435 s \n",
      "\n",
      "\n",
      "\tEpisode 3908 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5343],\n",
      "        [1.0000, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [1.0000, 0.5923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47527551651001 \tStep Time:  0.00598454475402832 s \tTotal Time:  25.125227451324463 s \n",
      "\n",
      "\n",
      "\tEpisode 3909 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.5349]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5015],\n",
      "        [1.0000, 0.5335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492619514465332 \tStep Time:  0.00698089599609375 s \tTotal Time:  25.132208347320557 s \n",
      "\n",
      "\n",
      "\tEpisode 3910 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5309],\n",
      "        [1.0000, 0.5576]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4704],\n",
      "        [1.0000, 0.6125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571433126926422 \tStep Time:  0.004986763000488281 s \tTotal Time:  25.137195110321045 s \n",
      "\n",
      "\n",
      "\tEpisode 3911 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.6065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4860],\n",
      "        [1.0000, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466938495635986 \tStep Time:  0.00598454475402832 s \tTotal Time:  25.143179655075073 s \n",
      "\n",
      "\n",
      "\tEpisode 3912 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4563],\n",
      "        [1.0000, 0.4382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5237],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513044238090515 \tStep Time:  0.00598597526550293 s \tTotal Time:  25.149165630340576 s \n",
      "\n",
      "\n",
      "\tEpisode 3913 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5569],\n",
      "        [1.0000, 0.6034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4459],\n",
      "        [1.0000, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510426998138428 \tStep Time:  0.006982326507568359 s \tTotal Time:  25.156147956848145 s \n",
      "\n",
      "\n",
      "\tEpisode 3914 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4025],\n",
      "        [1.0000, 0.4706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3982],\n",
      "        [1.0000, 0.6181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.377754271030426 \tStep Time:  0.007032871246337891 s \tTotal Time:  25.163180828094482 s \n",
      "\n",
      "\n",
      "\tEpisode 3915 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4508],\n",
      "        [1.0000, 0.4501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49970430135727 \tStep Time:  0.005983114242553711 s \tTotal Time:  25.169163942337036 s \n",
      "\n",
      "\n",
      "\tEpisode 3916 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4172],\n",
      "        [1.0000, 0.3888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3627],\n",
      "        [1.0000, 0.4730]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510233879089355 \tStep Time:  0.008009195327758789 s \tTotal Time:  25.177173137664795 s \n",
      "\n",
      "\n",
      "\tEpisode 3917 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5739],\n",
      "        [1.0000, 0.4225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3561],\n",
      "        [1.0000, 0.4689]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538485050201416 \tStep Time:  0.006955862045288086 s \tTotal Time:  25.184128999710083 s \n",
      "\n",
      "\n",
      "\tEpisode 3918 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4358],\n",
      "        [1.0000, 0.5703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4266],\n",
      "        [1.0000, 0.3494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549940466880798 \tStep Time:  0.006977081298828125 s \tTotal Time:  25.19110608100891 s \n",
      "\n",
      "\n",
      "\tEpisode 3919 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5284],\n",
      "        [1.0000, 0.5874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5250],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54512071609497 \tStep Time:  0.006981849670410156 s \tTotal Time:  25.19808793067932 s \n",
      "\n",
      "\n",
      "\tEpisode 3920 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5137],\n",
      "        [1.0000, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3899],\n",
      "        [1.0000, 0.3481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.670125961303711 \tStep Time:  0.005983591079711914 s \tTotal Time:  25.204071521759033 s \n",
      "\n",
      "\n",
      "\tEpisode 3921 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5146],\n",
      "        [1.0000, 0.4853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497877597808838 \tStep Time:  0.007978677749633789 s \tTotal Time:  25.212050199508667 s \n",
      "\n",
      "\n",
      "\tEpisode 3922 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.4824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4795],\n",
      "        [1.0000, 0.4315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472789764404297 \tStep Time:  0.0069811344146728516 s \tTotal Time:  25.21903133392334 s \n",
      "\n",
      "\n",
      "\tEpisode 3923 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4235],\n",
      "        [1.0000, 0.5393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5757],\n",
      "        [1.0000, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54495906829834 \tStep Time:  0.006982326507568359 s \tTotal Time:  25.226013660430908 s \n",
      "\n",
      "\n",
      "\tEpisode 3924 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5235],\n",
      "        [1.0000, 0.4749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5743],\n",
      "        [1.0000, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437575340270996 \tStep Time:  0.006981372833251953 s \tTotal Time:  25.233991622924805 s \n",
      "\n",
      "\n",
      "\tEpisode 3925 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.4364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5548],\n",
      "        [1.0000, 0.4974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49903267621994 \tStep Time:  0.007978439331054688 s \tTotal Time:  25.24197006225586 s \n",
      "\n",
      "\n",
      "\tEpisode 3926 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5410],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5303],\n",
      "        [1.0000, 0.5363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483349978923798 \tStep Time:  0.006981372833251953 s \tTotal Time:  25.24895143508911 s \n",
      "\n",
      "\n",
      "\tEpisode 3927 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5142],\n",
      "        [1.0000, 0.4164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5295],\n",
      "        [1.0000, 0.5610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437005519866943 \tStep Time:  0.006982564926147461 s \tTotal Time:  25.25593400001526 s \n",
      "\n",
      "\n",
      "\tEpisode 3928 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4085],\n",
      "        [1.0000, 0.4455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.4335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484458446502686 \tStep Time:  0.007978677749633789 s \tTotal Time:  25.263912677764893 s \n",
      "\n",
      "\n",
      "\tEpisode 3929 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6449],\n",
      "        [1.0000, 0.3868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5271],\n",
      "        [1.0000, 0.4447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.356605529785156 \tStep Time:  0.006980419158935547 s \tTotal Time:  25.270893096923828 s \n",
      "\n",
      "\n",
      "\tEpisode 3930 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5628],\n",
      "        [1.0000, 0.6382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3870],\n",
      "        [1.0000, 0.4836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512736320495605 \tStep Time:  0.007978200912475586 s \tTotal Time:  25.278871297836304 s \n",
      "\n",
      "\n",
      "\tEpisode 3931 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5977],\n",
      "        [1.0000, 0.3294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5839],\n",
      "        [1.0000, 0.3553]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520353555679321 \tStep Time:  0.0059850215911865234 s \tTotal Time:  25.285853147506714 s \n",
      "\n",
      "\n",
      "\tEpisode 3932 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5713],\n",
      "        [1.0000, 0.4670]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3776],\n",
      "        [1.0000, 0.4499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502200484275818 \tStep Time:  0.007978439331054688 s \tTotal Time:  25.29383158683777 s \n",
      "\n",
      "\n",
      "\tEpisode 3933 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5664],\n",
      "        [1.0000, 0.4461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4499],\n",
      "        [1.0000, 0.2972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.400886058807373 \tStep Time:  0.006982326507568359 s \tTotal Time:  25.300813913345337 s \n",
      "\n",
      "\n",
      "\tEpisode 3934 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4352],\n",
      "        [1.0000, 0.4606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5203],\n",
      "        [1.0000, 0.6200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486908197402954 \tStep Time:  0.010968923568725586 s \tTotal Time:  25.311782836914062 s \n",
      "\n",
      "\n",
      "\tEpisode 3935 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4358],\n",
      "        [1.0000, 0.7020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5508],\n",
      "        [1.0000, 0.5821]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533750057220459 \tStep Time:  0.007015228271484375 s \tTotal Time:  25.318798065185547 s \n",
      "\n",
      "\n",
      "\tEpisode 3936 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5998],\n",
      "        [1.0000, 0.3637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5989],\n",
      "        [1.0000, 0.5972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.417187690734863 \tStep Time:  0.006983518600463867 s \tTotal Time:  25.32677912712097 s \n",
      "\n",
      "\n",
      "\tEpisode 3937 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7135],\n",
      "        [1.0000, 0.6394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2815],\n",
      "        [1.0000, 0.5298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.296008110046387 \tStep Time:  0.0059795379638671875 s \tTotal Time:  25.33275866508484 s \n",
      "\n",
      "\n",
      "\tEpisode 3938 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6286],\n",
      "        [1.0000, 0.7282]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6624],\n",
      "        [1.0000, 0.7406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59836483001709 \tStep Time:  0.005952358245849609 s \tTotal Time:  25.33871102333069 s \n",
      "\n",
      "\n",
      "\tEpisode 3939 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5645],\n",
      "        [1.0000, 0.6342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3108],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.415609002113342 \tStep Time:  0.007978439331054688 s \tTotal Time:  25.346689462661743 s \n",
      "\n",
      "\n",
      "\tEpisode 3940 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3422],\n",
      "        [1.0000, 0.6316]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5210],\n",
      "        [1.0000, 0.6758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.620098114013672 \tStep Time:  0.012968778610229492 s \tTotal Time:  25.359658241271973 s \n",
      "\n",
      "\n",
      "\tEpisode 3941 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2575],\n",
      "        [1.0000, 0.5875]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4559],\n",
      "        [1.0000, 0.2274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.641527712345123 \tStep Time:  0.008974075317382812 s \tTotal Time:  25.36962866783142 s \n",
      "\n",
      "\n",
      "\tEpisode 3942 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4303],\n",
      "        [1.0000, 0.3818]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7012],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.662725448608398 \tStep Time:  0.010973215103149414 s \tTotal Time:  25.38060188293457 s \n",
      "\n",
      "\n",
      "\tEpisode 3943 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5833],\n",
      "        [1.0000, 0.3124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2701],\n",
      "        [1.0000, 0.3584]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495372593402863 \tStep Time:  0.007977485656738281 s \tTotal Time:  25.38857936859131 s \n",
      "\n",
      "\n",
      "\tEpisode 3944 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4821],\n",
      "        [1.0000, 0.2570]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6248],\n",
      "        [1.0000, 0.6700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.294041633605957 \tStep Time:  0.00797891616821289 s \tTotal Time:  25.39655828475952 s \n",
      "\n",
      "\n",
      "\tEpisode 3945 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3679],\n",
      "        [1.0000, 0.5869]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4645],\n",
      "        [1.0000, 0.5674]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575005054473877 \tStep Time:  0.008977174758911133 s \tTotal Time:  25.405535459518433 s \n",
      "\n",
      "\n",
      "\tEpisode 3946 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2354],\n",
      "        [1.0000, 0.3137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7127],\n",
      "        [1.0000, 0.7557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.650537371635437 \tStep Time:  0.008008003234863281 s \tTotal Time:  25.413543462753296 s \n",
      "\n",
      "\n",
      "\tEpisode 3947 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7215],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5734],\n",
      "        [1.0000, 0.4806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493934631347656 \tStep Time:  0.007946014404296875 s \tTotal Time:  25.421489477157593 s \n",
      "\n",
      "\n",
      "\tEpisode 3948 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3050],\n",
      "        [1.0000, 0.4111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5846],\n",
      "        [1.0000, 0.5843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50059849023819 \tStep Time:  0.007979154586791992 s \tTotal Time:  25.429468631744385 s \n",
      "\n",
      "\n",
      "\tEpisode 3949 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5260],\n",
      "        [1.0000, 0.6383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3214],\n",
      "        [1.0000, 0.4328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44394302368164 \tStep Time:  0.0060176849365234375 s \tTotal Time:  25.435486316680908 s \n",
      "\n",
      "\n",
      "\tEpisode 3950 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2445],\n",
      "        [1.0000, 0.5872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4538],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.363219618797302 \tStep Time:  0.007944583892822266 s \tTotal Time:  25.44343090057373 s \n",
      "\n",
      "\n",
      "\tEpisode 3951 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7549],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4839],\n",
      "        [1.0000, 0.5542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465267181396484 \tStep Time:  0.007015705108642578 s \tTotal Time:  25.450446605682373 s \n",
      "\n",
      "\n",
      "\tEpisode 3952 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5915],\n",
      "        [1.0000, 0.4261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2349],\n",
      "        [1.0000, 0.5641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458757877349854 \tStep Time:  0.0069484710693359375 s \tTotal Time:  25.45739507675171 s \n",
      "\n",
      "\n",
      "\tEpisode 3953 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4064],\n",
      "        [1.0000, 0.2743]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2697],\n",
      "        [1.0000, 0.3174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546677589416504 \tStep Time:  0.00797724723815918 s \tTotal Time:  25.465372323989868 s \n",
      "\n",
      "\n",
      "\tEpisode 3954 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4253],\n",
      "        [1.0000, 0.6719]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4780],\n",
      "        [1.0000, 0.5584]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.635858058929443 \tStep Time:  0.00598454475402832 s \tTotal Time:  25.471356868743896 s \n",
      "\n",
      "\n",
      "\tEpisode 3955 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.5768]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3415],\n",
      "        [1.0000, 0.5583]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.408333539962769 \tStep Time:  0.007984399795532227 s \tTotal Time:  25.47934126853943 s \n",
      "\n",
      "\n",
      "\tEpisode 3956 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4453],\n",
      "        [1.0000, 0.3642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4792],\n",
      "        [1.0000, 0.2751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62168538570404 \tStep Time:  0.005981922149658203 s \tTotal Time:  25.485323190689087 s \n",
      "\n",
      "\n",
      "\tEpisode 3957 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2767],\n",
      "        [1.0000, 0.2837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4598],\n",
      "        [1.0000, 0.6082]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50798511505127 \tStep Time:  0.007979869842529297 s \tTotal Time:  25.493303060531616 s \n",
      "\n",
      "\n",
      "\tEpisode 3958 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.5905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5340],\n",
      "        [1.0000, 0.7092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.401338577270508 \tStep Time:  0.005981922149658203 s \tTotal Time:  25.499284982681274 s \n",
      "\n",
      "\n",
      "\tEpisode 3959 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5455],\n",
      "        [1.0000, 0.2881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2853],\n",
      "        [1.0000, 0.6954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.252389907836914 \tStep Time:  0.006981611251831055 s \tTotal Time:  25.506266593933105 s \n",
      "\n",
      "\n",
      "\tEpisode 3960 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4920],\n",
      "        [1.0000, 0.3486]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.7223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.362491130828857 \tStep Time:  0.0060846805572509766 s \tTotal Time:  25.512351274490356 s \n",
      "\n",
      "\n",
      "\tEpisode 3961 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2927],\n",
      "        [1.0000, 0.4161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6015],\n",
      "        [1.0000, 0.2419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.826421737670898 \tStep Time:  0.006009101867675781 s \tTotal Time:  25.51937174797058 s \n",
      "\n",
      "\n",
      "\tEpisode 3962 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6078],\n",
      "        [1.0000, 0.2951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6023],\n",
      "        [1.0000, 0.3327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.854502618312836 \tStep Time:  0.005950450897216797 s \tTotal Time:  25.525322198867798 s \n",
      "\n",
      "\n",
      "\tEpisode 3963 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3387],\n",
      "        [1.0000, 0.3932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3661],\n",
      "        [1.0000, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46675992012024 \tStep Time:  0.006058454513549805 s \tTotal Time:  25.532410144805908 s \n",
      "\n",
      "\n",
      "\tEpisode 3964 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7090],\n",
      "        [1.0000, 0.4166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7029],\n",
      "        [1.0000, 0.3739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557602405548096 \tStep Time:  0.0059528350830078125 s \tTotal Time:  25.538362979888916 s \n",
      "\n",
      "\n",
      "\tEpisode 3965 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6541],\n",
      "        [1.0000, 0.7917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6862],\n",
      "        [1.0000, 0.3923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.692665576934814 \tStep Time:  0.007016897201538086 s \tTotal Time:  25.545379877090454 s \n",
      "\n",
      "\n",
      "\tEpisode 3966 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6030],\n",
      "        [1.0000, 0.5474]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.5432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472897052764893 \tStep Time:  0.0059473514556884766 s \tTotal Time:  25.551327228546143 s \n",
      "\n",
      "\n",
      "\tEpisode 3967 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5415],\n",
      "        [1.0000, 0.6115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4324],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54209280014038 \tStep Time:  0.005984783172607422 s \tTotal Time:  25.55731201171875 s \n",
      "\n",
      "\n",
      "\tEpisode 3968 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5373],\n",
      "        [1.0000, 0.7269]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5439],\n",
      "        [1.0000, 0.5900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499088287353516 \tStep Time:  0.0069806575775146484 s \tTotal Time:  25.564292669296265 s \n",
      "\n",
      "\n",
      "\tEpisode 3969 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6090],\n",
      "        [1.0000, 0.4537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4424],\n",
      "        [1.0000, 0.4970]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422117710113525 \tStep Time:  0.005983591079711914 s \tTotal Time:  25.570276260375977 s \n",
      "\n",
      "\n",
      "\tEpisode 3970 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4629],\n",
      "        [1.0000, 0.4421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5725],\n",
      "        [1.0000, 0.5558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524379253387451 \tStep Time:  0.0069811344146728516 s \tTotal Time:  25.57825493812561 s \n",
      "\n",
      "\n",
      "\tEpisode 3971 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4955],\n",
      "        [1.0000, 0.5960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4795],\n",
      "        [1.0000, 0.4902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472519874572754 \tStep Time:  0.005984306335449219 s \tTotal Time:  25.58423924446106 s \n",
      "\n",
      "\n",
      "\tEpisode 3972 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5315],\n",
      "        [1.0000, 0.5280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4660],\n",
      "        [1.0000, 0.6059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.456238269805908 \tStep Time:  0.007013797760009766 s \tTotal Time:  25.59125304222107 s \n",
      "\n",
      "\n",
      "\tEpisode 3973 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6480],\n",
      "        [1.0000, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4743],\n",
      "        [1.0000, 0.4151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.427717685699463 \tStep Time:  0.007946968078613281 s \tTotal Time:  25.599200010299683 s \n",
      "\n",
      "\n",
      "\tEpisode 3974 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6273],\n",
      "        [1.0000, 0.7123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6343],\n",
      "        [1.0000, 0.4975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.656855583190918 \tStep Time:  0.007978677749633789 s \tTotal Time:  25.607178688049316 s \n",
      "\n",
      "\n",
      "\tEpisode 3975 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4836],\n",
      "        [1.0000, 0.5703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5383],\n",
      "        [1.0000, 0.5261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515590190887451 \tStep Time:  0.012001991271972656 s \tTotal Time:  25.61918067932129 s \n",
      "\n",
      "\n",
      "\tEpisode 3976 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3837],\n",
      "        [1.0000, 0.5030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6741],\n",
      "        [1.0000, 0.4362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592799663543701 \tStep Time:  0.007108926773071289 s \tTotal Time:  25.62628960609436 s \n",
      "\n",
      "\n",
      "\tEpisode 3977 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6661],\n",
      "        [1.0000, 0.4086]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.4090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.635075390338898 \tStep Time:  0.008003950119018555 s \tTotal Time:  25.63429355621338 s \n",
      "\n",
      "\n",
      "\tEpisode 3978 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5585],\n",
      "        [1.0000, 0.5949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5461],\n",
      "        [1.0000, 0.4056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447393894195557 \tStep Time:  0.006825685501098633 s \tTotal Time:  25.641119241714478 s \n",
      "\n",
      "\n",
      "\tEpisode 3979 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5810],\n",
      "        [1.0000, 0.5716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3631],\n",
      "        [1.0000, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464788019657135 \tStep Time:  0.006514310836791992 s \tTotal Time:  25.647757291793823 s \n",
      "\n",
      "\n",
      "\tEpisode 3980 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4378],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3942],\n",
      "        [1.0000, 0.4985]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53859806060791 \tStep Time:  0.006020784378051758 s \tTotal Time:  25.653778076171875 s \n",
      "\n",
      "\n",
      "\tEpisode 3981 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3414],\n",
      "        [1.0000, 0.6200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [1.0000, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.668941855430603 \tStep Time:  0.0069501399993896484 s \tTotal Time:  25.660728216171265 s \n",
      "\n",
      "\n",
      "\tEpisode 3982 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3460],\n",
      "        [1.0000, 0.6131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6297],\n",
      "        [1.0000, 0.4310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.306747436523438 \tStep Time:  0.005983591079711914 s \tTotal Time:  25.66770601272583 s \n",
      "\n",
      "\n",
      "\tEpisode 3983 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4853],\n",
      "        [1.0000, 0.4866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4477],\n",
      "        [1.0000, 0.4585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50546646118164 \tStep Time:  0.006982088088989258 s \tTotal Time:  25.67468810081482 s \n",
      "\n",
      "\n",
      "\tEpisode 3984 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3420],\n",
      "        [1.0000, 0.3380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4194],\n",
      "        [1.0000, 0.3944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47630500793457 \tStep Time:  0.006981849670410156 s \tTotal Time:  25.68166995048523 s \n",
      "\n",
      "\n",
      "\tEpisode 3985 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.5540]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5045],\n",
      "        [1.0000, 0.5588]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530506610870361 \tStep Time:  0.0069811344146728516 s \tTotal Time:  25.688651084899902 s \n",
      "\n",
      "\n",
      "\tEpisode 3986 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3398],\n",
      "        [1.0000, 0.3662]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6087],\n",
      "        [1.0000, 0.4246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.378254413604736 \tStep Time:  0.006986141204833984 s \tTotal Time:  25.695637226104736 s \n",
      "\n",
      "\n",
      "\tEpisode 3987 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3441],\n",
      "        [1.0000, 0.5522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5354],\n",
      "        [1.0000, 0.3564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546772301197052 \tStep Time:  0.006981372833251953 s \tTotal Time:  25.70261859893799 s \n",
      "\n",
      "\n",
      "\tEpisode 3988 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4578],\n",
      "        [1.0000, 0.4242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5276],\n",
      "        [1.0000, 0.4589]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458922386169434 \tStep Time:  0.006981372833251953 s \tTotal Time:  25.70959997177124 s \n",
      "\n",
      "\n",
      "\tEpisode 3989 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4060],\n",
      "        [1.0000, 0.3479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5788],\n",
      "        [1.0000, 0.3926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426294803619385 \tStep Time:  0.006982088088989258 s \tTotal Time:  25.71658205986023 s \n",
      "\n",
      "\n",
      "\tEpisode 3990 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5617],\n",
      "        [1.0000, 0.4758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4296],\n",
      "        [1.0000, 0.4199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5558500289917 \tStep Time:  0.007979154586791992 s \tTotal Time:  25.72456121444702 s \n",
      "\n",
      "\n",
      "\tEpisode 3991 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [1.0000, 0.5990]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6057],\n",
      "        [1.0000, 0.3301]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447765409946442 \tStep Time:  0.0069463253021240234 s \tTotal Time:  25.731507539749146 s \n",
      "\n",
      "\n",
      "\tEpisode 3992 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3604],\n",
      "        [1.0000, 0.5664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5807],\n",
      "        [1.0000, 0.5640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442342281341553 \tStep Time:  0.006981372833251953 s \tTotal Time:  25.738488912582397 s \n",
      "\n",
      "\n",
      "\tEpisode 3993 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3312],\n",
      "        [1.0000, 0.5671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6126],\n",
      "        [1.0000, 0.3205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.28946304321289 \tStep Time:  0.007978439331054688 s \tTotal Time:  25.746467351913452 s \n",
      "\n",
      "\n",
      "\tEpisode 3994 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5732],\n",
      "        [1.0000, 0.6295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6130],\n",
      "        [1.0000, 0.5695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53583812713623 \tStep Time:  0.0059854984283447266 s \tTotal Time:  25.752452850341797 s \n",
      "\n",
      "\n",
      "\tEpisode 3995 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5759],\n",
      "        [1.0000, 0.6395]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6337],\n",
      "        [1.0000, 0.4421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464191913604736 \tStep Time:  0.006981372833251953 s \tTotal Time:  25.760430574417114 s \n",
      "\n",
      "\n",
      "\tEpisode 3996 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5389],\n",
      "        [1.0000, 0.2845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4032],\n",
      "        [1.0000, 0.4592]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.64462947845459 \tStep Time:  0.0069811344146728516 s \tTotal Time:  25.767411708831787 s \n",
      "\n",
      "\n",
      "\tEpisode 3997 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6737],\n",
      "        [1.0000, 0.4385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4671],\n",
      "        [1.0000, 0.3502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.362443447113037 \tStep Time:  0.007886648178100586 s \tTotal Time:  25.775298357009888 s \n",
      "\n",
      "\n",
      "\tEpisode 3998 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4380],\n",
      "        [1.0000, 0.3481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [1.0000, 0.6225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419625163078308 \tStep Time:  0.007072925567626953 s \tTotal Time:  25.782371282577515 s \n",
      "\n",
      "\n",
      "\tEpisode 3999 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6877],\n",
      "        [1.0000, 0.2424]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4343],\n",
      "        [1.0000, 0.6499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47102975845337 \tStep Time:  0.006983280181884766 s \tTotal Time:  25.7893545627594 s \n",
      "\n",
      "\n",
      "\tEpisode 4000 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3007],\n",
      "        [1.0000, 0.6989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3893],\n",
      "        [1.0000, 0.4836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.632461547851562 \tStep Time:  0.007977724075317383 s \tTotal Time:  25.797332286834717 s \n",
      "\n",
      "\n",
      "\tEpisode 4001 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2504],\n",
      "        [1.0000, 0.7395]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3555],\n",
      "        [1.0000, 0.5996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.234835624694824 \tStep Time:  0.006981611251831055 s \tTotal Time:  25.804313898086548 s \n",
      "\n",
      "\n",
      "\tEpisode 4002 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3984],\n",
      "        [1.0000, 0.6138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6330],\n",
      "        [1.0000, 0.5878]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451459407806396 \tStep Time:  0.007977962493896484 s \tTotal Time:  25.812291860580444 s \n",
      "\n",
      "\n",
      "\tEpisode 4003 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5933],\n",
      "        [1.0000, 0.3516]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5409],\n",
      "        [1.0000, 0.5735]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.618367195129395 \tStep Time:  0.0070133209228515625 s \tTotal Time:  25.819305181503296 s \n",
      "\n",
      "\n",
      "\tEpisode 4004 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5147],\n",
      "        [1.0000, 0.4275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6967],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66922378540039 \tStep Time:  0.007947206497192383 s \tTotal Time:  25.82725238800049 s \n",
      "\n",
      "\n",
      "\tEpisode 4005 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6775],\n",
      "        [1.0000, 0.4065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5553],\n",
      "        [1.0000, 0.5749]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.687333583831787 \tStep Time:  0.006984233856201172 s \tTotal Time:  25.83423662185669 s \n",
      "\n",
      "\n",
      "\tEpisode 4006 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5665],\n",
      "        [1.0000, 0.4558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7533],\n",
      "        [1.0000, 0.6115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440247058868408 \tStep Time:  0.0069828033447265625 s \tTotal Time:  25.841219425201416 s \n",
      "\n",
      "\n",
      "\tEpisode 4007 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3003],\n",
      "        [1.0000, 0.5428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5964],\n",
      "        [1.0000, 0.6437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.357885837554932 \tStep Time:  0.006983757019042969 s \tTotal Time:  25.84820318222046 s \n",
      "\n",
      "\n",
      "\tEpisode 4008 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6486],\n",
      "        [1.0000, 0.6383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4749],\n",
      "        [1.0000, 0.3267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.623876571655273 \tStep Time:  0.0059833526611328125 s \tTotal Time:  25.854186534881592 s \n",
      "\n",
      "\n",
      "\tEpisode 4009 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5486],\n",
      "        [1.0000, 0.4975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4564],\n",
      "        [1.0000, 0.5444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548338890075684 \tStep Time:  0.0070459842681884766 s \tTotal Time:  25.86123251914978 s \n",
      "\n",
      "\n",
      "\tEpisode 4010 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5744],\n",
      "        [1.0000, 0.3539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4267],\n",
      "        [1.0000, 0.5596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36268174648285 \tStep Time:  0.005980730056762695 s \tTotal Time:  25.867213249206543 s \n",
      "\n",
      "\n",
      "\tEpisode 4011 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5468],\n",
      "        [1.0000, 0.3842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6602],\n",
      "        [1.0000, 0.7975]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599817037582397 \tStep Time:  0.006981849670410156 s \tTotal Time:  25.874195098876953 s \n",
      "\n",
      "\n",
      "\tEpisode 4012 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2326],\n",
      "        [1.0000, 0.4915]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6405],\n",
      "        [1.0000, 0.5945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.68008542060852 \tStep Time:  0.00703120231628418 s \tTotal Time:  25.88155460357666 s \n",
      "\n",
      "\n",
      "\tEpisode 4013 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4646],\n",
      "        [1.0000, 0.7017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5635],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504907131195068 \tStep Time:  0.005984783172607422 s \tTotal Time:  25.887539386749268 s \n",
      "\n",
      "\n",
      "\tEpisode 4014 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5867],\n",
      "        [1.0000, 0.5853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6161],\n",
      "        [1.0000, 0.5691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523379921913147 \tStep Time:  0.0069806575775146484 s \tTotal Time:  25.894520044326782 s \n",
      "\n",
      "\n",
      "\tEpisode 4015 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5403],\n",
      "        [1.0000, 0.5608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5373],\n",
      "        [1.0000, 0.5208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545294761657715 \tStep Time:  0.008977174758911133 s \tTotal Time:  25.903497219085693 s \n",
      "\n",
      "\n",
      "\tEpisode 4016 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6858],\n",
      "        [1.0000, 0.5824]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5335],\n",
      "        [1.0000, 0.3678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518627643585205 \tStep Time:  0.007977724075317383 s \tTotal Time:  25.91147494316101 s \n",
      "\n",
      "\n",
      "\tEpisode 4017 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4037],\n",
      "        [1.0000, 0.5387]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.4344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.641807079315186 \tStep Time:  0.007978439331054688 s \tTotal Time:  25.919453382492065 s \n",
      "\n",
      "\n",
      "\tEpisode 4018 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3461],\n",
      "        [1.0000, 0.5109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.4090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526864528656006 \tStep Time:  0.0069806575775146484 s \tTotal Time:  25.92643404006958 s \n",
      "\n",
      "\n",
      "\tEpisode 4019 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3183],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4836],\n",
      "        [1.0000, 0.4069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492959976196289 \tStep Time:  0.0070188045501708984 s \tTotal Time:  25.93345284461975 s \n",
      "\n",
      "\n",
      "\tEpisode 4020 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4809],\n",
      "        [1.0000, 0.3695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5437],\n",
      "        [1.0000, 0.3214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550428867340088 \tStep Time:  0.00695037841796875 s \tTotal Time:  25.94040322303772 s \n",
      "\n",
      "\n",
      "\tEpisode 4021 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4773],\n",
      "        [1.0000, 0.3301]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4669],\n",
      "        [1.0000, 0.3868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509333848953247 \tStep Time:  0.0069789886474609375 s \tTotal Time:  25.94738221168518 s \n",
      "\n",
      "\n",
      "\tEpisode 4022 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4840],\n",
      "        [1.0000, 0.4774]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3767],\n",
      "        [1.0000, 0.4728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577946186065674 \tStep Time:  0.006982088088989258 s \tTotal Time:  25.95436429977417 s \n",
      "\n",
      "\n",
      "\tEpisode 4023 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4754],\n",
      "        [1.0000, 0.3540]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.4744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463456690311432 \tStep Time:  0.00698089599609375 s \tTotal Time:  25.961345195770264 s \n",
      "\n",
      "\n",
      "\tEpisode 4024 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4763],\n",
      "        [1.0000, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4760],\n",
      "        [1.0000, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516819953918457 \tStep Time:  0.008976221084594727 s \tTotal Time:  25.97032141685486 s \n",
      "\n",
      "\n",
      "\tEpisode 4025 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4786],\n",
      "        [1.0000, 0.4833]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4677],\n",
      "        [1.0000, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49954640865326 \tStep Time:  0.007978200912475586 s \tTotal Time:  25.978299617767334 s \n",
      "\n",
      "\n",
      "\tEpisode 4026 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [1.0000, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.4930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515018939971924 \tStep Time:  0.005984306335449219 s \tTotal Time:  25.984283924102783 s \n",
      "\n",
      "\n",
      "\tEpisode 4027 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5103],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4386],\n",
      "        [1.0000, 0.5091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542656898498535 \tStep Time:  0.008977651596069336 s \tTotal Time:  25.993261575698853 s \n",
      "\n",
      "\n",
      "\tEpisode 4028 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4215],\n",
      "        [1.0000, 0.5117]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4666],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4821298122406 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.000242710113525 s \n",
      "\n",
      "\n",
      "\tEpisode 4029 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5064],\n",
      "        [1.0000, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4019],\n",
      "        [1.0000, 0.4042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.406877994537354 \tStep Time:  0.01196742057800293 s \tTotal Time:  26.01221013069153 s \n",
      "\n",
      "\n",
      "\tEpisode 4030 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3950],\n",
      "        [1.0000, 0.5406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5243],\n",
      "        [1.0000, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584832310676575 \tStep Time:  0.01296544075012207 s \tTotal Time:  26.02517557144165 s \n",
      "\n",
      "\n",
      "\tEpisode 4031 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4153],\n",
      "        [1.0000, 0.4682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4625],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488026142120361 \tStep Time:  0.022938251495361328 s \tTotal Time:  26.04811382293701 s \n",
      "\n",
      "\n",
      "\tEpisode 4032 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5075],\n",
      "        [1.0000, 0.5015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5127],\n",
      "        [1.0000, 0.5395]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524176597595215 \tStep Time:  0.007979154586791992 s \tTotal Time:  26.056092977523804 s \n",
      "\n",
      "\n",
      "\tEpisode 4033 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5430],\n",
      "        [1.0000, 0.4621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487214028835297 \tStep Time:  0.006985902786254883 s \tTotal Time:  26.06307888031006 s \n",
      "\n",
      "\n",
      "\tEpisode 4034 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5270],\n",
      "        [1.0000, 0.5239]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5434],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483911752700806 \tStep Time:  0.005982637405395508 s \tTotal Time:  26.069061517715454 s \n",
      "\n",
      "\n",
      "\tEpisode 4035 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5347],\n",
      "        [1.0000, 0.5429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5306],\n",
      "        [1.0000, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49436092376709 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.076042890548706 s \n",
      "\n",
      "\n",
      "\tEpisode 4036 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4512],\n",
      "        [1.0000, 0.5135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48252820968628 \tStep Time:  0.006982088088989258 s \tTotal Time:  26.083024978637695 s \n",
      "\n",
      "\n",
      "\tEpisode 4037 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4452],\n",
      "        [1.0000, 0.5280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.4402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50706285238266 \tStep Time:  0.0059833526611328125 s \tTotal Time:  26.089008331298828 s \n",
      "\n",
      "\n",
      "\tEpisode 4038 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5378],\n",
      "        [1.0000, 0.4381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5330],\n",
      "        [1.0000, 0.4270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4041748046875 \tStep Time:  0.006982326507568359 s \tTotal Time:  26.0969877243042 s \n",
      "\n",
      "\n",
      "\tEpisode 4039 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5341],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4853],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494009017944336 \tStep Time:  0.007012128829956055 s \tTotal Time:  26.103999853134155 s \n",
      "\n",
      "\n",
      "\tEpisode 4040 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.4241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4440],\n",
      "        [1.0000, 0.4033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.589957475662231 \tStep Time:  0.005990743637084961 s \tTotal Time:  26.10999059677124 s \n",
      "\n",
      "\n",
      "\tEpisode 4041 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4300],\n",
      "        [1.0000, 0.4870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4993],\n",
      "        [1.0000, 0.5332]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460350036621094 \tStep Time:  0.006976604461669922 s \tTotal Time:  26.11696720123291 s \n",
      "\n",
      "\n",
      "\tEpisode 4042 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4770],\n",
      "        [1.0000, 0.5172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5390],\n",
      "        [1.0000, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518177509307861 \tStep Time:  0.005949974060058594 s \tTotal Time:  26.12291717529297 s \n",
      "\n",
      "\n",
      "\tEpisode 4043 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4512],\n",
      "        [1.0000, 0.4840]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5281],\n",
      "        [1.0000, 0.4538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480602264404297 \tStep Time:  0.0069828033447265625 s \tTotal Time:  26.129899978637695 s \n",
      "\n",
      "\n",
      "\tEpisode 4044 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.5358]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4174],\n",
      "        [1.0000, 0.4131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51334524154663 \tStep Time:  0.0069806575775146484 s \tTotal Time:  26.13688063621521 s \n",
      "\n",
      "\n",
      "\tEpisode 4045 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5435],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4946],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50541353225708 \tStep Time:  0.006981849670410156 s \tTotal Time:  26.14386248588562 s \n",
      "\n",
      "\n",
      "\tEpisode 4046 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5445],\n",
      "        [1.0000, 0.5526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3806],\n",
      "        [1.0000, 0.5455]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.434499740600586 \tStep Time:  0.007977962493896484 s \tTotal Time:  26.151840448379517 s \n",
      "\n",
      "\n",
      "\tEpisode 4047 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4417],\n",
      "        [1.0000, 0.5600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3801],\n",
      "        [1.0000, 0.3843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46146833896637 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.15882158279419 s \n",
      "\n",
      "\n",
      "\tEpisode 4048 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3972],\n",
      "        [1.0000, 0.3825]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4418],\n",
      "        [1.0000, 0.4350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52091121673584 \tStep Time:  0.005984067916870117 s \tTotal Time:  26.16480565071106 s \n",
      "\n",
      "\n",
      "\tEpisode 4049 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5762],\n",
      "        [1.0000, 0.5620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6309],\n",
      "        [1.0000, 0.5866]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536877155303955 \tStep Time:  0.00698089599609375 s \tTotal Time:  26.172784328460693 s \n",
      "\n",
      "\n",
      "\tEpisode 4050 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4766],\n",
      "        [1.0000, 0.5449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4847],\n",
      "        [1.0000, 0.5660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581477642059326 \tStep Time:  0.0069806575775146484 s \tTotal Time:  26.179764986038208 s \n",
      "\n",
      "\n",
      "\tEpisode 4051 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5550],\n",
      "        [1.0000, 0.5856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4116],\n",
      "        [1.0000, 0.5764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.450458288192749 \tStep Time:  0.005985260009765625 s \tTotal Time:  26.185750246047974 s \n",
      "\n",
      "\n",
      "\tEpisode 4052 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5759],\n",
      "        [1.0000, 0.5776]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4096],\n",
      "        [1.0000, 0.5401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.617315769195557 \tStep Time:  0.00797724723815918 s \tTotal Time:  26.193727493286133 s \n",
      "\n",
      "\n",
      "\tEpisode 4053 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5340],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51144790649414 \tStep Time:  0.005984067916870117 s \tTotal Time:  26.199711561203003 s \n",
      "\n",
      "\n",
      "\tEpisode 4054 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3583],\n",
      "        [1.0000, 0.5460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4115],\n",
      "        [1.0000, 0.5457]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.35868501663208 \tStep Time:  0.006981849670410156 s \tTotal Time:  26.206693410873413 s \n",
      "\n",
      "\n",
      "\tEpisode 4055 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5266],\n",
      "        [1.0000, 0.5392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5500],\n",
      "        [1.0000, 0.5549]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48859453201294 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.213674783706665 s \n",
      "\n",
      "\n",
      "\tEpisode 4056 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4006],\n",
      "        [1.0000, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4946],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587801456451416 \tStep Time:  0.008010625839233398 s \tTotal Time:  26.2216854095459 s \n",
      "\n",
      "\n",
      "\tEpisode 4057 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5278],\n",
      "        [1.0000, 0.4931]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5440],\n",
      "        [1.0000, 0.5331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527131140232086 \tStep Time:  0.006983518600463867 s \tTotal Time:  26.228668928146362 s \n",
      "\n",
      "\n",
      "\tEpisode 4058 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4910],\n",
      "        [1.0000, 0.4653]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4637],\n",
      "        [1.0000, 0.4090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489013195037842 \tStep Time:  0.0059812068939208984 s \tTotal Time:  26.234650135040283 s \n",
      "\n",
      "\n",
      "\tEpisode 4059 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4643],\n",
      "        [1.0000, 0.3112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4697],\n",
      "        [1.0000, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447556138038635 \tStep Time:  0.007151126861572266 s \tTotal Time:  26.241801261901855 s \n",
      "\n",
      "\n",
      "\tEpisode 4060 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4746],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505601406097412 \tStep Time:  0.006780385971069336 s \tTotal Time:  26.248581647872925 s \n",
      "\n",
      "\n",
      "\tEpisode 4061 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5416],\n",
      "        [1.0000, 0.4036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5336],\n",
      "        [1.0000, 0.4798]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481205940246582 \tStep Time:  0.005983829498291016 s \tTotal Time:  26.254565477371216 s \n",
      "\n",
      "\n",
      "\tEpisode 4062 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4835],\n",
      "        [1.0000, 0.5246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5206],\n",
      "        [1.0000, 0.3788]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564742147922516 \tStep Time:  0.007982492446899414 s \tTotal Time:  26.262547969818115 s \n",
      "\n",
      "\n",
      "\tEpisode 4063 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4465],\n",
      "        [1.0000, 0.5278]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.4450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517341613769531 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.269529104232788 s \n",
      "\n",
      "\n",
      "\tEpisode 4064 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5128],\n",
      "        [1.0000, 0.4646]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3757],\n",
      "        [1.0000, 0.3396]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531421184539795 \tStep Time:  0.006982088088989258 s \tTotal Time:  26.276511192321777 s \n",
      "\n",
      "\n",
      "\tEpisode 4065 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5451],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.3893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.594342708587646 \tStep Time:  0.0069844722747802734 s \tTotal Time:  26.283495664596558 s \n",
      "\n",
      "\n",
      "\tEpisode 4066 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5280],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5362],\n",
      "        [1.0000, 0.5034]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517467975616455 \tStep Time:  0.005983591079711914 s \tTotal Time:  26.28947925567627 s \n",
      "\n",
      "\n",
      "\tEpisode 4067 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5438],\n",
      "        [1.0000, 0.3596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.4664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568089485168457 \tStep Time:  0.006982088088989258 s \tTotal Time:  26.29646134376526 s \n",
      "\n",
      "\n",
      "\tEpisode 4068 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5694],\n",
      "        [1.0000, 0.3232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5542],\n",
      "        [1.0000, 0.3267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550754487514496 \tStep Time:  0.0059833526611328125 s \tTotal Time:  26.30244469642639 s \n",
      "\n",
      "\n",
      "\tEpisode 4069 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5638],\n",
      "        [1.0000, 0.4611]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3036],\n",
      "        [1.0000, 0.5293]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6919105052948 \tStep Time:  0.006982088088989258 s \tTotal Time:  26.30942678451538 s \n",
      "\n",
      "\n",
      "\tEpisode 4070 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4017],\n",
      "        [1.0000, 0.5580]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4233],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524840831756592 \tStep Time:  0.006980419158935547 s \tTotal Time:  26.316407203674316 s \n",
      "\n",
      "\n",
      "\tEpisode 4071 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5189],\n",
      "        [1.0000, 0.4466]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5726],\n",
      "        [1.0000, 0.5726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476389408111572 \tStep Time:  0.005984067916870117 s \tTotal Time:  26.322391271591187 s \n",
      "\n",
      "\n",
      "\tEpisode 4072 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4020],\n",
      "        [1.0000, 0.5494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5801],\n",
      "        [1.0000, 0.5586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581929206848145 \tStep Time:  0.006981611251831055 s \tTotal Time:  26.329372882843018 s \n",
      "\n",
      "\n",
      "\tEpisode 4073 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4652],\n",
      "        [1.0000, 0.4880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5827],\n",
      "        [1.0000, 0.5734]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498884201049805 \tStep Time:  0.005983829498291016 s \tTotal Time:  26.33535671234131 s \n",
      "\n",
      "\n",
      "\tEpisode 4074 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4778],\n",
      "        [1.0000, 0.5395]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4901],\n",
      "        [1.0000, 0.5838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433173298835754 \tStep Time:  0.005983114242553711 s \tTotal Time:  26.34233808517456 s \n",
      "\n",
      "\n",
      "\tEpisode 4075 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4895],\n",
      "        [1.0000, 0.5759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4887],\n",
      "        [1.0000, 0.5782]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59837818145752 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.349319458007812 s \n",
      "\n",
      "\n",
      "\tEpisode 4076 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5595],\n",
      "        [1.0000, 0.5839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4624],\n",
      "        [1.0000, 0.5259]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532122194766998 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.356300830841064 s \n",
      "\n",
      "\n",
      "\tEpisode 4077 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5772],\n",
      "        [1.0000, 0.5766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5427],\n",
      "        [1.0000, 0.5838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544727325439453 \tStep Time:  0.005984783172607422 s \tTotal Time:  26.362285614013672 s \n",
      "\n",
      "\n",
      "\tEpisode 4078 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5513],\n",
      "        [1.0000, 0.4767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.5293]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475116729736328 \tStep Time:  0.006980180740356445 s \tTotal Time:  26.36926579475403 s \n",
      "\n",
      "\n",
      "\tEpisode 4079 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5528],\n",
      "        [1.0000, 0.5693]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4717],\n",
      "        [1.0000, 0.5781]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555857002735138 \tStep Time:  0.007979869842529297 s \tTotal Time:  26.377245664596558 s \n",
      "\n",
      "\n",
      "\tEpisode 4080 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4815],\n",
      "        [1.0000, 0.5752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4862],\n",
      "        [1.0000, 0.5516]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497535049915314 \tStep Time:  0.00698089599609375 s \tTotal Time:  26.38422656059265 s \n",
      "\n",
      "\n",
      "\tEpisode 4081 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5724],\n",
      "        [1.0000, 0.5686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4942],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52206140756607 \tStep Time:  0.007981061935424805 s \tTotal Time:  26.392207622528076 s \n",
      "\n",
      "\n",
      "\tEpisode 4082 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5373],\n",
      "        [1.0000, 0.5701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4236],\n",
      "        [1.0000, 0.5746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457563042640686 \tStep Time:  0.006979942321777344 s \tTotal Time:  26.399187564849854 s \n",
      "\n",
      "\n",
      "\tEpisode 4083 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5602],\n",
      "        [1.0000, 0.5678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5688],\n",
      "        [1.0000, 0.4722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.554428815841675 \tStep Time:  0.007979393005371094 s \tTotal Time:  26.407166957855225 s \n",
      "\n",
      "\n",
      "\tEpisode 4084 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4237],\n",
      "        [1.0000, 0.5601]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4699],\n",
      "        [1.0000, 0.5621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.395723164081573 \tStep Time:  0.007976770401000977 s \tTotal Time:  26.415143728256226 s \n",
      "\n",
      "\n",
      "\tEpisode 4085 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4257],\n",
      "        [1.0000, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5553],\n",
      "        [1.0000, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465516090393066 \tStep Time:  0.006981611251831055 s \tTotal Time:  26.422125339508057 s \n",
      "\n",
      "\n",
      "\tEpisode 4086 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4677],\n",
      "        [1.0000, 0.5370]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5249],\n",
      "        [1.0000, 0.5593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490778982639313 \tStep Time:  0.011968374252319336 s \tTotal Time:  26.43509078025818 s \n",
      "\n",
      "\n",
      "\tEpisode 4087 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5429],\n",
      "        [1.0000, 0.5594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5455],\n",
      "        [1.0000, 0.3736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420036315917969 \tStep Time:  0.007978439331054688 s \tTotal Time:  26.443069219589233 s \n",
      "\n",
      "\n",
      "\tEpisode 4088 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5025],\n",
      "        [1.0000, 0.5517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6367],\n",
      "        [1.0000, 0.4476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445121765136719 \tStep Time:  0.0069806575775146484 s \tTotal Time:  26.450049877166748 s \n",
      "\n",
      "\n",
      "\tEpisode 4089 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3969],\n",
      "        [1.0000, 0.5230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5580],\n",
      "        [1.0000, 0.4237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519397258758545 \tStep Time:  0.006981849670410156 s \tTotal Time:  26.457031726837158 s \n",
      "\n",
      "\n",
      "\tEpisode 4090 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5658],\n",
      "        [1.0000, 0.3591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5645],\n",
      "        [1.0000, 0.5618]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416319489479065 \tStep Time:  0.007977962493896484 s \tTotal Time:  26.465009689331055 s \n",
      "\n",
      "\n",
      "\tEpisode 4091 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3273],\n",
      "        [1.0000, 0.3073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5790],\n",
      "        [1.0000, 0.4551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.340572834014893 \tStep Time:  0.0060193538665771484 s \tTotal Time:  26.471029043197632 s \n",
      "\n",
      "\n",
      "\tEpisode 4092 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5768],\n",
      "        [1.0000, 0.5507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5849],\n",
      "        [1.0000, 0.1626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.371354341506958 \tStep Time:  0.007016420364379883 s \tTotal Time:  26.47804546356201 s \n",
      "\n",
      "\n",
      "\tEpisode 4093 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5948],\n",
      "        [1.0000, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4378],\n",
      "        [1.0000, 0.4870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.409383296966553 \tStep Time:  0.0070133209228515625 s \tTotal Time:  26.485058784484863 s \n",
      "\n",
      "\n",
      "\tEpisode 4094 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3059],\n",
      "        [1.0000, 0.3084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7398],\n",
      "        [1.0000, 0.7826]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  14.064697265625 \tStep Time:  0.006985187530517578 s \tTotal Time:  26.49204397201538 s \n",
      "\n",
      "\n",
      "\tEpisode 4095 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4051],\n",
      "        [1.0000, 0.4518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5840],\n",
      "        [1.0000, 0.4700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498526453971863 \tStep Time:  0.0069773197174072266 s \tTotal Time:  26.499021291732788 s \n",
      "\n",
      "\n",
      "\tEpisode 4096 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6107],\n",
      "        [1.0000, 0.3406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546843528747559 \tStep Time:  0.0059871673583984375 s \tTotal Time:  26.505008459091187 s \n",
      "\n",
      "\n",
      "\tEpisode 4097 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5962],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5504],\n",
      "        [1.0000, 0.3412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485237121582031 \tStep Time:  0.006947040557861328 s \tTotal Time:  26.511955499649048 s \n",
      "\n",
      "\n",
      "\tEpisode 4098 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3118],\n",
      "        [1.0000, 0.2649]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7715],\n",
      "        [1.0000, 0.5855]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.722264289855957 \tStep Time:  0.006015777587890625 s \tTotal Time:  26.51797127723694 s \n",
      "\n",
      "\n",
      "\tEpisode 4099 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6365],\n",
      "        [1.0000, 0.5698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5840],\n",
      "        [1.0000, 0.5882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509279251098633 \tStep Time:  0.006951570510864258 s \tTotal Time:  26.524922847747803 s \n",
      "\n",
      "\n",
      "\tEpisode 4100 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3657],\n",
      "        [1.0000, 0.5688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3574],\n",
      "        [1.0000, 0.3150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436494827270508 \tStep Time:  0.007013082504272461 s \tTotal Time:  26.531935930252075 s \n",
      "\n",
      "\n",
      "\tEpisode 4101 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5544],\n",
      "        [1.0000, 0.6437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5679],\n",
      "        [1.0000, 0.4127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.657977819442749 \tStep Time:  0.0059814453125 s \tTotal Time:  26.537917375564575 s \n",
      "\n",
      "\n",
      "\tEpisode 4102 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5638],\n",
      "        [1.0000, 0.5364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.5277]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537654399871826 \tStep Time:  0.007947206497192383 s \tTotal Time:  26.545864582061768 s \n",
      "\n",
      "\n",
      "\tEpisode 4103 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5451],\n",
      "        [1.0000, 0.5621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5592],\n",
      "        [1.0000, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536739349365234 \tStep Time:  0.0059854984283447266 s \tTotal Time:  26.551850080490112 s \n",
      "\n",
      "\n",
      "\tEpisode 4104 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5273],\n",
      "        [1.0000, 0.5564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5302],\n",
      "        [1.0000, 0.3504]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44550085067749 \tStep Time:  0.006980419158935547 s \tTotal Time:  26.558830499649048 s \n",
      "\n",
      "\n",
      "\tEpisode 4105 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4970],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3706],\n",
      "        [1.0000, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.624004364013672 \tStep Time:  0.006981611251831055 s \tTotal Time:  26.56581211090088 s \n",
      "\n",
      "\n",
      "\tEpisode 4106 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5424],\n",
      "        [1.0000, 0.4465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5492],\n",
      "        [1.0000, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.594418466091156 \tStep Time:  0.0059833526611328125 s \tTotal Time:  26.57179546356201 s \n",
      "\n",
      "\n",
      "\tEpisode 4107 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3838],\n",
      "        [1.0000, 0.3247]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5224],\n",
      "        [1.0000, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527247905731201 \tStep Time:  0.008976221084594727 s \tTotal Time:  26.580771684646606 s \n",
      "\n",
      "\n",
      "\tEpisode 4108 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5484],\n",
      "        [1.0000, 0.5335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5474],\n",
      "        [1.0000, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.566232204437256 \tStep Time:  0.0069806575775146484 s \tTotal Time:  26.58775234222412 s \n",
      "\n",
      "\n",
      "\tEpisode 4109 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3727],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503213882446289 \tStep Time:  0.006982326507568359 s \tTotal Time:  26.59473466873169 s \n",
      "\n",
      "\n",
      "\tEpisode 4110 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5502],\n",
      "        [1.0000, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3410],\n",
      "        [1.0000, 0.3972]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564111709594727 \tStep Time:  0.006980180740356445 s \tTotal Time:  26.601714849472046 s \n",
      "\n",
      "\n",
      "\tEpisode 4111 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5193],\n",
      "        [1.0000, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5302],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523894786834717 \tStep Time:  0.006981849670410156 s \tTotal Time:  26.609694004058838 s \n",
      "\n",
      "\n",
      "\tEpisode 4112 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5002],\n",
      "        [1.0000, 0.5563]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5568],\n",
      "        [1.0000, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526415824890137 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.61667513847351 s \n",
      "\n",
      "\n",
      "\tEpisode 4113 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4300],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3649],\n",
      "        [1.0000, 0.4925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.426884651184082 \tStep Time:  0.007979631423950195 s \tTotal Time:  26.62465476989746 s \n",
      "\n",
      "\n",
      "\tEpisode 4114 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4293],\n",
      "        [1.0000, 0.5491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5608],\n",
      "        [1.0000, 0.5647]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6013822555542 \tStep Time:  0.006981849670410156 s \tTotal Time:  26.632633447647095 s \n",
      "\n",
      "\n",
      "\tEpisode 4115 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5653],\n",
      "        [1.0000, 0.5628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4179],\n",
      "        [1.0000, 0.2741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474012851715088 \tStep Time:  0.006979942321777344 s \tTotal Time:  26.639613389968872 s \n",
      "\n",
      "\n",
      "\tEpisode 4116 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5654],\n",
      "        [1.0000, 0.5409]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.5554]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494905471801758 \tStep Time:  0.007016897201538086 s \tTotal Time:  26.64663028717041 s \n",
      "\n",
      "\n",
      "\tEpisode 4117 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5610],\n",
      "        [1.0000, 0.5500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465525150299072 \tStep Time:  0.006015777587890625 s \tTotal Time:  26.6526460647583 s \n",
      "\n",
      "\n",
      "\tEpisode 4118 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5543],\n",
      "        [1.0000, 0.5538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4675],\n",
      "        [1.0000, 0.4609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512685120105743 \tStep Time:  0.005953311920166016 s \tTotal Time:  26.658599376678467 s \n",
      "\n",
      "\n",
      "\tEpisode 4119 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5305],\n",
      "        [1.0000, 0.5043]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4260],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493384540081024 \tStep Time:  0.00601506233215332 s \tTotal Time:  26.66461443901062 s \n",
      "\n",
      "\n",
      "\tEpisode 4120 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5503],\n",
      "        [1.0000, 0.5438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4618],\n",
      "        [1.0000, 0.5361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474906921386719 \tStep Time:  0.005985736846923828 s \tTotal Time:  26.670600175857544 s \n",
      "\n",
      "\n",
      "\tEpisode 4121 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4742],\n",
      "        [1.0000, 0.5458]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4491],\n",
      "        [1.0000, 0.3418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503809690475464 \tStep Time:  0.006951093673706055 s \tTotal Time:  26.67755126953125 s \n",
      "\n",
      "\n",
      "\tEpisode 4122 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5248],\n",
      "        [1.0000, 0.5391]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5233],\n",
      "        [1.0000, 0.5462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5093554854393 \tStep Time:  0.005986213684082031 s \tTotal Time:  26.683537483215332 s \n",
      "\n",
      "\n",
      "\tEpisode 4123 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5207],\n",
      "        [1.0000, 0.3473]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4538],\n",
      "        [1.0000, 0.3898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588114142417908 \tStep Time:  0.006981611251831055 s \tTotal Time:  26.690519094467163 s \n",
      "\n",
      "\n",
      "\tEpisode 4124 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5442],\n",
      "        [1.0000, 0.4621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5384],\n",
      "        [1.0000, 0.4951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494348049163818 \tStep Time:  0.006983041763305664 s \tTotal Time:  26.69750213623047 s \n",
      "\n",
      "\n",
      "\tEpisode 4125 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.5334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5246],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457964062690735 \tStep Time:  0.0060520172119140625 s \tTotal Time:  26.703554153442383 s \n",
      "\n",
      "\n",
      "\tEpisode 4126 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5444],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5388],\n",
      "        [1.0000, 0.4628]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583325982093811 \tStep Time:  0.006948232650756836 s \tTotal Time:  26.71050238609314 s \n",
      "\n",
      "\n",
      "\tEpisode 4127 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4460],\n",
      "        [1.0000, 0.5439]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5447],\n",
      "        [1.0000, 0.4012]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493619918823242 \tStep Time:  0.005983829498291016 s \tTotal Time:  26.71648621559143 s \n",
      "\n",
      "\n",
      "\tEpisode 4128 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5366],\n",
      "        [1.0000, 0.4691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4650],\n",
      "        [1.0000, 0.5522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510631561279297 \tStep Time:  0.00598454475402832 s \tTotal Time:  26.72247076034546 s \n",
      "\n",
      "\n",
      "\tEpisode 4129 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5528],\n",
      "        [1.0000, 0.4591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4669],\n",
      "        [1.0000, 0.5531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511290907859802 \tStep Time:  0.007978200912475586 s \tTotal Time:  26.730448961257935 s \n",
      "\n",
      "\n",
      "\tEpisode 4130 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4780],\n",
      "        [1.0000, 0.4569]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5387],\n",
      "        [1.0000, 0.5417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435317516326904 \tStep Time:  0.006016969680786133 s \tTotal Time:  26.73646593093872 s \n",
      "\n",
      "\n",
      "\tEpisode 4131 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5461],\n",
      "        [1.0000, 0.5045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4458],\n",
      "        [1.0000, 0.4259]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597435474395752 \tStep Time:  0.005994081497192383 s \tTotal Time:  26.742460012435913 s \n",
      "\n",
      "\n",
      "\tEpisode 4132 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4320],\n",
      "        [1.0000, 0.3341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.5429]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4689359664917 \tStep Time:  0.005972862243652344 s \tTotal Time:  26.748432874679565 s \n",
      "\n",
      "\n",
      "\tEpisode 4133 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4614],\n",
      "        [1.0000, 0.5253]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5366],\n",
      "        [1.0000, 0.4827]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510248184204102 \tStep Time:  0.0059833526611328125 s \tTotal Time:  26.7544162273407 s \n",
      "\n",
      "\n",
      "\tEpisode 4134 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5172],\n",
      "        [1.0000, 0.4382]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5452],\n",
      "        [1.0000, 0.4642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50671911239624 \tStep Time:  0.006982326507568359 s \tTotal Time:  26.761398553848267 s \n",
      "\n",
      "\n",
      "\tEpisode 4135 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5485],\n",
      "        [1.0000, 0.5365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5493],\n",
      "        [1.0000, 0.5496]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500165462493896 \tStep Time:  0.004987955093383789 s \tTotal Time:  26.76638650894165 s \n",
      "\n",
      "\n",
      "\tEpisode 4136 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5487],\n",
      "        [1.0000, 0.5489]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5488],\n",
      "        [1.0000, 0.5257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517428874969482 \tStep Time:  0.004950284957885742 s \tTotal Time:  26.772337198257446 s \n",
      "\n",
      "\n",
      "\tEpisode 4137 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5027],\n",
      "        [1.0000, 0.5444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4756],\n",
      "        [1.0000, 0.5456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44861388206482 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.780315399169922 s \n",
      "\n",
      "\n",
      "\tEpisode 4138 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5208],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5419],\n",
      "        [1.0000, 0.5277]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527040481567383 \tStep Time:  0.005984067916870117 s \tTotal Time:  26.786299467086792 s \n",
      "\n",
      "\n",
      "\tEpisode 4139 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5455],\n",
      "        [1.0000, 0.4029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5199],\n",
      "        [1.0000, 0.4117]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63784795999527 \tStep Time:  0.0059854984283447266 s \tTotal Time:  26.79328227043152 s \n",
      "\n",
      "\n",
      "\tEpisode 4140 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3760],\n",
      "        [1.0000, 0.4295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4683],\n",
      "        [1.0000, 0.4700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545658588409424 \tStep Time:  0.006979703903198242 s \tTotal Time:  26.800261974334717 s \n",
      "\n",
      "\n",
      "\tEpisode 4141 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4390],\n",
      "        [1.0000, 0.4285]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3019],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.655939042568207 \tStep Time:  0.006981611251831055 s \tTotal Time:  26.807243585586548 s \n",
      "\n",
      "\n",
      "\tEpisode 4142 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4678],\n",
      "        [1.0000, 0.4677]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4885],\n",
      "        [1.0000, 0.5477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537894368171692 \tStep Time:  0.005983829498291016 s \tTotal Time:  26.81322741508484 s \n",
      "\n",
      "\n",
      "\tEpisode 4143 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5586],\n",
      "        [1.0000, 0.5187]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4840],\n",
      "        [1.0000, 0.5388]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561227321624756 \tStep Time:  0.0069806575775146484 s \tTotal Time:  26.820208072662354 s \n",
      "\n",
      "\n",
      "\tEpisode 4144 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5630],\n",
      "        [1.0000, 0.5328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4412],\n",
      "        [1.0000, 0.4280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50412368774414 \tStep Time:  0.006981611251831055 s \tTotal Time:  26.827189683914185 s \n",
      "\n",
      "\n",
      "\tEpisode 4145 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5671],\n",
      "        [1.0000, 0.5499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4904],\n",
      "        [1.0000, 0.5610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482717335224152 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.834170818328857 s \n",
      "\n",
      "\n",
      "\tEpisode 4146 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5417],\n",
      "        [1.0000, 0.4218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3885],\n",
      "        [1.0000, 0.4751]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468033790588379 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.84115219116211 s \n",
      "\n",
      "\n",
      "\tEpisode 4147 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5823],\n",
      "        [1.0000, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5680],\n",
      "        [1.0000, 0.5692]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536466598510742 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.84813356399536 s \n",
      "\n",
      "\n",
      "\tEpisode 4148 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5742],\n",
      "        [1.0000, 0.5735]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5785],\n",
      "        [1.0000, 0.5585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510519027709961 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.855114698410034 s \n",
      "\n",
      "\n",
      "\tEpisode 4149 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5925],\n",
      "        [1.0000, 0.5741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5169],\n",
      "        [1.0000, 0.4878]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505757689476013 \tStep Time:  0.00797891616821289 s \tTotal Time:  26.863093614578247 s \n",
      "\n",
      "\n",
      "\tEpisode 4150 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5697],\n",
      "        [1.0000, 0.4525]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.5806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504177570343018 \tStep Time:  0.005983829498291016 s \tTotal Time:  26.869077444076538 s \n",
      "\n",
      "\n",
      "\tEpisode 4151 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5124],\n",
      "        [1.0000, 0.6936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5067],\n",
      "        [1.0000, 0.5717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.651115894317627 \tStep Time:  0.00798177719116211 s \tTotal Time:  26.8770592212677 s \n",
      "\n",
      "\n",
      "\tEpisode 4152 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5617],\n",
      "        [1.0000, 0.5631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476666450500488 \tStep Time:  0.006978511810302734 s \tTotal Time:  26.884037733078003 s \n",
      "\n",
      "\n",
      "\tEpisode 4153 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5577],\n",
      "        [1.0000, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5506],\n",
      "        [1.0000, 0.5513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485231399536133 \tStep Time:  0.006981849670410156 s \tTotal Time:  26.891019582748413 s \n",
      "\n",
      "\n",
      "\tEpisode 4154 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4701],\n",
      "        [1.0000, 0.5334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4955],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548630595207214 \tStep Time:  0.007977962493896484 s \tTotal Time:  26.89899754524231 s \n",
      "\n",
      "\n",
      "\tEpisode 4155 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4249],\n",
      "        [1.0000, 0.4893]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5357],\n",
      "        [1.0000, 0.5212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550013780593872 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.90597891807556 s \n",
      "\n",
      "\n",
      "\tEpisode 4156 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4810],\n",
      "        [1.0000, 0.4668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5024],\n",
      "        [1.0000, 0.5334]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498069763183594 \tStep Time:  0.0069811344146728516 s \tTotal Time:  26.912960052490234 s \n",
      "\n",
      "\n",
      "\tEpisode 4157 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4351],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4912],\n",
      "        [1.0000, 0.4481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519582748413086 \tStep Time:  0.006981372833251953 s \tTotal Time:  26.919941425323486 s \n",
      "\n",
      "\n",
      "\tEpisode 4158 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4519],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5304],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544987201690674 \tStep Time:  0.009974241256713867 s \tTotal Time:  26.9299156665802 s \n",
      "\n",
      "\n",
      "\tEpisode 4159 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5313],\n",
      "        [1.0000, 0.5243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5236],\n",
      "        [1.0000, 0.4634]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481620132923126 \tStep Time:  0.00797891616821289 s \tTotal Time:  26.937894582748413 s \n",
      "\n",
      "\n",
      "\tEpisode 4160 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4704],\n",
      "        [1.0000, 0.4622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5160],\n",
      "        [1.0000, 0.5210]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51170951128006 \tStep Time:  0.007013559341430664 s \tTotal Time:  26.944908142089844 s \n",
      "\n",
      "\n",
      "\tEpisode 4161 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4731],\n",
      "        [1.0000, 0.4787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5187],\n",
      "        [1.0000, 0.5001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49401479959488 \tStep Time:  0.005983829498291016 s \tTotal Time:  26.9518883228302 s \n",
      "\n",
      "\n",
      "\tEpisode 4162 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4802],\n",
      "        [1.0000, 0.5247]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4884],\n",
      "        [1.0000, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488946676254272 \tStep Time:  0.006442546844482422 s \tTotal Time:  26.958330869674683 s \n",
      "\n",
      "\n",
      "\tEpisode 4163 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5146],\n",
      "        [1.0000, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5234],\n",
      "        [1.0000, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51896721124649 \tStep Time:  0.005605220794677734 s \tTotal Time:  26.96393609046936 s \n",
      "\n",
      "\n",
      "\tEpisode 4164 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4504],\n",
      "        [1.0000, 0.4607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5215],\n",
      "        [1.0000, 0.4150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570223331451416 \tStep Time:  0.005980730056762695 s \tTotal Time:  26.969916820526123 s \n",
      "\n",
      "\n",
      "\tEpisode 4165 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5217],\n",
      "        [1.0000, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533355236053467 \tStep Time:  0.007947921752929688 s \tTotal Time:  26.977864742279053 s \n",
      "\n",
      "\n",
      "\tEpisode 4166 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5264],\n",
      "        [1.0000, 0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5339],\n",
      "        [1.0000, 0.5326]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521432876586914 \tStep Time:  0.006987810134887695 s \tTotal Time:  26.98485255241394 s \n",
      "\n",
      "\n",
      "\tEpisode 4167 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5235],\n",
      "        [1.0000, 0.4965]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4836],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514033019542694 \tStep Time:  0.00598454475402832 s \tTotal Time:  26.99083709716797 s \n",
      "\n",
      "\n",
      "\tEpisode 4168 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4604],\n",
      "        [1.0000, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5292],\n",
      "        [1.0000, 0.5071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54198932647705 \tStep Time:  0.007978439331054688 s \tTotal Time:  26.998815536499023 s \n",
      "\n",
      "\n",
      "\tEpisode 4169 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4417],\n",
      "        [1.0000, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4521],\n",
      "        [1.0000, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496901512145996 \tStep Time:  0.005982637405395508 s \tTotal Time:  27.00479817390442 s \n",
      "\n",
      "\n",
      "\tEpisode 4170 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4863],\n",
      "        [1.0000, 0.4449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4839],\n",
      "        [1.0000, 0.4604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497241497039795 \tStep Time:  0.006981611251831055 s \tTotal Time:  27.01177978515625 s \n",
      "\n",
      "\n",
      "\tEpisode 4171 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.4562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5245],\n",
      "        [1.0000, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478924572467804 \tStep Time:  0.00698089599609375 s \tTotal Time:  27.018760681152344 s \n",
      "\n",
      "\n",
      "\tEpisode 4172 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4563],\n",
      "        [1.0000, 0.4739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5265],\n",
      "        [1.0000, 0.4445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460212230682373 \tStep Time:  0.006981849670410156 s \tTotal Time:  27.025742530822754 s \n",
      "\n",
      "\n",
      "\tEpisode 4173 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4683],\n",
      "        [1.0000, 0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5273],\n",
      "        [1.0000, 0.5036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518733024597168 \tStep Time:  0.00701594352722168 s \tTotal Time:  27.032758474349976 s \n",
      "\n",
      "\n",
      "\tEpisode 4174 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5129],\n",
      "        [1.0000, 0.4780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5270],\n",
      "        [1.0000, 0.5305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536659240722656 \tStep Time:  0.006978511810302734 s \tTotal Time:  27.03973698616028 s \n",
      "\n",
      "\n",
      "\tEpisode 4175 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4374],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4292],\n",
      "        [1.0000, 0.4753]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560532867908478 \tStep Time:  0.00794839859008789 s \tTotal Time:  27.047685384750366 s \n",
      "\n",
      "\n",
      "\tEpisode 4176 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4793],\n",
      "        [1.0000, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5187],\n",
      "        [1.0000, 0.5275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485010862350464 \tStep Time:  0.005982637405395508 s \tTotal Time:  27.05366802215576 s \n",
      "\n",
      "\n",
      "\tEpisode 4177 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4467],\n",
      "        [1.0000, 0.4694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.4494]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.468082189559937 \tStep Time:  0.006018161773681641 s \tTotal Time:  27.059686183929443 s \n",
      "\n",
      "\n",
      "\tEpisode 4178 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4274],\n",
      "        [1.0000, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5181],\n",
      "        [1.0000, 0.5148]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581470012664795 \tStep Time:  0.006034374237060547 s \tTotal Time:  27.066714763641357 s \n",
      "\n",
      "\n",
      "\tEpisode 4179 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4865],\n",
      "        [1.0000, 0.5353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5458],\n",
      "        [1.0000, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531225204467773 \tStep Time:  0.005973339080810547 s \tTotal Time:  27.072688102722168 s \n",
      "\n",
      "\n",
      "\tEpisode 4180 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4452],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5284],\n",
      "        [1.0000, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47090470790863 \tStep Time:  0.0069866180419921875 s \tTotal Time:  27.07967472076416 s \n",
      "\n",
      "\n",
      "\tEpisode 4181 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4964],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514561176300049 \tStep Time:  0.005987644195556641 s \tTotal Time:  27.085662364959717 s \n",
      "\n",
      "\n",
      "\tEpisode 4182 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.4797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5388],\n",
      "        [1.0000, 0.5364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480374336242676 \tStep Time:  0.005983829498291016 s \tTotal Time:  27.091646194458008 s \n",
      "\n",
      "\n",
      "\tEpisode 4183 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4733],\n",
      "        [1.0000, 0.5377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.4858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51607084274292 \tStep Time:  0.00601649284362793 s \tTotal Time:  27.097662687301636 s \n",
      "\n",
      "\n",
      "\tEpisode 4184 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4966],\n",
      "        [1.0000, 0.5275]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4811],\n",
      "        [1.0000, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515224933624268 \tStep Time:  0.005985736846923828 s \tTotal Time:  27.10364842414856 s \n",
      "\n",
      "\n",
      "\tEpisode 4185 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5339],\n",
      "        [1.0000, 0.4721]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4840],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522727012634277 \tStep Time:  0.0069484710693359375 s \tTotal Time:  27.110596895217896 s \n",
      "\n",
      "\n",
      "\tEpisode 4186 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4019],\n",
      "        [1.0000, 0.5238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5182],\n",
      "        [1.0000, 0.4561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47943925857544 \tStep Time:  0.007977485656738281 s \tTotal Time:  27.118574380874634 s \n",
      "\n",
      "\n",
      "\tEpisode 4187 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4759],\n",
      "        [1.0000, 0.4348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4951],\n",
      "        [1.0000, 0.4555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485795021057129 \tStep Time:  0.006982326507568359 s \tTotal Time:  27.125556707382202 s \n",
      "\n",
      "\n",
      "\tEpisode 4188 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5136],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4928],\n",
      "        [1.0000, 0.5220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482588291168213 \tStep Time:  0.006980419158935547 s \tTotal Time:  27.132537126541138 s \n",
      "\n",
      "\n",
      "\tEpisode 4189 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.5250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4639],\n",
      "        [1.0000, 0.4599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490252494812012 \tStep Time:  0.0069811344146728516 s \tTotal Time:  27.13951826095581 s \n",
      "\n",
      "\n",
      "\tEpisode 4190 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4751],\n",
      "        [1.0000, 0.4483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5253],\n",
      "        [1.0000, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497734785079956 \tStep Time:  0.005983829498291016 s \tTotal Time:  27.1455020904541 s \n",
      "\n",
      "\n",
      "\tEpisode 4191 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4960],\n",
      "        [1.0000, 0.5132]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5083],\n",
      "        [1.0000, 0.5271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486978471279144 \tStep Time:  0.005984067916870117 s \tTotal Time:  27.15148615837097 s \n",
      "\n",
      "\n",
      "\tEpisode 4192 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2838],\n",
      "        [1.0000, 0.5290]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5461],\n",
      "        [1.0000, 0.3945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595438003540039 \tStep Time:  0.006981372833251953 s \tTotal Time:  27.158467531204224 s \n",
      "\n",
      "\n",
      "\tEpisode 4193 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5607],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.5526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54466199874878 \tStep Time:  0.005984067916870117 s \tTotal Time:  27.164451599121094 s \n",
      "\n",
      "\n",
      "\tEpisode 4194 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4822],\n",
      "        [1.0000, 0.3524]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4957],\n",
      "        [1.0000, 0.5329]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.420680522918701 \tStep Time:  0.005984783172607422 s \tTotal Time:  27.1704363822937 s \n",
      "\n",
      "\n",
      "\tEpisode 4195 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3773],\n",
      "        [1.0000, 0.5631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5367],\n",
      "        [1.0000, 0.5694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438885390758514 \tStep Time:  0.007978677749633789 s \tTotal Time:  27.178415060043335 s \n",
      "\n",
      "\n",
      "\tEpisode 4196 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5604],\n",
      "        [1.0000, 0.5646]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5664],\n",
      "        [1.0000, 0.5717]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510796785354614 \tStep Time:  0.006981372833251953 s \tTotal Time:  27.185396432876587 s \n",
      "\n",
      "\n",
      "\tEpisode 4197 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5737],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5580],\n",
      "        [1.0000, 0.5722]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553975105285645 \tStep Time:  0.0139617919921875 s \tTotal Time:  27.199358224868774 s \n",
      "\n",
      "\n",
      "\tEpisode 4198 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.6044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4021],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508974552154541 \tStep Time:  0.010007381439208984 s \tTotal Time:  27.209365606307983 s \n",
      "\n",
      "\n",
      "\tEpisode 4199 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5571],\n",
      "        [1.0000, 0.5608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2675],\n",
      "        [1.0000, 0.5521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.693488121032715 \tStep Time:  0.00797891616821289 s \tTotal Time:  27.217344522476196 s \n",
      "\n",
      "\n",
      "\tEpisode 4200 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3057],\n",
      "        [1.0000, 0.5464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5130],\n",
      "        [1.0000, 0.5252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439448356628418 \tStep Time:  0.010936737060546875 s \tTotal Time:  27.228281259536743 s \n",
      "\n",
      "\n",
      "\tEpisode 4201 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3222],\n",
      "        [1.0000, 0.4721]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3494],\n",
      "        [1.0000, 0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36649763584137 \tStep Time:  0.007979393005371094 s \tTotal Time:  27.236260652542114 s \n",
      "\n",
      "\n",
      "\tEpisode 4202 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5218],\n",
      "        [1.0000, 0.4696]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3766],\n",
      "        [1.0000, 0.5318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4616117477417 \tStep Time:  0.017950773239135742 s \tTotal Time:  27.25421142578125 s \n",
      "\n",
      "\n",
      "\tEpisode 4203 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5294],\n",
      "        [1.0000, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3203],\n",
      "        [1.0000, 0.3520]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558692216873169 \tStep Time:  0.008976221084594727 s \tTotal Time:  27.263187646865845 s \n",
      "\n",
      "\n",
      "\tEpisode 4204 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5391],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4023],\n",
      "        [1.0000, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.430711090564728 \tStep Time:  0.008975505828857422 s \tTotal Time:  27.272163152694702 s \n",
      "\n",
      "\n",
      "\tEpisode 4205 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4616],\n",
      "        [1.0000, 0.3310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5188],\n",
      "        [1.0000, 0.5414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.386317253112793 \tStep Time:  0.010971784591674805 s \tTotal Time:  27.283134937286377 s \n",
      "\n",
      "\n",
      "\tEpisode 4206 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5288],\n",
      "        [1.0000, 0.4164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3547],\n",
      "        [1.0000, 0.5364]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.668701767921448 \tStep Time:  0.009972333908081055 s \tTotal Time:  27.293107271194458 s \n",
      "\n",
      "\n",
      "\tEpisode 4207 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5503],\n",
      "        [1.0000, 0.4183]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3457],\n",
      "        [1.0000, 0.5295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.36790931224823 \tStep Time:  0.007978677749633789 s \tTotal Time:  27.301085948944092 s \n",
      "\n",
      "\n",
      "\tEpisode 4208 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4982],\n",
      "        [1.0000, 0.5538]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5212],\n",
      "        [1.0000, 0.5631]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517798244953156 \tStep Time:  0.007979154586791992 s \tTotal Time:  27.309065103530884 s \n",
      "\n",
      "\n",
      "\tEpisode 4209 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5641],\n",
      "        [1.0000, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4926],\n",
      "        [1.0000, 0.2902]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.459084868431091 \tStep Time:  0.011968851089477539 s \tTotal Time:  27.32103395462036 s \n",
      "\n",
      "\n",
      "\tEpisode 4210 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5682],\n",
      "        [1.0000, 0.5656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5534],\n",
      "        [1.0000, 0.3294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.413875699043274 \tStep Time:  0.010970115661621094 s \tTotal Time:  27.332004070281982 s \n",
      "\n",
      "\n",
      "\tEpisode 4211 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5176],\n",
      "        [1.0000, 0.5797]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5767],\n",
      "        [1.0000, 0.3762]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.652463376522064 \tStep Time:  0.007977485656738281 s \tTotal Time:  27.33998155593872 s \n",
      "\n",
      "\n",
      "\tEpisode 4212 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3885],\n",
      "        [1.0000, 0.5813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5842],\n",
      "        [1.0000, 0.5416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402604818344116 \tStep Time:  0.007978439331054688 s \tTotal Time:  27.347959995269775 s \n",
      "\n",
      "\n",
      "\tEpisode 4213 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3557],\n",
      "        [1.0000, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5580],\n",
      "        [1.0000, 0.5853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.648419857025146 \tStep Time:  0.006981372833251953 s \tTotal Time:  27.354941368103027 s \n",
      "\n",
      "\n",
      "\tEpisode 4214 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4993],\n",
      "        [1.0000, 0.5432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5786],\n",
      "        [1.0000, 0.4172]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4138503074646 \tStep Time:  0.006981372833251953 s \tTotal Time:  27.36292028427124 s \n",
      "\n",
      "\n",
      "\tEpisode 4215 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5713],\n",
      "        [1.0000, 0.5556]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5611],\n",
      "        [1.0000, 0.2921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.663029491901398 \tStep Time:  0.007978677749633789 s \tTotal Time:  27.370898962020874 s \n",
      "\n",
      "\n",
      "\tEpisode 4216 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5396],\n",
      "        [1.0000, 0.5812]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5747],\n",
      "        [1.0000, 0.5852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53818142414093 \tStep Time:  0.007978677749633789 s \tTotal Time:  27.378877639770508 s \n",
      "\n",
      "\n",
      "\tEpisode 4217 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5840],\n",
      "        [1.0000, 0.5794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4452],\n",
      "        [1.0000, 0.5718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.579846978187561 \tStep Time:  0.007978439331054688 s \tTotal Time:  27.386856079101562 s \n",
      "\n",
      "\n",
      "\tEpisode 4218 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4479],\n",
      "        [1.0000, 0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5677],\n",
      "        [1.0000, 0.4314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.415884971618652 \tStep Time:  0.008976221084594727 s \tTotal Time:  27.395832300186157 s \n",
      "\n",
      "\n",
      "\tEpisode 4219 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3716],\n",
      "        [1.0000, 0.5274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5203],\n",
      "        [1.0000, 0.4465]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49130916595459 \tStep Time:  0.009974479675292969 s \tTotal Time:  27.40580677986145 s \n",
      "\n",
      "\n",
      "\tEpisode 4220 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5221],\n",
      "        [1.0000, 0.5566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5304],\n",
      "        [1.0000, 0.5493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51333999633789 \tStep Time:  0.009973287582397461 s \tTotal Time:  27.415780067443848 s \n",
      "\n",
      "\n",
      "\tEpisode 4221 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5425],\n",
      "        [1.0000, 0.5668]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.5752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.457922458648682 \tStep Time:  0.009973764419555664 s \tTotal Time:  27.425753831863403 s \n",
      "\n",
      "\n",
      "\tEpisode 4222 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5333],\n",
      "        [1.0000, 0.5623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.4220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572330355644226 \tStep Time:  0.009974002838134766 s \tTotal Time:  27.436724185943604 s \n",
      "\n",
      "\n",
      "\tEpisode 4223 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5299],\n",
      "        [1.0000, 0.4147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3199],\n",
      "        [1.0000, 0.5404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.35794723033905 \tStep Time:  0.010970115661621094 s \tTotal Time:  27.447694301605225 s \n",
      "\n",
      "\n",
      "\tEpisode 4224 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4585],\n",
      "        [1.0000, 0.5407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.4743]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509811401367188 \tStep Time:  0.009973526000976562 s \tTotal Time:  27.4576678276062 s \n",
      "\n",
      "\n",
      "\tEpisode 4225 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5493],\n",
      "        [1.0000, 0.4514]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3379],\n",
      "        [1.0000, 0.4304]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.64774227142334 \tStep Time:  0.008975505828857422 s \tTotal Time:  27.46664333343506 s \n",
      "\n",
      "\n",
      "\tEpisode 4226 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4454],\n",
      "        [1.0000, 0.3138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5347],\n",
      "        [1.0000, 0.5683]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.585037231445312 \tStep Time:  0.008977890014648438 s \tTotal Time:  27.475621223449707 s \n",
      "\n",
      "\n",
      "\tEpisode 4227 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4051],\n",
      "        [1.0000, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5540],\n",
      "        [1.0000, 0.5529]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464708805084229 \tStep Time:  0.008974552154541016 s \tTotal Time:  27.484595775604248 s \n",
      "\n",
      "\n",
      "\tEpisode 4228 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5660],\n",
      "        [1.0000, 0.5579]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5199],\n",
      "        [1.0000, 0.5503]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491000652313232 \tStep Time:  0.007978439331054688 s \tTotal Time:  27.492574214935303 s \n",
      "\n",
      "\n",
      "\tEpisode 4229 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5664],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5601],\n",
      "        [1.0000, 0.5414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574618577957153 \tStep Time:  0.007978200912475586 s \tTotal Time:  27.50055241584778 s \n",
      "\n",
      "\n",
      "\tEpisode 4230 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5463],\n",
      "        [1.0000, 0.5573]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5526],\n",
      "        [1.0000, 0.5517]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523368656635284 \tStep Time:  0.007979393005371094 s \tTotal Time:  27.50853180885315 s \n",
      "\n",
      "\n",
      "\tEpisode 4231 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.4600]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5514],\n",
      "        [1.0000, 0.5483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539151549339294 \tStep Time:  0.008046150207519531 s \tTotal Time:  27.51657795906067 s \n",
      "\n",
      "\n",
      "\tEpisode 4232 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.5542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4547],\n",
      "        [1.0000, 0.5519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478273570537567 \tStep Time:  0.0069713592529296875 s \tTotal Time:  27.5235493183136 s \n",
      "\n",
      "\n",
      "\tEpisode 4233 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4503],\n",
      "        [1.0000, 0.4982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4479],\n",
      "        [1.0000, 0.5362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5455322265625 \tStep Time:  0.007956504821777344 s \tTotal Time:  27.531505823135376 s \n",
      "\n",
      "\n",
      "\tEpisode 4234 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.5348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4639],\n",
      "        [1.0000, 0.4178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552745878696442 \tStep Time:  0.007979154586791992 s \tTotal Time:  27.539484977722168 s \n",
      "\n",
      "\n",
      "\tEpisode 4235 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4356],\n",
      "        [1.0000, 0.4775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.5401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44395923614502 \tStep Time:  0.007977962493896484 s \tTotal Time:  27.547462940216064 s \n",
      "\n",
      "\n",
      "\tEpisode 4236 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5006],\n",
      "        [1.0000, 0.4271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5357],\n",
      "        [1.0000, 0.4140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50514143705368 \tStep Time:  0.007979154586791992 s \tTotal Time:  27.555442094802856 s \n",
      "\n",
      "\n",
      "\tEpisode 4237 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5424],\n",
      "        [1.0000, 0.5434]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4811],\n",
      "        [1.0000, 0.4431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506928443908691 \tStep Time:  0.008975744247436523 s \tTotal Time:  27.564417839050293 s \n",
      "\n",
      "\n",
      "\tEpisode 4238 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4860],\n",
      "        [1.0000, 0.5377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5330],\n",
      "        [1.0000, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567555904388428 \tStep Time:  0.007978677749633789 s \tTotal Time:  27.572396516799927 s \n",
      "\n",
      "\n",
      "\tEpisode 4239 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5039],\n",
      "        [1.0000, 0.5422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5266],\n",
      "        [1.0000, 0.4650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532164096832275 \tStep Time:  0.00997304916381836 s \tTotal Time:  27.582369565963745 s \n",
      "\n",
      "\n",
      "\tEpisode 4240 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4360],\n",
      "        [1.0000, 0.5408]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4576],\n",
      "        [1.0000, 0.4526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.585667610168457 \tStep Time:  0.00797891616821289 s \tTotal Time:  27.590348482131958 s \n",
      "\n",
      "\n",
      "\tEpisode 4241 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5400],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.5381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522305965423584 \tStep Time:  0.010970354080200195 s \tTotal Time:  27.601318836212158 s \n",
      "\n",
      "\n",
      "\tEpisode 4242 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.5376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5359],\n",
      "        [1.0000, 0.5320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535695970058441 \tStep Time:  0.00998687744140625 s \tTotal Time:  27.611305713653564 s \n",
      "\n",
      "\n",
      "\tEpisode 4243 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5405],\n",
      "        [1.0000, 0.5240]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5401],\n",
      "        [1.0000, 0.4065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610677242279053 \tStep Time:  0.017940044403076172 s \tTotal Time:  27.62924575805664 s \n",
      "\n",
      "\n",
      "\tEpisode 4244 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5382],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3649],\n",
      "        [1.0000, 0.4670]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63555383682251 \tStep Time:  0.013961315155029297 s \tTotal Time:  27.64320707321167 s \n",
      "\n",
      "\n",
      "\tEpisode 4245 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4922],\n",
      "        [1.0000, 0.4256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4460],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.586423218250275 \tStep Time:  0.013963937759399414 s \tTotal Time:  27.65717101097107 s \n",
      "\n",
      "\n",
      "\tEpisode 4246 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4885],\n",
      "        [1.0000, 0.4590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5405],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502948939800262 \tStep Time:  0.01296377182006836 s \tTotal Time:  27.670134782791138 s \n",
      "\n",
      "\n",
      "\tEpisode 4247 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5347],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.5081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537718772888184 \tStep Time:  0.01795172691345215 s \tTotal Time:  27.68808650970459 s \n",
      "\n",
      "\n",
      "\tEpisode 4248 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4899],\n",
      "        [1.0000, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.4629]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532346248626709 \tStep Time:  0.008976936340332031 s \tTotal Time:  27.697063446044922 s \n",
      "\n",
      "\n",
      "\tEpisode 4249 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5028]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5234],\n",
      "        [1.0000, 0.4300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551240921020508 \tStep Time:  0.007977485656738281 s \tTotal Time:  27.70504093170166 s \n",
      "\n",
      "\n",
      "\tEpisode 4250 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5398],\n",
      "        [1.0000, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4477],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.548172950744629 \tStep Time:  0.00797891616821289 s \tTotal Time:  27.713019847869873 s \n",
      "\n",
      "\n",
      "\tEpisode 4251 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5459],\n",
      "        [1.0000, 0.5456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5435],\n",
      "        [1.0000, 0.5385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513091087341309 \tStep Time:  0.007979154586791992 s \tTotal Time:  27.720999002456665 s \n",
      "\n",
      "\n",
      "\tEpisode 4252 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4914],\n",
      "        [1.0000, 0.5249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5023],\n",
      "        [1.0000, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491423606872559 \tStep Time:  0.00797891616821289 s \tTotal Time:  27.728977918624878 s \n",
      "\n",
      "\n",
      "\tEpisode 4253 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5384],\n",
      "        [1.0000, 0.4728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4416],\n",
      "        [1.0000, 0.5068]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.440570533275604 \tStep Time:  0.00698399543762207 s \tTotal Time:  27.73695945739746 s \n",
      "\n",
      "\n",
      "\tEpisode 4254 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5399],\n",
      "        [1.0000, 0.5340]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5436],\n",
      "        [1.0000, 0.5291]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494219362735748 \tStep Time:  0.00897669792175293 s \tTotal Time:  27.745936155319214 s \n",
      "\n",
      "\n",
      "\tEpisode 4255 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5390],\n",
      "        [1.0000, 0.5250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5351],\n",
      "        [1.0000, 0.4441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452284336090088 \tStep Time:  0.007977724075317383 s \tTotal Time:  27.75391387939453 s \n",
      "\n",
      "\n",
      "\tEpisode 4256 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5469],\n",
      "        [1.0000, 0.4462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3994],\n",
      "        [1.0000, 0.5331]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479456901550293 \tStep Time:  0.008976936340332031 s \tTotal Time:  27.762890815734863 s \n",
      "\n",
      "\n",
      "\tEpisode 4257 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5657],\n",
      "        [1.0000, 0.5695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5197],\n",
      "        [1.0000, 0.5795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491647243499756 \tStep Time:  0.007978677749633789 s \tTotal Time:  27.770869493484497 s \n",
      "\n",
      "\n",
      "\tEpisode 4258 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5594],\n",
      "        [1.0000, 0.5324]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5693],\n",
      "        [1.0000, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516523361206055 \tStep Time:  0.008977174758911133 s \tTotal Time:  27.779846668243408 s \n",
      "\n",
      "\n",
      "\tEpisode 4259 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5687],\n",
      "        [1.0000, 0.5858]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5307],\n",
      "        [1.0000, 0.5394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551136493682861 \tStep Time:  0.007977008819580078 s \tTotal Time:  27.78782367706299 s \n",
      "\n",
      "\n",
      "\tEpisode 4260 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5513],\n",
      "        [1.0000, 0.5889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5837],\n",
      "        [1.0000, 0.5246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496036052703857 \tStep Time:  0.00897526741027832 s \tTotal Time:  27.796798944473267 s \n",
      "\n",
      "\n",
      "\tEpisode 4261 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5336],\n",
      "        [1.0000, 0.5522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5935],\n",
      "        [1.0000, 0.5759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497498989105225 \tStep Time:  0.007979631423950195 s \tTotal Time:  27.804778575897217 s \n",
      "\n",
      "\n",
      "\tEpisode 4262 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5650],\n",
      "        [1.0000, 0.2907]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5471],\n",
      "        [1.0000, 0.3800]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499018669128418 \tStep Time:  0.008975982666015625 s \tTotal Time:  27.813754558563232 s \n",
      "\n",
      "\n",
      "\tEpisode 4263 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2927],\n",
      "        [1.0000, 0.5644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4940],\n",
      "        [1.0000, 0.5145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.379250526428223 \tStep Time:  0.007978439331054688 s \tTotal Time:  27.821732997894287 s \n",
      "\n",
      "\n",
      "\tEpisode 4264 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3495],\n",
      "        [1.0000, 0.4546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3229],\n",
      "        [1.0000, 0.3251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.623409748077393 \tStep Time:  0.008975744247436523 s \tTotal Time:  27.831705808639526 s \n",
      "\n",
      "\n",
      "\tEpisode 4265 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3157],\n",
      "        [1.0000, 0.5690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3471],\n",
      "        [1.0000, 0.5927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.288393020629883 \tStep Time:  0.007978439331054688 s \tTotal Time:  27.83968424797058 s \n",
      "\n",
      "\n",
      "\tEpisode 4266 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5151],\n",
      "        [1.0000, 0.5287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4631],\n",
      "        [1.0000, 0.5585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549610137939453 \tStep Time:  0.00897669792175293 s \tTotal Time:  27.848660945892334 s \n",
      "\n",
      "\n",
      "\tEpisode 4267 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5887],\n",
      "        [1.0000, 0.5976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4632],\n",
      "        [1.0000, 0.4886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509310722351074 \tStep Time:  0.007979393005371094 s \tTotal Time:  27.856640338897705 s \n",
      "\n",
      "\n",
      "\tEpisode 4268 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5646],\n",
      "        [1.0000, 0.6669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5769],\n",
      "        [1.0000, 0.5758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490141868591309 \tStep Time:  0.00897526741027832 s \tTotal Time:  27.865615606307983 s \n",
      "\n",
      "\n",
      "\tEpisode 4269 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3744],\n",
      "        [1.0000, 0.3763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6214],\n",
      "        [1.0000, 0.5979]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523950576782227 \tStep Time:  0.006981849670410156 s \tTotal Time:  27.872597455978394 s \n",
      "\n",
      "\n",
      "\tEpisode 4270 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4655],\n",
      "        [1.0000, 0.5897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3498],\n",
      "        [1.0000, 0.6046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476592540740967 \tStep Time:  0.008976221084594727 s \tTotal Time:  27.88257074356079 s \n",
      "\n",
      "\n",
      "\tEpisode 4271 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6188],\n",
      "        [1.0000, 0.7051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5367],\n",
      "        [1.0000, 0.6594]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447038292884827 \tStep Time:  0.007977724075317383 s \tTotal Time:  27.89054846763611 s \n",
      "\n",
      "\n",
      "\tEpisode 4272 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6378],\n",
      "        [1.0000, 0.4604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5261],\n",
      "        [1.0000, 0.4445]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.584905624389648 \tStep Time:  0.008981466293334961 s \tTotal Time:  27.899529933929443 s \n",
      "\n",
      "\n",
      "\tEpisode 4273 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1965],\n",
      "        [1.0000, 0.6927]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5592],\n",
      "        [1.0000, 0.3852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.248920977115631 \tStep Time:  0.0069844722747802734 s \tTotal Time:  27.906514406204224 s \n",
      "\n",
      "\n",
      "\tEpisode 4274 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6384],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.4385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.407222747802734 \tStep Time:  0.009976863861083984 s \tTotal Time:  27.916491270065308 s \n",
      "\n",
      "\n",
      "\tEpisode 4275 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4863],\n",
      "        [1.0000, 0.5958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5675],\n",
      "        [1.0000, 0.5888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56992781162262 \tStep Time:  0.008981704711914062 s \tTotal Time:  27.92547297477722 s \n",
      "\n",
      "\n",
      "\tEpisode 4276 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6038],\n",
      "        [1.0000, 0.4153]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4973],\n",
      "        [1.0000, 0.5771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49967622756958 \tStep Time:  0.008970499038696289 s \tTotal Time:  27.934443473815918 s \n",
      "\n",
      "\n",
      "\tEpisode 4277 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.0386],\n",
      "        [1.0000, 0.5805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4386],\n",
      "        [1.0000, 0.2768]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46069210767746 \tStep Time:  0.007982015609741211 s \tTotal Time:  27.94242548942566 s \n",
      "\n",
      "\n",
      "\tEpisode 4278 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3900],\n",
      "        [1.0000, 0.4015]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6755],\n",
      "        [1.0000, 0.3669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.395175457000732 \tStep Time:  0.008972883224487305 s \tTotal Time:  27.951398372650146 s \n",
      "\n",
      "\n",
      "\tEpisode 4279 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3667],\n",
      "        [1.0000, 0.2392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6637],\n",
      "        [1.0000, 0.4272]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.634037613868713 \tStep Time:  0.006983518600463867 s \tTotal Time:  27.95838189125061 s \n",
      "\n",
      "\n",
      "\tEpisode 4280 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4556],\n",
      "        [1.0000, 0.1476]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6107],\n",
      "        [1.0000, 0.3716]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.331460952758789 \tStep Time:  0.00897359848022461 s \tTotal Time:  27.967355489730835 s \n",
      "\n",
      "\n",
      "\tEpisode 4281 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5749],\n",
      "        [1.0000, 0.5498]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5629],\n",
      "        [1.0000, 0.6425]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.591562509536743 \tStep Time:  0.007981300354003906 s \tTotal Time:  27.97533679008484 s \n",
      "\n",
      "\n",
      "\tEpisode 4282 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6209],\n",
      "        [1.0000, 0.3560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6343],\n",
      "        [1.0000, 0.6149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.41112995147705 \tStep Time:  0.00798177719116211 s \tTotal Time:  27.983318567276 s \n",
      "\n",
      "\n",
      "\tEpisode 4283 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4324],\n",
      "        [1.0000, 0.4401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2701],\n",
      "        [1.0000, 0.6271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.382774829864502 \tStep Time:  0.00799250602722168 s \tTotal Time:  27.991311073303223 s \n",
      "\n",
      "\n",
      "\tEpisode 4284 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6936],\n",
      "        [1.0000, 0.5173]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4536],\n",
      "        [1.0000, 0.5436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.647546291351318 \tStep Time:  0.008960962295532227 s \tTotal Time:  28.000272035598755 s \n",
      "\n",
      "\n",
      "\tEpisode 4285 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4598],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5788],\n",
      "        [1.0000, 0.5330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45497989654541 \tStep Time:  0.006981372833251953 s \tTotal Time:  28.007253408432007 s \n",
      "\n",
      "\n",
      "\tEpisode 4286 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4048],\n",
      "        [1.0000, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3603],\n",
      "        [1.0000, 0.6200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.372190475463867 \tStep Time:  0.00897836685180664 s \tTotal Time:  28.016231775283813 s \n",
      "\n",
      "\n",
      "\tEpisode 4287 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4384],\n",
      "        [1.0000, 0.6072]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5361],\n",
      "        [1.0000, 0.6480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.396524786949158 \tStep Time:  0.006981372833251953 s \tTotal Time:  28.023213148117065 s \n",
      "\n",
      "\n",
      "\tEpisode 4288 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2015],\n",
      "        [1.0000, 0.5899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5076],\n",
      "        [1.0000, 0.3658]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544729232788086 \tStep Time:  0.009084463119506836 s \tTotal Time:  28.032297611236572 s \n",
      "\n",
      "\n",
      "\tEpisode 4289 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3113],\n",
      "        [1.0000, 0.2996]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6447],\n",
      "        [1.0000, 0.3154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.431990027427673 \tStep Time:  0.008974790573120117 s \tTotal Time:  28.041272401809692 s \n",
      "\n",
      "\n",
      "\tEpisode 4290 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5980],\n",
      "        [1.0000, 0.2932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6576],\n",
      "        [1.0000, 0.6487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.391145706176758 \tStep Time:  0.008975982666015625 s \tTotal Time:  28.050248384475708 s \n",
      "\n",
      "\n",
      "\tEpisode 4291 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2744],\n",
      "        [1.0000, 0.4984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6384],\n",
      "        [1.0000, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.389415383338928 \tStep Time:  0.012966394424438477 s \tTotal Time:  28.063214778900146 s \n",
      "\n",
      "\n",
      "\tEpisode 4292 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3581],\n",
      "        [1.0000, 0.5487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4045],\n",
      "        [1.0000, 0.7220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.624419331550598 \tStep Time:  0.006980419158935547 s \tTotal Time:  28.070195198059082 s \n",
      "\n",
      "\n",
      "\tEpisode 4293 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7966],\n",
      "        [1.0000, 0.7354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4575],\n",
      "        [1.0000, 0.3085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.237112045288086 \tStep Time:  0.008976459503173828 s \tTotal Time:  28.079171657562256 s \n",
      "\n",
      "\n",
      "\tEpisode 4294 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6462],\n",
      "        [1.0000, 0.5921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1698],\n",
      "        [1.0000, 0.6423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.354362487792969 \tStep Time:  0.006981611251831055 s \tTotal Time:  28.086153268814087 s \n",
      "\n",
      "\n",
      "\tEpisode 4295 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6533],\n",
      "        [1.0000, 0.3033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5985],\n",
      "        [1.0000, 0.4298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535014629364014 \tStep Time:  0.007977962493896484 s \tTotal Time:  28.094131231307983 s \n",
      "\n",
      "\n",
      "\tEpisode 4296 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7338],\n",
      "        [1.0000, 0.8346]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6944],\n",
      "        [1.0000, 0.6982]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550959587097168 \tStep Time:  0.007978677749633789 s \tTotal Time:  28.102109909057617 s \n",
      "\n",
      "\n",
      "\tEpisode 4297 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6562],\n",
      "        [1.0000, 0.2365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4489],\n",
      "        [1.0000, 0.4089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.575613498687744 \tStep Time:  0.0069849491119384766 s \tTotal Time:  28.109094858169556 s \n",
      "\n",
      "\n",
      "\tEpisode 4298 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4413],\n",
      "        [1.0000, 0.6759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3570],\n",
      "        [1.0000, 0.7383]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592765808105469 \tStep Time:  0.00797581672668457 s \tTotal Time:  28.11707067489624 s \n",
      "\n",
      "\n",
      "\tEpisode 4299 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6084],\n",
      "        [1.0000, 0.2243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4992],\n",
      "        [1.0000, 0.3853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.832025051116943 \tStep Time:  0.009104013442993164 s \tTotal Time:  28.126174688339233 s \n",
      "\n",
      "\n",
      "\tEpisode 4300 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6612],\n",
      "        [1.0000, 0.6136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5620],\n",
      "        [1.0000, 0.4101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.722582817077637 \tStep Time:  0.009844541549682617 s \tTotal Time:  28.136019229888916 s \n",
      "\n",
      "\n",
      "\tEpisode 4301 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5770],\n",
      "        [1.0000, 0.5795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3640],\n",
      "        [1.0000, 0.6431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.436471462249756 \tStep Time:  0.007980585098266602 s \tTotal Time:  28.143999814987183 s \n",
      "\n",
      "\n",
      "\tEpisode 4302 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3366],\n",
      "        [1.0000, 0.6111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4890],\n",
      "        [1.0000, 0.6001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483718872070312 \tStep Time:  0.006979465484619141 s \tTotal Time:  28.1509792804718 s \n",
      "\n",
      "\n",
      "\tEpisode 4303 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2190],\n",
      "        [1.0000, 0.5319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5684],\n",
      "        [1.0000, 0.5746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.422734379768372 \tStep Time:  0.007980585098266602 s \tTotal Time:  28.15895986557007 s \n",
      "\n",
      "\n",
      "\tEpisode 4304 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5422],\n",
      "        [1.0000, 0.3889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3637],\n",
      "        [1.0000, 0.4531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.435847520828247 \tStep Time:  0.006980419158935547 s \tTotal Time:  28.16693663597107 s \n",
      "\n",
      "\n",
      "\tEpisode 4305 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2924],\n",
      "        [1.0000, 0.2196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5396],\n",
      "        [1.0000, 0.4899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60656750202179 \tStep Time:  0.007979869842529297 s \tTotal Time:  28.1749165058136 s \n",
      "\n",
      "\n",
      "\tEpisode 4306 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3331],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5969],\n",
      "        [1.0000, 0.6057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483877122402191 \tStep Time:  0.00797581672668457 s \tTotal Time:  28.18389129638672 s \n",
      "\n",
      "\n",
      "\tEpisode 4307 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1673],\n",
      "        [1.0000, 0.4698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6417],\n",
      "        [1.0000, 0.4810]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39514446258545 \tStep Time:  0.00897836685180664 s \tTotal Time:  28.192869663238525 s \n",
      "\n",
      "\n",
      "\tEpisode 4308 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [1.0000, 0.2552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4500],\n",
      "        [1.0000, 0.5959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.537261962890625 \tStep Time:  0.01096963882446289 s \tTotal Time:  28.20383930206299 s \n",
      "\n",
      "\n",
      "\tEpisode 4309 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3378],\n",
      "        [1.0000, 0.2967]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3922],\n",
      "        [1.0000, 0.2763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510594964027405 \tStep Time:  0.008980512619018555 s \tTotal Time:  28.212819814682007 s \n",
      "\n",
      "\n",
      "\tEpisode 4310 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5649],\n",
      "        [1.0000, 0.4805]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7587],\n",
      "        [1.0000, 0.5540]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.62278139591217 \tStep Time:  0.007979154586791992 s \tTotal Time:  28.2207989692688 s \n",
      "\n",
      "\n",
      "\tEpisode 4311 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5508],\n",
      "        [1.0000, 0.4419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5613],\n",
      "        [1.0000, 0.4057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.668784618377686 \tStep Time:  0.010972023010253906 s \tTotal Time:  28.231770992279053 s \n",
      "\n",
      "\n",
      "\tEpisode 4312 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6754],\n",
      "        [1.0000, 0.5254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4741],\n",
      "        [1.0000, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.66668325662613 \tStep Time:  0.011967182159423828 s \tTotal Time:  28.243738174438477 s \n",
      "\n",
      "\n",
      "\tEpisode 4313 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4781],\n",
      "        [1.0000, 0.3642]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4368],\n",
      "        [1.0000, 0.6419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.449498176574707 \tStep Time:  0.012103557586669922 s \tTotal Time:  28.255841732025146 s \n",
      "\n",
      "\n",
      "\tEpisode 4314 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5700],\n",
      "        [1.0000, 0.5781]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6691],\n",
      "        [1.0000, 0.3158]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.394436836242676 \tStep Time:  0.00897526741027832 s \tTotal Time:  28.264816999435425 s \n",
      "\n",
      "\n",
      "\tEpisode 4315 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3449],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.636410653591156 \tStep Time:  0.007980108261108398 s \tTotal Time:  28.272797107696533 s \n",
      "\n",
      "\n",
      "\tEpisode 4316 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6345],\n",
      "        [1.0000, 0.5222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3956],\n",
      "        [1.0000, 0.6984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.357682228088379 \tStep Time:  0.010968446731567383 s \tTotal Time:  28.2837655544281 s \n",
      "\n",
      "\n",
      "\tEpisode 4317 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7430],\n",
      "        [1.0000, 0.3081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5396],\n",
      "        [1.0000, 0.4126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.310526609420776 \tStep Time:  0.009974002838134766 s \tTotal Time:  28.293739557266235 s \n",
      "\n",
      "\n",
      "\tEpisode 4318 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4922],\n",
      "        [1.0000, 0.6402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5071],\n",
      "        [1.0000, 0.5404]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.630025863647461 \tStep Time:  0.007979393005371094 s \tTotal Time:  28.301718950271606 s \n",
      "\n",
      "\n",
      "\tEpisode 4319 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6214],\n",
      "        [1.0000, 0.6200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5176],\n",
      "        [1.0000, 0.4943]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553121566772461 \tStep Time:  0.011966466903686523 s \tTotal Time:  28.313685417175293 s \n",
      "\n",
      "\n",
      "\tEpisode 4320 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4430],\n",
      "        [1.0000, 0.3085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4371],\n",
      "        [1.0000, 0.5771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564732193946838 \tStep Time:  0.007980108261108398 s \tTotal Time:  28.3216655254364 s \n",
      "\n",
      "\n",
      "\tEpisode 4321 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7335],\n",
      "        [1.0000, 0.6245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7264],\n",
      "        [1.0000, 0.4174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.825946807861328 \tStep Time:  0.008985280990600586 s \tTotal Time:  28.330650806427002 s \n",
      "\n",
      "\n",
      "\tEpisode 4322 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6135],\n",
      "        [1.0000, 0.6156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4238],\n",
      "        [1.0000, 0.5253]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43938398361206 \tStep Time:  0.0069773197174072266 s \tTotal Time:  28.33762812614441 s \n",
      "\n",
      "\n",
      "\tEpisode 4323 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6102],\n",
      "        [1.0000, 0.6307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5367],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545119047164917 \tStep Time:  0.00897669792175293 s \tTotal Time:  28.346604824066162 s \n",
      "\n",
      "\n",
      "\tEpisode 4324 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5454],\n",
      "        [1.0000, 0.5883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5651],\n",
      "        [1.0000, 0.3021]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.444538116455078 \tStep Time:  0.008977651596069336 s \tTotal Time:  28.35558247566223 s \n",
      "\n",
      "\n",
      "\tEpisode 4325 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5892],\n",
      "        [1.0000, 0.3153]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5371],\n",
      "        [1.0000, 0.6022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.669799327850342 \tStep Time:  0.00897359848022461 s \tTotal Time:  28.364556074142456 s \n",
      "\n",
      "\n",
      "\tEpisode 4326 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5493],\n",
      "        [1.0000, 0.5679]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5596],\n",
      "        [1.0000, 0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496871948242188 \tStep Time:  0.0069811344146728516 s \tTotal Time:  28.37153720855713 s \n",
      "\n",
      "\n",
      "\tEpisode 4327 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.5384]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3781],\n",
      "        [1.0000, 0.5425]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465454578399658 \tStep Time:  0.00797891616821289 s \tTotal Time:  28.379516124725342 s \n",
      "\n",
      "\n",
      "\tEpisode 4328 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.5438]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5135],\n",
      "        [1.0000, 0.3416]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.42332124710083 \tStep Time:  0.007016897201538086 s \tTotal Time:  28.38653302192688 s \n",
      "\n",
      "\n",
      "\tEpisode 4329 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2374],\n",
      "        [1.0000, 0.5984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5005],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466245174407959 \tStep Time:  0.006947517395019531 s \tTotal Time:  28.3934805393219 s \n",
      "\n",
      "\n",
      "\tEpisode 4330 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5076],\n",
      "        [1.0000, 0.3393]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5399],\n",
      "        [1.0000, 0.5311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44930225610733 \tStep Time:  0.006980180740356445 s \tTotal Time:  28.400460720062256 s \n",
      "\n",
      "\n",
      "\tEpisode 4331 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1605],\n",
      "        [1.0000, 0.5303]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5369],\n",
      "        [1.0000, 0.5170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.748750686645508 \tStep Time:  0.0079803466796875 s \tTotal Time:  28.408441066741943 s \n",
      "\n",
      "\n",
      "\tEpisode 4332 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5220],\n",
      "        [1.0000, 0.5386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5266],\n",
      "        [1.0000, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524208545684814 \tStep Time:  0.006979227066040039 s \tTotal Time:  28.415420293807983 s \n",
      "\n",
      "\n",
      "\tEpisode 4333 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3936],\n",
      "        [1.0000, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5342],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.454567432403564 \tStep Time:  0.005984306335449219 s \tTotal Time:  28.421404600143433 s \n",
      "\n",
      "\n",
      "\tEpisode 4334 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3904],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.5262]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4773850440979 \tStep Time:  0.00798177719116211 s \tTotal Time:  28.430383443832397 s \n",
      "\n",
      "\n",
      "\tEpisode 4335 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.1571],\n",
      "        [1.0000, 0.4443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5316],\n",
      "        [1.0000, 0.5279]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.724168300628662 \tStep Time:  0.005980730056762695 s \tTotal Time:  28.43636417388916 s \n",
      "\n",
      "\n",
      "\tEpisode 4336 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5380],\n",
      "        [1.0000, 0.5374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5228],\n",
      "        [1.0000, 0.5332]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52422285079956 \tStep Time:  0.007979154586791992 s \tTotal Time:  28.444343328475952 s \n",
      "\n",
      "\n",
      "\tEpisode 4337 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5392],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3070],\n",
      "        [1.0000, 0.4228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497127056121826 \tStep Time:  0.007978439331054688 s \tTotal Time:  28.452321767807007 s \n",
      "\n",
      "\n",
      "\tEpisode 4338 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5329],\n",
      "        [1.0000, 0.3764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5350],\n",
      "        [1.0000, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443807303905487 \tStep Time:  0.010973215103149414 s \tTotal Time:  28.463294982910156 s \n",
      "\n",
      "\n",
      "\tEpisode 4339 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4535],\n",
      "        [1.0000, 0.5348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5364],\n",
      "        [1.0000, 0.4682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51696503162384 \tStep Time:  0.010002851486206055 s \tTotal Time:  28.473297834396362 s \n",
      "\n",
      "\n",
      "\tEpisode 4340 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4986],\n",
      "        [1.0000, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5364],\n",
      "        [1.0000, 0.5249]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489942073822021 \tStep Time:  0.008982181549072266 s \tTotal Time:  28.482280015945435 s \n",
      "\n",
      "\n",
      "\tEpisode 4341 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3687],\n",
      "        [1.0000, 0.5246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4250],\n",
      "        [1.0000, 0.4105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592415511608124 \tStep Time:  0.0069751739501953125 s \tTotal Time:  28.48925518989563 s \n",
      "\n",
      "\n",
      "\tEpisode 4342 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2985],\n",
      "        [1.0000, 0.4662]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3084],\n",
      "        [1.0000, 0.3646]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601004600524902 \tStep Time:  0.007978439331054688 s \tTotal Time:  28.497233629226685 s \n",
      "\n",
      "\n",
      "\tEpisode 4343 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2790],\n",
      "        [1.0000, 0.5296]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2585],\n",
      "        [1.0000, 0.5454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.298241138458252 \tStep Time:  0.005987405776977539 s \tTotal Time:  28.503221035003662 s \n",
      "\n",
      "\n",
      "\tEpisode 4344 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5405],\n",
      "        [1.0000, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4014],\n",
      "        [1.0000, 0.3487]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.366503238677979 \tStep Time:  0.00794363021850586 s \tTotal Time:  28.511164665222168 s \n",
      "\n",
      "\n",
      "\tEpisode 4345 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5259],\n",
      "        [1.0000, 0.4235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5756],\n",
      "        [1.0000, 0.5666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.415335655212402 \tStep Time:  0.00698089599609375 s \tTotal Time:  28.51814556121826 s \n",
      "\n",
      "\n",
      "\tEpisode 4346 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4411],\n",
      "        [1.0000, 0.5931]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5738],\n",
      "        [1.0000, 0.3003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.75413990020752 \tStep Time:  0.006982564926147461 s \tTotal Time:  28.52512812614441 s \n",
      "\n",
      "\n",
      "\tEpisode 4347 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5949],\n",
      "        [1.0000, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476893424987793 \tStep Time:  0.006987810134887695 s \tTotal Time:  28.532115936279297 s \n",
      "\n",
      "\n",
      "\tEpisode 4348 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5819],\n",
      "        [1.0000, 0.5980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6311],\n",
      "        [1.0000, 0.5442]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523988246917725 \tStep Time:  0.007978439331054688 s \tTotal Time:  28.54009437561035 s \n",
      "\n",
      "\n",
      "\tEpisode 4349 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3963],\n",
      "        [1.0000, 0.5199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6012],\n",
      "        [1.0000, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539125919342041 \tStep Time:  0.006981611251831055 s \tTotal Time:  28.547075986862183 s \n",
      "\n",
      "\n",
      "\tEpisode 4350 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3666],\n",
      "        [1.0000, 0.6009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3162],\n",
      "        [1.0000, 0.5961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572535514831543 \tStep Time:  0.006981849670410156 s \tTotal Time:  28.554057836532593 s \n",
      "\n",
      "\n",
      "\tEpisode 4351 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5965],\n",
      "        [1.0000, 0.5561]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3849],\n",
      "        [1.0000, 0.5557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.423076152801514 \tStep Time:  0.005993366241455078 s \tTotal Time:  28.560051202774048 s \n",
      "\n",
      "\n",
      "\tEpisode 4352 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5637],\n",
      "        [1.0000, 0.4452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4447],\n",
      "        [1.0000, 0.5173]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496217966079712 \tStep Time:  0.006016254425048828 s \tTotal Time:  28.56705617904663 s \n",
      "\n",
      "\n",
      "\tEpisode 4353 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4280],\n",
      "        [1.0000, 0.6031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5232],\n",
      "        [1.0000, 0.3761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.688750743865967 \tStep Time:  0.007946014404296875 s \tTotal Time:  28.575002193450928 s \n",
      "\n",
      "\n",
      "\tEpisode 4354 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5897],\n",
      "        [1.0000, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6025],\n",
      "        [1.0000, 0.4502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501411437988281 \tStep Time:  0.008992671966552734 s \tTotal Time:  28.58399486541748 s \n",
      "\n",
      "\n",
      "\tEpisode 4355 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4832],\n",
      "        [1.0000, 0.5853]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5941],\n",
      "        [1.0000, 0.4179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50028657913208 \tStep Time:  0.007961511611938477 s \tTotal Time:  28.59195637702942 s \n",
      "\n",
      "\n",
      "\tEpisode 4356 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5234],\n",
      "        [1.0000, 0.5669]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5756],\n",
      "        [1.0000, 0.5901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515082001686096 \tStep Time:  0.007979631423950195 s \tTotal Time:  28.59993600845337 s \n",
      "\n",
      "\n",
      "\tEpisode 4357 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5819],\n",
      "        [1.0000, 0.5836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5885],\n",
      "        [1.0000, 0.5785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.541745662689209 \tStep Time:  0.00698089599609375 s \tTotal Time:  28.606916904449463 s \n",
      "\n",
      "\n",
      "\tEpisode 4358 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5793],\n",
      "        [1.0000, 0.5258]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5806],\n",
      "        [1.0000, 0.5739]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564405858516693 \tStep Time:  0.007978200912475586 s \tTotal Time:  28.61489510536194 s \n",
      "\n",
      "\n",
      "\tEpisode 4359 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4138],\n",
      "        [1.0000, 0.5599]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5477],\n",
      "        [1.0000, 0.5707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58861494064331 \tStep Time:  0.00797891616821289 s \tTotal Time:  28.62287402153015 s \n",
      "\n",
      "\n",
      "\tEpisode 4360 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5390],\n",
      "        [1.0000, 0.5559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5581],\n",
      "        [1.0000, 0.4418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486957311630249 \tStep Time:  0.00897526741027832 s \tTotal Time:  28.63184928894043 s \n",
      "\n",
      "\n",
      "\tEpisode 4361 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.5526]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5339],\n",
      "        [1.0000, 0.5092]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53164291381836 \tStep Time:  0.006981372833251953 s \tTotal Time:  28.63883066177368 s \n",
      "\n",
      "\n",
      "\tEpisode 4362 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4223],\n",
      "        [1.0000, 0.5251]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5206],\n",
      "        [1.0000, 0.4578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534037113189697 \tStep Time:  0.014960050582885742 s \tTotal Time:  28.653790712356567 s \n",
      "\n",
      "\n",
      "\tEpisode 4363 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5276],\n",
      "        [1.0000, 0.5174]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3511],\n",
      "        [1.0000, 0.5295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.610028624534607 \tStep Time:  0.012967109680175781 s \tTotal Time:  28.666757822036743 s \n",
      "\n",
      "\n",
      "\tEpisode 4364 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5394],\n",
      "        [1.0000, 0.5427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5362],\n",
      "        [1.0000, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.560056030750275 \tStep Time:  0.011969327926635742 s \tTotal Time:  28.67872714996338 s \n",
      "\n",
      "\n",
      "\tEpisode 4365 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.5357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5249],\n",
      "        [1.0000, 0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535541534423828 \tStep Time:  0.009972333908081055 s \tTotal Time:  28.68869948387146 s \n",
      "\n",
      "\n",
      "\tEpisode 4366 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3940],\n",
      "        [1.0000, 0.4690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4045],\n",
      "        [1.0000, 0.5216]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44430261850357 \tStep Time:  0.008975505828857422 s \tTotal Time:  28.697674989700317 s \n",
      "\n",
      "\n",
      "\tEpisode 4367 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4789],\n",
      "        [1.0000, 0.5150]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5065],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544128358364105 \tStep Time:  0.006981372833251953 s \tTotal Time:  28.70465636253357 s \n",
      "\n",
      "\n",
      "\tEpisode 4368 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.4071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3745],\n",
      "        [1.0000, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.403808534145355 \tStep Time:  0.008976221084594727 s \tTotal Time:  28.713632583618164 s \n",
      "\n",
      "\n",
      "\tEpisode 4369 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4950],\n",
      "        [1.0000, 0.5241]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5153],\n",
      "        [1.0000, 0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52375841140747 \tStep Time:  0.007978439331054688 s \tTotal Time:  28.72161102294922 s \n",
      "\n",
      "\n",
      "\tEpisode 4370 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4622],\n",
      "        [1.0000, 0.5276]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5171],\n",
      "        [1.0000, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555113434791565 \tStep Time:  0.008976221084594727 s \tTotal Time:  28.730587244033813 s \n",
      "\n",
      "\n",
      "\tEpisode 4371 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.4545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5315],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50233256816864 \tStep Time:  0.007978439331054688 s \tTotal Time:  28.738565683364868 s \n",
      "\n",
      "\n",
      "\tEpisode 4372 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4108],\n",
      "        [1.0000, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5298],\n",
      "        [1.0000, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.58281296491623 \tStep Time:  0.00797891616821289 s \tTotal Time:  28.74654459953308 s \n",
      "\n",
      "\n",
      "\tEpisode 4373 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5035],\n",
      "        [1.0000, 0.4053]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5183],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.460611343383789 \tStep Time:  0.007978439331054688 s \tTotal Time:  28.754523038864136 s \n",
      "\n",
      "\n",
      "\tEpisode 4374 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4002],\n",
      "        [1.0000, 0.4053]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.5326]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.488196313381195 \tStep Time:  0.007980823516845703 s \tTotal Time:  28.76250386238098 s \n",
      "\n",
      "\n",
      "\tEpisode 4375 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4948],\n",
      "        [1.0000, 0.5141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5268],\n",
      "        [1.0000, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51332002878189 \tStep Time:  0.007976770401000977 s \tTotal Time:  28.770480632781982 s \n",
      "\n",
      "\n",
      "\tEpisode 4376 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5404],\n",
      "        [1.0000, 0.5242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5123],\n",
      "        [1.0000, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51305866241455 \tStep Time:  0.009973287582397461 s \tTotal Time:  28.78045392036438 s \n",
      "\n",
      "\n",
      "\tEpisode 4377 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4499],\n",
      "        [1.0000, 0.5456]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4802],\n",
      "        [1.0000, 0.4177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497089684009552 \tStep Time:  0.008974075317382812 s \tTotal Time:  28.789427995681763 s \n",
      "\n",
      "\n",
      "\tEpisode 4378 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4104],\n",
      "        [1.0000, 0.4754]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5551],\n",
      "        [1.0000, 0.5190]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523146092891693 \tStep Time:  0.008986473083496094 s \tTotal Time:  28.79841446876526 s \n",
      "\n",
      "\n",
      "\tEpisode 4379 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5316],\n",
      "        [1.0000, 0.5428]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4146],\n",
      "        [1.0000, 0.5796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558041095733643 \tStep Time:  0.007975339889526367 s \tTotal Time:  28.806389808654785 s \n",
      "\n",
      "\n",
      "\tEpisode 4380 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5184],\n",
      "        [1.0000, 0.5593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5532],\n",
      "        [1.0000, 0.3974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455722332000732 \tStep Time:  0.008974790573120117 s \tTotal Time:  28.815364599227905 s \n",
      "\n",
      "\n",
      "\tEpisode 4381 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5958],\n",
      "        [1.0000, 0.4153]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5266],\n",
      "        [1.0000, 0.5246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.601863861083984 \tStep Time:  0.007979154586791992 s \tTotal Time:  28.823343753814697 s \n",
      "\n",
      "\n",
      "\tEpisode 4382 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5251],\n",
      "        [1.0000, 0.5232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5196],\n",
      "        [1.0000, 0.4000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571669578552246 \tStep Time:  0.009007453918457031 s \tTotal Time:  28.832351207733154 s \n",
      "\n",
      "\n",
      "\tEpisode 4383 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.4713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3973],\n",
      "        [1.0000, 0.5742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.617412149906158 \tStep Time:  0.008975028991699219 s \tTotal Time:  28.841326236724854 s \n",
      "\n",
      "\n",
      "\tEpisode 4384 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4177],\n",
      "        [1.0000, 0.5401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5592],\n",
      "        [1.0000, 0.4246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500759601593018 \tStep Time:  0.007970809936523438 s \tTotal Time:  28.849297046661377 s \n",
      "\n",
      "\n",
      "\tEpisode 4385 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3937],\n",
      "        [1.0000, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4591],\n",
      "        [1.0000, 0.5518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.634640276432037 \tStep Time:  0.008975505828857422 s \tTotal Time:  28.858272552490234 s \n",
      "\n",
      "\n",
      "\tEpisode 4386 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5516],\n",
      "        [1.0000, 0.5017]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5461],\n",
      "        [1.0000, 0.5167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496836185455322 \tStep Time:  0.01296544075012207 s \tTotal Time:  28.871237993240356 s \n",
      "\n",
      "\n",
      "\tEpisode 4387 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5351],\n",
      "        [1.0000, 0.5344]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5655],\n",
      "        [1.0000, 0.5610]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538954734802246 \tStep Time:  0.010971307754516602 s \tTotal Time:  28.882209300994873 s \n",
      "\n",
      "\n",
      "\tEpisode 4388 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5441],\n",
      "        [1.0000, 0.5403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5384],\n",
      "        [1.0000, 0.5313]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505605697631836 \tStep Time:  0.00797891616821289 s \tTotal Time:  28.890188217163086 s \n",
      "\n",
      "\n",
      "\tEpisode 4389 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5409],\n",
      "        [1.0000, 0.5244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5320],\n",
      "        [1.0000, 0.5268]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509449005126953 \tStep Time:  0.010970354080200195 s \tTotal Time:  28.901158571243286 s \n",
      "\n",
      "\n",
      "\tEpisode 4390 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5086],\n",
      "        [1.0000, 0.4657]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4752],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51005071401596 \tStep Time:  0.00798797607421875 s \tTotal Time:  28.909146547317505 s \n",
      "\n",
      "\n",
      "\tEpisode 4391 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5019],\n",
      "        [1.0000, 0.5377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4722],\n",
      "        [1.0000, 0.4999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471025943756104 \tStep Time:  0.007983207702636719 s \tTotal Time:  28.91712975502014 s \n",
      "\n",
      "\n",
      "\tEpisode 4392 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4551],\n",
      "        [1.0000, 0.5447]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4848],\n",
      "        [1.0000, 0.5318]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483824729919434 \tStep Time:  0.0069811344146728516 s \tTotal Time:  28.924110889434814 s \n",
      "\n",
      "\n",
      "\tEpisode 4393 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6800],\n",
      "        [1.0000, 0.4542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4948],\n",
      "        [1.0000, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574189186096191 \tStep Time:  0.008989572525024414 s \tTotal Time:  28.93310046195984 s \n",
      "\n",
      "\n",
      "\tEpisode 4394 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5436],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3913],\n",
      "        [1.0000, 0.4520]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512041628360748 \tStep Time:  0.007976531982421875 s \tTotal Time:  28.94107699394226 s \n",
      "\n",
      "\n",
      "\tEpisode 4395 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4859],\n",
      "        [1.0000, 0.4225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5014],\n",
      "        [1.0000, 0.5265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550236105918884 \tStep Time:  0.008985519409179688 s \tTotal Time:  28.95006251335144 s \n",
      "\n",
      "\n",
      "\tEpisode 4396 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5687],\n",
      "        [1.0000, 0.5389]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.5793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559549331665039 \tStep Time:  0.010004997253417969 s \tTotal Time:  28.96006751060486 s \n",
      "\n",
      "\n",
      "\tEpisode 4397 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5346],\n",
      "        [1.0000, 0.5246]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5394],\n",
      "        [1.0000, 0.4760]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472919464111328 \tStep Time:  0.009940147399902344 s \tTotal Time:  28.97000765800476 s \n",
      "\n",
      "\n",
      "\tEpisode 4398 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2813],\n",
      "        [1.0000, 0.4081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3875],\n",
      "        [1.0000, 0.5368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.653783798217773 \tStep Time:  0.007978439331054688 s \tTotal Time:  28.977986097335815 s \n",
      "\n",
      "\n",
      "\tEpisode 4399 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4463],\n",
      "        [1.0000, 0.4152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5606],\n",
      "        [1.0000, 0.4992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494288444519043 \tStep Time:  0.008975744247436523 s \tTotal Time:  28.986961841583252 s \n",
      "\n",
      "\n",
      "\tEpisode 4400 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5373],\n",
      "        [1.0000, 0.4481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5359],\n",
      "        [1.0000, 0.6221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510284662246704 \tStep Time:  0.008975982666015625 s \tTotal Time:  28.995937824249268 s \n",
      "\n",
      "\n",
      "\tEpisode 4401 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5338],\n",
      "        [1.0000, 0.5345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4294],\n",
      "        [1.0000, 0.4497]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516828417778015 \tStep Time:  0.00897526741027832 s \tTotal Time:  29.004913091659546 s \n",
      "\n",
      "\n",
      "\tEpisode 4402 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5299],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4321],\n",
      "        [1.0000, 0.5233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.538670063018799 \tStep Time:  0.00897669792175293 s \tTotal Time:  29.0138897895813 s \n",
      "\n",
      "\n",
      "\tEpisode 4403 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5061],\n",
      "        [1.0000, 0.5230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5240],\n",
      "        [1.0000, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50339126586914 \tStep Time:  0.009972333908081055 s \tTotal Time:  29.02386212348938 s \n",
      "\n",
      "\n",
      "\tEpisode 4404 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5202],\n",
      "        [1.0000, 0.4370]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5112],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47397780418396 \tStep Time:  0.0109710693359375 s \tTotal Time:  29.034833192825317 s \n",
      "\n",
      "\n",
      "\tEpisode 4405 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4948],\n",
      "        [1.0000, 0.5238]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.4482]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534112453460693 \tStep Time:  0.010970592498779297 s \tTotal Time:  29.045803785324097 s \n",
      "\n",
      "\n",
      "\tEpisode 4406 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5122],\n",
      "        [1.0000, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5057],\n",
      "        [1.0000, 0.5151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497161746025085 \tStep Time:  0.007978677749633789 s \tTotal Time:  29.05378246307373 s \n",
      "\n",
      "\n",
      "\tEpisode 4407 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5268],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515364646911621 \tStep Time:  0.009973287582397461 s \tTotal Time:  29.063755750656128 s \n",
      "\n",
      "\n",
      "\tEpisode 4408 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5068],\n",
      "        [1.0000, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5136],\n",
      "        [1.0000, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521534442901611 \tStep Time:  0.00897669792175293 s \tTotal Time:  29.07273244857788 s \n",
      "\n",
      "\n",
      "\tEpisode 4409 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5194],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5075],\n",
      "        [1.0000, 0.4310]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474593162536621 \tStep Time:  0.008977651596069336 s \tTotal Time:  29.08171010017395 s \n",
      "\n",
      "\n",
      "\tEpisode 4410 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [1.0000, 0.4357]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471717357635498 \tStep Time:  0.007010698318481445 s \tTotal Time:  29.08872079849243 s \n",
      "\n",
      "\n",
      "\tEpisode 4411 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5110],\n",
      "        [1.0000, 0.3160]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5007],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.608591556549072 \tStep Time:  0.00794363021850586 s \tTotal Time:  29.097665548324585 s \n",
      "\n",
      "\n",
      "\tEpisode 4412 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4986],\n",
      "        [1.0000, 0.4721]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5207],\n",
      "        [1.0000, 0.4286]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45135873556137 \tStep Time:  0.006981611251831055 s \tTotal Time:  29.104647159576416 s \n",
      "\n",
      "\n",
      "\tEpisode 4413 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5002],\n",
      "        [1.0000, 0.5161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516066551208496 \tStep Time:  0.007978439331054688 s \tTotal Time:  29.11262559890747 s \n",
      "\n",
      "\n",
      "\tEpisode 4414 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4923],\n",
      "        [1.0000, 0.4794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4861],\n",
      "        [1.0000, 0.5214]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526869893074036 \tStep Time:  0.007979393005371094 s \tTotal Time:  29.120604991912842 s \n",
      "\n",
      "\n",
      "\tEpisode 4415 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4483],\n",
      "        [1.0000, 0.4742]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5302],\n",
      "        [1.0000, 0.5299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572860717773438 \tStep Time:  0.010970592498779297 s \tTotal Time:  29.13157558441162 s \n",
      "\n",
      "\n",
      "\tEpisode 4416 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5198],\n",
      "        [1.0000, 0.5250]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4688],\n",
      "        [1.0000, 0.5305]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531471312046051 \tStep Time:  0.007982254028320312 s \tTotal Time:  29.13955783843994 s \n",
      "\n",
      "\n",
      "\tEpisode 4417 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4954],\n",
      "        [1.0000, 0.4493]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5241],\n",
      "        [1.0000, 0.4533]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492231667041779 \tStep Time:  0.009973526000976562 s \tTotal Time:  29.150529384613037 s \n",
      "\n",
      "\n",
      "\tEpisode 4418 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5180],\n",
      "        [1.0000, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5071],\n",
      "        [1.0000, 0.4694]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477884292602539 \tStep Time:  0.008977413177490234 s \tTotal Time:  29.159506797790527 s \n",
      "\n",
      "\n",
      "\tEpisode 4419 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5226],\n",
      "        [1.0000, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5342],\n",
      "        [1.0000, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50623893737793 \tStep Time:  0.007977724075317383 s \tTotal Time:  29.167484521865845 s \n",
      "\n",
      "\n",
      "\tEpisode 4420 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5089],\n",
      "        [1.0000, 0.5368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5250],\n",
      "        [1.0000, 0.5312]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513722896575928 \tStep Time:  0.008974552154541016 s \tTotal Time:  29.176459074020386 s \n",
      "\n",
      "\n",
      "\tEpisode 4421 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [1.0000, 0.5271]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5222],\n",
      "        [1.0000, 0.5327]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485794067382812 \tStep Time:  0.010972023010253906 s \tTotal Time:  29.18743109703064 s \n",
      "\n",
      "\n",
      "\tEpisode 4422 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.4845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5426],\n",
      "        [1.0000, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534110069274902 \tStep Time:  0.01096963882446289 s \tTotal Time:  29.198400735855103 s \n",
      "\n",
      "\n",
      "\tEpisode 4423 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5420],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5308],\n",
      "        [1.0000, 0.4695]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476520538330078 \tStep Time:  0.00897669792175293 s \tTotal Time:  29.207377433776855 s \n",
      "\n",
      "\n",
      "\tEpisode 4424 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5437],\n",
      "        [1.0000, 0.5584]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5266],\n",
      "        [1.0000, 0.5280]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497635841369629 \tStep Time:  0.00797724723815918 s \tTotal Time:  29.215354681015015 s \n",
      "\n",
      "\n",
      "\tEpisode 4425 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.5314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5486],\n",
      "        [1.0000, 0.5330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500234603881836 \tStep Time:  0.008974075317382812 s \tTotal Time:  29.225327968597412 s \n",
      "\n",
      "\n",
      "\tEpisode 4426 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4914],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5258],\n",
      "        [1.0000, 0.4924]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507208287715912 \tStep Time:  0.009974956512451172 s \tTotal Time:  29.235302925109863 s \n",
      "\n",
      "\n",
      "\tEpisode 4427 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4996],\n",
      "        [1.0000, 0.4380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545730113983154 \tStep Time:  0.007979393005371094 s \tTotal Time:  29.243282318115234 s \n",
      "\n",
      "\n",
      "\tEpisode 4428 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5927],\n",
      "        [1.0000, 0.4836]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4725],\n",
      "        [1.0000, 0.4636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.43606948852539 \tStep Time:  0.00899052619934082 s \tTotal Time:  29.252272844314575 s \n",
      "\n",
      "\n",
      "\tEpisode 4429 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4775],\n",
      "        [1.0000, 0.4744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6038],\n",
      "        [1.0000, 0.5459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54000586271286 \tStep Time:  0.009960651397705078 s \tTotal Time:  29.26223349571228 s \n",
      "\n",
      "\n",
      "\tEpisode 4430 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4469],\n",
      "        [1.0000, 0.4322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5822],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481919765472412 \tStep Time:  0.025926828384399414 s \tTotal Time:  29.28816032409668 s \n",
      "\n",
      "\n",
      "\tEpisode 4431 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5332],\n",
      "        [1.0000, 0.3944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5386],\n",
      "        [1.0000, 0.4513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.402363359928131 \tStep Time:  0.013963699340820312 s \tTotal Time:  29.3021240234375 s \n",
      "\n",
      "\n",
      "\tEpisode 4432 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4805],\n",
      "        [1.0000, 0.6197]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.5423]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523109436035156 \tStep Time:  0.012964725494384766 s \tTotal Time:  29.315088748931885 s \n",
      "\n",
      "\n",
      "\tEpisode 4433 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6372],\n",
      "        [1.0000, 0.4620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5332],\n",
      "        [1.0000, 0.4414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45500659942627 \tStep Time:  0.008975505828857422 s \tTotal Time:  29.324064254760742 s \n",
      "\n",
      "\n",
      "\tEpisode 4434 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.5812]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5275],\n",
      "        [1.0000, 0.5534]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506320536136627 \tStep Time:  0.009982824325561523 s \tTotal Time:  29.334047079086304 s \n",
      "\n",
      "\n",
      "\tEpisode 4435 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5016],\n",
      "        [1.0000, 0.5558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5745],\n",
      "        [1.0000, 0.4621]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481492400169373 \tStep Time:  0.00897073745727539 s \tTotal Time:  29.34301781654358 s \n",
      "\n",
      "\n",
      "\tEpisode 4436 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5421],\n",
      "        [1.0000, 0.5528]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6581],\n",
      "        [1.0000, 0.4041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.394294738769531 \tStep Time:  0.008977890014648438 s \tTotal Time:  29.351995706558228 s \n",
      "\n",
      "\n",
      "\tEpisode 4437 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4370],\n",
      "        [1.0000, 0.5453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4275],\n",
      "        [1.0000, 0.5274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507924437522888 \tStep Time:  0.010971307754516602 s \tTotal Time:  29.362967014312744 s \n",
      "\n",
      "\n",
      "\tEpisode 4438 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4613],\n",
      "        [1.0000, 0.5452]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2240],\n",
      "        [1.0000, 0.3146]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.793944835662842 \tStep Time:  0.008974313735961914 s \tTotal Time:  29.371941328048706 s \n",
      "\n",
      "\n",
      "\tEpisode 4439 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3871],\n",
      "        [1.0000, 0.4056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.2459],\n",
      "        [1.0000, 0.5352]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.705573081970215 \tStep Time:  0.00997471809387207 s \tTotal Time:  29.381916046142578 s \n",
      "\n",
      "\n",
      "\tEpisode 4440 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.4682]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4003],\n",
      "        [1.0000, 0.5135]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.447153985500336 \tStep Time:  0.007976531982421875 s \tTotal Time:  29.389892578125 s \n",
      "\n",
      "\n",
      "\tEpisode 4441 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4574],\n",
      "        [1.0000, 0.6252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3744],\n",
      "        [1.0000, 0.5258]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535683870315552 \tStep Time:  0.009970903396606445 s \tTotal Time:  29.400861501693726 s \n",
      "\n",
      "\n",
      "\tEpisode 4442 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4842],\n",
      "        [1.0000, 0.4335]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.4506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56260335445404 \tStep Time:  0.01197052001953125 s \tTotal Time:  29.412832021713257 s \n",
      "\n",
      "\n",
      "\tEpisode 4443 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5278],\n",
      "        [1.0000, 0.5207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5763],\n",
      "        [1.0000, 0.4107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55030632019043 \tStep Time:  0.012965202331542969 s \tTotal Time:  29.4257972240448 s \n",
      "\n",
      "\n",
      "\tEpisode 4444 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4507],\n",
      "        [1.0000, 0.4567]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5854],\n",
      "        [1.0000, 0.3078]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.393290996551514 \tStep Time:  0.00997161865234375 s \tTotal Time:  29.435768842697144 s \n",
      "\n",
      "\n",
      "\tEpisode 4445 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4239],\n",
      "        [1.0000, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4941],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496816158294678 \tStep Time:  0.008975505828857422 s \tTotal Time:  29.44574284553528 s \n",
      "\n",
      "\n",
      "\tEpisode 4446 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4622],\n",
      "        [1.0000, 0.3081]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5678],\n",
      "        [1.0000, 0.5437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.365552425384521 \tStep Time:  0.009972572326660156 s \tTotal Time:  29.45571541786194 s \n",
      "\n",
      "\n",
      "\tEpisode 4447 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3227],\n",
      "        [1.0000, 0.3245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4585],\n",
      "        [1.0000, 0.4560]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54384994506836 \tStep Time:  0.009974002838134766 s \tTotal Time:  29.466686010360718 s \n",
      "\n",
      "\n",
      "\tEpisode 4448 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3907],\n",
      "        [1.0000, 0.5259]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3754],\n",
      "        [1.0000, 0.5872]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.700730443000793 \tStep Time:  0.008975744247436523 s \tTotal Time:  29.475661754608154 s \n",
      "\n",
      "\n",
      "\tEpisode 4449 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5198],\n",
      "        [1.0000, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6313],\n",
      "        [1.0000, 0.5811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495929777622223 \tStep Time:  0.012965202331542969 s \tTotal Time:  29.488626956939697 s \n",
      "\n",
      "\n",
      "\tEpisode 4450 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4612],\n",
      "        [1.0000, 0.5433]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5200],\n",
      "        [1.0000, 0.5759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446483612060547 \tStep Time:  0.01197052001953125 s \tTotal Time:  29.50059747695923 s \n",
      "\n",
      "\n",
      "\tEpisode 4451 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4509],\n",
      "        [1.0000, 0.5466]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3550],\n",
      "        [1.0000, 0.5543]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.376506805419922 \tStep Time:  0.011967182159423828 s \tTotal Time:  29.512564659118652 s \n",
      "\n",
      "\n",
      "\tEpisode 4452 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4701],\n",
      "        [1.0000, 0.6835]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4822],\n",
      "        [1.0000, 0.3747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.386879920959473 \tStep Time:  0.008974552154541016 s \tTotal Time:  29.521539211273193 s \n",
      "\n",
      "\n",
      "\tEpisode 4453 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7966],\n",
      "        [1.0000, 0.5790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3276],\n",
      "        [1.0000, 0.3974]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.653528690338135 \tStep Time:  0.009972572326660156 s \tTotal Time:  29.531511783599854 s \n",
      "\n",
      "\n",
      "\tEpisode 4454 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4773],\n",
      "        [1.0000, 0.5941]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3432],\n",
      "        [1.0000, 0.3919]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569438457489014 \tStep Time:  0.008975982666015625 s \tTotal Time:  29.54048776626587 s \n",
      "\n",
      "\n",
      "\tEpisode 4455 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.5622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5480],\n",
      "        [1.0000, 0.7141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508104145526886 \tStep Time:  0.010971546173095703 s \tTotal Time:  29.551459312438965 s \n",
      "\n",
      "\n",
      "\tEpisode 4456 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5343],\n",
      "        [1.0000, 0.3392]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5211],\n",
      "        [1.0000, 0.5372]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446037292480469 \tStep Time:  0.00897669792175293 s \tTotal Time:  29.560436010360718 s \n",
      "\n",
      "\n",
      "\tEpisode 4457 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5702],\n",
      "        [1.0000, 0.4352]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4283],\n",
      "        [1.0000, 0.5652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.386366367340088 \tStep Time:  0.0069811344146728516 s \tTotal Time:  29.56741714477539 s \n",
      "\n",
      "\n",
      "\tEpisode 4458 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3558],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4465],\n",
      "        [1.0000, 0.4793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.619103908538818 \tStep Time:  0.006982088088989258 s \tTotal Time:  29.57439923286438 s \n",
      "\n",
      "\n",
      "\tEpisode 4459 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5999],\n",
      "        [1.0000, 0.3263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4378],\n",
      "        [1.0000, 0.5604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.613477170467377 \tStep Time:  0.008974552154541016 s \tTotal Time:  29.58337378501892 s \n",
      "\n",
      "\n",
      "\tEpisode 4460 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3512],\n",
      "        [1.0000, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4895],\n",
      "        [1.0000, 0.4350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.636478126049042 \tStep Time:  0.006981372833251953 s \tTotal Time:  29.590355157852173 s \n",
      "\n",
      "\n",
      "\tEpisode 4461 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4742],\n",
      "        [1.0000, 0.3756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6138],\n",
      "        [1.0000, 0.5676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45967173576355 \tStep Time:  0.007979154586791992 s \tTotal Time:  29.598334312438965 s \n",
      "\n",
      "\n",
      "\tEpisode 4462 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4055],\n",
      "        [1.0000, 0.6685]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5629],\n",
      "        [1.0000, 0.4446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.574203491210938 \tStep Time:  0.007979154586791992 s \tTotal Time:  29.606313467025757 s \n",
      "\n",
      "\n",
      "\tEpisode 4463 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4444],\n",
      "        [1.0000, 0.5656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.4368]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.439749240875244 \tStep Time:  0.009972810745239258 s \tTotal Time:  29.616286277770996 s \n",
      "\n",
      "\n",
      "\tEpisode 4464 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4060],\n",
      "        [1.0000, 0.5415]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5787],\n",
      "        [1.0000, 0.5362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.604302406311035 \tStep Time:  0.007978439331054688 s \tTotal Time:  29.62426471710205 s \n",
      "\n",
      "\n",
      "\tEpisode 4465 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6660],\n",
      "        [1.0000, 0.5794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4346],\n",
      "        [1.0000, 0.5591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.40494966506958 \tStep Time:  0.007978677749633789 s \tTotal Time:  29.632243394851685 s \n",
      "\n",
      "\n",
      "\tEpisode 4466 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5039],\n",
      "        [1.0000, 0.4118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5676],\n",
      "        [1.0000, 0.4123]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.555860042572021 \tStep Time:  0.006987333297729492 s \tTotal Time:  29.639230728149414 s \n",
      "\n",
      "\n",
      "\tEpisode 4467 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5746],\n",
      "        [1.0000, 0.5163]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5925],\n",
      "        [1.0000, 0.4398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490131855010986 \tStep Time:  0.007973432540893555 s \tTotal Time:  29.647204160690308 s \n",
      "\n",
      "\n",
      "\tEpisode 4468 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5404],\n",
      "        [1.0000, 0.5297]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5316],\n",
      "        [1.0000, 0.5228]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520602881908417 \tStep Time:  0.007977962493896484 s \tTotal Time:  29.655182123184204 s \n",
      "\n",
      "\n",
      "\tEpisode 4469 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5828],\n",
      "        [1.0000, 0.5590]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5308],\n",
      "        [1.0000, 0.4622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496046543121338 \tStep Time:  0.007978677749633789 s \tTotal Time:  29.663160800933838 s \n",
      "\n",
      "\n",
      "\tEpisode 4470 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4878],\n",
      "        [1.0000, 0.5524]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5284],\n",
      "        [1.0000, 0.5564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.559410572052002 \tStep Time:  0.007978439331054688 s \tTotal Time:  29.671139240264893 s \n",
      "\n",
      "\n",
      "\tEpisode 4471 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5236],\n",
      "        [1.0000, 0.5220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4396],\n",
      "        [1.0000, 0.5706]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528786182403564 \tStep Time:  0.013963699340820312 s \tTotal Time:  29.685102939605713 s \n",
      "\n",
      "\n",
      "\tEpisode 4472 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.5908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4708],\n",
      "        [1.0000, 0.5921]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.416267216205597 \tStep Time:  0.006979703903198242 s \tTotal Time:  29.69208264350891 s \n",
      "\n",
      "\n",
      "\tEpisode 4473 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5274],\n",
      "        [1.0000, 0.5414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4348],\n",
      "        [1.0000, 0.5341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.567560195922852 \tStep Time:  0.0069811344146728516 s \tTotal Time:  29.699063777923584 s \n",
      "\n",
      "\n",
      "\tEpisode 4474 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3767],\n",
      "        [1.0000, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4890],\n",
      "        [1.0000, 0.4949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452447056770325 \tStep Time:  0.006981372833251953 s \tTotal Time:  29.706045150756836 s \n",
      "\n",
      "\n",
      "\tEpisode 4475 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5274],\n",
      "        [1.0000, 0.5444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5293],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511810779571533 \tStep Time:  0.005984067916870117 s \tTotal Time:  29.712029218673706 s \n",
      "\n",
      "\n",
      "\tEpisode 4476 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4454],\n",
      "        [1.0000, 0.5270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5144],\n",
      "        [1.0000, 0.4555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495836734771729 \tStep Time:  0.005984067916870117 s \tTotal Time:  29.719010591506958 s \n",
      "\n",
      "\n",
      "\tEpisode 4477 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4645],\n",
      "        [1.0000, 0.5273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5534],\n",
      "        [1.0000, 0.4209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518568515777588 \tStep Time:  0.005984306335449219 s \tTotal Time:  29.724994897842407 s \n",
      "\n",
      "\n",
      "\tEpisode 4478 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4677],\n",
      "        [1.0000, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4772],\n",
      "        [1.0000, 0.5191]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522505283355713 \tStep Time:  0.007979154586791992 s \tTotal Time:  29.7329740524292 s \n",
      "\n",
      "\n",
      "\tEpisode 4479 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4060],\n",
      "        [1.0000, 0.4252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5125],\n",
      "        [1.0000, 0.5811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.470192015171051 \tStep Time:  0.007978439331054688 s \tTotal Time:  29.740952491760254 s \n",
      "\n",
      "\n",
      "\tEpisode 4480 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5293],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4350],\n",
      "        [1.0000, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547191143035889 \tStep Time:  0.00797891616821289 s \tTotal Time:  29.748931407928467 s \n",
      "\n",
      "\n",
      "\tEpisode 4481 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.4992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5349],\n",
      "        [1.0000, 0.5435]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499769806861877 \tStep Time:  0.007978677749633789 s \tTotal Time:  29.7569100856781 s \n",
      "\n",
      "\n",
      "\tEpisode 4482 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5159],\n",
      "        [1.0000, 0.4875]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4943],\n",
      "        [1.0000, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477048873901367 \tStep Time:  0.00897669792175293 s \tTotal Time:  29.765886783599854 s \n",
      "\n",
      "\n",
      "\tEpisode 4483 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4848],\n",
      "        [1.0000, 0.5640]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5365],\n",
      "        [1.0000, 0.4961]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565862655639648 \tStep Time:  0.007978439331054688 s \tTotal Time:  29.773865222930908 s \n",
      "\n",
      "\n",
      "\tEpisode 4484 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.4891]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.4295]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544265747070312 \tStep Time:  0.007978200912475586 s \tTotal Time:  29.781843423843384 s \n",
      "\n",
      "\n",
      "\tEpisode 4485 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5098],\n",
      "        [1.0000, 0.4315]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4894],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48957109451294 \tStep Time:  0.007978439331054688 s \tTotal Time:  29.78982186317444 s \n",
      "\n",
      "\n",
      "\tEpisode 4486 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5548],\n",
      "        [1.0000, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477900981903076 \tStep Time:  0.009974002838134766 s \tTotal Time:  29.799795866012573 s \n",
      "\n",
      "\n",
      "\tEpisode 4487 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4996],\n",
      "        [1.0000, 0.5265]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5254],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510106086730957 \tStep Time:  0.006980419158935547 s \tTotal Time:  29.80677628517151 s \n",
      "\n",
      "\n",
      "\tEpisode 4488 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4872],\n",
      "        [1.0000, 0.5068]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5616],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467087745666504 \tStep Time:  0.007979154586791992 s \tTotal Time:  29.815752744674683 s \n",
      "\n",
      "\n",
      "\tEpisode 4489 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.4652]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4889],\n",
      "        [1.0000, 0.4641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498205661773682 \tStep Time:  0.00698089599609375 s \tTotal Time:  29.822733640670776 s \n",
      "\n",
      "\n",
      "\tEpisode 4490 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.4453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4378],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510792255401611 \tStep Time:  0.009974241256713867 s \tTotal Time:  29.83270788192749 s \n",
      "\n",
      "\n",
      "\tEpisode 4491 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4981],\n",
      "        [1.0000, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5110],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503554463386536 \tStep Time:  0.006979703903198242 s \tTotal Time:  29.83968758583069 s \n",
      "\n",
      "\n",
      "\tEpisode 4492 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4717],\n",
      "        [1.0000, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4385],\n",
      "        [1.0000, 0.4969]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524262428283691 \tStep Time:  0.010971307754516602 s \tTotal Time:  29.850658893585205 s \n",
      "\n",
      "\n",
      "\tEpisode 4493 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5028],\n",
      "        [1.0000, 0.4960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5127],\n",
      "        [1.0000, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498253583908081 \tStep Time:  0.009975910186767578 s \tTotal Time:  29.860634803771973 s \n",
      "\n",
      "\n",
      "\tEpisode 4494 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5136],\n",
      "        [1.0000, 0.4780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5117],\n",
      "        [1.0000, 0.5165]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522199153900146 \tStep Time:  0.013960599899291992 s \tTotal Time:  29.874595403671265 s \n",
      "\n",
      "\n",
      "\tEpisode 4495 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5218]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.523615300655365 \tStep Time:  0.012964725494384766 s \tTotal Time:  29.88756012916565 s \n",
      "\n",
      "\n",
      "\tEpisode 4496 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5087],\n",
      "        [1.0000, 0.4840]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489927291870117 \tStep Time:  0.010970830917358398 s \tTotal Time:  29.898530960083008 s \n",
      "\n",
      "\n",
      "\tEpisode 4497 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.5691]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.5203]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484340190887451 \tStep Time:  0.008976221084594727 s \tTotal Time:  29.907507181167603 s \n",
      "\n",
      "\n",
      "\tEpisode 4498 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.5215]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5121],\n",
      "        [1.0000, 0.5114]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495368599891663 \tStep Time:  0.00897669792175293 s \tTotal Time:  29.916483879089355 s \n",
      "\n",
      "\n",
      "\tEpisode 4499 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5029],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [1.0000, 0.5087]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496064186096191 \tStep Time:  0.007977008819580078 s \tTotal Time:  29.924460887908936 s \n",
      "\n",
      "\n",
      "\tEpisode 4500 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5304],\n",
      "        [1.0000, 0.5686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5018],\n",
      "        [1.0000, 0.5141]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462498188018799 \tStep Time:  0.008976459503173828 s \tTotal Time:  29.93343734741211 s \n",
      "\n",
      "\n",
      "\tEpisode 4501 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5036],\n",
      "        [1.0000, 0.4930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.4947]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505685329437256 \tStep Time:  0.00797891616821289 s \tTotal Time:  29.941416263580322 s \n",
      "\n",
      "\n",
      "\tEpisode 4502 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4917],\n",
      "        [1.0000, 0.4920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5303],\n",
      "        [1.0000, 0.5234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504831314086914 \tStep Time:  0.007119655609130859 s \tTotal Time:  29.948535919189453 s \n",
      "\n",
      "\n",
      "\tEpisode 4503 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5672],\n",
      "        [1.0000, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5498],\n",
      "        [1.0000, 0.4828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442095756530762 \tStep Time:  0.0139617919921875 s \tTotal Time:  29.96249771118164 s \n",
      "\n",
      "\n",
      "\tEpisode 4504 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4955],\n",
      "        [1.0000, 0.5220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5496],\n",
      "        [1.0000, 0.4823]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455097675323486 \tStep Time:  0.015068292617797852 s \tTotal Time:  29.97756600379944 s \n",
      "\n",
      "\n",
      "\tEpisode 4505 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5390],\n",
      "        [1.0000, 0.5752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4543],\n",
      "        [1.0000, 0.5585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543245017528534 \tStep Time:  0.010971546173095703 s \tTotal Time:  29.988537549972534 s \n",
      "\n",
      "\n",
      "\tEpisode 4506 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5564],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4387],\n",
      "        [1.0000, 0.6660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490005493164062 \tStep Time:  0.008975744247436523 s \tTotal Time:  29.99751329421997 s \n",
      "\n",
      "\n",
      "\tEpisode 4507 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4614],\n",
      "        [1.0000, 0.6032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5421],\n",
      "        [1.0000, 0.4602]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.539623260498047 \tStep Time:  0.006981611251831055 s \tTotal Time:  30.005491495132446 s \n",
      "\n",
      "\n",
      "\tEpisode 4508 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4648],\n",
      "        [1.0000, 0.4245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563261032104492 \tStep Time:  0.00701451301574707 s \tTotal Time:  30.012506008148193 s \n",
      "\n",
      "\n",
      "\tEpisode 4509 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4467],\n",
      "        [1.0000, 0.5536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4233],\n",
      "        [1.0000, 0.5740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508426189422607 \tStep Time:  0.006979227066040039 s \tTotal Time:  30.019485235214233 s \n",
      "\n",
      "\n",
      "\tEpisode 4510 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4942],\n",
      "        [1.0000, 0.4268]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502401351928711 \tStep Time:  0.006951093673706055 s \tTotal Time:  30.02643632888794 s \n",
      "\n",
      "\n",
      "\tEpisode 4511 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4202],\n",
      "        [1.0000, 0.4662]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4953],\n",
      "        [1.0000, 0.4508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.551865339279175 \tStep Time:  0.007977485656738281 s \tTotal Time:  30.034413814544678 s \n",
      "\n",
      "\n",
      "\tEpisode 4512 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5886],\n",
      "        [1.0000, 0.5167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5744],\n",
      "        [1.0000, 0.4641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496668219566345 \tStep Time:  0.006981372833251953 s \tTotal Time:  30.04139518737793 s \n",
      "\n",
      "\n",
      "\tEpisode 4513 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5044],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4320],\n",
      "        [1.0000, 0.6908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572690963745117 \tStep Time:  0.007977724075317383 s \tTotal Time:  30.049372911453247 s \n",
      "\n",
      "\n",
      "\tEpisode 4514 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5338],\n",
      "        [1.0000, 0.4593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4370],\n",
      "        [1.0000, 0.4345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549870133399963 \tStep Time:  0.006982088088989258 s \tTotal Time:  30.056354999542236 s \n",
      "\n",
      "\n",
      "\tEpisode 4515 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6640],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5492],\n",
      "        [1.0000, 0.5653]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.624295234680176 \tStep Time:  0.008975505828857422 s \tTotal Time:  30.065330505371094 s \n",
      "\n",
      "\n",
      "\tEpisode 4516 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5421],\n",
      "        [1.0000, 0.4763]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4451],\n",
      "        [1.0000, 0.4667]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561835765838623 \tStep Time:  0.007978439331054688 s \tTotal Time:  30.07330894470215 s \n",
      "\n",
      "\n",
      "\tEpisode 4517 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4887],\n",
      "        [1.0000, 0.5233]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5209],\n",
      "        [1.0000, 0.4685]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496601104736328 \tStep Time:  0.007978677749633789 s \tTotal Time:  30.081287622451782 s \n",
      "\n",
      "\n",
      "\tEpisode 4518 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4943],\n",
      "        [1.0000, 0.4502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4462],\n",
      "        [1.0000, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504887580871582 \tStep Time:  0.012965917587280273 s \tTotal Time:  30.095251083374023 s \n",
      "\n",
      "\n",
      "\tEpisode 4519 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4750],\n",
      "        [1.0000, 0.4785]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4684],\n",
      "        [1.0000, 0.4537]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517027378082275 \tStep Time:  0.010972261428833008 s \tTotal Time:  30.106223344802856 s \n",
      "\n",
      "\n",
      "\tEpisode 4520 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.4757]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49735289812088 \tStep Time:  0.0069811344146728516 s \tTotal Time:  30.11320447921753 s \n",
      "\n",
      "\n",
      "\tEpisode 4521 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4772],\n",
      "        [1.0000, 0.4889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.4737]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493536353111267 \tStep Time:  0.0069811344146728516 s \tTotal Time:  30.120185613632202 s \n",
      "\n",
      "\n",
      "\tEpisode 4522 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4672],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4770],\n",
      "        [1.0000, 0.4617]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512039184570312 \tStep Time:  0.007279157638549805 s \tTotal Time:  30.127464771270752 s \n",
      "\n",
      "\n",
      "\tEpisode 4523 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4778],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4828],\n",
      "        [1.0000, 0.4809]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517412662506104 \tStep Time:  0.007693052291870117 s \tTotal Time:  30.135157823562622 s \n",
      "\n",
      "\n",
      "\tEpisode 4524 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4744],\n",
      "        [1.0000, 0.5039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4985],\n",
      "        [1.0000, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502337038516998 \tStep Time:  0.005983829498291016 s \tTotal Time:  30.141141653060913 s \n",
      "\n",
      "\n",
      "\tEpisode 4525 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.4771]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5120],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526107788085938 \tStep Time:  0.007986307144165039 s \tTotal Time:  30.149127960205078 s \n",
      "\n",
      "\n",
      "\tEpisode 4526 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5004],\n",
      "        [1.0000, 0.4976]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4973],\n",
      "        [1.0000, 0.5020]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498877882957458 \tStep Time:  0.00698542594909668 s \tTotal Time:  30.156113386154175 s \n",
      "\n",
      "\n",
      "\tEpisode 4527 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5055],\n",
      "        [1.0000, 0.5014]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4734],\n",
      "        [1.0000, 0.4994]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514211654663086 \tStep Time:  0.006981372833251953 s \tTotal Time:  30.163094758987427 s \n",
      "\n",
      "\n",
      "\tEpisode 4528 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5008],\n",
      "        [1.0000, 0.4909]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500411450862885 \tStep Time:  0.006982088088989258 s \tTotal Time:  30.170076847076416 s \n",
      "\n",
      "\n",
      "\tEpisode 4529 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4931],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495254516601562 \tStep Time:  0.007977485656738281 s \tTotal Time:  30.178054332733154 s \n",
      "\n",
      "\n",
      "\tEpisode 4530 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5078],\n",
      "        [1.0000, 0.5684]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5493],\n",
      "        [1.0000, 0.5837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480344295501709 \tStep Time:  0.006981611251831055 s \tTotal Time:  30.185035943984985 s \n",
      "\n",
      "\n",
      "\tEpisode 4531 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5249],\n",
      "        [1.0000, 0.5235]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503223419189453 \tStep Time:  0.006981372833251953 s \tTotal Time:  30.192017316818237 s \n",
      "\n",
      "\n",
      "\tEpisode 4532 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5615],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4867],\n",
      "        [1.0000, 0.6127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533798217773438 \tStep Time:  0.008975982666015625 s \tTotal Time:  30.200993299484253 s \n",
      "\n",
      "\n",
      "\tEpisode 4533 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4788],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4423],\n",
      "        [1.0000, 0.4506]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51244306564331 \tStep Time:  0.005984067916870117 s \tTotal Time:  30.206977367401123 s \n",
      "\n",
      "\n",
      "\tEpisode 4534 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4416],\n",
      "        [1.0000, 0.4568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5216],\n",
      "        [1.0000, 0.6524]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.653515815734863 \tStep Time:  0.0069866180419921875 s \tTotal Time:  30.214961290359497 s \n",
      "\n",
      "\n",
      "\tEpisode 4535 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5750],\n",
      "        [1.0000, 0.4834]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5123],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479680597782135 \tStep Time:  0.006981372833251953 s \tTotal Time:  30.22194266319275 s \n",
      "\n",
      "\n",
      "\tEpisode 4536 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4971],\n",
      "        [1.0000, 0.6759]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5018],\n",
      "        [1.0000, 0.5208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446645140647888 \tStep Time:  0.0069828033447265625 s \tTotal Time:  30.228925466537476 s \n",
      "\n",
      "\n",
      "\tEpisode 4537 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4989],\n",
      "        [1.0000, 0.5024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5058],\n",
      "        [1.0000, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507742881774902 \tStep Time:  0.008985042572021484 s \tTotal Time:  30.237910509109497 s \n",
      "\n",
      "\n",
      "\tEpisode 4538 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5031],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4938],\n",
      "        [1.0000, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519240856170654 \tStep Time:  0.006982088088989258 s \tTotal Time:  30.244892597198486 s \n",
      "\n",
      "\n",
      "\tEpisode 4539 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4975],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5009],\n",
      "        [1.0000, 0.5022]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505316257476807 \tStep Time:  0.006980180740356445 s \tTotal Time:  30.252869606018066 s \n",
      "\n",
      "\n",
      "\tEpisode 4540 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5037],\n",
      "        [1.0000, 0.4950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5010],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510849475860596 \tStep Time:  0.010974407196044922 s \tTotal Time:  30.26384401321411 s \n",
      "\n",
      "\n",
      "\tEpisode 4541 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5185],\n",
      "        [1.0000, 0.4981]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4910],\n",
      "        [1.0000, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520225048065186 \tStep Time:  0.010971546173095703 s \tTotal Time:  30.274815559387207 s \n",
      "\n",
      "\n",
      "\tEpisode 4542 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5165],\n",
      "        [1.0000, 0.5320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5122],\n",
      "        [1.0000, 0.5220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521695137023926 \tStep Time:  0.009968996047973633 s \tTotal Time:  30.28478455543518 s \n",
      "\n",
      "\n",
      "\tEpisode 4543 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4831],\n",
      "        [1.0000, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4758],\n",
      "        [1.0000, 0.6004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576054453849792 \tStep Time:  0.011968374252319336 s \tTotal Time:  30.297749757766724 s \n",
      "\n",
      "\n",
      "\tEpisode 4544 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5110],\n",
      "        [1.0000, 0.4930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4765],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502927601337433 \tStep Time:  0.012966394424438477 s \tTotal Time:  30.310716152191162 s \n",
      "\n",
      "\n",
      "\tEpisode 4545 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.4660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5061],\n",
      "        [1.0000, 0.4598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478617191314697 \tStep Time:  0.011970758438110352 s \tTotal Time:  30.322686910629272 s \n",
      "\n",
      "\n",
      "\tEpisode 4546 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4607],\n",
      "        [1.0000, 0.5168]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4551],\n",
      "        [1.0000, 0.4954]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498872935771942 \tStep Time:  0.011963605880737305 s \tTotal Time:  30.33465051651001 s \n",
      "\n",
      "\n",
      "\tEpisode 4547 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4711],\n",
      "        [1.0000, 0.4605]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5558],\n",
      "        [1.0000, 0.5373]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520236015319824 \tStep Time:  0.010974645614624023 s \tTotal Time:  30.345625162124634 s \n",
      "\n",
      "\n",
      "\tEpisode 4548 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4800],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4813],\n",
      "        [1.0000, 0.5379]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487833976745605 \tStep Time:  0.00997161865234375 s \tTotal Time:  30.355596780776978 s \n",
      "\n",
      "\n",
      "\tEpisode 4549 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4616],\n",
      "        [1.0000, 0.4764]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4805],\n",
      "        [1.0000, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502304077148438 \tStep Time:  0.008976221084594727 s \tTotal Time:  30.364573001861572 s \n",
      "\n",
      "\n",
      "\tEpisode 4550 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5397],\n",
      "        [1.0000, 0.4681]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4768],\n",
      "        [1.0000, 0.5347]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497910678386688 \tStep Time:  0.008974790573120117 s \tTotal Time:  30.373547792434692 s \n",
      "\n",
      "\n",
      "\tEpisode 4551 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5082],\n",
      "        [1.0000, 0.5326]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5033],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496846675872803 \tStep Time:  0.00897669792175293 s \tTotal Time:  30.382524490356445 s \n",
      "\n",
      "\n",
      "\tEpisode 4552 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5401],\n",
      "        [1.0000, 0.4882]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4709],\n",
      "        [1.0000, 0.5533]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.441184520721436 \tStep Time:  0.008974552154541016 s \tTotal Time:  30.391499042510986 s \n",
      "\n",
      "\n",
      "\tEpisode 4553 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4669],\n",
      "        [1.0000, 0.5330]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4592],\n",
      "        [1.0000, 0.5531]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.428248703479767 \tStep Time:  0.0109710693359375 s \tTotal Time:  30.402470111846924 s \n",
      "\n",
      "\n",
      "\tEpisode 4554 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5404],\n",
      "        [1.0000, 0.5199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489471912384033 \tStep Time:  0.007979631423950195 s \tTotal Time:  30.410449743270874 s \n",
      "\n",
      "\n",
      "\tEpisode 4555 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.4752]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5488],\n",
      "        [1.0000, 0.4804]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467282772064209 \tStep Time:  0.009973287582397461 s \tTotal Time:  30.421419858932495 s \n",
      "\n",
      "\n",
      "\tEpisode 4556 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6779],\n",
      "        [1.0000, 0.4646]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5049],\n",
      "        [1.0000, 0.5949]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543200492858887 \tStep Time:  0.011967658996582031 s \tTotal Time:  30.433387517929077 s \n",
      "\n",
      "\n",
      "\tEpisode 4557 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.4897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5131],\n",
      "        [1.0000, 0.3999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.571648120880127 \tStep Time:  0.014099597930908203 s \tTotal Time:  30.447487115859985 s \n",
      "\n",
      "\n",
      "\tEpisode 4558 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5637],\n",
      "        [1.0000, 0.4095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4094],\n",
      "        [1.0000, 0.3918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.431485831737518 \tStep Time:  0.016956329345703125 s \tTotal Time:  30.46444344520569 s \n",
      "\n",
      "\n",
      "\tEpisode 4559 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6076],\n",
      "        [1.0000, 0.4634]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5790],\n",
      "        [1.0000, 0.4897]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510082721710205 \tStep Time:  0.015957117080688477 s \tTotal Time:  30.480400562286377 s \n",
      "\n",
      "\n",
      "\tEpisode 4560 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7445],\n",
      "        [1.0000, 0.4875]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6577],\n",
      "        [1.0000, 0.7660]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507034420967102 \tStep Time:  0.015957355499267578 s \tTotal Time:  30.496357917785645 s \n",
      "\n",
      "\n",
      "\tEpisode 4561 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4309],\n",
      "        [1.0000, 0.6001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5228],\n",
      "        [1.0000, 0.4714]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.400841355323792 \tStep Time:  0.009973287582397461 s \tTotal Time:  30.506331205368042 s \n",
      "\n",
      "\n",
      "\tEpisode 4562 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4398],\n",
      "        [1.0000, 0.4558]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6700],\n",
      "        [1.0000, 0.3828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39319896697998 \tStep Time:  0.01196742057800293 s \tTotal Time:  30.518298625946045 s \n",
      "\n",
      "\n",
      "\tEpisode 4563 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6981],\n",
      "        [1.0000, 0.6376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.3564]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487813353538513 \tStep Time:  0.01995253562927246 s \tTotal Time:  30.538251161575317 s \n",
      "\n",
      "\n",
      "\tEpisode 4564 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3825],\n",
      "        [1.0000, 0.4413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.3593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517090797424316 \tStep Time:  0.014953851699829102 s \tTotal Time:  30.553205013275146 s \n",
      "\n",
      "\n",
      "\tEpisode 4565 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3643],\n",
      "        [1.0000, 0.3322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3601],\n",
      "        [1.0000, 0.6195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.6963529586792 \tStep Time:  0.009973287582397461 s \tTotal Time:  30.563178300857544 s \n",
      "\n",
      "\n",
      "\tEpisode 4566 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4304],\n",
      "        [1.0000, 0.4093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4933],\n",
      "        [1.0000, 0.4609]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461388111114502 \tStep Time:  0.006985187530517578 s \tTotal Time:  30.57016348838806 s \n",
      "\n",
      "\n",
      "\tEpisode 4567 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6130],\n",
      "        [1.0000, 0.5608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3600],\n",
      "        [1.0000, 0.5566]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458892226219177 \tStep Time:  0.007980108261108398 s \tTotal Time:  30.57814359664917 s \n",
      "\n",
      "\n",
      "\tEpisode 4568 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6536],\n",
      "        [1.0000, 0.5323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3859],\n",
      "        [1.0000, 0.6406]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45818567276001 \tStep Time:  0.007978677749633789 s \tTotal Time:  30.58711814880371 s \n",
      "\n",
      "\n",
      "\tEpisode 4569 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5969],\n",
      "        [1.0000, 0.5934]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4424],\n",
      "        [1.0000, 0.3570]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.337278366088867 \tStep Time:  0.006982326507568359 s \tTotal Time:  30.59410047531128 s \n",
      "\n",
      "\n",
      "\tEpisode 4570 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6725],\n",
      "        [1.0000, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5826],\n",
      "        [1.0000, 0.6366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581557273864746 \tStep Time:  0.0079803466796875 s \tTotal Time:  30.602080821990967 s \n",
      "\n",
      "\n",
      "\tEpisode 4571 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5195],\n",
      "        [1.0000, 0.4027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5575],\n",
      "        [1.0000, 0.4608]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.627047896385193 \tStep Time:  0.006981611251831055 s \tTotal Time:  30.609062433242798 s \n",
      "\n",
      "\n",
      "\tEpisode 4572 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4032],\n",
      "        [1.0000, 0.6224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3451],\n",
      "        [1.0000, 0.3637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.672494888305664 \tStep Time:  0.00897669792175293 s \tTotal Time:  30.61803913116455 s \n",
      "\n",
      "\n",
      "\tEpisode 4573 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4537],\n",
      "        [1.0000, 0.3522]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4024],\n",
      "        [1.0000, 0.3513]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583167433738708 \tStep Time:  0.0069811344146728516 s \tTotal Time:  30.625020265579224 s \n",
      "\n",
      "\n",
      "\tEpisode 4574 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3652],\n",
      "        [1.0000, 0.6790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5944],\n",
      "        [1.0000, 0.6345]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.385798275470734 \tStep Time:  0.007979154586791992 s \tTotal Time:  30.632999420166016 s \n",
      "\n",
      "\n",
      "\tEpisode 4575 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3914],\n",
      "        [1.0000, 0.4143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5110],\n",
      "        [1.0000, 0.5449]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495881795883179 \tStep Time:  0.007978200912475586 s \tTotal Time:  30.64097762107849 s \n",
      "\n",
      "\n",
      "\tEpisode 4576 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4860],\n",
      "        [1.0000, 0.3758]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6047],\n",
      "        [1.0000, 0.4864]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.417428970336914 \tStep Time:  0.007979154586791992 s \tTotal Time:  30.648956775665283 s \n",
      "\n",
      "\n",
      "\tEpisode 4577 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5083],\n",
      "        [1.0000, 0.3773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3721],\n",
      "        [1.0000, 0.6129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.35737133026123 \tStep Time:  0.006979942321777344 s \tTotal Time:  30.65593671798706 s \n",
      "\n",
      "\n",
      "\tEpisode 4578 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4093],\n",
      "        [1.0000, 0.3700]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6177],\n",
      "        [1.0000, 0.3740]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.653889179229736 \tStep Time:  0.007979154586791992 s \tTotal Time:  30.663915872573853 s \n",
      "\n",
      "\n",
      "\tEpisode 4579 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6303],\n",
      "        [1.0000, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3865],\n",
      "        [1.0000, 0.4008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.367667198181152 \tStep Time:  0.007978200912475586 s \tTotal Time:  30.671894073486328 s \n",
      "\n",
      "\n",
      "\tEpisode 4580 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3848],\n",
      "        [1.0000, 0.6328]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4526],\n",
      "        [1.0000, 0.4201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.606316566467285 \tStep Time:  0.007980585098266602 s \tTotal Time:  30.679874658584595 s \n",
      "\n",
      "\n",
      "\tEpisode 4581 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5308],\n",
      "        [1.0000, 0.3442]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5007],\n",
      "        [1.0000, 0.6394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564548015594482 \tStep Time:  0.007977724075317383 s \tTotal Time:  30.687852382659912 s \n",
      "\n",
      "\n",
      "\tEpisode 4582 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3801],\n",
      "        [1.0000, 0.6687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4026],\n",
      "        [1.0000, 0.3922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.421034336090088 \tStep Time:  0.007978439331054688 s \tTotal Time:  30.695830821990967 s \n",
      "\n",
      "\n",
      "\tEpisode 4583 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4143],\n",
      "        [1.0000, 0.3939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.6746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.717151641845703 \tStep Time:  0.007978200912475586 s \tTotal Time:  30.703809022903442 s \n",
      "\n",
      "\n",
      "\tEpisode 4584 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5310],\n",
      "        [1.0000, 0.4287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5949],\n",
      "        [1.0000, 0.4224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55599069595337 \tStep Time:  0.0079803466796875 s \tTotal Time:  30.71178936958313 s \n",
      "\n",
      "\n",
      "\tEpisode 4585 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6613],\n",
      "        [1.0000, 0.5832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5117],\n",
      "        [1.0000, 0.5025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.412708282470703 \tStep Time:  0.007977485656738281 s \tTotal Time:  30.719766855239868 s \n",
      "\n",
      "\n",
      "\tEpisode 4586 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6793],\n",
      "        [1.0000, 0.5323]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [1.0000, 0.5466]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.613443851470947 \tStep Time:  0.008976459503173828 s \tTotal Time:  30.728743314743042 s \n",
      "\n",
      "\n",
      "\tEpisode 4587 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4688],\n",
      "        [1.0000, 0.5185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4813],\n",
      "        [1.0000, 0.4636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478726387023926 \tStep Time:  0.00897526741027832 s \tTotal Time:  30.73771858215332 s \n",
      "\n",
      "\n",
      "\tEpisode 4588 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.6459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4436],\n",
      "        [1.0000, 0.6302]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54838752746582 \tStep Time:  0.008975744247436523 s \tTotal Time:  30.746694326400757 s \n",
      "\n",
      "\n",
      "\tEpisode 4589 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5786],\n",
      "        [1.0000, 0.6343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7173],\n",
      "        [1.0000, 0.4401]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.380417883396149 \tStep Time:  0.007979631423950195 s \tTotal Time:  30.754673957824707 s \n",
      "\n",
      "\n",
      "\tEpisode 4590 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4508],\n",
      "        [1.0000, 0.5803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4391],\n",
      "        [1.0000, 0.5234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494989037513733 \tStep Time:  0.008975744247436523 s \tTotal Time:  30.763649702072144 s \n",
      "\n",
      "\n",
      "\tEpisode 4591 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6373],\n",
      "        [1.0000, 0.4968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5862],\n",
      "        [1.0000, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489481449127197 \tStep Time:  0.007978677749633789 s \tTotal Time:  30.771628379821777 s \n",
      "\n",
      "\n",
      "\tEpisode 4592 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4321],\n",
      "        [1.0000, 0.4707]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5968],\n",
      "        [1.0000, 0.5799]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549598217010498 \tStep Time:  0.008975982666015625 s \tTotal Time:  30.780604362487793 s \n",
      "\n",
      "\n",
      "\tEpisode 4593 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4288],\n",
      "        [1.0000, 0.6248]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4603],\n",
      "        [1.0000, 0.6520]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533160209655762 \tStep Time:  0.007979154586791992 s \tTotal Time:  30.788583517074585 s \n",
      "\n",
      "\n",
      "\tEpisode 4594 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5966],\n",
      "        [1.0000, 0.6115]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5344],\n",
      "        [1.0000, 0.4536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.638609886169434 \tStep Time:  0.008975028991699219 s \tTotal Time:  30.797558546066284 s \n",
      "\n",
      "\n",
      "\tEpisode 4595 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6086],\n",
      "        [1.0000, 0.4545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4577],\n",
      "        [1.0000, 0.5508]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49899435043335 \tStep Time:  0.010973215103149414 s \tTotal Time:  30.808531761169434 s \n",
      "\n",
      "\n",
      "\tEpisode 4596 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4233],\n",
      "        [1.0000, 0.4480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4523],\n",
      "        [1.0000, 0.4595]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496998190879822 \tStep Time:  0.009969949722290039 s \tTotal Time:  30.818501710891724 s \n",
      "\n",
      "\n",
      "\tEpisode 4597 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4400],\n",
      "        [1.0000, 0.4502]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4325],\n",
      "        [1.0000, 0.4138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501409411430359 \tStep Time:  0.006016254425048828 s \tTotal Time:  30.824517965316772 s \n",
      "\n",
      "\n",
      "\tEpisode 4598 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5549],\n",
      "        [1.0000, 0.5984]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5627],\n",
      "        [1.0000, 0.4259]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47632110118866 \tStep Time:  0.007021427154541016 s \tTotal Time:  30.831539392471313 s \n",
      "\n",
      "\n",
      "\tEpisode 4599 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.4311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5580],\n",
      "        [1.0000, 0.5407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489128589630127 \tStep Time:  0.007944345474243164 s \tTotal Time:  30.839483737945557 s \n",
      "\n",
      "\n",
      "\tEpisode 4600 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4259],\n",
      "        [1.0000, 0.5655]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4504],\n",
      "        [1.0000, 0.5365]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511789321899414 \tStep Time:  0.008976221084594727 s \tTotal Time:  30.84845995903015 s \n",
      "\n",
      "\n",
      "\tEpisode 4601 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5368],\n",
      "        [1.0000, 0.5483]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4282],\n",
      "        [1.0000, 0.4273]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508093357086182 \tStep Time:  0.005984306335449219 s \tTotal Time:  30.8544442653656 s \n",
      "\n",
      "\n",
      "\tEpisode 4602 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4923],\n",
      "        [1.0000, 0.4343]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4902],\n",
      "        [1.0000, 0.4319]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512006282806396 \tStep Time:  0.007978200912475586 s \tTotal Time:  30.8634192943573 s \n",
      "\n",
      "\n",
      "\tEpisode 4603 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.4727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503548622131348 \tStep Time:  0.006981611251831055 s \tTotal Time:  30.87040090560913 s \n",
      "\n",
      "\n",
      "\tEpisode 4604 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5798],\n",
      "        [1.0000, 0.4317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4849],\n",
      "        [1.0000, 0.5075]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5196852684021 \tStep Time:  0.00798177719116211 s \tTotal Time:  30.878382682800293 s \n",
      "\n",
      "\n",
      "\tEpisode 4605 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4913],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [1.0000, 0.5198]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531618595123291 \tStep Time:  0.00797581672668457 s \tTotal Time:  30.886358499526978 s \n",
      "\n",
      "\n",
      "\tEpisode 4606 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4245],\n",
      "        [1.0000, 0.4806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4435],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.471812725067139 \tStep Time:  0.00698089599609375 s \tTotal Time:  30.89333939552307 s \n",
      "\n",
      "\n",
      "\tEpisode 4607 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4385],\n",
      "        [1.0000, 0.4468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4357],\n",
      "        [1.0000, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544827461242676 \tStep Time:  0.007979154586791992 s \tTotal Time:  30.901318550109863 s \n",
      "\n",
      "\n",
      "\tEpisode 4608 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4682],\n",
      "        [1.0000, 0.5546]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5480],\n",
      "        [1.0000, 0.4490]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52296781539917 \tStep Time:  0.0069806575775146484 s \tTotal Time:  30.908299207687378 s \n",
      "\n",
      "\n",
      "\tEpisode 4609 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4718],\n",
      "        [1.0000, 0.4266]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5299],\n",
      "        [1.0000, 0.4227]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432130575180054 \tStep Time:  0.007979393005371094 s \tTotal Time:  30.91627860069275 s \n",
      "\n",
      "\n",
      "\tEpisode 4610 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.5336]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4839],\n",
      "        [1.0000, 0.5193]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506013870239258 \tStep Time:  0.007978439331054688 s \tTotal Time:  30.924257040023804 s \n",
      "\n",
      "\n",
      "\tEpisode 4611 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4329],\n",
      "        [1.0000, 0.4136]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4109],\n",
      "        [1.0000, 0.5220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.472152709960938 \tStep Time:  0.007980823516845703 s \tTotal Time:  30.93223786354065 s \n",
      "\n",
      "\n",
      "\tEpisode 4612 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6056],\n",
      "        [1.0000, 0.5164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3944],\n",
      "        [1.0000, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424447000026703 \tStep Time:  0.007982015609741211 s \tTotal Time:  30.94021987915039 s \n",
      "\n",
      "\n",
      "\tEpisode 4613 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5804],\n",
      "        [1.0000, 0.5576]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5643],\n",
      "        [1.0000, 0.4287]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.438948631286621 \tStep Time:  0.00797891616821289 s \tTotal Time:  30.948198795318604 s \n",
      "\n",
      "\n",
      "\tEpisode 4614 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4204],\n",
      "        [1.0000, 0.4900]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4483],\n",
      "        [1.0000, 0.3122]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.599081993103027 \tStep Time:  0.006981372833251953 s \tTotal Time:  30.955180168151855 s \n",
      "\n",
      "\n",
      "\tEpisode 4615 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5968],\n",
      "        [1.0000, 0.4212]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4354],\n",
      "        [1.0000, 0.4039]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588837027549744 \tStep Time:  0.007978677749633789 s \tTotal Time:  30.96315884590149 s \n",
      "\n",
      "\n",
      "\tEpisode 4616 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4284],\n",
      "        [1.0000, 0.5939]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4106],\n",
      "        [1.0000, 0.4041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.433539986610413 \tStep Time:  0.006981849670410156 s \tTotal Time:  30.9701406955719 s \n",
      "\n",
      "\n",
      "\tEpisode 4617 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4288],\n",
      "        [1.0000, 0.4177]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3997],\n",
      "        [1.0000, 0.4390]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533142030239105 \tStep Time:  0.007977724075317383 s \tTotal Time:  30.978118419647217 s \n",
      "\n",
      "\n",
      "\tEpisode 4618 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4266],\n",
      "        [1.0000, 0.6548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6200],\n",
      "        [1.0000, 0.4606]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56329482793808 \tStep Time:  0.006981372833251953 s \tTotal Time:  30.98509979248047 s \n",
      "\n",
      "\n",
      "\tEpisode 4619 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5322],\n",
      "        [1.0000, 0.6466]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4156],\n",
      "        [1.0000, 0.4176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.692851543426514 \tStep Time:  0.007977962493896484 s \tTotal Time:  30.993077754974365 s \n",
      "\n",
      "\n",
      "\tEpisode 4620 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4558],\n",
      "        [1.0000, 0.4407]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5310],\n",
      "        [1.0000, 0.5196]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513960838317871 \tStep Time:  0.007979154586791992 s \tTotal Time:  31.001056909561157 s \n",
      "\n",
      "\n",
      "\tEpisode 4621 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5999],\n",
      "        [1.0000, 0.5801]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4622],\n",
      "        [1.0000, 0.5641]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48221755027771 \tStep Time:  0.007978200912475586 s \tTotal Time:  31.009035110473633 s \n",
      "\n",
      "\n",
      "\tEpisode 4622 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5086],\n",
      "        [1.0000, 0.6178]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4766],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4417142868042 \tStep Time:  0.010970592498779297 s \tTotal Time:  31.020005702972412 s \n",
      "\n",
      "\n",
      "\tEpisode 4623 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.5461]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5444],\n",
      "        [1.0000, 0.6376]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4498872756958 \tStep Time:  0.009974479675292969 s \tTotal Time:  31.029980182647705 s \n",
      "\n",
      "\n",
      "\tEpisode 4624 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6156],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.4989]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56501293182373 \tStep Time:  0.007977962493896484 s \tTotal Time:  31.0379581451416 s \n",
      "\n",
      "\n",
      "\tEpisode 4625 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.5231]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4905],\n",
      "        [1.0000, 0.5004]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5281982421875 \tStep Time:  0.010970354080200195 s \tTotal Time:  31.0489284992218 s \n",
      "\n",
      "\n",
      "\tEpisode 4626 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5672],\n",
      "        [1.0000, 0.5454]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4970],\n",
      "        [1.0000, 0.5886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576044082641602 \tStep Time:  0.00997304916381836 s \tTotal Time:  31.05890154838562 s \n",
      "\n",
      "\n",
      "\tEpisode 4627 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5471],\n",
      "        [1.0000, 0.5539]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5317],\n",
      "        [1.0000, 0.5880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520910263061523 \tStep Time:  0.014960527420043945 s \tTotal Time:  31.073862075805664 s \n",
      "\n",
      "\n",
      "\tEpisode 4628 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [1.0000, 0.5156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6217],\n",
      "        [1.0000, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572144508361816 \tStep Time:  0.014960050582885742 s \tTotal Time:  31.08882212638855 s \n",
      "\n",
      "\n",
      "\tEpisode 4629 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5378],\n",
      "        [1.0000, 0.5398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5532],\n",
      "        [1.0000, 0.5587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526561617851257 \tStep Time:  0.014959335327148438 s \tTotal Time:  31.1037814617157 s \n",
      "\n",
      "\n",
      "\tEpisode 4630 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5223],\n",
      "        [1.0000, 0.5501]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5242],\n",
      "        [1.0000, 0.5515]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507469654083252 \tStep Time:  0.025931119918823242 s \tTotal Time:  31.12971258163452 s \n",
      "\n",
      "\n",
      "\tEpisode 4631 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5846],\n",
      "        [1.0000, 0.6548]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4865],\n",
      "        [1.0000, 0.5320]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.411497592926025 \tStep Time:  0.010970592498779297 s \tTotal Time:  31.1406831741333 s \n",
      "\n",
      "\n",
      "\tEpisode 4632 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5566],\n",
      "        [1.0000, 0.5948]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4932],\n",
      "        [1.0000, 0.6099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612354755401611 \tStep Time:  0.008976459503173828 s \tTotal Time:  31.149659633636475 s \n",
      "\n",
      "\n",
      "\tEpisode 4633 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5263],\n",
      "        [1.0000, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5307],\n",
      "        [1.0000, 0.4673]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504431247711182 \tStep Time:  0.008976221084594727 s \tTotal Time:  31.15863585472107 s \n",
      "\n",
      "\n",
      "\tEpisode 4634 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.4626]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.6430]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.582054138183594 \tStep Time:  0.008976459503173828 s \tTotal Time:  31.167612314224243 s \n",
      "\n",
      "\n",
      "\tEpisode 4635 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5163],\n",
      "        [1.0000, 0.5024]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4435],\n",
      "        [1.0000, 0.4582]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52051055431366 \tStep Time:  0.007977962493896484 s \tTotal Time:  31.17559027671814 s \n",
      "\n",
      "\n",
      "\tEpisode 4636 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4906],\n",
      "        [1.0000, 0.4444]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4836],\n",
      "        [1.0000, 0.4832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487082481384277 \tStep Time:  0.007977485656738281 s \tTotal Time:  31.183567762374878 s \n",
      "\n",
      "\n",
      "\tEpisode 4637 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5139],\n",
      "        [1.0000, 0.4713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4331],\n",
      "        [1.0000, 0.5913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45257568359375 \tStep Time:  0.007012367248535156 s \tTotal Time:  31.190580129623413 s \n",
      "\n",
      "\n",
      "\tEpisode 4638 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5318],\n",
      "        [1.0000, 0.4210]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4666],\n",
      "        [1.0000, 0.4278]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552297115325928 \tStep Time:  0.007947444915771484 s \tTotal Time:  31.198527574539185 s \n",
      "\n",
      "\n",
      "\tEpisode 4639 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4387],\n",
      "        [1.0000, 0.4353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4508],\n",
      "        [1.0000, 0.6145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.605602025985718 \tStep Time:  0.006982088088989258 s \tTotal Time:  31.205509662628174 s \n",
      "\n",
      "\n",
      "\tEpisode 4640 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4835],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5088],\n",
      "        [1.0000, 0.4650]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467077493667603 \tStep Time:  0.006981849670410156 s \tTotal Time:  31.212491512298584 s \n",
      "\n",
      "\n",
      "\tEpisode 4641 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5287],\n",
      "        [1.0000, 0.4156]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4269],\n",
      "        [1.0000, 0.4932]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.434628963470459 \tStep Time:  0.007977962493896484 s \tTotal Time:  31.22046947479248 s \n",
      "\n",
      "\n",
      "\tEpisode 4642 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5544],\n",
      "        [1.0000, 0.4352]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5157],\n",
      "        [1.0000, 0.5850]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57965612411499 \tStep Time:  0.00598454475402832 s \tTotal Time:  31.22645401954651 s \n",
      "\n",
      "\n",
      "\tEpisode 4643 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4172],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4755],\n",
      "        [1.0000, 0.5412]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50569748878479 \tStep Time:  0.006981372833251953 s \tTotal Time:  31.23343539237976 s \n",
      "\n",
      "\n",
      "\tEpisode 4644 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5769],\n",
      "        [1.0000, 0.4138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4687],\n",
      "        [1.0000, 0.4446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.583885669708252 \tStep Time:  0.005983114242553711 s \tTotal Time:  31.239418506622314 s \n",
      "\n",
      "\n",
      "\tEpisode 4645 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5457],\n",
      "        [1.0000, 0.5354]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4279],\n",
      "        [1.0000, 0.5222]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.564053058624268 \tStep Time:  0.007979869842529297 s \tTotal Time:  31.247398376464844 s \n",
      "\n",
      "\n",
      "\tEpisode 4646 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4353],\n",
      "        [1.0000, 0.5170]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4740],\n",
      "        [1.0000, 0.4672]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.550952434539795 \tStep Time:  0.007977962493896484 s \tTotal Time:  31.25537633895874 s \n",
      "\n",
      "\n",
      "\tEpisode 4647 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4392],\n",
      "        [1.0000, 0.4545]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4601],\n",
      "        [1.0000, 0.4294]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530837059020996 \tStep Time:  0.007979154586791992 s \tTotal Time:  31.263355493545532 s \n",
      "\n",
      "\n",
      "\tEpisode 4648 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4524],\n",
      "        [1.0000, 0.4391]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4553],\n",
      "        [1.0000, 0.4724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509572982788086 \tStep Time:  0.01196742057800293 s \tTotal Time:  31.275322914123535 s \n",
      "\n",
      "\n",
      "\tEpisode 4649 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4738],\n",
      "        [1.0000, 0.4380]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4467],\n",
      "        [1.0000, 0.5201]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.534595966339111 \tStep Time:  0.013963460922241211 s \tTotal Time:  31.289286375045776 s \n",
      "\n",
      "\n",
      "\tEpisode 4650 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4653],\n",
      "        [1.0000, 0.4710]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4933],\n",
      "        [1.0000, 0.4794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50567626953125 \tStep Time:  0.014959573745727539 s \tTotal Time:  31.304245948791504 s \n",
      "\n",
      "\n",
      "\tEpisode 4651 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4613],\n",
      "        [1.0000, 0.4744]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4655],\n",
      "        [1.0000, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530404567718506 \tStep Time:  0.013962745666503906 s \tTotal Time:  31.318208694458008 s \n",
      "\n",
      "\n",
      "\tEpisode 4652 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4895],\n",
      "        [1.0000, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516927361488342 \tStep Time:  0.012966156005859375 s \tTotal Time:  31.331174850463867 s \n",
      "\n",
      "\n",
      "\tEpisode 4653 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.5091]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4779],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49216103553772 \tStep Time:  0.012964725494384766 s \tTotal Time:  31.344139575958252 s \n",
      "\n",
      "\n",
      "\tEpisode 4654 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5118],\n",
      "        [1.0000, 0.4721]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4760],\n",
      "        [1.0000, 0.4986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501453399658203 \tStep Time:  0.011966705322265625 s \tTotal Time:  31.35710382461548 s \n",
      "\n",
      "\n",
      "\tEpisode 4655 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5511],\n",
      "        [1.0000, 0.5118]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5525],\n",
      "        [1.0000, 0.5398]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497581481933594 \tStep Time:  0.008977413177490234 s \tTotal Time:  31.36608123779297 s \n",
      "\n",
      "\n",
      "\tEpisode 4656 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4611],\n",
      "        [1.0000, 0.4880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5289],\n",
      "        [1.0000, 0.4822]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477335929870605 \tStep Time:  0.007979154586791992 s \tTotal Time:  31.37406039237976 s \n",
      "\n",
      "\n",
      "\tEpisode 4657 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.5607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5419],\n",
      "        [1.0000, 0.4488]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.543071269989014 \tStep Time:  0.007978439331054688 s \tTotal Time:  31.382038831710815 s \n",
      "\n",
      "\n",
      "\tEpisode 4658 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5416],\n",
      "        [1.0000, 0.4729]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4971],\n",
      "        [1.0000, 0.5780]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437694072723389 \tStep Time:  0.007978200912475586 s \tTotal Time:  31.39001703262329 s \n",
      "\n",
      "\n",
      "\tEpisode 4659 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6113],\n",
      "        [1.0000, 0.4848]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6701],\n",
      "        [1.0000, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568879425525665 \tStep Time:  0.008976221084594727 s \tTotal Time:  31.398993253707886 s \n",
      "\n",
      "\n",
      "\tEpisode 4660 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6459],\n",
      "        [1.0000, 0.5905]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5768],\n",
      "        [1.0000, 0.6421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527167797088623 \tStep Time:  0.00797891616821289 s \tTotal Time:  31.4069721698761 s \n",
      "\n",
      "\n",
      "\tEpisode 4661 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3898],\n",
      "        [1.0000, 0.6453]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6096],\n",
      "        [1.0000, 0.4698]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.727947235107422 \tStep Time:  0.009972095489501953 s \tTotal Time:  31.4169442653656 s \n",
      "\n",
      "\n",
      "\tEpisode 4662 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5914],\n",
      "        [1.0000, 0.5942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4790],\n",
      "        [1.0000, 0.3783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.592158734798431 \tStep Time:  0.007980823516845703 s \tTotal Time:  31.424925088882446 s \n",
      "\n",
      "\n",
      "\tEpisode 4663 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5953],\n",
      "        [1.0000, 0.5261]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3784],\n",
      "        [1.0000, 0.3886]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494221687316895 \tStep Time:  0.008975982666015625 s \tTotal Time:  31.434898138046265 s \n",
      "\n",
      "\n",
      "\tEpisode 4664 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5999],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5135],\n",
      "        [1.0000, 0.3851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49482649564743 \tStep Time:  0.008976459503173828 s \tTotal Time:  31.44387459754944 s \n",
      "\n",
      "\n",
      "\tEpisode 4665 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5289],\n",
      "        [1.0000, 0.5622]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5959],\n",
      "        [1.0000, 0.4550]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.588512897491455 \tStep Time:  0.010969161987304688 s \tTotal Time:  31.454843759536743 s \n",
      "\n",
      "\n",
      "\tEpisode 4666 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3898],\n",
      "        [1.0000, 0.5960]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5618],\n",
      "        [1.0000, 0.4149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.364233493804932 \tStep Time:  0.011968135833740234 s \tTotal Time:  31.466811895370483 s \n",
      "\n",
      "\n",
      "\tEpisode 4667 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3931],\n",
      "        [1.0000, 0.5741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3799],\n",
      "        [1.0000, 0.5422]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530649662017822 \tStep Time:  0.009974241256713867 s \tTotal Time:  31.476786136627197 s \n",
      "\n",
      "\n",
      "\tEpisode 4668 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3905],\n",
      "        [1.0000, 0.5459]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5190],\n",
      "        [1.0000, 0.3725]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545005321502686 \tStep Time:  0.010968923568725586 s \tTotal Time:  31.487755060195923 s \n",
      "\n",
      "\n",
      "\tEpisode 4669 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5895],\n",
      "        [1.0000, 0.6027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5683],\n",
      "        [1.0000, 0.5176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57472848892212 \tStep Time:  0.011967897415161133 s \tTotal Time:  31.499722957611084 s \n",
      "\n",
      "\n",
      "\tEpisode 4670 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5376],\n",
      "        [1.0000, 0.4474]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.3704]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511732578277588 \tStep Time:  0.008975982666015625 s \tTotal Time:  31.5086989402771 s \n",
      "\n",
      "\n",
      "\tEpisode 4671 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5637],\n",
      "        [1.0000, 0.5464]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3703],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.644780158996582 \tStep Time:  0.013962984085083008 s \tTotal Time:  31.522661924362183 s \n",
      "\n",
      "\n",
      "\tEpisode 4672 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5215],\n",
      "        [1.0000, 0.4103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4223],\n",
      "        [1.0000, 0.4562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597582399845123 \tStep Time:  0.009974002838134766 s \tTotal Time:  31.532635927200317 s \n",
      "\n",
      "\n",
      "\tEpisode 4673 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4395],\n",
      "        [1.0000, 0.3991]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3969],\n",
      "        [1.0000, 0.4968]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5512615442276 \tStep Time:  0.00797724723815918 s \tTotal Time:  31.540613174438477 s \n",
      "\n",
      "\n",
      "\tEpisode 4674 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4762],\n",
      "        [1.0000, 0.4867]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4782],\n",
      "        [1.0000, 0.4342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526718616485596 \tStep Time:  0.007978677749633789 s \tTotal Time:  31.54859185218811 s \n",
      "\n",
      "\n",
      "\tEpisode 4675 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.4268]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4880],\n",
      "        [1.0000, 0.4633]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510734021663666 \tStep Time:  0.006981849670410156 s \tTotal Time:  31.55557370185852 s \n",
      "\n",
      "\n",
      "\tEpisode 4676 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4610],\n",
      "        [1.0000, 0.4557]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4462],\n",
      "        [1.0000, 0.4587]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514579772949219 \tStep Time:  0.007979154586791992 s \tTotal Time:  31.563552856445312 s \n",
      "\n",
      "\n",
      "\tEpisode 4677 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4863],\n",
      "        [1.0000, 0.4413]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4641],\n",
      "        [1.0000, 0.4837]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.479109764099121 \tStep Time:  0.007977724075317383 s \tTotal Time:  31.57153058052063 s \n",
      "\n",
      "\n",
      "\tEpisode 4678 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4774],\n",
      "        [1.0000, 0.4775]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5600],\n",
      "        [1.0000, 0.4350]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.573292255401611 \tStep Time:  0.006982564926147461 s \tTotal Time:  31.578513145446777 s \n",
      "\n",
      "\n",
      "\tEpisode 4679 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4748],\n",
      "        [1.0000, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4833],\n",
      "        [1.0000, 0.4715]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516000270843506 \tStep Time:  0.007977962493896484 s \tTotal Time:  31.587487936019897 s \n",
      "\n",
      "\n",
      "\tEpisode 4680 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4051],\n",
      "        [1.0000, 0.4842]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4829],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56288194656372 \tStep Time:  0.007979869842529297 s \tTotal Time:  31.595467805862427 s \n",
      "\n",
      "\n",
      "\tEpisode 4681 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5063],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5143],\n",
      "        [1.0000, 0.5153]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515043497085571 \tStep Time:  0.00797724723815918 s \tTotal Time:  31.603445053100586 s \n",
      "\n",
      "\n",
      "\tEpisode 4682 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5077],\n",
      "        [1.0000, 0.4917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4927],\n",
      "        [1.0000, 0.4766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.519997596740723 \tStep Time:  0.007979393005371094 s \tTotal Time:  31.611424446105957 s \n",
      "\n",
      "\n",
      "\tEpisode 4683 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5132],\n",
      "        [1.0000, 0.4828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4977],\n",
      "        [1.0000, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497113227844238 \tStep Time:  0.007978200912475586 s \tTotal Time:  31.619402647018433 s \n",
      "\n",
      "\n",
      "\tEpisode 4684 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5214],\n",
      "        [1.0000, 0.4912]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5521],\n",
      "        [1.0000, 0.5366]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54643440246582 \tStep Time:  0.007979869842529297 s \tTotal Time:  31.627382516860962 s \n",
      "\n",
      "\n",
      "\tEpisode 4685 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5088],\n",
      "        [1.0000, 0.5208]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5535],\n",
      "        [1.0000, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524429857730865 \tStep Time:  0.006980180740356445 s \tTotal Time:  31.63436269760132 s \n",
      "\n",
      "\n",
      "\tEpisode 4686 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5383],\n",
      "        [1.0000, 0.5322]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.5371]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485909938812256 \tStep Time:  0.005984783172607422 s \tTotal Time:  31.640347480773926 s \n",
      "\n",
      "\n",
      "\tEpisode 4687 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5204],\n",
      "        [1.0000, 0.5399]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4923],\n",
      "        [1.0000, 0.5128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504230976104736 \tStep Time:  0.005984067916870117 s \tTotal Time:  31.64732789993286 s \n",
      "\n",
      "\n",
      "\tEpisode 4688 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5405],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.5441]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524087965488434 \tStep Time:  0.006981372833251953 s \tTotal Time:  31.654309272766113 s \n",
      "\n",
      "\n",
      "\tEpisode 4689 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5303],\n",
      "        [1.0000, 0.5230]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.495146751403809 \tStep Time:  0.0071184635162353516 s \tTotal Time:  31.66142773628235 s \n",
      "\n",
      "\n",
      "\tEpisode 4690 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5331],\n",
      "        [1.0000, 0.4727]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4864],\n",
      "        [1.0000, 0.4562]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.552186965942383 \tStep Time:  0.005846977233886719 s \tTotal Time:  31.667274713516235 s \n",
      "\n",
      "\n",
      "\tEpisode 4691 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5203],\n",
      "        [1.0000, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4658],\n",
      "        [1.0000, 0.4614]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503798007965088 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.674256324768066 s \n",
      "\n",
      "\n",
      "\tEpisode 4692 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4586],\n",
      "        [1.0000, 0.4696]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5147],\n",
      "        [1.0000, 0.5171]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.451308250427246 \tStep Time:  0.006987810134887695 s \tTotal Time:  31.682240962982178 s \n",
      "\n",
      "\n",
      "\tEpisode 4693 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4533],\n",
      "        [1.0000, 0.5224]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4976],\n",
      "        [1.0000, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542310953140259 \tStep Time:  0.006974935531616211 s \tTotal Time:  31.689215898513794 s \n",
      "\n",
      "\n",
      "\tEpisode 4694 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4561],\n",
      "        [1.0000, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5252],\n",
      "        [1.0000, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476737022399902 \tStep Time:  0.007979154586791992 s \tTotal Time:  31.697195053100586 s \n",
      "\n",
      "\n",
      "\tEpisode 4695 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4660],\n",
      "        [1.0000, 0.4574]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5275],\n",
      "        [1.0000, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.557871341705322 \tStep Time:  0.007978439331054688 s \tTotal Time:  31.70517349243164 s \n",
      "\n",
      "\n",
      "\tEpisode 4696 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4979],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4773],\n",
      "        [1.0000, 0.4838]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514730453491211 \tStep Time:  0.006982326507568359 s \tTotal Time:  31.71215581893921 s \n",
      "\n",
      "\n",
      "\tEpisode 4697 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5028],\n",
      "        [1.0000, 0.5050]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5076],\n",
      "        [1.0000, 0.5095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503906726837158 \tStep Time:  0.00797724723815918 s \tTotal Time:  31.720133066177368 s \n",
      "\n",
      "\n",
      "\tEpisode 4698 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4663],\n",
      "        [1.0000, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4981],\n",
      "        [1.0000, 0.5370]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.469740867614746 \tStep Time:  0.005984783172607422 s \tTotal Time:  31.726117849349976 s \n",
      "\n",
      "\n",
      "\tEpisode 4699 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.4832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5636],\n",
      "        [1.0000, 0.5229]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475899755954742 \tStep Time:  0.007978200912475586 s \tTotal Time:  31.73409605026245 s \n",
      "\n",
      "\n",
      "\tEpisode 4700 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.4888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3756],\n",
      "        [1.0000, 0.4718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.580242156982422 \tStep Time:  0.00598454475402832 s \tTotal Time:  31.74008059501648 s \n",
      "\n",
      "\n",
      "\tEpisode 4701 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4074],\n",
      "        [1.0000, 0.5144]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5091],\n",
      "        [1.0000, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.461402416229248 \tStep Time:  0.00698089599609375 s \tTotal Time:  31.747061491012573 s \n",
      "\n",
      "\n",
      "\tEpisode 4702 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6100],\n",
      "        [1.0000, 0.4930]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5397],\n",
      "        [1.0000, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.553788661956787 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.754043102264404 s \n",
      "\n",
      "\n",
      "\tEpisode 4703 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4740],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4124],\n",
      "        [1.0000, 0.6242]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542633533477783 \tStep Time:  0.005984067916870117 s \tTotal Time:  31.760027170181274 s \n",
      "\n",
      "\n",
      "\tEpisode 4704 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6633],\n",
      "        [1.0000, 0.5402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6008],\n",
      "        [1.0000, 0.4830]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.39903736114502 \tStep Time:  0.00698542594909668 s \tTotal Time:  31.76701259613037 s \n",
      "\n",
      "\n",
      "\tEpisode 4705 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4672],\n",
      "        [1.0000, 0.5300]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3518],\n",
      "        [1.0000, 0.5309]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.455422401428223 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.773994207382202 s \n",
      "\n",
      "\n",
      "\tEpisode 4706 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4763],\n",
      "        [1.0000, 0.4151]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [1.0000, 0.5326]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525990962982178 \tStep Time:  0.006983041763305664 s \tTotal Time:  31.780977249145508 s \n",
      "\n",
      "\n",
      "\tEpisode 4707 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5644],\n",
      "        [1.0000, 0.6232]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3967],\n",
      "        [1.0000, 0.3298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.594537258148193 \tStep Time:  0.006979942321777344 s \tTotal Time:  31.787957191467285 s \n",
      "\n",
      "\n",
      "\tEpisode 4708 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5262],\n",
      "        [1.0000, 0.4911]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5310],\n",
      "        [1.0000, 0.4475]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446503937244415 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.794938802719116 s \n",
      "\n",
      "\n",
      "\tEpisode 4709 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4869],\n",
      "        [1.0000, 0.4431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4687],\n",
      "        [1.0000, 0.6542]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.637505829334259 \tStep Time:  0.0069806575775146484 s \tTotal Time:  31.80191946029663 s \n",
      "\n",
      "\n",
      "\tEpisode 4710 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4292],\n",
      "        [1.0000, 0.5860]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5635],\n",
      "        [1.0000, 0.2933]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.618837833404541 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.808901071548462 s \n",
      "\n",
      "\n",
      "\tEpisode 4711 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4485],\n",
      "        [1.0000, 0.4796]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5165],\n",
      "        [1.0000, 0.5578]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.547096729278564 \tStep Time:  0.005984067916870117 s \tTotal Time:  31.814885139465332 s \n",
      "\n",
      "\n",
      "\tEpisode 4712 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5302],\n",
      "        [1.0000, 0.4489]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3611],\n",
      "        [1.0000, 0.5176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.398181915283203 \tStep Time:  0.005983591079711914 s \tTotal Time:  31.821866035461426 s \n",
      "\n",
      "\n",
      "\tEpisode 4713 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3080],\n",
      "        [1.0000, 0.5175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5272],\n",
      "        [1.0000, 0.4500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60453176498413 \tStep Time:  0.005985260009765625 s \tTotal Time:  31.82785129547119 s \n",
      "\n",
      "\n",
      "\tEpisode 4714 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5188],\n",
      "        [1.0000, 0.5875]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5285],\n",
      "        [1.0000, 0.5098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477406978607178 \tStep Time:  0.0069806575775146484 s \tTotal Time:  31.834831953048706 s \n",
      "\n",
      "\n",
      "\tEpisode 4715 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.6375]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5786],\n",
      "        [1.0000, 0.4386]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.445860385894775 \tStep Time:  0.006981372833251953 s \tTotal Time:  31.841813325881958 s \n",
      "\n",
      "\n",
      "\tEpisode 4716 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5148],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4421],\n",
      "        [1.0000, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47692346572876 \tStep Time:  0.005983114242553711 s \tTotal Time:  31.84779644012451 s \n",
      "\n",
      "\n",
      "\tEpisode 4717 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5385],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.4377]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525983810424805 \tStep Time:  0.005984067916870117 s \tTotal Time:  31.854777812957764 s \n",
      "\n",
      "\n",
      "\tEpisode 4718 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5599],\n",
      "        [1.0000, 0.5998]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4939],\n",
      "        [1.0000, 0.5436]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.452640533447266 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.861759424209595 s \n",
      "\n",
      "\n",
      "\tEpisode 4719 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4697],\n",
      "        [1.0000, 0.3676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5681],\n",
      "        [1.0000, 0.4509]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.432835102081299 \tStep Time:  0.005984306335449219 s \tTotal Time:  31.867743730545044 s \n",
      "\n",
      "\n",
      "\tEpisode 4720 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5666],\n",
      "        [1.0000, 0.6007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5484],\n",
      "        [1.0000, 0.5950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.565468430519104 \tStep Time:  0.006980419158935547 s \tTotal Time:  31.87472414970398 s \n",
      "\n",
      "\n",
      "\tEpisode 4721 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5369],\n",
      "        [1.0000, 0.5204]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3496],\n",
      "        [1.0000, 0.4629]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45519232749939 \tStep Time:  0.006981849670410156 s \tTotal Time:  31.88170599937439 s \n",
      "\n",
      "\n",
      "\tEpisode 4722 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3997],\n",
      "        [1.0000, 0.6756]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4774],\n",
      "        [1.0000, 0.4166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.699921667575836 \tStep Time:  0.006981849670410156 s \tTotal Time:  31.8886878490448 s \n",
      "\n",
      "\n",
      "\tEpisode 4723 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5231],\n",
      "        [1.0000, 0.4185]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4585],\n",
      "        [1.0000, 0.4417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493351459503174 \tStep Time:  0.0069806575775146484 s \tTotal Time:  31.895668506622314 s \n",
      "\n",
      "\n",
      "\tEpisode 4724 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3975],\n",
      "        [1.0000, 0.4829]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4918],\n",
      "        [1.0000, 0.4618]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54939579963684 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.902650117874146 s \n",
      "\n",
      "\n",
      "\tEpisode 4725 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4937],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4932],\n",
      "        [1.0000, 0.3966]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57911241054535 \tStep Time:  0.009974241256713867 s \tTotal Time:  31.91262435913086 s \n",
      "\n",
      "\n",
      "\tEpisode 4726 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3624],\n",
      "        [1.0000, 0.3755]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4995],\n",
      "        [1.0000, 0.5244]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506640553474426 \tStep Time:  0.007978200912475586 s \tTotal Time:  31.920602560043335 s \n",
      "\n",
      "\n",
      "\tEpisode 4727 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3684],\n",
      "        [1.0000, 0.3871]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4141],\n",
      "        [1.0000, 0.4482]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.581939697265625 \tStep Time:  0.006981849670410156 s \tTotal Time:  31.927584409713745 s \n",
      "\n",
      "\n",
      "\tEpisode 4728 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.5920]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.6299]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50663423538208 \tStep Time:  0.007977485656738281 s \tTotal Time:  31.935561895370483 s \n",
      "\n",
      "\n",
      "\tEpisode 4729 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5408],\n",
      "        [1.0000, 0.4045]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6325],\n",
      "        [1.0000, 0.5311]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.63477897644043 \tStep Time:  0.006981372833251953 s \tTotal Time:  31.942543268203735 s \n",
      "\n",
      "\n",
      "\tEpisode 4730 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5039],\n",
      "        [1.0000, 0.6032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5264],\n",
      "        [1.0000, 0.5341]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493272304534912 \tStep Time:  0.0069828033447265625 s \tTotal Time:  31.949526071548462 s \n",
      "\n",
      "\n",
      "\tEpisode 4731 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5411],\n",
      "        [1.0000, 0.5298]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5557],\n",
      "        [1.0000, 0.4547]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45684289932251 \tStep Time:  0.005983591079711914 s \tTotal Time:  31.956509113311768 s \n",
      "\n",
      "\n",
      "\tEpisode 4732 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5378],\n",
      "        [1.0000, 0.5367]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5353],\n",
      "        [1.0000, 0.5643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527194023132324 \tStep Time:  0.006981611251831055 s \tTotal Time:  31.9634907245636 s \n",
      "\n",
      "\n",
      "\tEpisode 4733 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5355]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5362],\n",
      "        [1.0000, 0.5277]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516735553741455 \tStep Time:  0.006986856460571289 s \tTotal Time:  31.97047758102417 s \n",
      "\n",
      "\n",
      "\tEpisode 4734 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4572],\n",
      "        [1.0000, 0.5338]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4770],\n",
      "        [1.0000, 0.5240]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52272641658783 \tStep Time:  0.005984306335449219 s \tTotal Time:  31.97646188735962 s \n",
      "\n",
      "\n",
      "\tEpisode 4735 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5148],\n",
      "        [1.0000, 0.4460]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5142],\n",
      "        [1.0000, 0.5254]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46854543685913 \tStep Time:  0.00797891616821289 s \tTotal Time:  31.984440803527832 s \n",
      "\n",
      "\n",
      "\tEpisode 4736 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.4356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5091],\n",
      "        [1.0000, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465907990932465 \tStep Time:  0.006980419158935547 s \tTotal Time:  31.991421222686768 s \n",
      "\n",
      "\n",
      "\tEpisode 4737 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4291],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5174],\n",
      "        [1.0000, 0.3887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526782035827637 \tStep Time:  0.007021427154541016 s \tTotal Time:  31.99844264984131 s \n",
      "\n",
      "\n",
      "\tEpisode 4738 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5211],\n",
      "        [1.0000, 0.4750]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.4659]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511915683746338 \tStep Time:  0.0069425106048583984 s \tTotal Time:  32.00538516044617 s \n",
      "\n",
      "\n",
      "\tEpisode 4739 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4950],\n",
      "        [1.0000, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.4111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45293802022934 \tStep Time:  0.007978439331054688 s \tTotal Time:  32.01336359977722 s \n",
      "\n",
      "\n",
      "\tEpisode 4740 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.5236]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4544],\n",
      "        [1.0000, 0.3794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.597011089324951 \tStep Time:  0.0069811344146728516 s \tTotal Time:  32.020344734191895 s \n",
      "\n",
      "\n",
      "\tEpisode 4741 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3390],\n",
      "        [1.0000, 0.4951]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5869],\n",
      "        [1.0000, 0.4619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.65996503829956 \tStep Time:  0.005984067916870117 s \tTotal Time:  32.026328802108765 s \n",
      "\n",
      "\n",
      "\tEpisode 4742 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4939],\n",
      "        [1.0000, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5286],\n",
      "        [1.0000, 0.5267]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532360553741455 \tStep Time:  0.008010149002075195 s \tTotal Time:  32.03433895111084 s \n",
      "\n",
      "\n",
      "\tEpisode 4743 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5498],\n",
      "        [1.0000, 0.5414]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5217],\n",
      "        [1.0000, 0.4770]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.490899562835693 \tStep Time:  0.006983757019042969 s \tTotal Time:  32.04132270812988 s \n",
      "\n",
      "\n",
      "\tEpisode 4744 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4906],\n",
      "        [1.0000, 0.5666]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5218],\n",
      "        [1.0000, 0.5339]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47251272201538 \tStep Time:  0.00698089599609375 s \tTotal Time:  32.04830360412598 s \n",
      "\n",
      "\n",
      "\tEpisode 4745 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.4643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5203],\n",
      "        [1.0000, 0.4643]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.558326721191406 \tStep Time:  0.0059816837310791016 s \tTotal Time:  32.054285287857056 s \n",
      "\n",
      "\n",
      "\tEpisode 4746 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.5013]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4814],\n",
      "        [1.0000, 0.5143]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516593933105469 \tStep Time:  0.00794672966003418 s \tTotal Time:  32.06223201751709 s \n",
      "\n",
      "\n",
      "\tEpisode 4747 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5107],\n",
      "        [1.0000, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.5112]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499190092086792 \tStep Time:  0.004987478256225586 s \tTotal Time:  32.067219495773315 s \n",
      "\n",
      "\n",
      "\tEpisode 4748 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4940],\n",
      "        [1.0000, 0.5164]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5041],\n",
      "        [1.0000, 0.5195]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4856538772583 \tStep Time:  0.005983114242553711 s \tTotal Time:  32.07320261001587 s \n",
      "\n",
      "\n",
      "\tEpisode 4749 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5215],\n",
      "        [1.0000, 0.5182]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505388736724854 \tStep Time:  0.006981611251831055 s \tTotal Time:  32.0801842212677 s \n",
      "\n",
      "\n",
      "\tEpisode 4750 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5101],\n",
      "        [1.0000, 0.5107]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.5200]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506606101989746 \tStep Time:  0.00698089599609375 s \tTotal Time:  32.087165117263794 s \n",
      "\n",
      "\n",
      "\tEpisode 4751 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4987],\n",
      "        [1.0000, 0.5205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5228],\n",
      "        [1.0000, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49843692779541 \tStep Time:  0.00698089599609375 s \tTotal Time:  32.09414601325989 s \n",
      "\n",
      "\n",
      "\tEpisode 4752 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5023],\n",
      "        [1.0000, 0.5237]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4986],\n",
      "        [1.0000, 0.5181]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503636360168457 \tStep Time:  0.006981372833251953 s \tTotal Time:  32.10112738609314 s \n",
      "\n",
      "\n",
      "\tEpisode 4753 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4875],\n",
      "        [1.0000, 0.5046]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5203],\n",
      "        [1.0000, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.485156774520874 \tStep Time:  0.006982326507568359 s \tTotal Time:  32.10810971260071 s \n",
      "\n",
      "\n",
      "\tEpisode 4754 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5174],\n",
      "        [1.0000, 0.4963]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507351875305176 \tStep Time:  0.0070154666900634766 s \tTotal Time:  32.11512517929077 s \n",
      "\n",
      "\n",
      "\tEpisode 4755 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4795],\n",
      "        [1.0000, 0.4983]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4975],\n",
      "        [1.0000, 0.4950]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.492828845977783 \tStep Time:  0.0069844722747802734 s \tTotal Time:  32.12210965156555 s \n",
      "\n",
      "\n",
      "\tEpisode 4756 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4801],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5064],\n",
      "        [1.0000, 0.5038]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51806640625 \tStep Time:  0.005947113037109375 s \tTotal Time:  32.12805676460266 s \n",
      "\n",
      "\n",
      "\tEpisode 4757 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4884],\n",
      "        [1.0000, 0.5033]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503897070884705 \tStep Time:  0.007978439331054688 s \tTotal Time:  32.136035203933716 s \n",
      "\n",
      "\n",
      "\tEpisode 4758 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.5001]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497444331645966 \tStep Time:  0.005983591079711914 s \tTotal Time:  32.14201879501343 s \n",
      "\n",
      "\n",
      "\tEpisode 4759 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4988],\n",
      "        [1.0000, 0.4787]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4891],\n",
      "        [1.0000, 0.4676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500511407852173 \tStep Time:  0.0069849491119384766 s \tTotal Time:  32.149003744125366 s \n",
      "\n",
      "\n",
      "\tEpisode 4760 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4995],\n",
      "        [1.0000, 0.4474]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4916],\n",
      "        [1.0000, 0.4986]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524110078811646 \tStep Time:  0.00598597526550293 s \tTotal Time:  32.15498971939087 s \n",
      "\n",
      "\n",
      "\tEpisode 4761 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5013],\n",
      "        [1.0000, 0.4852]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4869],\n",
      "        [1.0000, 0.4519]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526944041252136 \tStep Time:  0.00797891616821289 s \tTotal Time:  32.16296863555908 s \n",
      "\n",
      "\n",
      "\tEpisode 4762 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5009],\n",
      "        [1.0000, 0.5026]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.5104]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510128498077393 \tStep Time:  0.0069844722747802734 s \tTotal Time:  32.16995310783386 s \n",
      "\n",
      "\n",
      "\tEpisode 4763 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5067],\n",
      "        [1.0000, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4689],\n",
      "        [1.0000, 0.4841]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.531455039978027 \tStep Time:  0.006981849670410156 s \tTotal Time:  32.17693495750427 s \n",
      "\n",
      "\n",
      "\tEpisode 4764 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.4579]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4811],\n",
      "        [1.0000, 0.5036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478873074054718 \tStep Time:  0.006979942321777344 s \tTotal Time:  32.18391489982605 s \n",
      "\n",
      "\n",
      "\tEpisode 4765 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4963],\n",
      "        [1.0000, 0.4777]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5007],\n",
      "        [1.0000, 0.4732]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49635362625122 \tStep Time:  0.006981372833251953 s \tTotal Time:  32.1908962726593 s \n",
      "\n",
      "\n",
      "\tEpisode 4766 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5109],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5079],\n",
      "        [1.0000, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497840940952301 \tStep Time:  0.00797891616821289 s \tTotal Time:  32.198875188827515 s \n",
      "\n",
      "\n",
      "\tEpisode 4767 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.5093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5092],\n",
      "        [1.0000, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502275049686432 \tStep Time:  0.005983829498291016 s \tTotal Time:  32.204859018325806 s \n",
      "\n",
      "\n",
      "\tEpisode 4768 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5099],\n",
      "        [1.0000, 0.5095]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5052],\n",
      "        [1.0000, 0.4934]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511435985565186 \tStep Time:  0.007979154586791992 s \tTotal Time:  32.2128381729126 s \n",
      "\n",
      "\n",
      "\tEpisode 4769 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5077],\n",
      "        [1.0000, 0.4819]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4550],\n",
      "        [1.0000, 0.5032]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517566680908203 \tStep Time:  0.008975505828857422 s \tTotal Time:  32.221813678741455 s \n",
      "\n",
      "\n",
      "\tEpisode 4770 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4591],\n",
      "        [1.0000, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5301],\n",
      "        [1.0000, 0.4794]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54780626296997 \tStep Time:  0.007980108261108398 s \tTotal Time:  32.22979378700256 s \n",
      "\n",
      "\n",
      "\tEpisode 4771 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4636],\n",
      "        [1.0000, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5120],\n",
      "        [1.0000, 0.5125]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532008171081543 \tStep Time:  0.00701451301574707 s \tTotal Time:  32.23680830001831 s \n",
      "\n",
      "\n",
      "\tEpisode 4772 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5102],\n",
      "        [1.0000, 0.5130]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5089],\n",
      "        [1.0000, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505396842956543 \tStep Time:  0.006979227066040039 s \tTotal Time:  32.24378752708435 s \n",
      "\n",
      "\n",
      "\tEpisode 4773 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4812],\n",
      "        [1.0000, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5067],\n",
      "        [1.0000, 0.5176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5182204246521 \tStep Time:  0.006981611251831055 s \tTotal Time:  32.25076913833618 s \n",
      "\n",
      "\n",
      "\tEpisode 4774 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4840],\n",
      "        [1.0000, 0.4940]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4756],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.48069429397583 \tStep Time:  0.007978200912475586 s \tTotal Time:  32.25874733924866 s \n",
      "\n",
      "\n",
      "\tEpisode 4775 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4769],\n",
      "        [1.0000, 0.4942]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5159],\n",
      "        [1.0000, 0.5167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51133108139038 \tStep Time:  0.00894474983215332 s \tTotal Time:  32.26769208908081 s \n",
      "\n",
      "\n",
      "\tEpisode 4776 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4679],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4681],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.44984769821167 \tStep Time:  0.006981372833251953 s \tTotal Time:  32.27467346191406 s \n",
      "\n",
      "\n",
      "\tEpisode 4777 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4877],\n",
      "        [1.0000, 0.5356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5434],\n",
      "        [1.0000, 0.5348]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484151005744934 \tStep Time:  0.010004281997680664 s \tTotal Time:  32.28467774391174 s \n",
      "\n",
      "\n",
      "\tEpisode 4778 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5513],\n",
      "        [1.0000, 0.4468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4968],\n",
      "        [1.0000, 0.5552]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.529279172420502 \tStep Time:  0.006949186325073242 s \tTotal Time:  32.291626930236816 s \n",
      "\n",
      "\n",
      "\tEpisode 4779 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5573],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4349],\n",
      "        [1.0000, 0.4402]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53385591506958 \tStep Time:  0.007979631423950195 s \tTotal Time:  32.30060410499573 s \n",
      "\n",
      "\n",
      "\tEpisode 4780 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5768],\n",
      "        [1.0000, 0.4145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5584],\n",
      "        [1.0000, 0.4895]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.393922805786133 \tStep Time:  0.006980419158935547 s \tTotal Time:  32.30758452415466 s \n",
      "\n",
      "\n",
      "\tEpisode 4781 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4864],\n",
      "        [1.0000, 0.4993]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4341],\n",
      "        [1.0000, 0.4779]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.533342003822327 \tStep Time:  0.008011817932128906 s \tTotal Time:  32.31559634208679 s \n",
      "\n",
      "\n",
      "\tEpisode 4782 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5161],\n",
      "        [1.0000, 0.4270]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5115],\n",
      "        [1.0000, 0.6870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.648929119110107 \tStep Time:  0.010970115661621094 s \tTotal Time:  32.327531576156616 s \n",
      "\n",
      "\n",
      "\tEpisode 4783 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5083],\n",
      "        [1.0000, 0.6025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563325762748718 \tStep Time:  0.007980823516845703 s \tTotal Time:  32.33551239967346 s \n",
      "\n",
      "\n",
      "\tEpisode 4784 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4981],\n",
      "        [1.0000, 0.5072]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5386],\n",
      "        [1.0000, 0.4648]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504273891448975 \tStep Time:  0.008974075317382812 s \tTotal Time:  32.344486474990845 s \n",
      "\n",
      "\n",
      "\tEpisode 4785 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5563],\n",
      "        [1.0000, 0.4347]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4343],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.55519962310791 \tStep Time:  0.00798344612121582 s \tTotal Time:  32.35246992111206 s \n",
      "\n",
      "\n",
      "\tEpisode 4786 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4803],\n",
      "        [1.0000, 0.4845]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4291],\n",
      "        [1.0000, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53276014328003 \tStep Time:  0.006981372833251953 s \tTotal Time:  32.35945129394531 s \n",
      "\n",
      "\n",
      "\tEpisode 4787 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4425],\n",
      "        [1.0000, 0.4507]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5212],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.442203521728516 \tStep Time:  0.00698399543762207 s \tTotal Time:  32.366435289382935 s \n",
      "\n",
      "\n",
      "\tEpisode 4788 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5246],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4574],\n",
      "        [1.0000, 0.4926]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4820237159729 \tStep Time:  0.006981849670410156 s \tTotal Time:  32.373417139053345 s \n",
      "\n",
      "\n",
      "\tEpisode 4789 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4671],\n",
      "        [1.0000, 0.4500]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5153],\n",
      "        [1.0000, 0.5180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515267372131348 \tStep Time:  0.007979631423950195 s \tTotal Time:  32.381396770477295 s \n",
      "\n",
      "\n",
      "\tEpisode 4790 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4826],\n",
      "        [1.0000, 0.5027]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4587],\n",
      "        [1.0000, 0.4596]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51543140411377 \tStep Time:  0.006986379623413086 s \tTotal Time:  32.38838315010071 s \n",
      "\n",
      "\n",
      "\tEpisode 4791 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5265],\n",
      "        [1.0000, 0.4687]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.4879]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544431686401367 \tStep Time:  0.012971639633178711 s \tTotal Time:  32.40135478973389 s \n",
      "\n",
      "\n",
      "\tEpisode 4792 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5316],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.5220]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.513566434383392 \tStep Time:  0.01396322250366211 s \tTotal Time:  32.41531801223755 s \n",
      "\n",
      "\n",
      "\tEpisode 4793 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5097],\n",
      "        [1.0000, 0.5084]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4686],\n",
      "        [1.0000, 0.4953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518456220626831 \tStep Time:  0.009985685348510742 s \tTotal Time:  32.42530369758606 s \n",
      "\n",
      "\n",
      "\tEpisode 4794 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.4898]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5176],\n",
      "        [1.0000, 0.5225]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484018802642822 \tStep Time:  0.009960651397705078 s \tTotal Time:  32.435264348983765 s \n",
      "\n",
      "\n",
      "\tEpisode 4795 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5139],\n",
      "        [1.0000, 0.5154]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510522842407227 \tStep Time:  0.005995988845825195 s \tTotal Time:  32.44126033782959 s \n",
      "\n",
      "\n",
      "\tEpisode 4796 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5367],\n",
      "        [1.0000, 0.5010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5082],\n",
      "        [1.0000, 0.5188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51634407043457 \tStep Time:  0.00696873664855957 s \tTotal Time:  32.44822907447815 s \n",
      "\n",
      "\n",
      "\tEpisode 4797 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5142],\n",
      "        [1.0000, 0.5199]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5156],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504964232444763 \tStep Time:  0.005984306335449219 s \tTotal Time:  32.4542133808136 s \n",
      "\n",
      "\n",
      "\tEpisode 4798 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5188],\n",
      "        [1.0000, 0.5067]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512445449829102 \tStep Time:  0.006979465484619141 s \tTotal Time:  32.46119284629822 s \n",
      "\n",
      "\n",
      "\tEpisode 4799 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5095],\n",
      "        [1.0000, 0.5066]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.5131]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.487955093383789 \tStep Time:  0.008013248443603516 s \tTotal Time:  32.46920609474182 s \n",
      "\n",
      "\n",
      "\tEpisode 4800 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5035],\n",
      "        [1.0000, 0.5159]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.5240]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514267206192017 \tStep Time:  0.0059854984283447266 s \tTotal Time:  32.475191593170166 s \n",
      "\n",
      "\n",
      "\tEpisode 4801 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5060],\n",
      "        [1.0000, 0.5139]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5035],\n",
      "        [1.0000, 0.5108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500458717346191 \tStep Time:  0.00797891616821289 s \tTotal Time:  32.48317050933838 s \n",
      "\n",
      "\n",
      "\tEpisode 4802 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5105],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5033],\n",
      "        [1.0000, 0.5069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498963356018066 \tStep Time:  0.006978034973144531 s \tTotal Time:  32.49014854431152 s \n",
      "\n",
      "\n",
      "\tEpisode 4803 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5005],\n",
      "        [1.0000, 0.5108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5100],\n",
      "        [1.0000, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49782121181488 \tStep Time:  0.0069806575775146484 s \tTotal Time:  32.49712920188904 s \n",
      "\n",
      "\n",
      "\tEpisode 4804 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5032],\n",
      "        [1.0000, 0.5056]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5608],\n",
      "        [1.0000, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.473569869995117 \tStep Time:  0.00797891616821289 s \tTotal Time:  32.50510811805725 s \n",
      "\n",
      "\n",
      "\tEpisode 4805 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4868],\n",
      "        [1.0000, 0.5289]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5034],\n",
      "        [1.0000, 0.4623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526846408843994 \tStep Time:  0.007946491241455078 s \tTotal Time:  32.513054609298706 s \n",
      "\n",
      "\n",
      "\tEpisode 4806 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4765],\n",
      "        [1.0000, 0.5077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.5094]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516129493713379 \tStep Time:  0.006982088088989258 s \tTotal Time:  32.520036697387695 s \n",
      "\n",
      "\n",
      "\tEpisode 4807 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4569],\n",
      "        [1.0000, 0.4417]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5179],\n",
      "        [1.0000, 0.5121]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569255352020264 \tStep Time:  0.006980419158935547 s \tTotal Time:  32.52701711654663 s \n",
      "\n",
      "\n",
      "\tEpisode 4808 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5120]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5113],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502949714660645 \tStep Time:  0.007979393005371094 s \tTotal Time:  32.534996509552 s \n",
      "\n",
      "\n",
      "\tEpisode 4809 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4809],\n",
      "        [1.0000, 0.4913]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5144],\n",
      "        [1.0000, 0.5126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511452198028564 \tStep Time:  0.006981611251831055 s \tTotal Time:  32.54197812080383 s \n",
      "\n",
      "\n",
      "\tEpisode 4810 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4227],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5048]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.467838287353516 \tStep Time:  0.007978439331054688 s \tTotal Time:  32.54995656013489 s \n",
      "\n",
      "\n",
      "\tEpisode 4811 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5088],\n",
      "        [1.0000, 0.5085]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5067],\n",
      "        [1.0000, 0.5061]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503376007080078 \tStep Time:  0.00698089599609375 s \tTotal Time:  32.55693745613098 s \n",
      "\n",
      "\n",
      "\tEpisode 4812 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5556],\n",
      "        [1.0000, 0.5073]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.532604217529297 \tStep Time:  0.006982564926147461 s \tTotal Time:  32.56491708755493 s \n",
      "\n",
      "\n",
      "\tEpisode 4813 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5073],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5042],\n",
      "        [1.0000, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501863956451416 \tStep Time:  0.006979942321777344 s \tTotal Time:  32.57189702987671 s \n",
      "\n",
      "\n",
      "\tEpisode 4814 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5057],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4955],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50531530380249 \tStep Time:  0.006982088088989258 s \tTotal Time:  32.5788791179657 s \n",
      "\n",
      "\n",
      "\tEpisode 4815 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4757],\n",
      "        [1.0000, 0.4686]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4427],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46448278427124 \tStep Time:  0.006980419158935547 s \tTotal Time:  32.585859537124634 s \n",
      "\n",
      "\n",
      "\tEpisode 4816 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4658],\n",
      "        [1.0000, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4212],\n",
      "        [1.0000, 0.4925]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489375591278076 \tStep Time:  0.006981611251831055 s \tTotal Time:  32.592841148376465 s \n",
      "\n",
      "\n",
      "\tEpisode 4817 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5026],\n",
      "        [1.0000, 0.5037]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5323],\n",
      "        [1.0000, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47304916381836 \tStep Time:  0.006982564926147461 s \tTotal Time:  32.59982371330261 s \n",
      "\n",
      "\n",
      "\tEpisode 4818 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5120],\n",
      "        [1.0000, 0.5113]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5433],\n",
      "        [1.0000, 0.5499]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508255243301392 \tStep Time:  0.007977962493896484 s \tTotal Time:  32.60780167579651 s \n",
      "\n",
      "\n",
      "\tEpisode 4819 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4688],\n",
      "        [1.0000, 0.5101]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5129],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522718906402588 \tStep Time:  0.007978200912475586 s \tTotal Time:  32.615779876708984 s \n",
      "\n",
      "\n",
      "\tEpisode 4820 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4973],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5137],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.497063636779785 \tStep Time:  0.008978128433227539 s \tTotal Time:  32.62475800514221 s \n",
      "\n",
      "\n",
      "\tEpisode 4821 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3283],\n",
      "        [1.0000, 0.3952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5540],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.590977668762207 \tStep Time:  0.007979154586791992 s \tTotal Time:  32.632737159729004 s \n",
      "\n",
      "\n",
      "\tEpisode 4822 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4495],\n",
      "        [1.0000, 0.4607]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5023],\n",
      "        [1.0000, 0.5047]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503212809562683 \tStep Time:  0.006979942321777344 s \tTotal Time:  32.63971710205078 s \n",
      "\n",
      "\n",
      "\tEpisode 4823 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5061],\n",
      "        [1.0000, 0.4069]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.5161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.572102069854736 \tStep Time:  0.006983041763305664 s \tTotal Time:  32.64670014381409 s \n",
      "\n",
      "\n",
      "\tEpisode 4824 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5004],\n",
      "        [1.0000, 0.5256]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5593]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.486157894134521 \tStep Time:  0.006979703903198242 s \tTotal Time:  32.653679847717285 s \n",
      "\n",
      "\n",
      "\tEpisode 4825 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5145],\n",
      "        [1.0000, 0.5035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5195],\n",
      "        [1.0000, 0.5070]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.503409683704376 \tStep Time:  0.007979869842529297 s \tTotal Time:  32.661659717559814 s \n",
      "\n",
      "\n",
      "\tEpisode 4826 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5104],\n",
      "        [1.0000, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5185],\n",
      "        [1.0000, 0.5105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506875991821289 \tStep Time:  0.008006811141967773 s \tTotal Time:  32.66966652870178 s \n",
      "\n",
      "\n",
      "\tEpisode 4827 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5288],\n",
      "        [1.0000, 0.5124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496324896812439 \tStep Time:  0.004990816116333008 s \tTotal Time:  32.674657344818115 s \n",
      "\n",
      "\n",
      "\tEpisode 4828 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5132],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5167],\n",
      "        [1.0000, 0.5166]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502911806106567 \tStep Time:  0.007956504821777344 s \tTotal Time:  32.68261384963989 s \n",
      "\n",
      "\n",
      "\tEpisode 4829 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5146],\n",
      "        [1.0000, 0.5187]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.5108]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50066477060318 \tStep Time:  0.0059719085693359375 s \tTotal Time:  32.68858575820923 s \n",
      "\n",
      "\n",
      "\tEpisode 4830 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5209],\n",
      "        [1.0000, 0.5217]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5497],\n",
      "        [1.0000, 0.5213]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518787384033203 \tStep Time:  0.00698089599609375 s \tTotal Time:  32.69556665420532 s \n",
      "\n",
      "\n",
      "\tEpisode 4831 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5251],\n",
      "        [1.0000, 0.5556]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.4710]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504582405090332 \tStep Time:  0.0070154666900634766 s \tTotal Time:  32.702582120895386 s \n",
      "\n",
      "\n",
      "\tEpisode 4832 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5252],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5235],\n",
      "        [1.0000, 0.4731]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49414348602295 \tStep Time:  0.006983280181884766 s \tTotal Time:  32.70956540107727 s \n",
      "\n",
      "\n",
      "\tEpisode 4833 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5213],\n",
      "        [1.0000, 0.4944]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5109],\n",
      "        [1.0000, 0.4625]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.482590675354004 \tStep Time:  0.007946252822875977 s \tTotal Time:  32.71751165390015 s \n",
      "\n",
      "\n",
      "\tEpisode 4834 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5020],\n",
      "        [1.0000, 0.5828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4830],\n",
      "        [1.0000, 0.5304]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522644519805908 \tStep Time:  0.0069811344146728516 s \tTotal Time:  32.72449278831482 s \n",
      "\n",
      "\n",
      "\tEpisode 4835 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5231],\n",
      "        [1.0000, 0.5471]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5626],\n",
      "        [1.0000, 0.4935]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45969808101654 \tStep Time:  0.008976459503173828 s \tTotal Time:  32.73346924781799 s \n",
      "\n",
      "\n",
      "\tEpisode 4836 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4898],\n",
      "        [1.0000, 0.4290]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5045],\n",
      "        [1.0000, 0.6995]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.593512535095215 \tStep Time:  0.006979703903198242 s \tTotal Time:  32.74044895172119 s \n",
      "\n",
      "\n",
      "\tEpisode 4837 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6475],\n",
      "        [1.0000, 0.4793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4601],\n",
      "        [1.0000, 0.3263]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.546561181545258 \tStep Time:  0.007978677749633789 s \tTotal Time:  32.748427629470825 s \n",
      "\n",
      "\n",
      "\tEpisode 4838 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.4333]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3822],\n",
      "        [1.0000, 0.4828]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505847930908203 \tStep Time:  0.00698089599609375 s \tTotal Time:  32.75540852546692 s \n",
      "\n",
      "\n",
      "\tEpisode 4839 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4598],\n",
      "        [1.0000, 0.5381]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4980],\n",
      "        [1.0000, 0.5495]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.526835262775421 \tStep Time:  0.006981849670410156 s \tTotal Time:  32.76338815689087 s \n",
      "\n",
      "\n",
      "\tEpisode 4840 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4906],\n",
      "        [1.0000, 0.4555]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4458],\n",
      "        [1.0000, 0.5245]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52802562713623 \tStep Time:  0.007977962493896484 s \tTotal Time:  32.771366119384766 s \n",
      "\n",
      "\n",
      "\tEpisode 4841 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3886],\n",
      "        [1.0000, 0.4967]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3013],\n",
      "        [1.0000, 0.4042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.626161575317383 \tStep Time:  0.006982326507568359 s \tTotal Time:  32.778348445892334 s \n",
      "\n",
      "\n",
      "\tEpisode 4842 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4408],\n",
      "        [1.0000, 0.3710]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3784],\n",
      "        [1.0000, 0.4403]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.59293520450592 \tStep Time:  0.00797891616821289 s \tTotal Time:  32.78632736206055 s \n",
      "\n",
      "\n",
      "\tEpisode 4843 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4857],\n",
      "        [1.0000, 0.5395]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5153],\n",
      "        [1.0000, 0.4747]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493741989135742 \tStep Time:  0.006982088088989258 s \tTotal Time:  32.793309450149536 s \n",
      "\n",
      "\n",
      "\tEpisode 4844 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5040],\n",
      "        [1.0000, 0.5080]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5110],\n",
      "        [1.0000, 0.5149]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515914916992188 \tStep Time:  0.006979227066040039 s \tTotal Time:  32.800288677215576 s \n",
      "\n",
      "\n",
      "\tEpisode 4845 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5081],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5138],\n",
      "        [1.0000, 0.5176]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507383823394775 \tStep Time:  0.00797891616821289 s \tTotal Time:  32.80826759338379 s \n",
      "\n",
      "\n",
      "\tEpisode 4846 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5084],\n",
      "        [1.0000, 0.5152]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5259],\n",
      "        [1.0000, 0.5215]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504536151885986 \tStep Time:  0.007982015609741211 s \tTotal Time:  32.81624960899353 s \n",
      "\n",
      "\n",
      "\tEpisode 4847 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5155],\n",
      "        [1.0000, 0.5257]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5201],\n",
      "        [1.0000, 0.5054]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.518466770648956 \tStep Time:  0.006978750228881836 s \tTotal Time:  32.82322835922241 s \n",
      "\n",
      "\n",
      "\tEpisode 4848 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5335],\n",
      "        [1.0000, 0.5016]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5240],\n",
      "        [1.0000, 0.5096]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496856212615967 \tStep Time:  0.007977962493896484 s \tTotal Time:  32.83120632171631 s \n",
      "\n",
      "\n",
      "\tEpisode 4849 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5195],\n",
      "        [1.0000, 0.4790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.5129]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49120044708252 \tStep Time:  0.00698089599609375 s \tTotal Time:  32.8381872177124 s \n",
      "\n",
      "\n",
      "\tEpisode 4850 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5108],\n",
      "        [1.0000, 0.5088]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5444],\n",
      "        [1.0000, 0.5065]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521219909191132 \tStep Time:  0.007980585098266602 s \tTotal Time:  32.84616780281067 s \n",
      "\n",
      "\n",
      "\tEpisode 4851 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5185],\n",
      "        [1.0000, 0.5100]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5017],\n",
      "        [1.0000, 0.5746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483586311340332 \tStep Time:  0.006979465484619141 s \tTotal Time:  32.85314726829529 s \n",
      "\n",
      "\n",
      "\tEpisode 4852 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4926],\n",
      "        [1.0000, 0.5671]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4676],\n",
      "        [1.0000, 0.4583]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549840450286865 \tStep Time:  0.006981849670410156 s \tTotal Time:  32.8601291179657 s \n",
      "\n",
      "\n",
      "\tEpisode 4853 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4560],\n",
      "        [1.0000, 0.5219]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5088],\n",
      "        [1.0000, 0.4432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.570397853851318 \tStep Time:  0.007978439331054688 s \tTotal Time:  32.86810755729675 s \n",
      "\n",
      "\n",
      "\tEpisode 4854 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4808],\n",
      "        [1.0000, 0.4802]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5193],\n",
      "        [1.0000, 0.4923]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52483081817627 \tStep Time:  0.007978677749633789 s \tTotal Time:  32.87608623504639 s \n",
      "\n",
      "\n",
      "\tEpisode 4855 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4884],\n",
      "        [1.0000, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4714],\n",
      "        [1.0000, 0.5167]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498644351959229 \tStep Time:  0.007979393005371094 s \tTotal Time:  32.88406562805176 s \n",
      "\n",
      "\n",
      "\tEpisode 4856 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5051],\n",
      "        [1.0000, 0.4480]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5229],\n",
      "        [1.0000, 0.4076]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.536241054534912 \tStep Time:  0.009973764419555664 s \tTotal Time:  32.89403939247131 s \n",
      "\n",
      "\n",
      "\tEpisode 4857 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5126],\n",
      "        [1.0000, 0.5097]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5148],\n",
      "        [1.0000, 0.5140]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506895065307617 \tStep Time:  0.008976221084594727 s \tTotal Time:  32.90301561355591 s \n",
      "\n",
      "\n",
      "\tEpisode 4858 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4989],\n",
      "        [1.0000, 0.5134]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5092],\n",
      "        [1.0000, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49517673254013 \tStep Time:  0.009972333908081055 s \tTotal Time:  32.91298794746399 s \n",
      "\n",
      "\n",
      "\tEpisode 4859 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5114],\n",
      "        [1.0000, 0.4649]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4803],\n",
      "        [1.0000, 0.5180]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512624263763428 \tStep Time:  0.008976936340332031 s \tTotal Time:  32.92196488380432 s \n",
      "\n",
      "\n",
      "\tEpisode 4860 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4881],\n",
      "        [1.0000, 0.5162]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5623],\n",
      "        [1.0000, 0.5011]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.462157726287842 \tStep Time:  0.014959096908569336 s \tTotal Time:  32.93692398071289 s \n",
      "\n",
      "\n",
      "\tEpisode 4861 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5175],\n",
      "        [1.0000, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5150],\n",
      "        [1.0000, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501314640045166 \tStep Time:  0.008977413177490234 s \tTotal Time:  32.94590139389038 s \n",
      "\n",
      "\n",
      "\tEpisode 4862 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5331],\n",
      "        [1.0000, 0.5767]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5438],\n",
      "        [1.0000, 0.5012]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474328994750977 \tStep Time:  0.007977008819580078 s \tTotal Time:  32.95387840270996 s \n",
      "\n",
      "\n",
      "\tEpisode 4863 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5858],\n",
      "        [1.0000, 0.5000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5268]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535178661346436 \tStep Time:  0.007978677749633789 s \tTotal Time:  32.961857080459595 s \n",
      "\n",
      "\n",
      "\tEpisode 4864 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5337],\n",
      "        [1.0000, 0.4243]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4250],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.60303258895874 \tStep Time:  0.007978200912475586 s \tTotal Time:  32.96983528137207 s \n",
      "\n",
      "\n",
      "\tEpisode 4865 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5705],\n",
      "        [1.0000, 0.5421]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4644],\n",
      "        [1.0000, 0.6098]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56997299194336 \tStep Time:  0.006981372833251953 s \tTotal Time:  32.97681665420532 s \n",
      "\n",
      "\n",
      "\tEpisode 4866 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5064],\n",
      "        [1.0000, 0.4437]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4350],\n",
      "        [1.0000, 0.5221]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509616374969482 \tStep Time:  0.00797891616821289 s \tTotal Time:  32.984795570373535 s \n",
      "\n",
      "\n",
      "\tEpisode 4867 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5088],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5878],\n",
      "        [1.0000, 0.4505]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.446174204349518 \tStep Time:  0.0069811344146728516 s \tTotal Time:  32.99177670478821 s \n",
      "\n",
      "\n",
      "\tEpisode 4868 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5074],\n",
      "        [1.0000, 0.4928]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4671],\n",
      "        [1.0000, 0.4521]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542882919311523 \tStep Time:  0.007979154586791992 s \tTotal Time:  32.999755859375 s \n",
      "\n",
      "\n",
      "\tEpisode 4869 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4886],\n",
      "        [1.0000, 0.4559]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4763],\n",
      "        [1.0000, 0.4778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.522101938724518 \tStep Time:  0.007978439331054688 s \tTotal Time:  33.007734298706055 s \n",
      "\n",
      "\n",
      "\tEpisode 4870 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5352],\n",
      "        [1.0000, 0.4999]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5015],\n",
      "        [1.0000, 0.5059]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.525613725185394 \tStep Time:  0.007978439331054688 s \tTotal Time:  33.01571273803711 s \n",
      "\n",
      "\n",
      "\tEpisode 4871 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5533],\n",
      "        [1.0000, 0.5010]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5031]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480482697486877 \tStep Time:  0.006981372833251953 s \tTotal Time:  33.02269411087036 s \n",
      "\n",
      "\n",
      "\tEpisode 4872 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5098],\n",
      "        [1.0000, 0.5090]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5165],\n",
      "        [1.0000, 0.5044]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509374618530273 \tStep Time:  0.007979869842529297 s \tTotal Time:  33.03067398071289 s \n",
      "\n",
      "\n",
      "\tEpisode 4873 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4984],\n",
      "        [1.0000, 0.5036]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5022],\n",
      "        [1.0000, 0.5210]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511691391468048 \tStep Time:  0.007977485656738281 s \tTotal Time:  33.03865146636963 s \n",
      "\n",
      "\n",
      "\tEpisode 4874 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5039],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4804],\n",
      "        [1.0000, 0.4962]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517803192138672 \tStep Time:  0.0069828033447265625 s \tTotal Time:  33.045634269714355 s \n",
      "\n",
      "\n",
      "\tEpisode 4875 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4806],\n",
      "        [1.0000, 0.5145]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4927],\n",
      "        [1.0000, 0.4887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510620594024658 \tStep Time:  0.006979703903198242 s \tTotal Time:  33.052613973617554 s \n",
      "\n",
      "\n",
      "\tEpisode 4876 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4951],\n",
      "        [1.0000, 0.5064]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4869],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520102500915527 \tStep Time:  0.005984067916870117 s \tTotal Time:  33.058598041534424 s \n",
      "\n",
      "\n",
      "\tEpisode 4877 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4775],\n",
      "        [1.0000, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4994],\n",
      "        [1.0000, 0.4839]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.502376914024353 \tStep Time:  0.006981372833251953 s \tTotal Time:  33.065579414367676 s \n",
      "\n",
      "\n",
      "\tEpisode 4878 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5030],\n",
      "        [1.0000, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5103],\n",
      "        [1.0000, 0.5051]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49930453300476 \tStep Time:  0.006981849670410156 s \tTotal Time:  33.072561264038086 s \n",
      "\n",
      "\n",
      "\tEpisode 4879 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4837],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5149],\n",
      "        [1.0000, 0.5169]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51330190896988 \tStep Time:  0.006981372833251953 s \tTotal Time:  33.07954263687134 s \n",
      "\n",
      "\n",
      "\tEpisode 4880 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4875],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4967],\n",
      "        [1.0000, 0.5188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.51282685995102 \tStep Time:  0.006980419158935547 s \tTotal Time:  33.08652305603027 s \n",
      "\n",
      "\n",
      "\tEpisode 4881 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4974],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4886],\n",
      "        [1.0000, 0.4861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505505621433258 \tStep Time:  0.006981372833251953 s \tTotal Time:  33.093504428863525 s \n",
      "\n",
      "\n",
      "\tEpisode 4882 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4953],\n",
      "        [1.0000, 0.4806]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5340],\n",
      "        [1.0000, 0.4591]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494754791259766 \tStep Time:  0.007979631423950195 s \tTotal Time:  33.101484060287476 s \n",
      "\n",
      "\n",
      "\tEpisode 4883 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5312],\n",
      "        [1.0000, 0.5234]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4438],\n",
      "        [1.0000, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.549110412597656 \tStep Time:  0.010970830917358398 s \tTotal Time:  33.112454891204834 s \n",
      "\n",
      "\n",
      "\tEpisode 4884 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4907],\n",
      "        [1.0000, 0.4831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4511],\n",
      "        [1.0000, 0.5099]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527843952178955 \tStep Time:  0.014959573745727539 s \tTotal Time:  33.12741446495056 s \n",
      "\n",
      "\n",
      "\tEpisode 4885 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4882],\n",
      "        [1.0000, 0.5468]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4485],\n",
      "        [1.0000, 0.5479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.424943745136261 \tStep Time:  0.012965202331542969 s \tTotal Time:  33.140379667282104 s \n",
      "\n",
      "\n",
      "\tEpisode 4886 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4507],\n",
      "        [1.0000, 0.4551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5357],\n",
      "        [1.0000, 0.4491]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54988008737564 \tStep Time:  0.010971307754516602 s \tTotal Time:  33.15135097503662 s \n",
      "\n",
      "\n",
      "\tEpisode 4887 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4849],\n",
      "        [1.0000, 0.5431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5633],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.576384544372559 \tStep Time:  0.006980419158935547 s \tTotal Time:  33.15833139419556 s \n",
      "\n",
      "\n",
      "\tEpisode 4888 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4629],\n",
      "        [1.0000, 0.5463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5544],\n",
      "        [1.0000, 0.5448]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.458486557006836 \tStep Time:  0.007981538772583008 s \tTotal Time:  33.1673104763031 s \n",
      "\n",
      "\n",
      "\tEpisode 4889 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5426],\n",
      "        [1.0000, 0.5484]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5834],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.46947181224823 \tStep Time:  0.0069811344146728516 s \tTotal Time:  33.17429161071777 s \n",
      "\n",
      "\n",
      "\tEpisode 4890 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4623],\n",
      "        [1.0000, 0.4644]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4596],\n",
      "        [1.0000, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47630786895752 \tStep Time:  0.007979154586791992 s \tTotal Time:  33.182270765304565 s \n",
      "\n",
      "\n",
      "\tEpisode 4891 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5525],\n",
      "        [1.0000, 0.5147]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5466],\n",
      "        [1.0000, 0.4598]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.535052299499512 \tStep Time:  0.007978200912475586 s \tTotal Time:  33.19024896621704 s \n",
      "\n",
      "\n",
      "\tEpisode 4892 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5381],\n",
      "        [1.0000, 0.4477]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4835],\n",
      "        [1.0000, 0.4664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5408256649971 \tStep Time:  0.007979154586791992 s \tTotal Time:  33.19822812080383 s \n",
      "\n",
      "\n",
      "\tEpisode 4893 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4856],\n",
      "        [1.0000, 0.5802]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4843],\n",
      "        [1.0000, 0.5496]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.520609378814697 \tStep Time:  0.007977962493896484 s \tTotal Time:  33.20620608329773 s \n",
      "\n",
      "\n",
      "\tEpisode 4894 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4911],\n",
      "        [1.0000, 0.4817]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5484],\n",
      "        [1.0000, 0.5103]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517091751098633 \tStep Time:  0.007979869842529297 s \tTotal Time:  33.21418595314026 s \n",
      "\n",
      "\n",
      "\tEpisode 4895 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4623],\n",
      "        [1.0000, 0.5536]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5339],\n",
      "        [1.0000, 0.4724]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489956855773926 \tStep Time:  0.007977724075317383 s \tTotal Time:  33.222163677215576 s \n",
      "\n",
      "\n",
      "\tEpisode 4896 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4869],\n",
      "        [1.0000, 0.4778]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4845],\n",
      "        [1.0000, 0.5207]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.516369998455048 \tStep Time:  0.006982326507568359 s \tTotal Time:  33.229146003723145 s \n",
      "\n",
      "\n",
      "\tEpisode 4897 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4827],\n",
      "        [1.0000, 0.4959]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4862],\n",
      "        [1.0000, 0.5161]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.480758666992188 \tStep Time:  0.007978439331054688 s \tTotal Time:  33.2371244430542 s \n",
      "\n",
      "\n",
      "\tEpisode 4898 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5096],\n",
      "        [1.0000, 0.5239]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5242],\n",
      "        [1.0000, 0.5179]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498528242111206 \tStep Time:  0.0069811344146728516 s \tTotal Time:  33.24410557746887 s \n",
      "\n",
      "\n",
      "\tEpisode 4899 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4602],\n",
      "        [1.0000, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5506],\n",
      "        [1.0000, 0.5260]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5141282081604 \tStep Time:  0.007010459899902344 s \tTotal Time:  33.251116037368774 s \n",
      "\n",
      "\n",
      "\tEpisode 4900 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4628],\n",
      "        [1.0000, 0.4656]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5607],\n",
      "        [1.0000, 0.4880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.542827606201172 \tStep Time:  0.006983757019042969 s \tTotal Time:  33.25809979438782 s \n",
      "\n",
      "\n",
      "\tEpisode 4901 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5154],\n",
      "        [1.0000, 0.5019]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5015],\n",
      "        [1.0000, 0.5041]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507964611053467 \tStep Time:  0.006949901580810547 s \tTotal Time:  33.26504969596863 s \n",
      "\n",
      "\n",
      "\tEpisode 4902 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5000],\n",
      "        [1.0000, 0.4745]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4863],\n",
      "        [1.0000, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.517136335372925 \tStep Time:  0.010972023010253906 s \tTotal Time:  33.27602171897888 s \n",
      "\n",
      "\n",
      "\tEpisode 4903 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.4937]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4839],\n",
      "        [1.0000, 0.4586]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.521968841552734 \tStep Time:  0.009972572326660156 s \tTotal Time:  33.28599429130554 s \n",
      "\n",
      "\n",
      "\tEpisode 4904 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.5462]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4858],\n",
      "        [1.0000, 0.4945]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.476737022399902 \tStep Time:  0.009972333908081055 s \tTotal Time:  33.29596662521362 s \n",
      "\n",
      "\n",
      "\tEpisode 4905 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.4713]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4851],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.512675166130066 \tStep Time:  0.009974241256713867 s \tTotal Time:  33.30594086647034 s \n",
      "\n",
      "\n",
      "\tEpisode 4906 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5038],\n",
      "        [1.0000, 0.4936]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5046],\n",
      "        [1.0000, 0.5057]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.506447434425354 \tStep Time:  0.00897526741027832 s \tTotal Time:  33.314916133880615 s \n",
      "\n",
      "\n",
      "\tEpisode 4907 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.4790]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5224],\n",
      "        [1.0000, 0.5049]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.510275959968567 \tStep Time:  0.008976936340332031 s \tTotal Time:  33.32389307022095 s \n",
      "\n",
      "\n",
      "\tEpisode 4908 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.6128]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5018],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.569232106208801 \tStep Time:  0.008974552154541016 s \tTotal Time:  33.33286762237549 s \n",
      "\n",
      "\n",
      "\tEpisode 4909 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5093],\n",
      "        [1.0000, 0.5126]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5010],\n",
      "        [1.0000, 0.5035]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50039517879486 \tStep Time:  0.011969804763793945 s \tTotal Time:  33.34483742713928 s \n",
      "\n",
      "\n",
      "\tEpisode 4910 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5063],\n",
      "        [1.0000, 0.5007]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5089],\n",
      "        [1.0000, 0.5678]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.47660493850708 \tStep Time:  0.00797891616821289 s \tTotal Time:  33.352816343307495 s \n",
      "\n",
      "\n",
      "\tEpisode 4911 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4991],\n",
      "        [1.0000, 0.4992]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5095],\n",
      "        [1.0000, 0.4988]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49601924419403 \tStep Time:  0.00698089599609375 s \tTotal Time:  33.35979723930359 s \n",
      "\n",
      "\n",
      "\tEpisode 4912 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5215],\n",
      "        [1.0000, 0.5023]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5370],\n",
      "        [1.0000, 0.4980]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507152557373047 \tStep Time:  0.007978439331054688 s \tTotal Time:  33.367775678634644 s \n",
      "\n",
      "\n",
      "\tEpisode 4913 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5314],\n",
      "        [1.0000, 0.5553]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5064],\n",
      "        [1.0000, 0.4899]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52298367023468 \tStep Time:  0.0069811344146728516 s \tTotal Time:  33.374756813049316 s \n",
      "\n",
      "\n",
      "\tEpisode 4914 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4981],\n",
      "        [1.0000, 0.5274]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5257],\n",
      "        [1.0000, 0.4918]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.498885154724121 \tStep Time:  0.006982088088989258 s \tTotal Time:  33.381738901138306 s \n",
      "\n",
      "\n",
      "\tEpisode 4915 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5210],\n",
      "        [1.0000, 0.5018]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5743],\n",
      "        [1.0000, 0.5133]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54472428560257 \tStep Time:  0.0069806575775146484 s \tTotal Time:  33.38871955871582 s \n",
      "\n",
      "\n",
      "\tEpisode 4916 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4909],\n",
      "        [1.0000, 0.5532]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5186],\n",
      "        [1.0000, 0.4832]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.491437077522278 \tStep Time:  0.0069806575775146484 s \tTotal Time:  33.395700216293335 s \n",
      "\n",
      "\n",
      "\tEpisode 4917 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4827],\n",
      "        [1.0000, 0.4880]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4934],\n",
      "        [1.0000, 0.5446]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530526638031006 \tStep Time:  0.007054328918457031 s \tTotal Time:  33.40275454521179 s \n",
      "\n",
      "\n",
      "\tEpisode 4918 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4893],\n",
      "        [1.0000, 0.4863]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5427],\n",
      "        [1.0000, 0.4973]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.477567672729492 \tStep Time:  0.0069468021392822266 s \tTotal Time:  33.409701347351074 s \n",
      "\n",
      "\n",
      "\tEpisode 4919 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.5157]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4931],\n",
      "        [1.0000, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.499908447265625 \tStep Time:  0.006981611251831055 s \tTotal Time:  33.416682958602905 s \n",
      "\n",
      "\n",
      "\tEpisode 4920 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4843],\n",
      "        [1.0000, 0.5102]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5033],\n",
      "        [1.0000, 0.5089]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484793245792389 \tStep Time:  0.0069806575775146484 s \tTotal Time:  33.42366361618042 s \n",
      "\n",
      "\n",
      "\tEpisode 4921 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5500],\n",
      "        [1.0000, 0.5142]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4836],\n",
      "        [1.0000, 0.5040]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.463430404663086 \tStep Time:  0.007014274597167969 s \tTotal Time:  33.43067789077759 s \n",
      "\n",
      "\n",
      "\tEpisode 4922 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4896],\n",
      "        [1.0000, 0.4958]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5139],\n",
      "        [1.0000, 0.5481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.53991985321045 \tStep Time:  0.006983041763305664 s \tTotal Time:  33.437660932540894 s \n",
      "\n",
      "\n",
      "\tEpisode 4923 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5414],\n",
      "        [1.0000, 0.5184]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5057],\n",
      "        [1.0000, 0.5568]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501847743988037 \tStep Time:  0.006978511810302734 s \tTotal Time:  33.444639444351196 s \n",
      "\n",
      "\n",
      "\tEpisode 4924 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5180],\n",
      "        [1.0000, 0.4793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5027],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508652687072754 \tStep Time:  0.006982088088989258 s \tTotal Time:  33.451621532440186 s \n",
      "\n",
      "\n",
      "\tEpisode 4925 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4990],\n",
      "        [1.0000, 0.4746]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4779],\n",
      "        [1.0000, 0.4783]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509836196899414 \tStep Time:  0.006983041763305664 s \tTotal Time:  33.45860457420349 s \n",
      "\n",
      "\n",
      "\tEpisode 4926 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4731],\n",
      "        [1.0000, 0.4728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.501704216003418 \tStep Time:  0.006948232650756836 s \tTotal Time:  33.46555280685425 s \n",
      "\n",
      "\n",
      "\tEpisode 4927 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4895],\n",
      "        [1.0000, 0.4748]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4816],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.500604629516602 \tStep Time:  0.006980419158935547 s \tTotal Time:  33.472533226013184 s \n",
      "\n",
      "\n",
      "\tEpisode 4928 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4879],\n",
      "        [1.0000, 0.4964]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4842],\n",
      "        [1.0000, 0.5314]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.481676995754242 \tStep Time:  0.006982088088989258 s \tTotal Time:  33.47951531410217 s \n",
      "\n",
      "\n",
      "\tEpisode 4929 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4813],\n",
      "        [1.0000, 0.5620]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4921],\n",
      "        [1.0000, 0.4957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464999675750732 \tStep Time:  0.006292581558227539 s \tTotal Time:  33.48680520057678 s \n",
      "\n",
      "\n",
      "\tEpisode 4930 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7332],\n",
      "        [1.0000, 0.4881]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4834],\n",
      "        [1.0000, 0.4811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.412755489349365 \tStep Time:  0.0059854984283447266 s \tTotal Time:  33.49279069900513 s \n",
      "\n",
      "\n",
      "\tEpisode 4931 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5103],\n",
      "        [1.0000, 0.4890]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4753],\n",
      "        [1.0000, 0.4718]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530727863311768 \tStep Time:  0.00697779655456543 s \tTotal Time:  33.49976849555969 s \n",
      "\n",
      "\n",
      "\tEpisode 4932 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5324],\n",
      "        [1.0000, 0.4868]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5217],\n",
      "        [1.0000, 0.4793]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.504665613174438 \tStep Time:  0.005986213684082031 s \tTotal Time:  33.505754709243774 s \n",
      "\n",
      "\n",
      "\tEpisode 4933 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4935],\n",
      "        [1.0000, 0.4877]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4710],\n",
      "        [1.0000, 0.4922]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.511006355285645 \tStep Time:  0.0059850215911865234 s \tTotal Time:  33.51273465156555 s \n",
      "\n",
      "\n",
      "\tEpisode 4934 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4657],\n",
      "        [1.0000, 0.4623]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5085],\n",
      "        [1.0000, 0.4690]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.478147983551025 \tStep Time:  0.00698089599609375 s \tTotal Time:  33.519715547561646 s \n",
      "\n",
      "\n",
      "\tEpisode 4935 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4660],\n",
      "        [1.0000, 0.5188]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4726],\n",
      "        [1.0000, 0.4701]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475970268249512 \tStep Time:  0.007947444915771484 s \tTotal Time:  33.52766299247742 s \n",
      "\n",
      "\n",
      "\tEpisode 4936 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4647],\n",
      "        [1.0000, 0.4363]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4407],\n",
      "        [1.0000, 0.4463]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.52151346206665 \tStep Time:  0.006983041763305664 s \tTotal Time:  33.53464603424072 s \n",
      "\n",
      "\n",
      "\tEpisode 4937 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4196],\n",
      "        [1.0000, 0.4111]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4269],\n",
      "        [1.0000, 0.5307]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.45893669128418 \tStep Time:  0.006978750228881836 s \tTotal Time:  33.541624784469604 s \n",
      "\n",
      "\n",
      "\tEpisode 4938 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4039],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5255],\n",
      "        [1.0000, 0.4736]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.545885682106018 \tStep Time:  0.006982088088989258 s \tTotal Time:  33.548606872558594 s \n",
      "\n",
      "\n",
      "\tEpisode 4939 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4950],\n",
      "        [1.0000, 0.4071]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5316],\n",
      "        [1.0000, 0.4432]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514214992523193 \tStep Time:  0.0069806575775146484 s \tTotal Time:  33.55558753013611 s \n",
      "\n",
      "\n",
      "\tEpisode 4940 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3977],\n",
      "        [1.0000, 0.6518]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4096],\n",
      "        [1.0000, 0.4803]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437093019485474 \tStep Time:  0.005984067916870117 s \tTotal Time:  33.56157159805298 s \n",
      "\n",
      "\n",
      "\tEpisode 4941 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6023],\n",
      "        [1.0000, 0.5252]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.7314],\n",
      "        [1.0000, 0.6374]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466697692871094 \tStep Time:  0.006987333297729492 s \tTotal Time:  33.56855893135071 s \n",
      "\n",
      "\n",
      "\tEpisode 4942 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4471],\n",
      "        [1.0000, 0.4813]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4037],\n",
      "        [1.0000, 0.3957]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54983514547348 \tStep Time:  0.005985260009765625 s \tTotal Time:  33.574544191360474 s \n",
      "\n",
      "\n",
      "\tEpisode 4943 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4996],\n",
      "        [1.0000, 0.6175]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4724],\n",
      "        [1.0000, 0.4892]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.561282277107239 \tStep Time:  0.005982875823974609 s \tTotal Time:  33.58052706718445 s \n",
      "\n",
      "\n",
      "\tEpisode 4944 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5253],\n",
      "        [1.0000, 0.4664]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4150],\n",
      "        [1.0000, 0.5390]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.419032096862793 \tStep Time:  0.007429838180541992 s \tTotal Time:  33.58795690536499 s \n",
      "\n",
      "\n",
      "\tEpisode 4945 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4212],\n",
      "        [1.0000, 0.4124]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4435],\n",
      "        [1.0000, 0.5074]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.54100513458252 \tStep Time:  0.0069811344146728516 s \tTotal Time:  33.59493803977966 s \n",
      "\n",
      "\n",
      "\tEpisode 4946 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4791],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5674],\n",
      "        [1.0000, 0.4481]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.443654477596283 \tStep Time:  0.005983829498291016 s \tTotal Time:  33.600921869277954 s \n",
      "\n",
      "\n",
      "\tEpisode 4947 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4531],\n",
      "        [1.0000, 0.5883]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4835],\n",
      "        [1.0000, 0.5861]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.530677795410156 \tStep Time:  0.00598454475402832 s \tTotal Time:  33.60690641403198 s \n",
      "\n",
      "\n",
      "\tEpisode 4948 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4254],\n",
      "        [1.0000, 0.5917]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4187],\n",
      "        [1.0000, 0.4000]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.612215995788574 \tStep Time:  0.006982326507568359 s \tTotal Time:  33.61388874053955 s \n",
      "\n",
      "\n",
      "\tEpisode 4949 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5329],\n",
      "        [1.0000, 0.4661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5967],\n",
      "        [1.0000, 0.4604]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.412079811096191 \tStep Time:  0.007977962493896484 s \tTotal Time:  33.62186670303345 s \n",
      "\n",
      "\n",
      "\tEpisode 4950 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5266],\n",
      "        [1.0000, 0.4619]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5805],\n",
      "        [1.0000, 0.4884]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.524230480194092 \tStep Time:  0.008995532989501953 s \tTotal Time:  33.63086223602295 s \n",
      "\n",
      "\n",
      "\tEpisode 4951 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5669],\n",
      "        [1.0000, 0.6385]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.8045],\n",
      "        [1.0000, 0.5450]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.659635066986084 \tStep Time:  0.007990360260009766 s \tTotal Time:  33.63885259628296 s \n",
      "\n",
      "\n",
      "\tEpisode 4952 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4763],\n",
      "        [1.0000, 0.4904]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6218],\n",
      "        [1.0000, 0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.595468044281006 \tStep Time:  0.008943796157836914 s \tTotal Time:  33.647796392440796 s \n",
      "\n",
      "\n",
      "\tEpisode 4953 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4587],\n",
      "        [1.0000, 0.6025]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4601],\n",
      "        [1.0000, 0.4870]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.587592780590057 \tStep Time:  0.007980585098266602 s \tTotal Time:  33.65577697753906 s \n",
      "\n",
      "\n",
      "\tEpisode 4954 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4556],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4870],\n",
      "        [1.0000, 0.5060]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509756922721863 \tStep Time:  0.006980419158935547 s \tTotal Time:  33.662757396698 s \n",
      "\n",
      "\n",
      "\tEpisode 4955 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4839],\n",
      "        [1.0000, 0.4931]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4732],\n",
      "        [1.0000, 0.7353]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.427360534667969 \tStep Time:  0.00701451301574707 s \tTotal Time:  33.669771909713745 s \n",
      "\n",
      "\n",
      "\tEpisode 4956 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6669],\n",
      "        [1.0000, 0.5351]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4844],\n",
      "        [1.0000, 0.5029]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.585350155830383 \tStep Time:  0.006949901580810547 s \tTotal Time:  33.676721811294556 s \n",
      "\n",
      "\n",
      "\tEpisode 4957 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5119],\n",
      "        [1.0000, 0.5003]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6208],\n",
      "        [1.0000, 0.5952]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515090703964233 \tStep Time:  0.009011268615722656 s \tTotal Time:  33.68573307991028 s \n",
      "\n",
      "\n",
      "\tEpisode 4958 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4610],\n",
      "        [1.0000, 0.5109]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5012],\n",
      "        [1.0000, 0.4862]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494179725646973 \tStep Time:  0.00697636604309082 s \tTotal Time:  33.69270944595337 s \n",
      "\n",
      "\n",
      "\tEpisode 4959 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4864],\n",
      "        [1.0000, 0.4636]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4725],\n",
      "        [1.0000, 0.4830]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.528849303722382 \tStep Time:  0.007945537567138672 s \tTotal Time:  33.70065498352051 s \n",
      "\n",
      "\n",
      "\tEpisode 4960 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4653],\n",
      "        [1.0000, 0.5137]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4680],\n",
      "        [1.0000, 0.4672]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.489802360534668 \tStep Time:  0.008976936340332031 s \tTotal Time:  33.70963191986084 s \n",
      "\n",
      "\n",
      "\tEpisode 4961 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4786],\n",
      "        [1.0000, 0.5009]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4925],\n",
      "        [1.0000, 0.4795]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.493810832500458 \tStep Time:  0.01196908950805664 s \tTotal Time:  33.7216010093689 s \n",
      "\n",
      "\n",
      "\tEpisode 4962 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5599],\n",
      "        [1.0000, 0.4728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4716],\n",
      "        [1.0000, 0.4741]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.5535569190979 \tStep Time:  0.00997304916381836 s \tTotal Time:  33.731574058532715 s \n",
      "\n",
      "\n",
      "\tEpisode 4963 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6567],\n",
      "        [1.0000, 0.4443]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4535],\n",
      "        [1.0000, 0.5888]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.563261032104492 \tStep Time:  0.007977485656738281 s \tTotal Time:  33.73955154418945 s \n",
      "\n",
      "\n",
      "\tEpisode 4964 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4857],\n",
      "        [1.0000, 0.4851]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6310],\n",
      "        [1.0000, 0.4726]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4502032995224 \tStep Time:  0.011966943740844727 s \tTotal Time:  33.75251626968384 s \n",
      "\n",
      "\n",
      "\tEpisode 4965 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4738],\n",
      "        [1.0000, 0.4516]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4638],\n",
      "        [1.0000, 0.4723]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.50501012802124 \tStep Time:  0.008978605270385742 s \tTotal Time:  33.761494874954224 s \n",
      "\n",
      "\n",
      "\tEpisode 4966 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4590],\n",
      "        [1.0000, 0.4709]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4874],\n",
      "        [1.0000, 0.6761]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.437395095825195 \tStep Time:  0.00897526741027832 s \tTotal Time:  33.7704701423645 s \n",
      "\n",
      "\n",
      "\tEpisode 4967 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4953],\n",
      "        [1.0000, 0.4637]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5173],\n",
      "        [1.0000, 0.4757]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.483805775642395 \tStep Time:  0.007979154586791992 s \tTotal Time:  33.778449296951294 s \n",
      "\n",
      "\n",
      "\tEpisode 4968 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4876],\n",
      "        [1.0000, 0.4773]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5302],\n",
      "        [1.0000, 0.4479]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.465705394744873 \tStep Time:  0.009973764419555664 s \tTotal Time:  33.78842306137085 s \n",
      "\n",
      "\n",
      "\tEpisode 4969 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5029],\n",
      "        [1.0000, 0.5361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4508],\n",
      "        [1.0000, 0.4042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.602808475494385 \tStep Time:  0.007977724075317383 s \tTotal Time:  33.797398805618286 s \n",
      "\n",
      "\n",
      "\tEpisode 4970 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4774],\n",
      "        [1.0000, 0.4766]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.4077]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.494410991668701 \tStep Time:  0.007978200912475586 s \tTotal Time:  33.80537700653076 s \n",
      "\n",
      "\n",
      "\tEpisode 4971 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4118],\n",
      "        [1.0000, 0.5189]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5009],\n",
      "        [1.0000, 0.5361]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.474973678588867 \tStep Time:  0.008387088775634766 s \tTotal Time:  33.8137640953064 s \n",
      "\n",
      "\n",
      "\tEpisode 4972 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4802],\n",
      "        [1.0000, 0.4431]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5182],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.466087341308594 \tStep Time:  0.0075762271881103516 s \tTotal Time:  33.82134032249451 s \n",
      "\n",
      "\n",
      "\tEpisode 4973 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5355],\n",
      "        [1.0000, 0.4708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3780],\n",
      "        [1.0000, 0.7093]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.742900252342224 \tStep Time:  0.007979154586791992 s \tTotal Time:  33.8293194770813 s \n",
      "\n",
      "\n",
      "\tEpisode 4974 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4821],\n",
      "        [1.0000, 0.4394]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4006],\n",
      "        [1.0000, 0.6360]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.380486011505127 \tStep Time:  0.008975744247436523 s \tTotal Time:  33.838295221328735 s \n",
      "\n",
      "\n",
      "\tEpisode 4975 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4049],\n",
      "        [1.0000, 0.3987]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4494],\n",
      "        [1.0000, 0.4427]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514878988265991 \tStep Time:  0.009973287582397461 s \tTotal Time:  33.84826850891113 s \n",
      "\n",
      "\n",
      "\tEpisode 4976 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3857],\n",
      "        [1.0000, 0.6116]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4757],\n",
      "        [1.0000, 0.4676]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.407401084899902 \tStep Time:  0.008982181549072266 s \tTotal Time:  33.857250690460205 s \n",
      "\n",
      "\n",
      "\tEpisode 4977 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6507],\n",
      "        [1.0000, 0.6498]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4715],\n",
      "        [1.0000, 0.5419]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.577491700649261 \tStep Time:  0.008975744247436523 s \tTotal Time:  33.86622643470764 s \n",
      "\n",
      "\n",
      "\tEpisode 4978 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4544],\n",
      "        [1.0000, 0.4585]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5458],\n",
      "        [1.0000, 0.4908]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.578454494476318 \tStep Time:  0.006981611251831055 s \tTotal Time:  33.87320804595947 s \n",
      "\n",
      "\n",
      "\tEpisode 4979 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5877],\n",
      "        [1.0000, 0.4789]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5282],\n",
      "        [1.0000, 0.5209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.56811410188675 \tStep Time:  0.007979393005371094 s \tTotal Time:  33.881187438964844 s \n",
      "\n",
      "\n",
      "\tEpisode 4980 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4186],\n",
      "        [1.0000, 0.4205]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4303],\n",
      "        [1.0000, 0.4683]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.515997767448425 \tStep Time:  0.009010553359985352 s \tTotal Time:  33.89019799232483 s \n",
      "\n",
      "\n",
      "\tEpisode 4981 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3587],\n",
      "        [1.0000, 0.5317]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5493],\n",
      "        [1.0000, 0.4856]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4540376663208 \tStep Time:  0.0059506893157958984 s \tTotal Time:  33.897143602371216 s \n",
      "\n",
      "\n",
      "\tEpisode 4982 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4706],\n",
      "        [1.0000, 0.5042]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.4209]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49626874923706 \tStep Time:  0.007978677749633789 s \tTotal Time:  33.90512228012085 s \n",
      "\n",
      "\n",
      "\tEpisode 4983 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4822],\n",
      "        [1.0000, 0.4194]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.3963],\n",
      "        [1.0000, 0.6708]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.464681148529053 \tStep Time:  0.006981849670410156 s \tTotal Time:  33.91210412979126 s \n",
      "\n",
      "\n",
      "\tEpisode 4984 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5328],\n",
      "        [1.0000, 0.4914]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5176],\n",
      "        [1.0000, 0.5728]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.568071365356445 \tStep Time:  0.007977962493896484 s \tTotal Time:  33.920082092285156 s \n",
      "\n",
      "\n",
      "\tEpisode 4985 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4499],\n",
      "        [1.0000, 0.5342]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4440],\n",
      "        [1.0000, 0.4874]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.49305248260498 \tStep Time:  0.008977174758911133 s \tTotal Time:  33.92905926704407 s \n",
      "\n",
      "\n",
      "\tEpisode 4986 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5187],\n",
      "        [1.0000, 0.5138]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5037],\n",
      "        [1.0000, 0.4931]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.527835369110107 \tStep Time:  0.008976221084594727 s \tTotal Time:  33.93803548812866 s \n",
      "\n",
      "\n",
      "\tEpisode 4987 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5303],\n",
      "        [1.0000, 0.5362]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4979],\n",
      "        [1.0000, 0.5661]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.475962162017822 \tStep Time:  0.008975028991699219 s \tTotal Time:  33.94701051712036 s \n",
      "\n",
      "\n",
      "\tEpisode 4988 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5178],\n",
      "        [1.0000, 0.4105]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5118],\n",
      "        [1.0000, 0.5281]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.57295572757721 \tStep Time:  0.008974552154541016 s \tTotal Time:  33.9559850692749 s \n",
      "\n",
      "\n",
      "\tEpisode 4989 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4701],\n",
      "        [1.0000, 0.4831]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6422],\n",
      "        [1.0000, 0.7356]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.598252773284912 \tStep Time:  0.009972810745239258 s \tTotal Time:  33.96595788002014 s \n",
      "\n",
      "\n",
      "\tEpisode 4990 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4076],\n",
      "        [1.0000, 0.5418]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4999],\n",
      "        [1.0000, 0.4953]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.4548978805542 \tStep Time:  0.008975982666015625 s \tTotal Time:  33.97493386268616 s \n",
      "\n",
      "\n",
      "\tEpisode 4991 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5091],\n",
      "        [1.0000, 0.5551]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.6219],\n",
      "        [1.0000, 0.4127]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.64695692062378 \tStep Time:  0.008976221084594727 s \tTotal Time:  33.98391008377075 s \n",
      "\n",
      "\n",
      "\tEpisode 4992 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4745],\n",
      "        [1.0000, 0.4811]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4849],\n",
      "        [1.0000, 0.4903]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.505481243133545 \tStep Time:  0.010970830917358398 s \tTotal Time:  33.99488091468811 s \n",
      "\n",
      "\n",
      "\tEpisode 4993 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4291],\n",
      "        [1.0000, 0.4847]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4853],\n",
      "        [1.0000, 0.4816]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.544445216655731 \tStep Time:  0.009385108947753906 s \tTotal Time:  34.004266023635864 s \n",
      "\n",
      "\n",
      "\tEpisode 4994 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4804],\n",
      "        [1.0000, 0.4876]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4915],\n",
      "        [1.0000, 0.4846]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [2., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.509445190429688 \tStep Time:  0.006968021392822266 s \tTotal Time:  34.01123404502869 s \n",
      "\n",
      "\n",
      "\tEpisode 4995 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4838],\n",
      "        [1.0000, 0.4843]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4898],\n",
      "        [1.0000, 0.4885]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.508111894130707 \tStep Time:  0.00696873664855957 s \tTotal Time:  34.018202781677246 s \n",
      "\n",
      "\n",
      "\tEpisode 4996 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4892],\n",
      "        [1.0000, 0.4901]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.4887]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.507112979888916 \tStep Time:  0.009955406188964844 s \tTotal Time:  34.02815818786621 s \n",
      "\n",
      "\n",
      "\tEpisode 4997 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4660],\n",
      "        [1.0000, 0.4889]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [5., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4668],\n",
      "        [1.0000, 0.4688]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [4., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496916770935059 \tStep Time:  0.012964248657226562 s \tTotal Time:  34.04112243652344 s \n",
      "\n",
      "\n",
      "\tEpisode 4998 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4908],\n",
      "        [1.0000, 0.5002]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [1., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4872],\n",
      "        [1.0000, 0.4807]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.514530181884766 \tStep Time:  0.01196908950805664 s \tTotal Time:  34.053091526031494 s \n",
      "\n",
      "\n",
      "\tEpisode 4999 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4923],\n",
      "        [1.0000, 0.5058]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[1., 0.],\n",
      "        [2., 1.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.4684],\n",
      "        [1.0000, 0.4703]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[4., 0.],\n",
      "        [5., 1.]])\n",
      "\tEpisode Result \tAverage Loss:  13.496902465820312 \tStep Time:  0.010970830917358398 s \tTotal Time:  34.06406235694885 s \n",
      "\n",
      "\n",
      "\tEpisode 5000 / 5000\n",
      "\t\tBatch 1 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5059],\n",
      "        [1.0000, 0.5030]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[5., 1.],\n",
      "        [4., 0.]])\n",
      "\t\tBatch 2 / 2\n",
      "reconstructed_output tensor([[1.0000, 0.5069],\n",
      "        [1.0000, 0.4729]], grad_fn=<SigmoidBackward0>)\n",
      "input_tensor tensor([[2., 1.],\n",
      "        [1., 0.]])\n",
      "\tEpisode Result \tAverage Loss:  13.484954118728638 \tStep Time:  0.008974075317382812 s \tTotal Time:  34.073036432266235 s \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26.969908237457275"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "train(model, optimizer, train_loader, epochs=500, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Supplier Data:\n",
      "[[0.99113023 0.45929772]]\n"
     ]
    }
   ],
   "source": [
    "# Generate a new sample\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(1, latent_dim)\n",
    "    generated_supplier = model.decode(sample).numpy()\n",
    "\n",
    "# Print or use the generated supplier data as needed\n",
    "print(\"Generated Supplier Data:\")\n",
    "print(generated_supplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Result Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(model, scale=5.0, n=25, digit_size=28, figsize=15):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "\n",
    "    # construct a grid \n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = torch.tensor([[xi, yi]], dtype=torch.float).to(device)\n",
    "            x_decoded = model.decode(z_sample)\n",
    "            digit = x_decoded[0].detach().cpu().reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size : (i + 1) * digit_size, j * digit_size : (j + 1) * digit_size,] = digit\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    plt.title('VAE Latent Space Visualization')\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"mean, z [0]\")\n",
    "    plt.ylabel(\"var, z [1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 1x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplot_latent_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mplot_latent_space\u001b[1;34m(model, scale, n, digit_size, figsize)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, xi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(grid_x):\n\u001b[0;32m     11\u001b[0m     z_sample \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[xi, yi]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m     x_decoded \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     digit \u001b[38;5;241m=\u001b[39m x_decoded[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mreshape(digit_size, digit_size)\n\u001b[0;32m     14\u001b[0m     figure[i \u001b[38;5;241m*\u001b[39m digit_size : (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m digit_size, j \u001b[38;5;241m*\u001b[39m digit_size : (j \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m digit_size,] \u001b[38;5;241m=\u001b[39m digit\n",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m, in \u001b[0;36mVAE.decode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Decoder forward pass\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     z \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m     recon_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc4(z))  \u001b[38;5;66;03m# Assuming input features are normalized between 0 and 1\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m recon_x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 1x512)"
     ]
    }
   ],
   "source": [
    "plot_latent_space(model, scale=5.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
